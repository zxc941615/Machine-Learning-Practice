{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "D:\\anaconda\\envs\\pytorch\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "stock_path = \"C:/Users/acer/Desktop/LAB/lab2_2.csv\"\n",
    "stock_df = pd.read_csv(stock_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'], format=\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.DataFrame(stock_df[[\"Open\",\"High\",\"Low\",\"Close\",\"Change(%)\",\"market_Change(%)\",\"Low\",\"K\",\"D\",\"SMA\",\"EMA\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\",\"RISE\"]].values,columns = [\"Open\",\"High\",\"Low\",\"Close\",\"Change(%)\",\"market_Change(%)\",\"Low\",\"K\",\"D\",\"SMA\",\"EMA\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\",\"RISE\"],index = stock_df[\"Date\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Change(%)</th>\n",
       "      <th>market_Change(%)</th>\n",
       "      <th>Low</th>\n",
       "      <th>K</th>\n",
       "      <th>D</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>K(%)</th>\n",
       "      <th>D(%)</th>\n",
       "      <th>SMA(%)</th>\n",
       "      <th>EMA(%)</th>\n",
       "      <th>RISE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>27.514700</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>27.567301</td>\n",
       "      <td>1.158100</td>\n",
       "      <td>-0.173601</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>58.376016</td>\n",
       "      <td>52.792005</td>\n",
       "      <td>25.414129</td>\n",
       "      <td>25.414129</td>\n",
       "      <td>16.752032</td>\n",
       "      <td>5.584011</td>\n",
       "      <td>1.432282</td>\n",
       "      <td>1.432282</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>27.251699</td>\n",
       "      <td>29.461300</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>29.461300</td>\n",
       "      <td>6.870455</td>\n",
       "      <td>-1.357631</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>72.250677</td>\n",
       "      <td>59.278229</td>\n",
       "      <td>25.977800</td>\n",
       "      <td>26.425921</td>\n",
       "      <td>23.767743</td>\n",
       "      <td>12.286375</td>\n",
       "      <td>2.217945</td>\n",
       "      <td>3.981222</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>30.197800</td>\n",
       "      <td>31.513100</td>\n",
       "      <td>30.197800</td>\n",
       "      <td>31.513100</td>\n",
       "      <td>6.964391</td>\n",
       "      <td>1.139757</td>\n",
       "      <td>30.197800</td>\n",
       "      <td>81.500452</td>\n",
       "      <td>66.685637</td>\n",
       "      <td>27.060057</td>\n",
       "      <td>27.697716</td>\n",
       "      <td>12.802336</td>\n",
       "      <td>12.496000</td>\n",
       "      <td>4.166085</td>\n",
       "      <td>4.812679</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>6.844449</td>\n",
       "      <td>0.807109</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>87.666968</td>\n",
       "      <td>73.679414</td>\n",
       "      <td>28.405343</td>\n",
       "      <td>29.190787</td>\n",
       "      <td>7.566236</td>\n",
       "      <td>10.487681</td>\n",
       "      <td>4.971480</td>\n",
       "      <td>5.390591</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-18</th>\n",
       "      <td>35.774399</td>\n",
       "      <td>35.984901</td>\n",
       "      <td>34.564400</td>\n",
       "      <td>35.984901</td>\n",
       "      <td>6.875269</td>\n",
       "      <td>-0.228187</td>\n",
       "      <td>34.564400</td>\n",
       "      <td>91.777978</td>\n",
       "      <td>79.712269</td>\n",
       "      <td>30.137700</td>\n",
       "      <td>30.889315</td>\n",
       "      <td>4.689350</td>\n",
       "      <td>8.187979</td>\n",
       "      <td>6.098702</td>\n",
       "      <td>5.818715</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>56.599998</td>\n",
       "      <td>57.400002</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.706717</td>\n",
       "      <td>0.265105</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>81.904480</td>\n",
       "      <td>77.871076</td>\n",
       "      <td>55.357143</td>\n",
       "      <td>55.617953</td>\n",
       "      <td>0.879924</td>\n",
       "      <td>2.658650</td>\n",
       "      <td>1.174935</td>\n",
       "      <td>0.835216</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>59.200001</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>58.299999</td>\n",
       "      <td>2.280700</td>\n",
       "      <td>-0.059294</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>83.018279</td>\n",
       "      <td>79.586810</td>\n",
       "      <td>56.057142</td>\n",
       "      <td>56.288464</td>\n",
       "      <td>1.359875</td>\n",
       "      <td>2.203301</td>\n",
       "      <td>1.264515</td>\n",
       "      <td>1.205567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>58.599998</td>\n",
       "      <td>59.599998</td>\n",
       "      <td>58.400002</td>\n",
       "      <td>59.400002</td>\n",
       "      <td>1.886798</td>\n",
       "      <td>0.754770</td>\n",
       "      <td>58.400002</td>\n",
       "      <td>87.603605</td>\n",
       "      <td>82.259075</td>\n",
       "      <td>56.842857</td>\n",
       "      <td>57.066349</td>\n",
       "      <td>5.523272</td>\n",
       "      <td>3.357673</td>\n",
       "      <td>1.401631</td>\n",
       "      <td>1.381961</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>61.799999</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>9.932661</td>\n",
       "      <td>-0.316085</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>91.735736</td>\n",
       "      <td>85.417962</td>\n",
       "      <td>58.442857</td>\n",
       "      <td>59.124762</td>\n",
       "      <td>4.716851</td>\n",
       "      <td>3.840169</td>\n",
       "      <td>2.814779</td>\n",
       "      <td>3.607053</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>65.300003</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>3.369061</td>\n",
       "      <td>-0.466512</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>94.490491</td>\n",
       "      <td>88.442138</td>\n",
       "      <td>60.071429</td>\n",
       "      <td>61.218572</td>\n",
       "      <td>3.002924</td>\n",
       "      <td>3.540445</td>\n",
       "      <td>2.786605</td>\n",
       "      <td>3.541341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2283 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Change(%)  \\\n",
       "2010-01-12  27.514700  28.830000  27.251699  27.567301   1.158100   \n",
       "2010-01-13  27.251699  29.461300  27.251699  29.461300   6.870455   \n",
       "2010-01-14  30.197800  31.513100  30.197800  31.513100   6.964391   \n",
       "2010-01-15  33.669998  33.669998  33.669998  33.669998   6.844449   \n",
       "2010-01-18  35.774399  35.984901  34.564400  35.984901   6.875269   \n",
       "...               ...        ...        ...        ...        ...   \n",
       "2019-12-25  56.599998  57.400002  56.299999  57.000000   0.706717   \n",
       "2019-12-26  58.000000  59.200001  57.599998  58.299999   2.280700   \n",
       "2019-12-27  58.599998  59.599998  58.400002  59.400002   1.886798   \n",
       "2019-12-30  61.799999  65.300003  61.799999  65.300003   9.932661   \n",
       "2019-12-31  65.300003  67.500000  64.500000  67.500000   3.369061   \n",
       "\n",
       "            market_Change(%)        Low          K          D        SMA  \\\n",
       "2010-01-12         -0.173601  27.251699  58.376016  52.792005  25.414129   \n",
       "2010-01-13         -1.357631  27.251699  72.250677  59.278229  25.977800   \n",
       "2010-01-14          1.139757  30.197800  81.500452  66.685637  27.060057   \n",
       "2010-01-15          0.807109  33.669998  87.666968  73.679414  28.405343   \n",
       "2010-01-18         -0.228187  34.564400  91.777978  79.712269  30.137700   \n",
       "...                      ...        ...        ...        ...        ...   \n",
       "2019-12-25          0.265105  56.299999  81.904480  77.871076  55.357143   \n",
       "2019-12-26         -0.059294  57.599998  83.018279  79.586810  56.057142   \n",
       "2019-12-27          0.754770  58.400002  87.603605  82.259075  56.842857   \n",
       "2019-12-30         -0.316085  61.799999  91.735736  85.417962  58.442857   \n",
       "2019-12-31         -0.466512  64.500000  94.490491  88.442138  60.071429   \n",
       "\n",
       "                  EMA       K(%)       D(%)    SMA(%)    EMA(%)  RISE  \n",
       "2010-01-12  25.414129  16.752032   5.584011  1.432282  1.432282   1.0  \n",
       "2010-01-13  26.425921  23.767743  12.286375  2.217945  3.981222   1.0  \n",
       "2010-01-14  27.697716  12.802336  12.496000  4.166085  4.812679   1.0  \n",
       "2010-01-15  29.190787   7.566236  10.487681  4.971480  5.390591   1.0  \n",
       "2010-01-18  30.889315   4.689350   8.187979  6.098702  5.818715   1.0  \n",
       "...               ...        ...        ...       ...       ...   ...  \n",
       "2019-12-25  55.617953   0.879924   2.658650  1.174935  0.835216   1.0  \n",
       "2019-12-26  56.288464   1.359875   2.203301  1.264515  1.205567   1.0  \n",
       "2019-12-27  57.066349   5.523272   3.357673  1.401631  1.381961   1.0  \n",
       "2019-12-30  59.124762   4.716851   3.840169  2.814779  3.607053   1.0  \n",
       "2019-12-31  61.218572   3.002924   3.540445  2.786605  3.541341   1.0  \n",
       "\n",
       "[2283 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr = stock_df.corr(method=\"kendall\")\n",
    "sns.heatmap(corr,cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler(feature_range = (-1,1))\n",
    "minmax_df = scaler.fit_transform(stock_df[[\"Change(%)\",\"market_Change(%)\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\"]])\n",
    "minmax_df = pd.DataFrame(minmax_df,columns = [\"Change(%)\",\"market_Change(%)\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\"],index = stock_df.index)\n",
    "minmax_df[\"RISE\"] = stock_df[\"RISE\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Change(%)</th>\n",
       "      <th>market_Change(%)</th>\n",
       "      <th>K(%)</th>\n",
       "      <th>D(%)</th>\n",
       "      <th>SMA(%)</th>\n",
       "      <th>EMA(%)</th>\n",
       "      <th>RISE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>0.117873</td>\n",
       "      <td>0.129217</td>\n",
       "      <td>-0.619633</td>\n",
       "      <td>-0.354385</td>\n",
       "      <td>0.082550</td>\n",
       "      <td>0.140285</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>0.690164</td>\n",
       "      <td>-0.088579</td>\n",
       "      <td>-0.566353</td>\n",
       "      <td>-0.209309</td>\n",
       "      <td>0.237017</td>\n",
       "      <td>0.639862</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>0.699575</td>\n",
       "      <td>0.370801</td>\n",
       "      <td>-0.649628</td>\n",
       "      <td>-0.204772</td>\n",
       "      <td>0.620034</td>\n",
       "      <td>0.802823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>0.687559</td>\n",
       "      <td>0.309613</td>\n",
       "      <td>-0.689393</td>\n",
       "      <td>-0.248243</td>\n",
       "      <td>0.778381</td>\n",
       "      <td>0.916090</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-18</th>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.119176</td>\n",
       "      <td>-0.711241</td>\n",
       "      <td>-0.298021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>0.072651</td>\n",
       "      <td>0.209914</td>\n",
       "      <td>-0.740172</td>\n",
       "      <td>-0.417706</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>0.230340</td>\n",
       "      <td>0.150243</td>\n",
       "      <td>-0.736527</td>\n",
       "      <td>-0.427562</td>\n",
       "      <td>0.049566</td>\n",
       "      <td>0.095850</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.190877</td>\n",
       "      <td>0.299985</td>\n",
       "      <td>-0.704908</td>\n",
       "      <td>-0.402575</td>\n",
       "      <td>0.076524</td>\n",
       "      <td>0.130423</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.996951</td>\n",
       "      <td>0.103008</td>\n",
       "      <td>-0.711033</td>\n",
       "      <td>-0.392131</td>\n",
       "      <td>0.354358</td>\n",
       "      <td>0.566528</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.339378</td>\n",
       "      <td>0.075337</td>\n",
       "      <td>-0.724049</td>\n",
       "      <td>-0.398619</td>\n",
       "      <td>0.348819</td>\n",
       "      <td>0.553648</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2283 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Change(%)  market_Change(%)      K(%)      D(%)    SMA(%)  \\\n",
       "2010-01-12   0.117873          0.129217 -0.619633 -0.354385  0.082550   \n",
       "2010-01-13   0.690164         -0.088579 -0.566353 -0.209309  0.237017   \n",
       "2010-01-14   0.699575          0.370801 -0.649628 -0.204772  0.620034   \n",
       "2010-01-15   0.687559          0.309613 -0.689393 -0.248243  0.778381   \n",
       "2010-01-18   0.690647          0.119176 -0.711241 -0.298021  1.000000   \n",
       "...               ...               ...       ...       ...       ...   \n",
       "2019-12-25   0.072651          0.209914 -0.740172 -0.417706  0.031954   \n",
       "2019-12-26   0.230340          0.150243 -0.736527 -0.427562  0.049566   \n",
       "2019-12-27   0.190877          0.299985 -0.704908 -0.402575  0.076524   \n",
       "2019-12-30   0.996951          0.103008 -0.711033 -0.392131  0.354358   \n",
       "2019-12-31   0.339378          0.075337 -0.724049 -0.398619  0.348819   \n",
       "\n",
       "              EMA(%)  RISE  \n",
       "2010-01-12  0.140285   1.0  \n",
       "2010-01-13  0.639862   1.0  \n",
       "2010-01-14  0.802823   1.0  \n",
       "2010-01-15  0.916090   1.0  \n",
       "2010-01-18  1.000000   1.0  \n",
       "...              ...   ...  \n",
       "2019-12-25  0.023264   1.0  \n",
       "2019-12-26  0.095850   1.0  \n",
       "2019-12-27  0.130423   1.0  \n",
       "2019-12-30  0.566528   1.0  \n",
       "2019-12-31  0.553648   1.0  \n",
       "\n",
       "[2283 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 200\n",
    "BATCH_SIZE = 16\n",
    "TIME_STEP = 7\n",
    "INPUT_SIZE = 6\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of feature:  torch.Size([2276, 7, 6])\n",
      "size of label:  torch.Size([2276])\n"
     ]
    }
   ],
   "source": [
    "# declear training features data\n",
    "features = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i-TIME_STEP:i][[\"Change(%)\",\"market_Change(%)\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = torch.FloatTensor(features)\n",
    "print(\"size of feature: \",features.size())\n",
    "\n",
    "# declear trainging labels data\n",
    "labels = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i:i+1][\"RISE\"]\n",
    "    labels.append(x.tolist())\n",
    "labels = torch.LongTensor(labels).view(-1)\n",
    "print(\"size of label: \",labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random select 90% training set index and 10% testing set index \n",
    "\n",
    "x = np.linspace(0,2275,2276).tolist()\n",
    "for i in range(0,2276):\n",
    "    x[i] = int(x[i])\n",
    "\n",
    "np.random.shuffle(x)\n",
    "training, test = x[:2000], x[2000:]\n",
    "test.sort()\n",
    "training.sort()\n",
    "train_features = features[training]\n",
    "train_labels = labels[training]\n",
    "test_features = features[test]\n",
    "test_labels = labels[test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(train_features,train_labels)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.lstm1 = torch.nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm2 = torch.nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm3 = torch.nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=6,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "    def forward(self,x):\n",
    "        lstm_out,_ = self.lstm1(x,None)\n",
    "        lstm_out = self.relu(lstm_out)\n",
    "        lstm_out,_ = self.lstm2(lstm_out,None)\n",
    "        lstm_out = self.relu(lstm_out)\n",
    "        lstm_out,_ = self.lstm3(lstm_out,None)\n",
    "        return lstm_out[:,-1,:]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.lstm1 = torch.nn.LSTM(\n",
    "            input_size=6,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm2 = torch.nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.out = torch.nn.Linear(64,INPUT_SIZE)\n",
    "    def forward(self,x):\n",
    "        lstm_out,_ = self.lstm1(x,None)\n",
    "        lstm_out = self.relu(lstm_out)\n",
    "        lstm_out,_ = self.lstm2(lstm_out,None)\n",
    "        \n",
    "        return self.out(lstm_out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LstmAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    def forward(self, x):\n",
    "        e = self.encoder(x)\n",
    "        e2 = e.view(-1,1,6)\n",
    "        e2 = e2.repeat(1,TIME_STEP,1)\n",
    "        d = self.decoder(e2)\n",
    "        return e,torch.squeeze(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmAE(\n",
      "  (encoder): Encoder(\n",
      "    (lstm1): LSTM(6, 64, batch_first=True)\n",
      "    (lstm2): LSTM(64, 32, batch_first=True)\n",
      "    (lstm3): LSTM(32, 6, batch_first=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm1): LSTM(6, 32, batch_first=True)\n",
      "    (lstm2): LSTM(32, 64, batch_first=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (out): Linear(in_features=64, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LstmAE()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.9,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   10|Train Avg Loss: 0.2275 \n",
      "Epoch:    1|steps:   20|Train Avg Loss: 0.1704 \n",
      "Epoch:    1|steps:   30|Train Avg Loss: 0.1026 \n",
      "Epoch:    1|steps:   40|Train Avg Loss: 0.0872 \n",
      "Epoch:    1|steps:   50|Train Avg Loss: 0.0824 \n",
      "Epoch:    1|steps:   60|Train Avg Loss: 0.0796 \n",
      "Epoch:    1|steps:   70|Train Avg Loss: 0.0692 \n",
      "Epoch:    1|steps:   80|Train Avg Loss: 0.0763 \n",
      "Epoch:    1|steps:   90|Train Avg Loss: 0.0614 \n",
      "Epoch:    1|steps:  100|Train Avg Loss: 0.0628 \n",
      "Epoch:    1|steps:  110|Train Avg Loss: 0.0583 \n",
      "Epoch:    1|steps:  120|Train Avg Loss: 0.0594 \n",
      "Epoch:    2|steps:   10|Train Avg Loss: 0.0562 \n",
      "Epoch:    2|steps:   20|Train Avg Loss: 0.0519 \n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.0510 \n",
      "Epoch:    2|steps:   40|Train Avg Loss: 0.0488 \n",
      "Epoch:    2|steps:   50|Train Avg Loss: 0.0468 \n",
      "Epoch:    2|steps:   60|Train Avg Loss: 0.0452 \n",
      "Epoch:    2|steps:   70|Train Avg Loss: 0.0454 \n",
      "Epoch:    2|steps:   80|Train Avg Loss: 0.0461 \n",
      "Epoch:    2|steps:   90|Train Avg Loss: 0.0450 \n",
      "Epoch:    2|steps:  100|Train Avg Loss: 0.0440 \n",
      "Epoch:    2|steps:  110|Train Avg Loss: 0.0399 \n",
      "Epoch:    2|steps:  120|Train Avg Loss: 0.0388 \n",
      "Epoch:    3|steps:   10|Train Avg Loss: 0.0412 \n",
      "Epoch:    3|steps:   20|Train Avg Loss: 0.0389 \n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.0389 \n",
      "Epoch:    3|steps:   40|Train Avg Loss: 0.0439 \n",
      "Epoch:    3|steps:   50|Train Avg Loss: 0.0390 \n",
      "Epoch:    3|steps:   60|Train Avg Loss: 0.0424 \n",
      "Epoch:    3|steps:   70|Train Avg Loss: 0.0358 \n",
      "Epoch:    3|steps:   80|Train Avg Loss: 0.0380 \n",
      "Epoch:    3|steps:   90|Train Avg Loss: 0.0372 \n",
      "Epoch:    3|steps:  100|Train Avg Loss: 0.0326 \n",
      "Epoch:    3|steps:  110|Train Avg Loss: 0.0337 \n",
      "Epoch:    3|steps:  120|Train Avg Loss: 0.0322 \n",
      "Epoch:    4|steps:   10|Train Avg Loss: 0.0340 \n",
      "Epoch:    4|steps:   20|Train Avg Loss: 0.0321 \n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.0328 \n",
      "Epoch:    4|steps:   40|Train Avg Loss: 0.0313 \n",
      "Epoch:    4|steps:   50|Train Avg Loss: 0.0309 \n",
      "Epoch:    4|steps:   60|Train Avg Loss: 0.0304 \n",
      "Epoch:    4|steps:   70|Train Avg Loss: 0.0323 \n",
      "Epoch:    4|steps:   80|Train Avg Loss: 0.0305 \n",
      "Epoch:    4|steps:   90|Train Avg Loss: 0.0339 \n",
      "Epoch:    4|steps:  100|Train Avg Loss: 0.0284 \n",
      "Epoch:    4|steps:  110|Train Avg Loss: 0.0297 \n",
      "Epoch:    4|steps:  120|Train Avg Loss: 0.0296 \n",
      "Epoch:    5|steps:   10|Train Avg Loss: 0.0295 \n",
      "Epoch:    5|steps:   20|Train Avg Loss: 0.0298 \n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.0276 \n",
      "Epoch:    5|steps:   40|Train Avg Loss: 0.0283 \n",
      "Epoch:    5|steps:   50|Train Avg Loss: 0.0272 \n",
      "Epoch:    5|steps:   60|Train Avg Loss: 0.0314 \n",
      "Epoch:    5|steps:   70|Train Avg Loss: 0.0299 \n",
      "Epoch:    5|steps:   80|Train Avg Loss: 0.0265 \n",
      "Epoch:    5|steps:   90|Train Avg Loss: 0.0285 \n",
      "Epoch:    5|steps:  100|Train Avg Loss: 0.0297 \n",
      "Epoch:    5|steps:  110|Train Avg Loss: 0.0278 \n",
      "Epoch:    5|steps:  120|Train Avg Loss: 0.0268 \n",
      "Epoch:    6|steps:   10|Train Avg Loss: 0.0276 \n",
      "Epoch:    6|steps:   20|Train Avg Loss: 0.0268 \n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.0254 \n",
      "Epoch:    6|steps:   40|Train Avg Loss: 0.0252 \n",
      "Epoch:    6|steps:   50|Train Avg Loss: 0.0235 \n",
      "Epoch:    6|steps:   60|Train Avg Loss: 0.0260 \n",
      "Epoch:    6|steps:   70|Train Avg Loss: 0.0297 \n",
      "Epoch:    6|steps:   80|Train Avg Loss: 0.0295 \n",
      "Epoch:    6|steps:   90|Train Avg Loss: 0.0318 \n",
      "Epoch:    6|steps:  100|Train Avg Loss: 0.0235 \n",
      "Epoch:    6|steps:  110|Train Avg Loss: 0.0266 \n",
      "Epoch:    6|steps:  120|Train Avg Loss: 0.0289 \n",
      "Epoch:    7|steps:   10|Train Avg Loss: 0.0258 \n",
      "Epoch:    7|steps:   20|Train Avg Loss: 0.0266 \n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.0244 \n",
      "Epoch:    7|steps:   40|Train Avg Loss: 0.0262 \n",
      "Epoch:    7|steps:   50|Train Avg Loss: 0.0280 \n",
      "Epoch:    7|steps:   60|Train Avg Loss: 0.0256 \n",
      "Epoch:    7|steps:   70|Train Avg Loss: 0.0259 \n",
      "Epoch:    7|steps:   80|Train Avg Loss: 0.0264 \n",
      "Epoch:    7|steps:   90|Train Avg Loss: 0.0253 \n",
      "Epoch:    7|steps:  100|Train Avg Loss: 0.0266 \n",
      "Epoch:    7|steps:  110|Train Avg Loss: 0.0263 \n",
      "Epoch:    7|steps:  120|Train Avg Loss: 0.0236 \n",
      "Epoch:    8|steps:   10|Train Avg Loss: 0.0247 \n",
      "Epoch:    8|steps:   20|Train Avg Loss: 0.0229 \n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.0270 \n",
      "Epoch:    8|steps:   40|Train Avg Loss: 0.0265 \n",
      "Epoch:    8|steps:   50|Train Avg Loss: 0.0251 \n",
      "Epoch:    8|steps:   60|Train Avg Loss: 0.0244 \n",
      "Epoch:    8|steps:   70|Train Avg Loss: 0.0238 \n",
      "Epoch:    8|steps:   80|Train Avg Loss: 0.0241 \n",
      "Epoch:    8|steps:   90|Train Avg Loss: 0.0257 \n",
      "Epoch:    8|steps:  100|Train Avg Loss: 0.0267 \n",
      "Epoch:    8|steps:  110|Train Avg Loss: 0.0231 \n",
      "Epoch:    8|steps:  120|Train Avg Loss: 0.0257 \n",
      "Epoch:    9|steps:   10|Train Avg Loss: 0.0222 \n",
      "Epoch:    9|steps:   20|Train Avg Loss: 0.0242 \n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.0269 \n",
      "Epoch:    9|steps:   40|Train Avg Loss: 0.0293 \n",
      "Epoch:    9|steps:   50|Train Avg Loss: 0.0224 \n",
      "Epoch:    9|steps:   60|Train Avg Loss: 0.0234 \n",
      "Epoch:    9|steps:   70|Train Avg Loss: 0.0232 \n",
      "Epoch:    9|steps:   80|Train Avg Loss: 0.0226 \n",
      "Epoch:    9|steps:   90|Train Avg Loss: 0.0243 \n",
      "Epoch:    9|steps:  100|Train Avg Loss: 0.0246 \n",
      "Epoch:    9|steps:  110|Train Avg Loss: 0.0230 \n",
      "Epoch:    9|steps:  120|Train Avg Loss: 0.0264 \n",
      "Epoch:   10|steps:   10|Train Avg Loss: 0.0230 \n",
      "Epoch:   10|steps:   20|Train Avg Loss: 0.0221 \n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.0222 \n",
      "Epoch:   10|steps:   40|Train Avg Loss: 0.0252 \n",
      "Epoch:   10|steps:   50|Train Avg Loss: 0.0263 \n",
      "Epoch:   10|steps:   60|Train Avg Loss: 0.0223 \n",
      "Epoch:   10|steps:   70|Train Avg Loss: 0.0243 \n",
      "Epoch:   10|steps:   80|Train Avg Loss: 0.0240 \n",
      "Epoch:   10|steps:   90|Train Avg Loss: 0.0241 \n",
      "Epoch:   10|steps:  100|Train Avg Loss: 0.0271 \n",
      "Epoch:   10|steps:  110|Train Avg Loss: 0.0244 \n",
      "Epoch:   10|steps:  120|Train Avg Loss: 0.0233 \n",
      "Epoch:   11|steps:   10|Train Avg Loss: 0.0243 \n",
      "Epoch:   11|steps:   20|Train Avg Loss: 0.0209 \n",
      "Epoch:   11|steps:   30|Train Avg Loss: 0.0225 \n",
      "Epoch:   11|steps:   40|Train Avg Loss: 0.0230 \n",
      "Epoch:   11|steps:   50|Train Avg Loss: 0.0244 \n",
      "Epoch:   11|steps:   60|Train Avg Loss: 0.0232 \n",
      "Epoch:   11|steps:   70|Train Avg Loss: 0.0239 \n",
      "Epoch:   11|steps:   80|Train Avg Loss: 0.0257 \n",
      "Epoch:   11|steps:   90|Train Avg Loss: 0.0216 \n",
      "Epoch:   11|steps:  100|Train Avg Loss: 0.0255 \n",
      "Epoch:   11|steps:  110|Train Avg Loss: 0.0225 \n",
      "Epoch:   11|steps:  120|Train Avg Loss: 0.0232 \n",
      "Epoch:   12|steps:   10|Train Avg Loss: 0.0240 \n",
      "Epoch:   12|steps:   20|Train Avg Loss: 0.0230 \n",
      "Epoch:   12|steps:   30|Train Avg Loss: 0.0250 \n",
      "Epoch:   12|steps:   40|Train Avg Loss: 0.0217 \n",
      "Epoch:   12|steps:   50|Train Avg Loss: 0.0239 \n",
      "Epoch:   12|steps:   60|Train Avg Loss: 0.0204 \n",
      "Epoch:   12|steps:   70|Train Avg Loss: 0.0211 \n",
      "Epoch:   12|steps:   80|Train Avg Loss: 0.0254 \n",
      "Epoch:   12|steps:   90|Train Avg Loss: 0.0238 \n",
      "Epoch:   12|steps:  100|Train Avg Loss: 0.0212 \n",
      "Epoch:   12|steps:  110|Train Avg Loss: 0.0212 \n",
      "Epoch:   12|steps:  120|Train Avg Loss: 0.0227 \n",
      "Epoch:   13|steps:   10|Train Avg Loss: 0.0215 \n",
      "Epoch:   13|steps:   20|Train Avg Loss: 0.0223 \n",
      "Epoch:   13|steps:   30|Train Avg Loss: 0.0202 \n",
      "Epoch:   13|steps:   40|Train Avg Loss: 0.0211 \n",
      "Epoch:   13|steps:   50|Train Avg Loss: 0.0196 \n",
      "Epoch:   13|steps:   60|Train Avg Loss: 0.0219 \n",
      "Epoch:   13|steps:   70|Train Avg Loss: 0.0227 \n",
      "Epoch:   13|steps:   80|Train Avg Loss: 0.0218 \n",
      "Epoch:   13|steps:   90|Train Avg Loss: 0.0215 \n",
      "Epoch:   13|steps:  100|Train Avg Loss: 0.0217 \n",
      "Epoch:   13|steps:  110|Train Avg Loss: 0.0235 \n",
      "Epoch:   13|steps:  120|Train Avg Loss: 0.0240 \n",
      "Epoch:   14|steps:   10|Train Avg Loss: 0.0200 \n",
      "Epoch:   14|steps:   20|Train Avg Loss: 0.0225 \n",
      "Epoch:   14|steps:   30|Train Avg Loss: 0.0223 \n",
      "Epoch:   14|steps:   40|Train Avg Loss: 0.0201 \n",
      "Epoch:   14|steps:   50|Train Avg Loss: 0.0216 \n",
      "Epoch:   14|steps:   60|Train Avg Loss: 0.0198 \n",
      "Epoch:   14|steps:   70|Train Avg Loss: 0.0215 \n",
      "Epoch:   14|steps:   80|Train Avg Loss: 0.0218 \n",
      "Epoch:   14|steps:   90|Train Avg Loss: 0.0227 \n",
      "Epoch:   14|steps:  100|Train Avg Loss: 0.0223 \n",
      "Epoch:   14|steps:  110|Train Avg Loss: 0.0191 \n",
      "Epoch:   14|steps:  120|Train Avg Loss: 0.0200 \n",
      "Epoch:   15|steps:   10|Train Avg Loss: 0.0229 \n",
      "Epoch:   15|steps:   20|Train Avg Loss: 0.0215 \n",
      "Epoch:   15|steps:   30|Train Avg Loss: 0.0218 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   15|steps:   40|Train Avg Loss: 0.0214 \n",
      "Epoch:   15|steps:   50|Train Avg Loss: 0.0202 \n",
      "Epoch:   15|steps:   60|Train Avg Loss: 0.0213 \n",
      "Epoch:   15|steps:   70|Train Avg Loss: 0.0210 \n",
      "Epoch:   15|steps:   80|Train Avg Loss: 0.0209 \n",
      "Epoch:   15|steps:   90|Train Avg Loss: 0.0192 \n",
      "Epoch:   15|steps:  100|Train Avg Loss: 0.0193 \n",
      "Epoch:   15|steps:  110|Train Avg Loss: 0.0208 \n",
      "Epoch:   15|steps:  120|Train Avg Loss: 0.0191 \n",
      "Epoch:   16|steps:   10|Train Avg Loss: 0.0195 \n",
      "Epoch:   16|steps:   20|Train Avg Loss: 0.0206 \n",
      "Epoch:   16|steps:   30|Train Avg Loss: 0.0214 \n",
      "Epoch:   16|steps:   40|Train Avg Loss: 0.0195 \n",
      "Epoch:   16|steps:   50|Train Avg Loss: 0.0198 \n",
      "Epoch:   16|steps:   60|Train Avg Loss: 0.0209 \n",
      "Epoch:   16|steps:   70|Train Avg Loss: 0.0201 \n",
      "Epoch:   16|steps:   80|Train Avg Loss: 0.0191 \n",
      "Epoch:   16|steps:   90|Train Avg Loss: 0.0225 \n",
      "Epoch:   16|steps:  100|Train Avg Loss: 0.0206 \n",
      "Epoch:   16|steps:  110|Train Avg Loss: 0.0205 \n",
      "Epoch:   16|steps:  120|Train Avg Loss: 0.0190 \n",
      "Epoch:   17|steps:   10|Train Avg Loss: 0.0209 \n",
      "Epoch:   17|steps:   20|Train Avg Loss: 0.0203 \n",
      "Epoch:   17|steps:   30|Train Avg Loss: 0.0167 \n",
      "Epoch:   17|steps:   40|Train Avg Loss: 0.0224 \n",
      "Epoch:   17|steps:   50|Train Avg Loss: 0.0205 \n",
      "Epoch:   17|steps:   60|Train Avg Loss: 0.0213 \n",
      "Epoch:   17|steps:   70|Train Avg Loss: 0.0215 \n",
      "Epoch:   17|steps:   80|Train Avg Loss: 0.0196 \n",
      "Epoch:   17|steps:   90|Train Avg Loss: 0.0215 \n",
      "Epoch:   17|steps:  100|Train Avg Loss: 0.0194 \n",
      "Epoch:   17|steps:  110|Train Avg Loss: 0.0198 \n",
      "Epoch:   17|steps:  120|Train Avg Loss: 0.0215 \n",
      "Epoch:   18|steps:   10|Train Avg Loss: 0.0186 \n",
      "Epoch:   18|steps:   20|Train Avg Loss: 0.0204 \n",
      "Epoch:   18|steps:   30|Train Avg Loss: 0.0204 \n",
      "Epoch:   18|steps:   40|Train Avg Loss: 0.0196 \n",
      "Epoch:   18|steps:   50|Train Avg Loss: 0.0228 \n",
      "Epoch:   18|steps:   60|Train Avg Loss: 0.0209 \n",
      "Epoch:   18|steps:   70|Train Avg Loss: 0.0200 \n",
      "Epoch:   18|steps:   80|Train Avg Loss: 0.0206 \n",
      "Epoch:   18|steps:   90|Train Avg Loss: 0.0175 \n",
      "Epoch:   18|steps:  100|Train Avg Loss: 0.0210 \n",
      "Epoch:   18|steps:  110|Train Avg Loss: 0.0208 \n",
      "Epoch:   18|steps:  120|Train Avg Loss: 0.0175 \n",
      "Epoch:   19|steps:   10|Train Avg Loss: 0.0189 \n",
      "Epoch:   19|steps:   20|Train Avg Loss: 0.0200 \n",
      "Epoch:   19|steps:   30|Train Avg Loss: 0.0192 \n",
      "Epoch:   19|steps:   40|Train Avg Loss: 0.0204 \n",
      "Epoch:   19|steps:   50|Train Avg Loss: 0.0184 \n",
      "Epoch:   19|steps:   60|Train Avg Loss: 0.0208 \n",
      "Epoch:   19|steps:   70|Train Avg Loss: 0.0193 \n",
      "Epoch:   19|steps:   80|Train Avg Loss: 0.0201 \n",
      "Epoch:   19|steps:   90|Train Avg Loss: 0.0195 \n",
      "Epoch:   19|steps:  100|Train Avg Loss: 0.0189 \n",
      "Epoch:   19|steps:  110|Train Avg Loss: 0.0206 \n",
      "Epoch:   19|steps:  120|Train Avg Loss: 0.0217 \n",
      "Epoch:   20|steps:   10|Train Avg Loss: 0.0195 \n",
      "Epoch:   20|steps:   20|Train Avg Loss: 0.0209 \n",
      "Epoch:   20|steps:   30|Train Avg Loss: 0.0210 \n",
      "Epoch:   20|steps:   40|Train Avg Loss: 0.0188 \n",
      "Epoch:   20|steps:   50|Train Avg Loss: 0.0212 \n",
      "Epoch:   20|steps:   60|Train Avg Loss: 0.0189 \n",
      "Epoch:   20|steps:   70|Train Avg Loss: 0.0192 \n",
      "Epoch:   20|steps:   80|Train Avg Loss: 0.0181 \n",
      "Epoch:   20|steps:   90|Train Avg Loss: 0.0190 \n",
      "Epoch:   20|steps:  100|Train Avg Loss: 0.0205 \n",
      "Epoch:   20|steps:  110|Train Avg Loss: 0.0182 \n",
      "Epoch:   20|steps:  120|Train Avg Loss: 0.0198 \n",
      "Epoch:   21|steps:   10|Train Avg Loss: 0.0192 \n",
      "Epoch:   21|steps:   20|Train Avg Loss: 0.0203 \n",
      "Epoch:   21|steps:   30|Train Avg Loss: 0.0181 \n",
      "Epoch:   21|steps:   40|Train Avg Loss: 0.0200 \n",
      "Epoch:   21|steps:   50|Train Avg Loss: 0.0212 \n",
      "Epoch:   21|steps:   60|Train Avg Loss: 0.0202 \n",
      "Epoch:   21|steps:   70|Train Avg Loss: 0.0183 \n",
      "Epoch:   21|steps:   80|Train Avg Loss: 0.0209 \n",
      "Epoch:   21|steps:   90|Train Avg Loss: 0.0185 \n",
      "Epoch:   21|steps:  100|Train Avg Loss: 0.0198 \n",
      "Epoch:   21|steps:  110|Train Avg Loss: 0.0184 \n",
      "Epoch:   21|steps:  120|Train Avg Loss: 0.0183 \n",
      "Epoch:   22|steps:   10|Train Avg Loss: 0.0198 \n",
      "Epoch:   22|steps:   20|Train Avg Loss: 0.0182 \n",
      "Epoch:   22|steps:   30|Train Avg Loss: 0.0196 \n",
      "Epoch:   22|steps:   40|Train Avg Loss: 0.0188 \n",
      "Epoch:   22|steps:   50|Train Avg Loss: 0.0210 \n",
      "Epoch:   22|steps:   60|Train Avg Loss: 0.0189 \n",
      "Epoch:   22|steps:   70|Train Avg Loss: 0.0183 \n",
      "Epoch:   22|steps:   80|Train Avg Loss: 0.0197 \n",
      "Epoch:   22|steps:   90|Train Avg Loss: 0.0211 \n",
      "Epoch:   22|steps:  100|Train Avg Loss: 0.0215 \n",
      "Epoch:   22|steps:  110|Train Avg Loss: 0.0179 \n",
      "Epoch:   22|steps:  120|Train Avg Loss: 0.0177 \n",
      "Epoch:   23|steps:   10|Train Avg Loss: 0.0195 \n",
      "Epoch:   23|steps:   20|Train Avg Loss: 0.0186 \n",
      "Epoch:   23|steps:   30|Train Avg Loss: 0.0200 \n",
      "Epoch:   23|steps:   40|Train Avg Loss: 0.0190 \n",
      "Epoch:   23|steps:   50|Train Avg Loss: 0.0209 \n",
      "Epoch:   23|steps:   60|Train Avg Loss: 0.0189 \n",
      "Epoch:   23|steps:   70|Train Avg Loss: 0.0191 \n",
      "Epoch:   23|steps:   80|Train Avg Loss: 0.0193 \n",
      "Epoch:   23|steps:   90|Train Avg Loss: 0.0175 \n",
      "Epoch:   23|steps:  100|Train Avg Loss: 0.0200 \n",
      "Epoch:   23|steps:  110|Train Avg Loss: 0.0203 \n",
      "Epoch:   23|steps:  120|Train Avg Loss: 0.0167 \n",
      "Epoch:   24|steps:   10|Train Avg Loss: 0.0174 \n",
      "Epoch:   24|steps:   20|Train Avg Loss: 0.0202 \n",
      "Epoch:   24|steps:   30|Train Avg Loss: 0.0194 \n",
      "Epoch:   24|steps:   40|Train Avg Loss: 0.0185 \n",
      "Epoch:   24|steps:   50|Train Avg Loss: 0.0191 \n",
      "Epoch:   24|steps:   60|Train Avg Loss: 0.0192 \n",
      "Epoch:   24|steps:   70|Train Avg Loss: 0.0192 \n",
      "Epoch:   24|steps:   80|Train Avg Loss: 0.0182 \n",
      "Epoch:   24|steps:   90|Train Avg Loss: 0.0177 \n",
      "Epoch:   24|steps:  100|Train Avg Loss: 0.0194 \n",
      "Epoch:   24|steps:  110|Train Avg Loss: 0.0222 \n",
      "Epoch:   24|steps:  120|Train Avg Loss: 0.0180 \n",
      "Epoch:   25|steps:   10|Train Avg Loss: 0.0178 \n",
      "Epoch:   25|steps:   20|Train Avg Loss: 0.0185 \n",
      "Epoch:   25|steps:   30|Train Avg Loss: 0.0167 \n",
      "Epoch:   25|steps:   40|Train Avg Loss: 0.0179 \n",
      "Epoch:   25|steps:   50|Train Avg Loss: 0.0204 \n",
      "Epoch:   25|steps:   60|Train Avg Loss: 0.0218 \n",
      "Epoch:   25|steps:   70|Train Avg Loss: 0.0195 \n",
      "Epoch:   25|steps:   80|Train Avg Loss: 0.0189 \n",
      "Epoch:   25|steps:   90|Train Avg Loss: 0.0186 \n",
      "Epoch:   25|steps:  100|Train Avg Loss: 0.0199 \n",
      "Epoch:   25|steps:  110|Train Avg Loss: 0.0183 \n",
      "Epoch:   25|steps:  120|Train Avg Loss: 0.0185 \n",
      "Epoch:   26|steps:   10|Train Avg Loss: 0.0174 \n",
      "Epoch:   26|steps:   20|Train Avg Loss: 0.0198 \n",
      "Epoch:   26|steps:   30|Train Avg Loss: 0.0183 \n",
      "Epoch:   26|steps:   40|Train Avg Loss: 0.0191 \n",
      "Epoch:   26|steps:   50|Train Avg Loss: 0.0186 \n",
      "Epoch:   26|steps:   60|Train Avg Loss: 0.0192 \n",
      "Epoch:   26|steps:   70|Train Avg Loss: 0.0202 \n",
      "Epoch:   26|steps:   80|Train Avg Loss: 0.0185 \n",
      "Epoch:   26|steps:   90|Train Avg Loss: 0.0194 \n",
      "Epoch:   26|steps:  100|Train Avg Loss: 0.0190 \n",
      "Epoch:   26|steps:  110|Train Avg Loss: 0.0185 \n",
      "Epoch:   26|steps:  120|Train Avg Loss: 0.0188 \n",
      "Epoch:   27|steps:   10|Train Avg Loss: 0.0188 \n",
      "Epoch:   27|steps:   20|Train Avg Loss: 0.0194 \n",
      "Epoch:   27|steps:   30|Train Avg Loss: 0.0215 \n",
      "Epoch:   27|steps:   40|Train Avg Loss: 0.0196 \n",
      "Epoch:   27|steps:   50|Train Avg Loss: 0.0193 \n",
      "Epoch:   27|steps:   60|Train Avg Loss: 0.0176 \n",
      "Epoch:   27|steps:   70|Train Avg Loss: 0.0182 \n",
      "Epoch:   27|steps:   80|Train Avg Loss: 0.0172 \n",
      "Epoch:   27|steps:   90|Train Avg Loss: 0.0190 \n",
      "Epoch:   27|steps:  100|Train Avg Loss: 0.0188 \n",
      "Epoch:   27|steps:  110|Train Avg Loss: 0.0182 \n",
      "Epoch:   27|steps:  120|Train Avg Loss: 0.0186 \n",
      "Epoch:   28|steps:   10|Train Avg Loss: 0.0193 \n",
      "Epoch:   28|steps:   20|Train Avg Loss: 0.0186 \n",
      "Epoch:   28|steps:   30|Train Avg Loss: 0.0177 \n",
      "Epoch:   28|steps:   40|Train Avg Loss: 0.0191 \n",
      "Epoch:   28|steps:   50|Train Avg Loss: 0.0198 \n",
      "Epoch:   28|steps:   60|Train Avg Loss: 0.0182 \n",
      "Epoch:   28|steps:   70|Train Avg Loss: 0.0183 \n",
      "Epoch:   28|steps:   80|Train Avg Loss: 0.0173 \n",
      "Epoch:   28|steps:   90|Train Avg Loss: 0.0187 \n",
      "Epoch:   28|steps:  100|Train Avg Loss: 0.0190 \n",
      "Epoch:   28|steps:  110|Train Avg Loss: 0.0183 \n",
      "Epoch:   28|steps:  120|Train Avg Loss: 0.0199 \n",
      "Epoch:   29|steps:   10|Train Avg Loss: 0.0196 \n",
      "Epoch:   29|steps:   20|Train Avg Loss: 0.0173 \n",
      "Epoch:   29|steps:   30|Train Avg Loss: 0.0190 \n",
      "Epoch:   29|steps:   40|Train Avg Loss: 0.0177 \n",
      "Epoch:   29|steps:   50|Train Avg Loss: 0.0193 \n",
      "Epoch:   29|steps:   60|Train Avg Loss: 0.0184 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   29|steps:   70|Train Avg Loss: 0.0181 \n",
      "Epoch:   29|steps:   80|Train Avg Loss: 0.0183 \n",
      "Epoch:   29|steps:   90|Train Avg Loss: 0.0188 \n",
      "Epoch:   29|steps:  100|Train Avg Loss: 0.0186 \n",
      "Epoch:   29|steps:  110|Train Avg Loss: 0.0189 \n",
      "Epoch:   29|steps:  120|Train Avg Loss: 0.0185 \n",
      "Epoch:   30|steps:   10|Train Avg Loss: 0.0183 \n",
      "Epoch:   30|steps:   20|Train Avg Loss: 0.0187 \n",
      "Epoch:   30|steps:   30|Train Avg Loss: 0.0178 \n",
      "Epoch:   30|steps:   40|Train Avg Loss: 0.0184 \n",
      "Epoch:   30|steps:   50|Train Avg Loss: 0.0177 \n",
      "Epoch:   30|steps:   60|Train Avg Loss: 0.0189 \n",
      "Epoch:   30|steps:   70|Train Avg Loss: 0.0172 \n",
      "Epoch:   30|steps:   80|Train Avg Loss: 0.0184 \n",
      "Epoch:   30|steps:   90|Train Avg Loss: 0.0185 \n",
      "Epoch:   30|steps:  100|Train Avg Loss: 0.0187 \n",
      "Epoch:   30|steps:  110|Train Avg Loss: 0.0208 \n",
      "Epoch:   30|steps:  120|Train Avg Loss: 0.0185 \n",
      "Epoch:   31|steps:   10|Train Avg Loss: 0.0170 \n",
      "Epoch:   31|steps:   20|Train Avg Loss: 0.0183 \n",
      "Epoch:   31|steps:   30|Train Avg Loss: 0.0182 \n",
      "Epoch:   31|steps:   40|Train Avg Loss: 0.0192 \n",
      "Epoch:   31|steps:   50|Train Avg Loss: 0.0185 \n",
      "Epoch:   31|steps:   60|Train Avg Loss: 0.0183 \n",
      "Epoch:   31|steps:   70|Train Avg Loss: 0.0171 \n",
      "Epoch:   31|steps:   80|Train Avg Loss: 0.0184 \n",
      "Epoch:   31|steps:   90|Train Avg Loss: 0.0198 \n",
      "Epoch:   31|steps:  100|Train Avg Loss: 0.0196 \n",
      "Epoch:   31|steps:  110|Train Avg Loss: 0.0172 \n",
      "Epoch:   31|steps:  120|Train Avg Loss: 0.0205 \n",
      "Epoch:   32|steps:   10|Train Avg Loss: 0.0199 \n",
      "Epoch:   32|steps:   20|Train Avg Loss: 0.0184 \n",
      "Epoch:   32|steps:   30|Train Avg Loss: 0.0186 \n",
      "Epoch:   32|steps:   40|Train Avg Loss: 0.0179 \n",
      "Epoch:   32|steps:   50|Train Avg Loss: 0.0188 \n",
      "Epoch:   32|steps:   60|Train Avg Loss: 0.0186 \n",
      "Epoch:   32|steps:   70|Train Avg Loss: 0.0186 \n",
      "Epoch:   32|steps:   80|Train Avg Loss: 0.0171 \n",
      "Epoch:   32|steps:   90|Train Avg Loss: 0.0183 \n",
      "Epoch:   32|steps:  100|Train Avg Loss: 0.0185 \n",
      "Epoch:   32|steps:  110|Train Avg Loss: 0.0174 \n",
      "Epoch:   32|steps:  120|Train Avg Loss: 0.0181 \n",
      "Epoch:   33|steps:   10|Train Avg Loss: 0.0164 \n",
      "Epoch:   33|steps:   20|Train Avg Loss: 0.0183 \n",
      "Epoch:   33|steps:   30|Train Avg Loss: 0.0193 \n",
      "Epoch:   33|steps:   40|Train Avg Loss: 0.0187 \n",
      "Epoch:   33|steps:   50|Train Avg Loss: 0.0199 \n",
      "Epoch:   33|steps:   60|Train Avg Loss: 0.0179 \n",
      "Epoch:   33|steps:   70|Train Avg Loss: 0.0196 \n",
      "Epoch:   33|steps:   80|Train Avg Loss: 0.0169 \n",
      "Epoch:   33|steps:   90|Train Avg Loss: 0.0187 \n",
      "Epoch:   33|steps:  100|Train Avg Loss: 0.0177 \n",
      "Epoch:   33|steps:  110|Train Avg Loss: 0.0173 \n",
      "Epoch:   33|steps:  120|Train Avg Loss: 0.0175 \n",
      "Epoch:   34|steps:   10|Train Avg Loss: 0.0185 \n",
      "Epoch:   34|steps:   20|Train Avg Loss: 0.0175 \n",
      "Epoch:   34|steps:   30|Train Avg Loss: 0.0176 \n",
      "Epoch:   34|steps:   40|Train Avg Loss: 0.0200 \n",
      "Epoch:   34|steps:   50|Train Avg Loss: 0.0178 \n",
      "Epoch:   34|steps:   60|Train Avg Loss: 0.0188 \n",
      "Epoch:   34|steps:   70|Train Avg Loss: 0.0174 \n",
      "Epoch:   34|steps:   80|Train Avg Loss: 0.0171 \n",
      "Epoch:   34|steps:   90|Train Avg Loss: 0.0173 \n",
      "Epoch:   34|steps:  100|Train Avg Loss: 0.0174 \n",
      "Epoch:   34|steps:  110|Train Avg Loss: 0.0187 \n",
      "Epoch:   34|steps:  120|Train Avg Loss: 0.0188 \n",
      "Epoch:   35|steps:   10|Train Avg Loss: 0.0173 \n",
      "Epoch:   35|steps:   20|Train Avg Loss: 0.0155 \n",
      "Epoch:   35|steps:   30|Train Avg Loss: 0.0184 \n",
      "Epoch:   35|steps:   40|Train Avg Loss: 0.0203 \n",
      "Epoch:   35|steps:   50|Train Avg Loss: 0.0172 \n",
      "Epoch:   35|steps:   60|Train Avg Loss: 0.0193 \n",
      "Epoch:   35|steps:   70|Train Avg Loss: 0.0188 \n",
      "Epoch:   35|steps:   80|Train Avg Loss: 0.0167 \n",
      "Epoch:   35|steps:   90|Train Avg Loss: 0.0186 \n",
      "Epoch:   35|steps:  100|Train Avg Loss: 0.0192 \n",
      "Epoch:   35|steps:  110|Train Avg Loss: 0.0184 \n",
      "Epoch:   35|steps:  120|Train Avg Loss: 0.0170 \n",
      "Epoch:   36|steps:   10|Train Avg Loss: 0.0183 \n",
      "Epoch:   36|steps:   20|Train Avg Loss: 0.0173 \n",
      "Epoch:   36|steps:   30|Train Avg Loss: 0.0197 \n",
      "Epoch:   36|steps:   40|Train Avg Loss: 0.0197 \n",
      "Epoch:   36|steps:   50|Train Avg Loss: 0.0163 \n",
      "Epoch:   36|steps:   60|Train Avg Loss: 0.0182 \n",
      "Epoch:   36|steps:   70|Train Avg Loss: 0.0172 \n",
      "Epoch:   36|steps:   80|Train Avg Loss: 0.0173 \n",
      "Epoch:   36|steps:   90|Train Avg Loss: 0.0167 \n",
      "Epoch:   36|steps:  100|Train Avg Loss: 0.0206 \n",
      "Epoch:   36|steps:  110|Train Avg Loss: 0.0162 \n",
      "Epoch:   36|steps:  120|Train Avg Loss: 0.0174 \n",
      "Epoch:   37|steps:   10|Train Avg Loss: 0.0177 \n",
      "Epoch:   37|steps:   20|Train Avg Loss: 0.0189 \n",
      "Epoch:   37|steps:   30|Train Avg Loss: 0.0179 \n",
      "Epoch:   37|steps:   40|Train Avg Loss: 0.0167 \n",
      "Epoch:   37|steps:   50|Train Avg Loss: 0.0170 \n",
      "Epoch:   37|steps:   60|Train Avg Loss: 0.0185 \n",
      "Epoch:   37|steps:   70|Train Avg Loss: 0.0191 \n",
      "Epoch:   37|steps:   80|Train Avg Loss: 0.0175 \n",
      "Epoch:   37|steps:   90|Train Avg Loss: 0.0175 \n",
      "Epoch:   37|steps:  100|Train Avg Loss: 0.0176 \n",
      "Epoch:   37|steps:  110|Train Avg Loss: 0.0168 \n",
      "Epoch:   37|steps:  120|Train Avg Loss: 0.0170 \n",
      "Epoch:   38|steps:   10|Train Avg Loss: 0.0180 \n",
      "Epoch:   38|steps:   20|Train Avg Loss: 0.0178 \n",
      "Epoch:   38|steps:   30|Train Avg Loss: 0.0169 \n",
      "Epoch:   38|steps:   40|Train Avg Loss: 0.0165 \n",
      "Epoch:   38|steps:   50|Train Avg Loss: 0.0167 \n",
      "Epoch:   38|steps:   60|Train Avg Loss: 0.0163 \n",
      "Epoch:   38|steps:   70|Train Avg Loss: 0.0165 \n",
      "Epoch:   38|steps:   80|Train Avg Loss: 0.0179 \n",
      "Epoch:   38|steps:   90|Train Avg Loss: 0.0184 \n",
      "Epoch:   38|steps:  100|Train Avg Loss: 0.0182 \n",
      "Epoch:   38|steps:  110|Train Avg Loss: 0.0180 \n",
      "Epoch:   38|steps:  120|Train Avg Loss: 0.0162 \n",
      "Epoch:   39|steps:   10|Train Avg Loss: 0.0185 \n",
      "Epoch:   39|steps:   20|Train Avg Loss: 0.0160 \n",
      "Epoch:   39|steps:   30|Train Avg Loss: 0.0152 \n",
      "Epoch:   39|steps:   40|Train Avg Loss: 0.0169 \n",
      "Epoch:   39|steps:   50|Train Avg Loss: 0.0156 \n",
      "Epoch:   39|steps:   60|Train Avg Loss: 0.0160 \n",
      "Epoch:   39|steps:   70|Train Avg Loss: 0.0164 \n",
      "Epoch:   39|steps:   80|Train Avg Loss: 0.0175 \n",
      "Epoch:   39|steps:   90|Train Avg Loss: 0.0156 \n",
      "Epoch:   39|steps:  100|Train Avg Loss: 0.0182 \n",
      "Epoch:   39|steps:  110|Train Avg Loss: 0.0177 \n",
      "Epoch:   39|steps:  120|Train Avg Loss: 0.0180 \n",
      "Epoch:   40|steps:   10|Train Avg Loss: 0.0172 \n",
      "Epoch:   40|steps:   20|Train Avg Loss: 0.0157 \n",
      "Epoch:   40|steps:   30|Train Avg Loss: 0.0166 \n",
      "Epoch:   40|steps:   40|Train Avg Loss: 0.0149 \n",
      "Epoch:   40|steps:   50|Train Avg Loss: 0.0153 \n",
      "Epoch:   40|steps:   60|Train Avg Loss: 0.0158 \n",
      "Epoch:   40|steps:   70|Train Avg Loss: 0.0148 \n",
      "Epoch:   40|steps:   80|Train Avg Loss: 0.0164 \n",
      "Epoch:   40|steps:   90|Train Avg Loss: 0.0158 \n",
      "Epoch:   40|steps:  100|Train Avg Loss: 0.0174 \n",
      "Epoch:   40|steps:  110|Train Avg Loss: 0.0176 \n",
      "Epoch:   40|steps:  120|Train Avg Loss: 0.0175 \n",
      "Epoch:   41|steps:   10|Train Avg Loss: 0.0169 \n",
      "Epoch:   41|steps:   20|Train Avg Loss: 0.0152 \n",
      "Epoch:   41|steps:   30|Train Avg Loss: 0.0151 \n",
      "Epoch:   41|steps:   40|Train Avg Loss: 0.0149 \n",
      "Epoch:   41|steps:   50|Train Avg Loss: 0.0175 \n",
      "Epoch:   41|steps:   60|Train Avg Loss: 0.0153 \n",
      "Epoch:   41|steps:   70|Train Avg Loss: 0.0156 \n",
      "Epoch:   41|steps:   80|Train Avg Loss: 0.0155 \n",
      "Epoch:   41|steps:   90|Train Avg Loss: 0.0157 \n",
      "Epoch:   41|steps:  100|Train Avg Loss: 0.0148 \n",
      "Epoch:   41|steps:  110|Train Avg Loss: 0.0167 \n",
      "Epoch:   41|steps:  120|Train Avg Loss: 0.0161 \n",
      "Epoch:   42|steps:   10|Train Avg Loss: 0.0153 \n",
      "Epoch:   42|steps:   20|Train Avg Loss: 0.0143 \n",
      "Epoch:   42|steps:   30|Train Avg Loss: 0.0165 \n",
      "Epoch:   42|steps:   40|Train Avg Loss: 0.0153 \n",
      "Epoch:   42|steps:   50|Train Avg Loss: 0.0145 \n",
      "Epoch:   42|steps:   60|Train Avg Loss: 0.0154 \n",
      "Epoch:   42|steps:   70|Train Avg Loss: 0.0157 \n",
      "Epoch:   42|steps:   80|Train Avg Loss: 0.0162 \n",
      "Epoch:   42|steps:   90|Train Avg Loss: 0.0159 \n",
      "Epoch:   42|steps:  100|Train Avg Loss: 0.0136 \n",
      "Epoch:   42|steps:  110|Train Avg Loss: 0.0144 \n",
      "Epoch:   42|steps:  120|Train Avg Loss: 0.0167 \n",
      "Epoch:   43|steps:   10|Train Avg Loss: 0.0154 \n",
      "Epoch:   43|steps:   20|Train Avg Loss: 0.0166 \n",
      "Epoch:   43|steps:   30|Train Avg Loss: 0.0158 \n",
      "Epoch:   43|steps:   40|Train Avg Loss: 0.0153 \n",
      "Epoch:   43|steps:   50|Train Avg Loss: 0.0144 \n",
      "Epoch:   43|steps:   60|Train Avg Loss: 0.0155 \n",
      "Epoch:   43|steps:   70|Train Avg Loss: 0.0155 \n",
      "Epoch:   43|steps:   80|Train Avg Loss: 0.0160 \n",
      "Epoch:   43|steps:   90|Train Avg Loss: 0.0157 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   43|steps:  100|Train Avg Loss: 0.0155 \n",
      "Epoch:   43|steps:  110|Train Avg Loss: 0.0154 \n",
      "Epoch:   43|steps:  120|Train Avg Loss: 0.0148 \n",
      "Epoch:   44|steps:   10|Train Avg Loss: 0.0135 \n",
      "Epoch:   44|steps:   20|Train Avg Loss: 0.0149 \n",
      "Epoch:   44|steps:   30|Train Avg Loss: 0.0150 \n",
      "Epoch:   44|steps:   40|Train Avg Loss: 0.0150 \n",
      "Epoch:   44|steps:   50|Train Avg Loss: 0.0155 \n",
      "Epoch:   44|steps:   60|Train Avg Loss: 0.0151 \n",
      "Epoch:   44|steps:   70|Train Avg Loss: 0.0144 \n",
      "Epoch:   44|steps:   80|Train Avg Loss: 0.0150 \n",
      "Epoch:   44|steps:   90|Train Avg Loss: 0.0164 \n",
      "Epoch:   44|steps:  100|Train Avg Loss: 0.0153 \n",
      "Epoch:   44|steps:  110|Train Avg Loss: 0.0154 \n",
      "Epoch:   44|steps:  120|Train Avg Loss: 0.0162 \n",
      "Epoch:   45|steps:   10|Train Avg Loss: 0.0160 \n",
      "Epoch:   45|steps:   20|Train Avg Loss: 0.0158 \n",
      "Epoch:   45|steps:   30|Train Avg Loss: 0.0155 \n",
      "Epoch:   45|steps:   40|Train Avg Loss: 0.0144 \n",
      "Epoch:   45|steps:   50|Train Avg Loss: 0.0143 \n",
      "Epoch:   45|steps:   60|Train Avg Loss: 0.0156 \n",
      "Epoch:   45|steps:   70|Train Avg Loss: 0.0149 \n",
      "Epoch:   45|steps:   80|Train Avg Loss: 0.0138 \n",
      "Epoch:   45|steps:   90|Train Avg Loss: 0.0140 \n",
      "Epoch:   45|steps:  100|Train Avg Loss: 0.0148 \n",
      "Epoch:   45|steps:  110|Train Avg Loss: 0.0142 \n",
      "Epoch:   45|steps:  120|Train Avg Loss: 0.0164 \n",
      "Epoch:   46|steps:   10|Train Avg Loss: 0.0136 \n",
      "Epoch:   46|steps:   20|Train Avg Loss: 0.0154 \n",
      "Epoch:   46|steps:   30|Train Avg Loss: 0.0136 \n",
      "Epoch:   46|steps:   40|Train Avg Loss: 0.0147 \n",
      "Epoch:   46|steps:   50|Train Avg Loss: 0.0149 \n",
      "Epoch:   46|steps:   60|Train Avg Loss: 0.0156 \n",
      "Epoch:   46|steps:   70|Train Avg Loss: 0.0173 \n",
      "Epoch:   46|steps:   80|Train Avg Loss: 0.0130 \n",
      "Epoch:   46|steps:   90|Train Avg Loss: 0.0141 \n",
      "Epoch:   46|steps:  100|Train Avg Loss: 0.0148 \n",
      "Epoch:   46|steps:  110|Train Avg Loss: 0.0152 \n",
      "Epoch:   46|steps:  120|Train Avg Loss: 0.0160 \n",
      "Epoch:   47|steps:   10|Train Avg Loss: 0.0155 \n",
      "Epoch:   47|steps:   20|Train Avg Loss: 0.0145 \n",
      "Epoch:   47|steps:   30|Train Avg Loss: 0.0142 \n",
      "Epoch:   47|steps:   40|Train Avg Loss: 0.0152 \n",
      "Epoch:   47|steps:   50|Train Avg Loss: 0.0149 \n",
      "Epoch:   47|steps:   60|Train Avg Loss: 0.0150 \n",
      "Epoch:   47|steps:   70|Train Avg Loss: 0.0143 \n",
      "Epoch:   47|steps:   80|Train Avg Loss: 0.0149 \n",
      "Epoch:   47|steps:   90|Train Avg Loss: 0.0142 \n",
      "Epoch:   47|steps:  100|Train Avg Loss: 0.0139 \n",
      "Epoch:   47|steps:  110|Train Avg Loss: 0.0136 \n",
      "Epoch:   47|steps:  120|Train Avg Loss: 0.0146 \n",
      "Epoch:   48|steps:   10|Train Avg Loss: 0.0139 \n",
      "Epoch:   48|steps:   20|Train Avg Loss: 0.0145 \n",
      "Epoch:   48|steps:   30|Train Avg Loss: 0.0137 \n",
      "Epoch:   48|steps:   40|Train Avg Loss: 0.0150 \n",
      "Epoch:   48|steps:   50|Train Avg Loss: 0.0138 \n",
      "Epoch:   48|steps:   60|Train Avg Loss: 0.0161 \n",
      "Epoch:   48|steps:   70|Train Avg Loss: 0.0134 \n",
      "Epoch:   48|steps:   80|Train Avg Loss: 0.0155 \n",
      "Epoch:   48|steps:   90|Train Avg Loss: 0.0149 \n",
      "Epoch:   48|steps:  100|Train Avg Loss: 0.0136 \n",
      "Epoch:   48|steps:  110|Train Avg Loss: 0.0152 \n",
      "Epoch:   48|steps:  120|Train Avg Loss: 0.0135 \n",
      "Epoch:   49|steps:   10|Train Avg Loss: 0.0129 \n",
      "Epoch:   49|steps:   20|Train Avg Loss: 0.0156 \n",
      "Epoch:   49|steps:   30|Train Avg Loss: 0.0136 \n",
      "Epoch:   49|steps:   40|Train Avg Loss: 0.0144 \n",
      "Epoch:   49|steps:   50|Train Avg Loss: 0.0140 \n",
      "Epoch:   49|steps:   60|Train Avg Loss: 0.0152 \n",
      "Epoch:   49|steps:   70|Train Avg Loss: 0.0146 \n",
      "Epoch:   49|steps:   80|Train Avg Loss: 0.0138 \n",
      "Epoch:   49|steps:   90|Train Avg Loss: 0.0140 \n",
      "Epoch:   49|steps:  100|Train Avg Loss: 0.0148 \n",
      "Epoch:   49|steps:  110|Train Avg Loss: 0.0149 \n",
      "Epoch:   49|steps:  120|Train Avg Loss: 0.0155 \n",
      "Epoch:   50|steps:   10|Train Avg Loss: 0.0149 \n",
      "Epoch:   50|steps:   20|Train Avg Loss: 0.0131 \n",
      "Epoch:   50|steps:   30|Train Avg Loss: 0.0148 \n",
      "Epoch:   50|steps:   40|Train Avg Loss: 0.0147 \n",
      "Epoch:   50|steps:   50|Train Avg Loss: 0.0143 \n",
      "Epoch:   50|steps:   60|Train Avg Loss: 0.0143 \n",
      "Epoch:   50|steps:   70|Train Avg Loss: 0.0145 \n",
      "Epoch:   50|steps:   80|Train Avg Loss: 0.0131 \n",
      "Epoch:   50|steps:   90|Train Avg Loss: 0.0150 \n",
      "Epoch:   50|steps:  100|Train Avg Loss: 0.0141 \n",
      "Epoch:   50|steps:  110|Train Avg Loss: 0.0140 \n",
      "Epoch:   50|steps:  120|Train Avg Loss: 0.0148 \n",
      "Epoch:   51|steps:   10|Train Avg Loss: 0.0141 \n",
      "Epoch:   51|steps:   20|Train Avg Loss: 0.0144 \n",
      "Epoch:   51|steps:   30|Train Avg Loss: 0.0141 \n",
      "Epoch:   51|steps:   40|Train Avg Loss: 0.0142 \n",
      "Epoch:   51|steps:   50|Train Avg Loss: 0.0150 \n",
      "Epoch:   51|steps:   60|Train Avg Loss: 0.0144 \n",
      "Epoch:   51|steps:   70|Train Avg Loss: 0.0134 \n",
      "Epoch:   51|steps:   80|Train Avg Loss: 0.0154 \n",
      "Epoch:   51|steps:   90|Train Avg Loss: 0.0139 \n",
      "Epoch:   51|steps:  100|Train Avg Loss: 0.0146 \n",
      "Epoch:   51|steps:  110|Train Avg Loss: 0.0152 \n",
      "Epoch:   51|steps:  120|Train Avg Loss: 0.0142 \n",
      "Epoch:   52|steps:   10|Train Avg Loss: 0.0122 \n",
      "Epoch:   52|steps:   20|Train Avg Loss: 0.0141 \n",
      "Epoch:   52|steps:   30|Train Avg Loss: 0.0159 \n",
      "Epoch:   52|steps:   40|Train Avg Loss: 0.0156 \n",
      "Epoch:   52|steps:   50|Train Avg Loss: 0.0146 \n",
      "Epoch:   52|steps:   60|Train Avg Loss: 0.0129 \n",
      "Epoch:   52|steps:   70|Train Avg Loss: 0.0143 \n",
      "Epoch:   52|steps:   80|Train Avg Loss: 0.0134 \n",
      "Epoch:   52|steps:   90|Train Avg Loss: 0.0148 \n",
      "Epoch:   52|steps:  100|Train Avg Loss: 0.0143 \n",
      "Epoch:   52|steps:  110|Train Avg Loss: 0.0138 \n",
      "Epoch:   52|steps:  120|Train Avg Loss: 0.0147 \n",
      "Epoch:   53|steps:   10|Train Avg Loss: 0.0130 \n",
      "Epoch:   53|steps:   20|Train Avg Loss: 0.0132 \n",
      "Epoch:   53|steps:   30|Train Avg Loss: 0.0145 \n",
      "Epoch:   53|steps:   40|Train Avg Loss: 0.0136 \n",
      "Epoch:   53|steps:   50|Train Avg Loss: 0.0148 \n",
      "Epoch:   53|steps:   60|Train Avg Loss: 0.0140 \n",
      "Epoch:   53|steps:   70|Train Avg Loss: 0.0154 \n",
      "Epoch:   53|steps:   80|Train Avg Loss: 0.0147 \n",
      "Epoch:   53|steps:   90|Train Avg Loss: 0.0150 \n",
      "Epoch:   53|steps:  100|Train Avg Loss: 0.0142 \n",
      "Epoch:   53|steps:  110|Train Avg Loss: 0.0143 \n",
      "Epoch:   53|steps:  120|Train Avg Loss: 0.0142 \n",
      "Epoch:   54|steps:   10|Train Avg Loss: 0.0154 \n",
      "Epoch:   54|steps:   20|Train Avg Loss: 0.0145 \n",
      "Epoch:   54|steps:   30|Train Avg Loss: 0.0137 \n",
      "Epoch:   54|steps:   40|Train Avg Loss: 0.0146 \n",
      "Epoch:   54|steps:   50|Train Avg Loss: 0.0143 \n",
      "Epoch:   54|steps:   60|Train Avg Loss: 0.0147 \n",
      "Epoch:   54|steps:   70|Train Avg Loss: 0.0141 \n",
      "Epoch:   54|steps:   80|Train Avg Loss: 0.0131 \n",
      "Epoch:   54|steps:   90|Train Avg Loss: 0.0127 \n",
      "Epoch:   54|steps:  100|Train Avg Loss: 0.0137 \n",
      "Epoch:   54|steps:  110|Train Avg Loss: 0.0128 \n",
      "Epoch:   54|steps:  120|Train Avg Loss: 0.0141 \n",
      "Epoch:   55|steps:   10|Train Avg Loss: 0.0137 \n",
      "Epoch:   55|steps:   20|Train Avg Loss: 0.0140 \n",
      "Epoch:   55|steps:   30|Train Avg Loss: 0.0136 \n",
      "Epoch:   55|steps:   40|Train Avg Loss: 0.0137 \n",
      "Epoch:   55|steps:   50|Train Avg Loss: 0.0134 \n",
      "Epoch:   55|steps:   60|Train Avg Loss: 0.0131 \n",
      "Epoch:   55|steps:   70|Train Avg Loss: 0.0137 \n",
      "Epoch:   55|steps:   80|Train Avg Loss: 0.0145 \n",
      "Epoch:   55|steps:   90|Train Avg Loss: 0.0141 \n",
      "Epoch:   55|steps:  100|Train Avg Loss: 0.0138 \n",
      "Epoch:   55|steps:  110|Train Avg Loss: 0.0153 \n",
      "Epoch:   55|steps:  120|Train Avg Loss: 0.0144 \n",
      "Epoch:   56|steps:   10|Train Avg Loss: 0.0137 \n",
      "Epoch:   56|steps:   20|Train Avg Loss: 0.0126 \n",
      "Epoch:   56|steps:   30|Train Avg Loss: 0.0133 \n",
      "Epoch:   56|steps:   40|Train Avg Loss: 0.0145 \n",
      "Epoch:   56|steps:   50|Train Avg Loss: 0.0155 \n",
      "Epoch:   56|steps:   60|Train Avg Loss: 0.0163 \n",
      "Epoch:   56|steps:   70|Train Avg Loss: 0.0131 \n",
      "Epoch:   56|steps:   80|Train Avg Loss: 0.0132 \n",
      "Epoch:   56|steps:   90|Train Avg Loss: 0.0133 \n",
      "Epoch:   56|steps:  100|Train Avg Loss: 0.0125 \n",
      "Epoch:   56|steps:  110|Train Avg Loss: 0.0137 \n",
      "Epoch:   56|steps:  120|Train Avg Loss: 0.0143 \n",
      "Epoch:   57|steps:   10|Train Avg Loss: 0.0146 \n",
      "Epoch:   57|steps:   20|Train Avg Loss: 0.0132 \n",
      "Epoch:   57|steps:   30|Train Avg Loss: 0.0145 \n",
      "Epoch:   57|steps:   40|Train Avg Loss: 0.0132 \n",
      "Epoch:   57|steps:   50|Train Avg Loss: 0.0141 \n",
      "Epoch:   57|steps:   60|Train Avg Loss: 0.0134 \n",
      "Epoch:   57|steps:   70|Train Avg Loss: 0.0140 \n",
      "Epoch:   57|steps:   80|Train Avg Loss: 0.0129 \n",
      "Epoch:   57|steps:   90|Train Avg Loss: 0.0142 \n",
      "Epoch:   57|steps:  100|Train Avg Loss: 0.0143 \n",
      "Epoch:   57|steps:  110|Train Avg Loss: 0.0157 \n",
      "Epoch:   57|steps:  120|Train Avg Loss: 0.0128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   58|steps:   10|Train Avg Loss: 0.0141 \n",
      "Epoch:   58|steps:   20|Train Avg Loss: 0.0126 \n",
      "Epoch:   58|steps:   30|Train Avg Loss: 0.0131 \n",
      "Epoch:   58|steps:   40|Train Avg Loss: 0.0135 \n",
      "Epoch:   58|steps:   50|Train Avg Loss: 0.0135 \n",
      "Epoch:   58|steps:   60|Train Avg Loss: 0.0139 \n",
      "Epoch:   58|steps:   70|Train Avg Loss: 0.0134 \n",
      "Epoch:   58|steps:   80|Train Avg Loss: 0.0162 \n",
      "Epoch:   58|steps:   90|Train Avg Loss: 0.0134 \n",
      "Epoch:   58|steps:  100|Train Avg Loss: 0.0143 \n",
      "Epoch:   58|steps:  110|Train Avg Loss: 0.0144 \n",
      "Epoch:   58|steps:  120|Train Avg Loss: 0.0140 \n",
      "Epoch:   59|steps:   10|Train Avg Loss: 0.0121 \n",
      "Epoch:   59|steps:   20|Train Avg Loss: 0.0140 \n",
      "Epoch:   59|steps:   30|Train Avg Loss: 0.0133 \n",
      "Epoch:   59|steps:   40|Train Avg Loss: 0.0147 \n",
      "Epoch:   59|steps:   50|Train Avg Loss: 0.0132 \n",
      "Epoch:   59|steps:   60|Train Avg Loss: 0.0132 \n",
      "Epoch:   59|steps:   70|Train Avg Loss: 0.0142 \n",
      "Epoch:   59|steps:   80|Train Avg Loss: 0.0131 \n",
      "Epoch:   59|steps:   90|Train Avg Loss: 0.0149 \n",
      "Epoch:   59|steps:  100|Train Avg Loss: 0.0156 \n",
      "Epoch:   59|steps:  110|Train Avg Loss: 0.0129 \n",
      "Epoch:   59|steps:  120|Train Avg Loss: 0.0135 \n",
      "Epoch:   60|steps:   10|Train Avg Loss: 0.0137 \n",
      "Epoch:   60|steps:   20|Train Avg Loss: 0.0143 \n",
      "Epoch:   60|steps:   30|Train Avg Loss: 0.0139 \n",
      "Epoch:   60|steps:   40|Train Avg Loss: 0.0139 \n",
      "Epoch:   60|steps:   50|Train Avg Loss: 0.0150 \n",
      "Epoch:   60|steps:   60|Train Avg Loss: 0.0134 \n",
      "Epoch:   60|steps:   70|Train Avg Loss: 0.0135 \n",
      "Epoch:   60|steps:   80|Train Avg Loss: 0.0124 \n",
      "Epoch:   60|steps:   90|Train Avg Loss: 0.0132 \n",
      "Epoch:   60|steps:  100|Train Avg Loss: 0.0144 \n",
      "Epoch:   60|steps:  110|Train Avg Loss: 0.0132 \n",
      "Epoch:   60|steps:  120|Train Avg Loss: 0.0132 \n",
      "Epoch:   61|steps:   10|Train Avg Loss: 0.0144 \n",
      "Epoch:   61|steps:   20|Train Avg Loss: 0.0140 \n",
      "Epoch:   61|steps:   30|Train Avg Loss: 0.0136 \n",
      "Epoch:   61|steps:   40|Train Avg Loss: 0.0125 \n",
      "Epoch:   61|steps:   50|Train Avg Loss: 0.0142 \n",
      "Epoch:   61|steps:   60|Train Avg Loss: 0.0144 \n",
      "Epoch:   61|steps:   70|Train Avg Loss: 0.0134 \n",
      "Epoch:   61|steps:   80|Train Avg Loss: 0.0137 \n",
      "Epoch:   61|steps:   90|Train Avg Loss: 0.0127 \n",
      "Epoch:   61|steps:  100|Train Avg Loss: 0.0144 \n",
      "Epoch:   61|steps:  110|Train Avg Loss: 0.0141 \n",
      "Epoch:   61|steps:  120|Train Avg Loss: 0.0121 \n",
      "Epoch:   62|steps:   10|Train Avg Loss: 0.0122 \n",
      "Epoch:   62|steps:   20|Train Avg Loss: 0.0144 \n",
      "Epoch:   62|steps:   30|Train Avg Loss: 0.0126 \n",
      "Epoch:   62|steps:   40|Train Avg Loss: 0.0133 \n",
      "Epoch:   62|steps:   50|Train Avg Loss: 0.0139 \n",
      "Epoch:   62|steps:   60|Train Avg Loss: 0.0136 \n",
      "Epoch:   62|steps:   70|Train Avg Loss: 0.0131 \n",
      "Epoch:   62|steps:   80|Train Avg Loss: 0.0134 \n",
      "Epoch:   62|steps:   90|Train Avg Loss: 0.0159 \n",
      "Epoch:   62|steps:  100|Train Avg Loss: 0.0141 \n",
      "Epoch:   62|steps:  110|Train Avg Loss: 0.0128 \n",
      "Epoch:   62|steps:  120|Train Avg Loss: 0.0129 \n",
      "Epoch:   63|steps:   10|Train Avg Loss: 0.0122 \n",
      "Epoch:   63|steps:   20|Train Avg Loss: 0.0130 \n",
      "Epoch:   63|steps:   30|Train Avg Loss: 0.0141 \n",
      "Epoch:   63|steps:   40|Train Avg Loss: 0.0128 \n",
      "Epoch:   63|steps:   50|Train Avg Loss: 0.0125 \n",
      "Epoch:   63|steps:   60|Train Avg Loss: 0.0138 \n",
      "Epoch:   63|steps:   70|Train Avg Loss: 0.0137 \n",
      "Epoch:   63|steps:   80|Train Avg Loss: 0.0133 \n",
      "Epoch:   63|steps:   90|Train Avg Loss: 0.0128 \n",
      "Epoch:   63|steps:  100|Train Avg Loss: 0.0146 \n",
      "Epoch:   63|steps:  110|Train Avg Loss: 0.0122 \n",
      "Epoch:   63|steps:  120|Train Avg Loss: 0.0150 \n",
      "Epoch:   64|steps:   10|Train Avg Loss: 0.0131 \n",
      "Epoch:   64|steps:   20|Train Avg Loss: 0.0126 \n",
      "Epoch:   64|steps:   30|Train Avg Loss: 0.0133 \n",
      "Epoch:   64|steps:   40|Train Avg Loss: 0.0145 \n",
      "Epoch:   64|steps:   50|Train Avg Loss: 0.0134 \n",
      "Epoch:   64|steps:   60|Train Avg Loss: 0.0126 \n",
      "Epoch:   64|steps:   70|Train Avg Loss: 0.0127 \n",
      "Epoch:   64|steps:   80|Train Avg Loss: 0.0136 \n",
      "Epoch:   64|steps:   90|Train Avg Loss: 0.0140 \n",
      "Epoch:   64|steps:  100|Train Avg Loss: 0.0130 \n",
      "Epoch:   64|steps:  110|Train Avg Loss: 0.0131 \n",
      "Epoch:   64|steps:  120|Train Avg Loss: 0.0140 \n",
      "Epoch:   65|steps:   10|Train Avg Loss: 0.0133 \n",
      "Epoch:   65|steps:   20|Train Avg Loss: 0.0137 \n",
      "Epoch:   65|steps:   30|Train Avg Loss: 0.0126 \n",
      "Epoch:   65|steps:   40|Train Avg Loss: 0.0121 \n",
      "Epoch:   65|steps:   50|Train Avg Loss: 0.0129 \n",
      "Epoch:   65|steps:   60|Train Avg Loss: 0.0134 \n",
      "Epoch:   65|steps:   70|Train Avg Loss: 0.0133 \n",
      "Epoch:   65|steps:   80|Train Avg Loss: 0.0139 \n",
      "Epoch:   65|steps:   90|Train Avg Loss: 0.0150 \n",
      "Epoch:   65|steps:  100|Train Avg Loss: 0.0139 \n",
      "Epoch:   65|steps:  110|Train Avg Loss: 0.0125 \n",
      "Epoch:   65|steps:  120|Train Avg Loss: 0.0137 \n",
      "Epoch:   66|steps:   10|Train Avg Loss: 0.0126 \n",
      "Epoch:   66|steps:   20|Train Avg Loss: 0.0130 \n",
      "Epoch:   66|steps:   30|Train Avg Loss: 0.0134 \n",
      "Epoch:   66|steps:   40|Train Avg Loss: 0.0132 \n",
      "Epoch:   66|steps:   50|Train Avg Loss: 0.0130 \n",
      "Epoch:   66|steps:   60|Train Avg Loss: 0.0133 \n",
      "Epoch:   66|steps:   70|Train Avg Loss: 0.0128 \n",
      "Epoch:   66|steps:   80|Train Avg Loss: 0.0135 \n",
      "Epoch:   66|steps:   90|Train Avg Loss: 0.0123 \n",
      "Epoch:   66|steps:  100|Train Avg Loss: 0.0142 \n",
      "Epoch:   66|steps:  110|Train Avg Loss: 0.0131 \n",
      "Epoch:   66|steps:  120|Train Avg Loss: 0.0145 \n",
      "Epoch:   67|steps:   10|Train Avg Loss: 0.0135 \n",
      "Epoch:   67|steps:   20|Train Avg Loss: 0.0139 \n",
      "Epoch:   67|steps:   30|Train Avg Loss: 0.0132 \n",
      "Epoch:   67|steps:   40|Train Avg Loss: 0.0130 \n",
      "Epoch:   67|steps:   50|Train Avg Loss: 0.0144 \n",
      "Epoch:   67|steps:   60|Train Avg Loss: 0.0128 \n",
      "Epoch:   67|steps:   70|Train Avg Loss: 0.0129 \n",
      "Epoch:   67|steps:   80|Train Avg Loss: 0.0139 \n",
      "Epoch:   67|steps:   90|Train Avg Loss: 0.0132 \n",
      "Epoch:   67|steps:  100|Train Avg Loss: 0.0134 \n",
      "Epoch:   67|steps:  110|Train Avg Loss: 0.0119 \n",
      "Epoch:   67|steps:  120|Train Avg Loss: 0.0131 \n",
      "Epoch:   68|steps:   10|Train Avg Loss: 0.0126 \n",
      "Epoch:   68|steps:   20|Train Avg Loss: 0.0138 \n",
      "Epoch:   68|steps:   30|Train Avg Loss: 0.0128 \n",
      "Epoch:   68|steps:   40|Train Avg Loss: 0.0128 \n",
      "Epoch:   68|steps:   50|Train Avg Loss: 0.0120 \n",
      "Epoch:   68|steps:   60|Train Avg Loss: 0.0124 \n",
      "Epoch:   68|steps:   70|Train Avg Loss: 0.0143 \n",
      "Epoch:   68|steps:   80|Train Avg Loss: 0.0122 \n",
      "Epoch:   68|steps:   90|Train Avg Loss: 0.0133 \n",
      "Epoch:   68|steps:  100|Train Avg Loss: 0.0134 \n",
      "Epoch:   68|steps:  110|Train Avg Loss: 0.0143 \n",
      "Epoch:   68|steps:  120|Train Avg Loss: 0.0133 \n",
      "Epoch:   69|steps:   10|Train Avg Loss: 0.0111 \n",
      "Epoch:   69|steps:   20|Train Avg Loss: 0.0128 \n",
      "Epoch:   69|steps:   30|Train Avg Loss: 0.0147 \n",
      "Epoch:   69|steps:   40|Train Avg Loss: 0.0127 \n",
      "Epoch:   69|steps:   50|Train Avg Loss: 0.0138 \n",
      "Epoch:   69|steps:   60|Train Avg Loss: 0.0136 \n",
      "Epoch:   69|steps:   70|Train Avg Loss: 0.0129 \n",
      "Epoch:   69|steps:   80|Train Avg Loss: 0.0127 \n",
      "Epoch:   69|steps:   90|Train Avg Loss: 0.0135 \n",
      "Epoch:   69|steps:  100|Train Avg Loss: 0.0127 \n",
      "Epoch:   69|steps:  110|Train Avg Loss: 0.0131 \n",
      "Epoch:   69|steps:  120|Train Avg Loss: 0.0126 \n",
      "Epoch:   70|steps:   10|Train Avg Loss: 0.0136 \n",
      "Epoch:   70|steps:   20|Train Avg Loss: 0.0133 \n",
      "Epoch:   70|steps:   30|Train Avg Loss: 0.0136 \n",
      "Epoch:   70|steps:   40|Train Avg Loss: 0.0135 \n",
      "Epoch:   70|steps:   50|Train Avg Loss: 0.0127 \n",
      "Epoch:   70|steps:   60|Train Avg Loss: 0.0129 \n",
      "Epoch:   70|steps:   70|Train Avg Loss: 0.0138 \n",
      "Epoch:   70|steps:   80|Train Avg Loss: 0.0136 \n",
      "Epoch:   70|steps:   90|Train Avg Loss: 0.0122 \n",
      "Epoch:   70|steps:  100|Train Avg Loss: 0.0140 \n",
      "Epoch:   70|steps:  110|Train Avg Loss: 0.0125 \n",
      "Epoch:   70|steps:  120|Train Avg Loss: 0.0123 \n",
      "Epoch:   71|steps:   10|Train Avg Loss: 0.0134 \n",
      "Epoch:   71|steps:   20|Train Avg Loss: 0.0131 \n",
      "Epoch:   71|steps:   30|Train Avg Loss: 0.0117 \n",
      "Epoch:   71|steps:   40|Train Avg Loss: 0.0126 \n",
      "Epoch:   71|steps:   50|Train Avg Loss: 0.0137 \n",
      "Epoch:   71|steps:   60|Train Avg Loss: 0.0131 \n",
      "Epoch:   71|steps:   70|Train Avg Loss: 0.0130 \n",
      "Epoch:   71|steps:   80|Train Avg Loss: 0.0136 \n",
      "Epoch:   71|steps:   90|Train Avg Loss: 0.0139 \n",
      "Epoch:   71|steps:  100|Train Avg Loss: 0.0124 \n",
      "Epoch:   71|steps:  110|Train Avg Loss: 0.0124 \n",
      "Epoch:   71|steps:  120|Train Avg Loss: 0.0117 \n",
      "Epoch:   72|steps:   10|Train Avg Loss: 0.0138 \n",
      "Epoch:   72|steps:   20|Train Avg Loss: 0.0124 \n",
      "Epoch:   72|steps:   30|Train Avg Loss: 0.0123 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   72|steps:   40|Train Avg Loss: 0.0116 \n",
      "Epoch:   72|steps:   50|Train Avg Loss: 0.0126 \n",
      "Epoch:   72|steps:   60|Train Avg Loss: 0.0132 \n",
      "Epoch:   72|steps:   70|Train Avg Loss: 0.0128 \n",
      "Epoch:   72|steps:   80|Train Avg Loss: 0.0132 \n",
      "Epoch:   72|steps:   90|Train Avg Loss: 0.0143 \n",
      "Epoch:   72|steps:  100|Train Avg Loss: 0.0133 \n",
      "Epoch:   72|steps:  110|Train Avg Loss: 0.0126 \n",
      "Epoch:   72|steps:  120|Train Avg Loss: 0.0133 \n",
      "Epoch:   73|steps:   10|Train Avg Loss: 0.0115 \n",
      "Epoch:   73|steps:   20|Train Avg Loss: 0.0136 \n",
      "Epoch:   73|steps:   30|Train Avg Loss: 0.0123 \n",
      "Epoch:   73|steps:   40|Train Avg Loss: 0.0127 \n",
      "Epoch:   73|steps:   50|Train Avg Loss: 0.0124 \n",
      "Epoch:   73|steps:   60|Train Avg Loss: 0.0124 \n",
      "Epoch:   73|steps:   70|Train Avg Loss: 0.0128 \n",
      "Epoch:   73|steps:   80|Train Avg Loss: 0.0128 \n",
      "Epoch:   73|steps:   90|Train Avg Loss: 0.0132 \n",
      "Epoch:   73|steps:  100|Train Avg Loss: 0.0129 \n",
      "Epoch:   73|steps:  110|Train Avg Loss: 0.0127 \n",
      "Epoch:   73|steps:  120|Train Avg Loss: 0.0126 \n",
      "Epoch:   74|steps:   10|Train Avg Loss: 0.0117 \n",
      "Epoch:   74|steps:   20|Train Avg Loss: 0.0126 \n",
      "Epoch:   74|steps:   30|Train Avg Loss: 0.0129 \n",
      "Epoch:   74|steps:   40|Train Avg Loss: 0.0125 \n",
      "Epoch:   74|steps:   50|Train Avg Loss: 0.0128 \n",
      "Epoch:   74|steps:   60|Train Avg Loss: 0.0130 \n",
      "Epoch:   74|steps:   70|Train Avg Loss: 0.0120 \n",
      "Epoch:   74|steps:   80|Train Avg Loss: 0.0134 \n",
      "Epoch:   74|steps:   90|Train Avg Loss: 0.0120 \n",
      "Epoch:   74|steps:  100|Train Avg Loss: 0.0133 \n",
      "Epoch:   74|steps:  110|Train Avg Loss: 0.0128 \n",
      "Epoch:   74|steps:  120|Train Avg Loss: 0.0143 \n",
      "Epoch:   75|steps:   10|Train Avg Loss: 0.0120 \n",
      "Epoch:   75|steps:   20|Train Avg Loss: 0.0139 \n",
      "Epoch:   75|steps:   30|Train Avg Loss: 0.0128 \n",
      "Epoch:   75|steps:   40|Train Avg Loss: 0.0124 \n",
      "Epoch:   75|steps:   50|Train Avg Loss: 0.0120 \n",
      "Epoch:   75|steps:   60|Train Avg Loss: 0.0118 \n",
      "Epoch:   75|steps:   70|Train Avg Loss: 0.0126 \n",
      "Epoch:   75|steps:   80|Train Avg Loss: 0.0135 \n",
      "Epoch:   75|steps:   90|Train Avg Loss: 0.0138 \n",
      "Epoch:   75|steps:  100|Train Avg Loss: 0.0127 \n",
      "Epoch:   75|steps:  110|Train Avg Loss: 0.0127 \n",
      "Epoch:   75|steps:  120|Train Avg Loss: 0.0123 \n",
      "Epoch:   76|steps:   10|Train Avg Loss: 0.0124 \n",
      "Epoch:   76|steps:   20|Train Avg Loss: 0.0132 \n",
      "Epoch:   76|steps:   30|Train Avg Loss: 0.0124 \n",
      "Epoch:   76|steps:   40|Train Avg Loss: 0.0127 \n",
      "Epoch:   76|steps:   50|Train Avg Loss: 0.0130 \n",
      "Epoch:   76|steps:   60|Train Avg Loss: 0.0118 \n",
      "Epoch:   76|steps:   70|Train Avg Loss: 0.0137 \n",
      "Epoch:   76|steps:   80|Train Avg Loss: 0.0124 \n",
      "Epoch:   76|steps:   90|Train Avg Loss: 0.0128 \n",
      "Epoch:   76|steps:  100|Train Avg Loss: 0.0129 \n",
      "Epoch:   76|steps:  110|Train Avg Loss: 0.0121 \n",
      "Epoch:   76|steps:  120|Train Avg Loss: 0.0127 \n",
      "Epoch:   77|steps:   10|Train Avg Loss: 0.0128 \n",
      "Epoch:   77|steps:   20|Train Avg Loss: 0.0128 \n",
      "Epoch:   77|steps:   30|Train Avg Loss: 0.0130 \n",
      "Epoch:   77|steps:   40|Train Avg Loss: 0.0113 \n",
      "Epoch:   77|steps:   50|Train Avg Loss: 0.0113 \n",
      "Epoch:   77|steps:   60|Train Avg Loss: 0.0134 \n",
      "Epoch:   77|steps:   70|Train Avg Loss: 0.0132 \n",
      "Epoch:   77|steps:   80|Train Avg Loss: 0.0124 \n",
      "Epoch:   77|steps:   90|Train Avg Loss: 0.0127 \n",
      "Epoch:   77|steps:  100|Train Avg Loss: 0.0127 \n",
      "Epoch:   77|steps:  110|Train Avg Loss: 0.0128 \n",
      "Epoch:   77|steps:  120|Train Avg Loss: 0.0130 \n",
      "Epoch:   78|steps:   10|Train Avg Loss: 0.0125 \n",
      "Epoch:   78|steps:   20|Train Avg Loss: 0.0142 \n",
      "Epoch:   78|steps:   30|Train Avg Loss: 0.0131 \n",
      "Epoch:   78|steps:   40|Train Avg Loss: 0.0130 \n",
      "Epoch:   78|steps:   50|Train Avg Loss: 0.0106 \n",
      "Epoch:   78|steps:   60|Train Avg Loss: 0.0118 \n",
      "Epoch:   78|steps:   70|Train Avg Loss: 0.0128 \n",
      "Epoch:   78|steps:   80|Train Avg Loss: 0.0119 \n",
      "Epoch:   78|steps:   90|Train Avg Loss: 0.0122 \n",
      "Epoch:   78|steps:  100|Train Avg Loss: 0.0125 \n",
      "Epoch:   78|steps:  110|Train Avg Loss: 0.0135 \n",
      "Epoch:   78|steps:  120|Train Avg Loss: 0.0126 \n",
      "Epoch:   79|steps:   10|Train Avg Loss: 0.0118 \n",
      "Epoch:   79|steps:   20|Train Avg Loss: 0.0121 \n",
      "Epoch:   79|steps:   30|Train Avg Loss: 0.0117 \n",
      "Epoch:   79|steps:   40|Train Avg Loss: 0.0120 \n",
      "Epoch:   79|steps:   50|Train Avg Loss: 0.0128 \n",
      "Epoch:   79|steps:   60|Train Avg Loss: 0.0141 \n",
      "Epoch:   79|steps:   70|Train Avg Loss: 0.0124 \n",
      "Epoch:   79|steps:   80|Train Avg Loss: 0.0127 \n",
      "Epoch:   79|steps:   90|Train Avg Loss: 0.0125 \n",
      "Epoch:   79|steps:  100|Train Avg Loss: 0.0134 \n",
      "Epoch:   79|steps:  110|Train Avg Loss: 0.0125 \n",
      "Epoch:   79|steps:  120|Train Avg Loss: 0.0131 \n",
      "Epoch:   80|steps:   10|Train Avg Loss: 0.0126 \n",
      "Epoch:   80|steps:   20|Train Avg Loss: 0.0121 \n",
      "Epoch:   80|steps:   30|Train Avg Loss: 0.0124 \n",
      "Epoch:   80|steps:   40|Train Avg Loss: 0.0124 \n",
      "Epoch:   80|steps:   50|Train Avg Loss: 0.0109 \n",
      "Epoch:   80|steps:   60|Train Avg Loss: 0.0129 \n",
      "Epoch:   80|steps:   70|Train Avg Loss: 0.0122 \n",
      "Epoch:   80|steps:   80|Train Avg Loss: 0.0127 \n",
      "Epoch:   80|steps:   90|Train Avg Loss: 0.0120 \n",
      "Epoch:   80|steps:  100|Train Avg Loss: 0.0132 \n",
      "Epoch:   80|steps:  110|Train Avg Loss: 0.0125 \n",
      "Epoch:   80|steps:  120|Train Avg Loss: 0.0125 \n",
      "Epoch:   81|steps:   10|Train Avg Loss: 0.0129 \n",
      "Epoch:   81|steps:   20|Train Avg Loss: 0.0113 \n",
      "Epoch:   81|steps:   30|Train Avg Loss: 0.0133 \n",
      "Epoch:   81|steps:   40|Train Avg Loss: 0.0125 \n",
      "Epoch:   81|steps:   50|Train Avg Loss: 0.0124 \n",
      "Epoch:   81|steps:   60|Train Avg Loss: 0.0129 \n",
      "Epoch:   81|steps:   70|Train Avg Loss: 0.0125 \n",
      "Epoch:   81|steps:   80|Train Avg Loss: 0.0121 \n",
      "Epoch:   81|steps:   90|Train Avg Loss: 0.0121 \n",
      "Epoch:   81|steps:  100|Train Avg Loss: 0.0114 \n",
      "Epoch:   81|steps:  110|Train Avg Loss: 0.0124 \n",
      "Epoch:   81|steps:  120|Train Avg Loss: 0.0122 \n",
      "Epoch:   82|steps:   10|Train Avg Loss: 0.0115 \n",
      "Epoch:   82|steps:   20|Train Avg Loss: 0.0115 \n",
      "Epoch:   82|steps:   30|Train Avg Loss: 0.0121 \n",
      "Epoch:   82|steps:   40|Train Avg Loss: 0.0125 \n",
      "Epoch:   82|steps:   50|Train Avg Loss: 0.0130 \n",
      "Epoch:   82|steps:   60|Train Avg Loss: 0.0120 \n",
      "Epoch:   82|steps:   70|Train Avg Loss: 0.0128 \n",
      "Epoch:   82|steps:   80|Train Avg Loss: 0.0113 \n",
      "Epoch:   82|steps:   90|Train Avg Loss: 0.0130 \n",
      "Epoch:   82|steps:  100|Train Avg Loss: 0.0127 \n",
      "Epoch:   82|steps:  110|Train Avg Loss: 0.0117 \n",
      "Epoch:   82|steps:  120|Train Avg Loss: 0.0134 \n",
      "Epoch:   83|steps:   10|Train Avg Loss: 0.0115 \n",
      "Epoch:   83|steps:   20|Train Avg Loss: 0.0116 \n",
      "Epoch:   83|steps:   30|Train Avg Loss: 0.0105 \n",
      "Epoch:   83|steps:   40|Train Avg Loss: 0.0127 \n",
      "Epoch:   83|steps:   50|Train Avg Loss: 0.0124 \n",
      "Epoch:   83|steps:   60|Train Avg Loss: 0.0119 \n",
      "Epoch:   83|steps:   70|Train Avg Loss: 0.0129 \n",
      "Epoch:   83|steps:   80|Train Avg Loss: 0.0113 \n",
      "Epoch:   83|steps:   90|Train Avg Loss: 0.0122 \n",
      "Epoch:   83|steps:  100|Train Avg Loss: 0.0120 \n",
      "Epoch:   83|steps:  110|Train Avg Loss: 0.0134 \n",
      "Epoch:   83|steps:  120|Train Avg Loss: 0.0130 \n",
      "Epoch:   84|steps:   10|Train Avg Loss: 0.0115 \n",
      "Epoch:   84|steps:   20|Train Avg Loss: 0.0107 \n",
      "Epoch:   84|steps:   30|Train Avg Loss: 0.0116 \n",
      "Epoch:   84|steps:   40|Train Avg Loss: 0.0123 \n",
      "Epoch:   84|steps:   50|Train Avg Loss: 0.0129 \n",
      "Epoch:   84|steps:   60|Train Avg Loss: 0.0120 \n",
      "Epoch:   84|steps:   70|Train Avg Loss: 0.0123 \n",
      "Epoch:   84|steps:   80|Train Avg Loss: 0.0127 \n",
      "Epoch:   84|steps:   90|Train Avg Loss: 0.0130 \n",
      "Epoch:   84|steps:  100|Train Avg Loss: 0.0119 \n",
      "Epoch:   84|steps:  110|Train Avg Loss: 0.0117 \n",
      "Epoch:   84|steps:  120|Train Avg Loss: 0.0128 \n",
      "Epoch:   85|steps:   10|Train Avg Loss: 0.0115 \n",
      "Epoch:   85|steps:   20|Train Avg Loss: 0.0122 \n",
      "Epoch:   85|steps:   30|Train Avg Loss: 0.0128 \n",
      "Epoch:   85|steps:   40|Train Avg Loss: 0.0116 \n",
      "Epoch:   85|steps:   50|Train Avg Loss: 0.0125 \n",
      "Epoch:   85|steps:   60|Train Avg Loss: 0.0121 \n",
      "Epoch:   85|steps:   70|Train Avg Loss: 0.0123 \n",
      "Epoch:   85|steps:   80|Train Avg Loss: 0.0114 \n",
      "Epoch:   85|steps:   90|Train Avg Loss: 0.0119 \n",
      "Epoch:   85|steps:  100|Train Avg Loss: 0.0116 \n",
      "Epoch:   85|steps:  110|Train Avg Loss: 0.0121 \n",
      "Epoch:   85|steps:  120|Train Avg Loss: 0.0113 \n",
      "Epoch:   86|steps:   10|Train Avg Loss: 0.0115 \n",
      "Epoch:   86|steps:   20|Train Avg Loss: 0.0114 \n",
      "Epoch:   86|steps:   30|Train Avg Loss: 0.0117 \n",
      "Epoch:   86|steps:   40|Train Avg Loss: 0.0113 \n",
      "Epoch:   86|steps:   50|Train Avg Loss: 0.0111 \n",
      "Epoch:   86|steps:   60|Train Avg Loss: 0.0114 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   86|steps:   70|Train Avg Loss: 0.0120 \n",
      "Epoch:   86|steps:   80|Train Avg Loss: 0.0116 \n",
      "Epoch:   86|steps:   90|Train Avg Loss: 0.0105 \n",
      "Epoch:   86|steps:  100|Train Avg Loss: 0.0114 \n",
      "Epoch:   86|steps:  110|Train Avg Loss: 0.0120 \n",
      "Epoch:   86|steps:  120|Train Avg Loss: 0.0132 \n",
      "Epoch:   87|steps:   10|Train Avg Loss: 0.0118 \n",
      "Epoch:   87|steps:   20|Train Avg Loss: 0.0110 \n",
      "Epoch:   87|steps:   30|Train Avg Loss: 0.0110 \n",
      "Epoch:   87|steps:   40|Train Avg Loss: 0.0112 \n",
      "Epoch:   87|steps:   50|Train Avg Loss: 0.0112 \n",
      "Epoch:   87|steps:   60|Train Avg Loss: 0.0113 \n",
      "Epoch:   87|steps:   70|Train Avg Loss: 0.0116 \n",
      "Epoch:   87|steps:   80|Train Avg Loss: 0.0120 \n",
      "Epoch:   87|steps:   90|Train Avg Loss: 0.0107 \n",
      "Epoch:   87|steps:  100|Train Avg Loss: 0.0119 \n",
      "Epoch:   87|steps:  110|Train Avg Loss: 0.0110 \n",
      "Epoch:   87|steps:  120|Train Avg Loss: 0.0121 \n",
      "Epoch:   88|steps:   10|Train Avg Loss: 0.0114 \n",
      "Epoch:   88|steps:   20|Train Avg Loss: 0.0113 \n",
      "Epoch:   88|steps:   30|Train Avg Loss: 0.0112 \n",
      "Epoch:   88|steps:   40|Train Avg Loss: 0.0127 \n",
      "Epoch:   88|steps:   50|Train Avg Loss: 0.0112 \n",
      "Epoch:   88|steps:   60|Train Avg Loss: 0.0122 \n",
      "Epoch:   88|steps:   70|Train Avg Loss: 0.0120 \n",
      "Epoch:   88|steps:   80|Train Avg Loss: 0.0107 \n",
      "Epoch:   88|steps:   90|Train Avg Loss: 0.0110 \n",
      "Epoch:   88|steps:  100|Train Avg Loss: 0.0106 \n",
      "Epoch:   88|steps:  110|Train Avg Loss: 0.0110 \n",
      "Epoch:   88|steps:  120|Train Avg Loss: 0.0116 \n",
      "Epoch:   89|steps:   10|Train Avg Loss: 0.0115 \n",
      "Epoch:   89|steps:   20|Train Avg Loss: 0.0110 \n",
      "Epoch:   89|steps:   30|Train Avg Loss: 0.0111 \n",
      "Epoch:   89|steps:   40|Train Avg Loss: 0.0117 \n",
      "Epoch:   89|steps:   50|Train Avg Loss: 0.0106 \n",
      "Epoch:   89|steps:   60|Train Avg Loss: 0.0109 \n",
      "Epoch:   89|steps:   70|Train Avg Loss: 0.0109 \n",
      "Epoch:   89|steps:   80|Train Avg Loss: 0.0121 \n",
      "Epoch:   89|steps:   90|Train Avg Loss: 0.0109 \n",
      "Epoch:   89|steps:  100|Train Avg Loss: 0.0115 \n",
      "Epoch:   89|steps:  110|Train Avg Loss: 0.0107 \n",
      "Epoch:   89|steps:  120|Train Avg Loss: 0.0111 \n",
      "Epoch:   90|steps:   10|Train Avg Loss: 0.0102 \n",
      "Epoch:   90|steps:   20|Train Avg Loss: 0.0115 \n",
      "Epoch:   90|steps:   30|Train Avg Loss: 0.0108 \n",
      "Epoch:   90|steps:   40|Train Avg Loss: 0.0118 \n",
      "Epoch:   90|steps:   50|Train Avg Loss: 0.0109 \n",
      "Epoch:   90|steps:   60|Train Avg Loss: 0.0103 \n",
      "Epoch:   90|steps:   70|Train Avg Loss: 0.0101 \n",
      "Epoch:   90|steps:   80|Train Avg Loss: 0.0114 \n",
      "Epoch:   90|steps:   90|Train Avg Loss: 0.0122 \n",
      "Epoch:   90|steps:  100|Train Avg Loss: 0.0111 \n",
      "Epoch:   90|steps:  110|Train Avg Loss: 0.0118 \n",
      "Epoch:   90|steps:  120|Train Avg Loss: 0.0107 \n",
      "Epoch:   91|steps:   10|Train Avg Loss: 0.0102 \n",
      "Epoch:   91|steps:   20|Train Avg Loss: 0.0109 \n",
      "Epoch:   91|steps:   30|Train Avg Loss: 0.0106 \n",
      "Epoch:   91|steps:   40|Train Avg Loss: 0.0104 \n",
      "Epoch:   91|steps:   50|Train Avg Loss: 0.0112 \n",
      "Epoch:   91|steps:   60|Train Avg Loss: 0.0118 \n",
      "Epoch:   91|steps:   70|Train Avg Loss: 0.0109 \n",
      "Epoch:   91|steps:   80|Train Avg Loss: 0.0109 \n",
      "Epoch:   91|steps:   90|Train Avg Loss: 0.0102 \n",
      "Epoch:   91|steps:  100|Train Avg Loss: 0.0113 \n",
      "Epoch:   91|steps:  110|Train Avg Loss: 0.0110 \n",
      "Epoch:   91|steps:  120|Train Avg Loss: 0.0109 \n",
      "Epoch:   92|steps:   10|Train Avg Loss: 0.0110 \n",
      "Epoch:   92|steps:   20|Train Avg Loss: 0.0099 \n",
      "Epoch:   92|steps:   30|Train Avg Loss: 0.0110 \n",
      "Epoch:   92|steps:   40|Train Avg Loss: 0.0098 \n",
      "Epoch:   92|steps:   50|Train Avg Loss: 0.0121 \n",
      "Epoch:   92|steps:   60|Train Avg Loss: 0.0097 \n",
      "Epoch:   92|steps:   70|Train Avg Loss: 0.0102 \n",
      "Epoch:   92|steps:   80|Train Avg Loss: 0.0110 \n",
      "Epoch:   92|steps:   90|Train Avg Loss: 0.0112 \n",
      "Epoch:   92|steps:  100|Train Avg Loss: 0.0111 \n",
      "Epoch:   92|steps:  110|Train Avg Loss: 0.0111 \n",
      "Epoch:   92|steps:  120|Train Avg Loss: 0.0105 \n",
      "Epoch:   93|steps:   10|Train Avg Loss: 0.0103 \n",
      "Epoch:   93|steps:   20|Train Avg Loss: 0.0109 \n",
      "Epoch:   93|steps:   30|Train Avg Loss: 0.0114 \n",
      "Epoch:   93|steps:   40|Train Avg Loss: 0.0110 \n",
      "Epoch:   93|steps:   50|Train Avg Loss: 0.0100 \n",
      "Epoch:   93|steps:   60|Train Avg Loss: 0.0104 \n",
      "Epoch:   93|steps:   70|Train Avg Loss: 0.0103 \n",
      "Epoch:   93|steps:   80|Train Avg Loss: 0.0100 \n",
      "Epoch:   93|steps:   90|Train Avg Loss: 0.0114 \n",
      "Epoch:   93|steps:  100|Train Avg Loss: 0.0116 \n",
      "Epoch:   93|steps:  110|Train Avg Loss: 0.0106 \n",
      "Epoch:   93|steps:  120|Train Avg Loss: 0.0109 \n",
      "Epoch:   94|steps:   10|Train Avg Loss: 0.0097 \n",
      "Epoch:   94|steps:   20|Train Avg Loss: 0.0118 \n",
      "Epoch:   94|steps:   30|Train Avg Loss: 0.0107 \n",
      "Epoch:   94|steps:   40|Train Avg Loss: 0.0105 \n",
      "Epoch:   94|steps:   50|Train Avg Loss: 0.0108 \n",
      "Epoch:   94|steps:   60|Train Avg Loss: 0.0105 \n",
      "Epoch:   94|steps:   70|Train Avg Loss: 0.0109 \n",
      "Epoch:   94|steps:   80|Train Avg Loss: 0.0107 \n",
      "Epoch:   94|steps:   90|Train Avg Loss: 0.0115 \n",
      "Epoch:   94|steps:  100|Train Avg Loss: 0.0107 \n",
      "Epoch:   94|steps:  110|Train Avg Loss: 0.0107 \n",
      "Epoch:   94|steps:  120|Train Avg Loss: 0.0108 \n",
      "Epoch:   95|steps:   10|Train Avg Loss: 0.0097 \n",
      "Epoch:   95|steps:   20|Train Avg Loss: 0.0106 \n",
      "Epoch:   95|steps:   30|Train Avg Loss: 0.0106 \n",
      "Epoch:   95|steps:   40|Train Avg Loss: 0.0111 \n",
      "Epoch:   95|steps:   50|Train Avg Loss: 0.0113 \n",
      "Epoch:   95|steps:   60|Train Avg Loss: 0.0112 \n",
      "Epoch:   95|steps:   70|Train Avg Loss: 0.0103 \n",
      "Epoch:   95|steps:   80|Train Avg Loss: 0.0109 \n",
      "Epoch:   95|steps:   90|Train Avg Loss: 0.0100 \n",
      "Epoch:   95|steps:  100|Train Avg Loss: 0.0099 \n",
      "Epoch:   95|steps:  110|Train Avg Loss: 0.0101 \n",
      "Epoch:   95|steps:  120|Train Avg Loss: 0.0112 \n",
      "Epoch:   96|steps:   10|Train Avg Loss: 0.0098 \n",
      "Epoch:   96|steps:   20|Train Avg Loss: 0.0100 \n",
      "Epoch:   96|steps:   30|Train Avg Loss: 0.0117 \n",
      "Epoch:   96|steps:   40|Train Avg Loss: 0.0098 \n",
      "Epoch:   96|steps:   50|Train Avg Loss: 0.0100 \n",
      "Epoch:   96|steps:   60|Train Avg Loss: 0.0099 \n",
      "Epoch:   96|steps:   70|Train Avg Loss: 0.0109 \n",
      "Epoch:   96|steps:   80|Train Avg Loss: 0.0104 \n",
      "Epoch:   96|steps:   90|Train Avg Loss: 0.0104 \n",
      "Epoch:   96|steps:  100|Train Avg Loss: 0.0114 \n",
      "Epoch:   96|steps:  110|Train Avg Loss: 0.0106 \n",
      "Epoch:   96|steps:  120|Train Avg Loss: 0.0108 \n",
      "Epoch:   97|steps:   10|Train Avg Loss: 0.0094 \n",
      "Epoch:   97|steps:   20|Train Avg Loss: 0.0114 \n",
      "Epoch:   97|steps:   30|Train Avg Loss: 0.0104 \n",
      "Epoch:   97|steps:   40|Train Avg Loss: 0.0103 \n",
      "Epoch:   97|steps:   50|Train Avg Loss: 0.0113 \n",
      "Epoch:   97|steps:   60|Train Avg Loss: 0.0109 \n",
      "Epoch:   97|steps:   70|Train Avg Loss: 0.0100 \n",
      "Epoch:   97|steps:   80|Train Avg Loss: 0.0108 \n",
      "Epoch:   97|steps:   90|Train Avg Loss: 0.0111 \n",
      "Epoch:   97|steps:  100|Train Avg Loss: 0.0109 \n",
      "Epoch:   97|steps:  110|Train Avg Loss: 0.0094 \n",
      "Epoch:   97|steps:  120|Train Avg Loss: 0.0104 \n",
      "Epoch:   98|steps:   10|Train Avg Loss: 0.0103 \n",
      "Epoch:   98|steps:   20|Train Avg Loss: 0.0100 \n",
      "Epoch:   98|steps:   30|Train Avg Loss: 0.0112 \n",
      "Epoch:   98|steps:   40|Train Avg Loss: 0.0105 \n",
      "Epoch:   98|steps:   50|Train Avg Loss: 0.0104 \n",
      "Epoch:   98|steps:   60|Train Avg Loss: 0.0103 \n",
      "Epoch:   98|steps:   70|Train Avg Loss: 0.0106 \n",
      "Epoch:   98|steps:   80|Train Avg Loss: 0.0112 \n",
      "Epoch:   98|steps:   90|Train Avg Loss: 0.0094 \n",
      "Epoch:   98|steps:  100|Train Avg Loss: 0.0098 \n",
      "Epoch:   98|steps:  110|Train Avg Loss: 0.0097 \n",
      "Epoch:   98|steps:  120|Train Avg Loss: 0.0112 \n",
      "Epoch:   99|steps:   10|Train Avg Loss: 0.0104 \n",
      "Epoch:   99|steps:   20|Train Avg Loss: 0.0108 \n",
      "Epoch:   99|steps:   30|Train Avg Loss: 0.0100 \n",
      "Epoch:   99|steps:   40|Train Avg Loss: 0.0103 \n",
      "Epoch:   99|steps:   50|Train Avg Loss: 0.0104 \n",
      "Epoch:   99|steps:   60|Train Avg Loss: 0.0113 \n",
      "Epoch:   99|steps:   70|Train Avg Loss: 0.0105 \n",
      "Epoch:   99|steps:   80|Train Avg Loss: 0.0100 \n",
      "Epoch:   99|steps:   90|Train Avg Loss: 0.0099 \n",
      "Epoch:   99|steps:  100|Train Avg Loss: 0.0104 \n",
      "Epoch:   99|steps:  110|Train Avg Loss: 0.0093 \n",
      "Epoch:   99|steps:  120|Train Avg Loss: 0.0101 \n",
      "Epoch:  100|steps:   10|Train Avg Loss: 0.0100 \n",
      "Epoch:  100|steps:   20|Train Avg Loss: 0.0103 \n",
      "Epoch:  100|steps:   30|Train Avg Loss: 0.0093 \n",
      "Epoch:  100|steps:   40|Train Avg Loss: 0.0105 \n",
      "Epoch:  100|steps:   50|Train Avg Loss: 0.0105 \n",
      "Epoch:  100|steps:   60|Train Avg Loss: 0.0101 \n",
      "Epoch:  100|steps:   70|Train Avg Loss: 0.0100 \n",
      "Epoch:  100|steps:   80|Train Avg Loss: 0.0097 \n",
      "Epoch:  100|steps:   90|Train Avg Loss: 0.0105 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  100|steps:  100|Train Avg Loss: 0.0113 \n",
      "Epoch:  100|steps:  110|Train Avg Loss: 0.0109 \n",
      "Epoch:  100|steps:  120|Train Avg Loss: 0.0106 \n",
      "Epoch:  101|steps:   10|Train Avg Loss: 0.0112 \n",
      "Epoch:  101|steps:   20|Train Avg Loss: 0.0105 \n",
      "Epoch:  101|steps:   30|Train Avg Loss: 0.0103 \n",
      "Epoch:  101|steps:   40|Train Avg Loss: 0.0105 \n",
      "Epoch:  101|steps:   50|Train Avg Loss: 0.0096 \n",
      "Epoch:  101|steps:   60|Train Avg Loss: 0.0109 \n",
      "Epoch:  101|steps:   70|Train Avg Loss: 0.0094 \n",
      "Epoch:  101|steps:   80|Train Avg Loss: 0.0108 \n",
      "Epoch:  101|steps:   90|Train Avg Loss: 0.0101 \n",
      "Epoch:  101|steps:  100|Train Avg Loss: 0.0099 \n",
      "Epoch:  101|steps:  110|Train Avg Loss: 0.0100 \n",
      "Epoch:  101|steps:  120|Train Avg Loss: 0.0112 \n",
      "Epoch:  102|steps:   10|Train Avg Loss: 0.0103 \n",
      "Epoch:  102|steps:   20|Train Avg Loss: 0.0098 \n",
      "Epoch:  102|steps:   30|Train Avg Loss: 0.0101 \n",
      "Epoch:  102|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  102|steps:   50|Train Avg Loss: 0.0094 \n",
      "Epoch:  102|steps:   60|Train Avg Loss: 0.0100 \n",
      "Epoch:  102|steps:   70|Train Avg Loss: 0.0106 \n",
      "Epoch:  102|steps:   80|Train Avg Loss: 0.0107 \n",
      "Epoch:  102|steps:   90|Train Avg Loss: 0.0109 \n",
      "Epoch:  102|steps:  100|Train Avg Loss: 0.0100 \n",
      "Epoch:  102|steps:  110|Train Avg Loss: 0.0101 \n",
      "Epoch:  102|steps:  120|Train Avg Loss: 0.0098 \n",
      "Epoch:  103|steps:   10|Train Avg Loss: 0.0099 \n",
      "Epoch:  103|steps:   20|Train Avg Loss: 0.0100 \n",
      "Epoch:  103|steps:   30|Train Avg Loss: 0.0102 \n",
      "Epoch:  103|steps:   40|Train Avg Loss: 0.0106 \n",
      "Epoch:  103|steps:   50|Train Avg Loss: 0.0101 \n",
      "Epoch:  103|steps:   60|Train Avg Loss: 0.0105 \n",
      "Epoch:  103|steps:   70|Train Avg Loss: 0.0102 \n",
      "Epoch:  103|steps:   80|Train Avg Loss: 0.0100 \n",
      "Epoch:  103|steps:   90|Train Avg Loss: 0.0100 \n",
      "Epoch:  103|steps:  100|Train Avg Loss: 0.0106 \n",
      "Epoch:  103|steps:  110|Train Avg Loss: 0.0095 \n",
      "Epoch:  103|steps:  120|Train Avg Loss: 0.0098 \n",
      "Epoch:  104|steps:   10|Train Avg Loss: 0.0103 \n",
      "Epoch:  104|steps:   20|Train Avg Loss: 0.0108 \n",
      "Epoch:  104|steps:   30|Train Avg Loss: 0.0103 \n",
      "Epoch:  104|steps:   40|Train Avg Loss: 0.0102 \n",
      "Epoch:  104|steps:   50|Train Avg Loss: 0.0104 \n",
      "Epoch:  104|steps:   60|Train Avg Loss: 0.0091 \n",
      "Epoch:  104|steps:   70|Train Avg Loss: 0.0097 \n",
      "Epoch:  104|steps:   80|Train Avg Loss: 0.0102 \n",
      "Epoch:  104|steps:   90|Train Avg Loss: 0.0097 \n",
      "Epoch:  104|steps:  100|Train Avg Loss: 0.0098 \n",
      "Epoch:  104|steps:  110|Train Avg Loss: 0.0104 \n",
      "Epoch:  104|steps:  120|Train Avg Loss: 0.0101 \n",
      "Epoch:  105|steps:   10|Train Avg Loss: 0.0106 \n",
      "Epoch:  105|steps:   20|Train Avg Loss: 0.0108 \n",
      "Epoch:  105|steps:   30|Train Avg Loss: 0.0092 \n",
      "Epoch:  105|steps:   40|Train Avg Loss: 0.0102 \n",
      "Epoch:  105|steps:   50|Train Avg Loss: 0.0095 \n",
      "Epoch:  105|steps:   60|Train Avg Loss: 0.0108 \n",
      "Epoch:  105|steps:   70|Train Avg Loss: 0.0105 \n",
      "Epoch:  105|steps:   80|Train Avg Loss: 0.0090 \n",
      "Epoch:  105|steps:   90|Train Avg Loss: 0.0100 \n",
      "Epoch:  105|steps:  100|Train Avg Loss: 0.0094 \n",
      "Epoch:  105|steps:  110|Train Avg Loss: 0.0095 \n",
      "Epoch:  105|steps:  120|Train Avg Loss: 0.0099 \n",
      "Epoch:  106|steps:   10|Train Avg Loss: 0.0110 \n",
      "Epoch:  106|steps:   20|Train Avg Loss: 0.0096 \n",
      "Epoch:  106|steps:   30|Train Avg Loss: 0.0099 \n",
      "Epoch:  106|steps:   40|Train Avg Loss: 0.0106 \n",
      "Epoch:  106|steps:   50|Train Avg Loss: 0.0102 \n",
      "Epoch:  106|steps:   60|Train Avg Loss: 0.0094 \n",
      "Epoch:  106|steps:   70|Train Avg Loss: 0.0096 \n",
      "Epoch:  106|steps:   80|Train Avg Loss: 0.0100 \n",
      "Epoch:  106|steps:   90|Train Avg Loss: 0.0098 \n",
      "Epoch:  106|steps:  100|Train Avg Loss: 0.0096 \n",
      "Epoch:  106|steps:  110|Train Avg Loss: 0.0101 \n",
      "Epoch:  106|steps:  120|Train Avg Loss: 0.0101 \n",
      "Epoch:  107|steps:   10|Train Avg Loss: 0.0097 \n",
      "Epoch:  107|steps:   20|Train Avg Loss: 0.0101 \n",
      "Epoch:  107|steps:   30|Train Avg Loss: 0.0093 \n",
      "Epoch:  107|steps:   40|Train Avg Loss: 0.0101 \n",
      "Epoch:  107|steps:   50|Train Avg Loss: 0.0099 \n",
      "Epoch:  107|steps:   60|Train Avg Loss: 0.0104 \n",
      "Epoch:  107|steps:   70|Train Avg Loss: 0.0092 \n",
      "Epoch:  107|steps:   80|Train Avg Loss: 0.0092 \n",
      "Epoch:  107|steps:   90|Train Avg Loss: 0.0103 \n",
      "Epoch:  107|steps:  100|Train Avg Loss: 0.0100 \n",
      "Epoch:  107|steps:  110|Train Avg Loss: 0.0098 \n",
      "Epoch:  107|steps:  120|Train Avg Loss: 0.0105 \n",
      "Epoch:  108|steps:   10|Train Avg Loss: 0.0098 \n",
      "Epoch:  108|steps:   20|Train Avg Loss: 0.0104 \n",
      "Epoch:  108|steps:   30|Train Avg Loss: 0.0094 \n",
      "Epoch:  108|steps:   40|Train Avg Loss: 0.0094 \n",
      "Epoch:  108|steps:   50|Train Avg Loss: 0.0100 \n",
      "Epoch:  108|steps:   60|Train Avg Loss: 0.0098 \n",
      "Epoch:  108|steps:   70|Train Avg Loss: 0.0099 \n",
      "Epoch:  108|steps:   80|Train Avg Loss: 0.0100 \n",
      "Epoch:  108|steps:   90|Train Avg Loss: 0.0107 \n",
      "Epoch:  108|steps:  100|Train Avg Loss: 0.0092 \n",
      "Epoch:  108|steps:  110|Train Avg Loss: 0.0097 \n",
      "Epoch:  108|steps:  120|Train Avg Loss: 0.0092 \n",
      "Epoch:  109|steps:   10|Train Avg Loss: 0.0097 \n",
      "Epoch:  109|steps:   20|Train Avg Loss: 0.0102 \n",
      "Epoch:  109|steps:   30|Train Avg Loss: 0.0097 \n",
      "Epoch:  109|steps:   40|Train Avg Loss: 0.0095 \n",
      "Epoch:  109|steps:   50|Train Avg Loss: 0.0099 \n",
      "Epoch:  109|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  109|steps:   70|Train Avg Loss: 0.0098 \n",
      "Epoch:  109|steps:   80|Train Avg Loss: 0.0106 \n",
      "Epoch:  109|steps:   90|Train Avg Loss: 0.0103 \n",
      "Epoch:  109|steps:  100|Train Avg Loss: 0.0096 \n",
      "Epoch:  109|steps:  110|Train Avg Loss: 0.0100 \n",
      "Epoch:  109|steps:  120|Train Avg Loss: 0.0102 \n",
      "Epoch:  110|steps:   10|Train Avg Loss: 0.0096 \n",
      "Epoch:  110|steps:   20|Train Avg Loss: 0.0096 \n",
      "Epoch:  110|steps:   30|Train Avg Loss: 0.0100 \n",
      "Epoch:  110|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  110|steps:   50|Train Avg Loss: 0.0094 \n",
      "Epoch:  110|steps:   60|Train Avg Loss: 0.0099 \n",
      "Epoch:  110|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  110|steps:   80|Train Avg Loss: 0.0092 \n",
      "Epoch:  110|steps:   90|Train Avg Loss: 0.0104 \n",
      "Epoch:  110|steps:  100|Train Avg Loss: 0.0110 \n",
      "Epoch:  110|steps:  110|Train Avg Loss: 0.0104 \n",
      "Epoch:  110|steps:  120|Train Avg Loss: 0.0094 \n",
      "Epoch:  111|steps:   10|Train Avg Loss: 0.0101 \n",
      "Epoch:  111|steps:   20|Train Avg Loss: 0.0098 \n",
      "Epoch:  111|steps:   30|Train Avg Loss: 0.0094 \n",
      "Epoch:  111|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  111|steps:   50|Train Avg Loss: 0.0101 \n",
      "Epoch:  111|steps:   60|Train Avg Loss: 0.0095 \n",
      "Epoch:  111|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  111|steps:   80|Train Avg Loss: 0.0089 \n",
      "Epoch:  111|steps:   90|Train Avg Loss: 0.0104 \n",
      "Epoch:  111|steps:  100|Train Avg Loss: 0.0096 \n",
      "Epoch:  111|steps:  110|Train Avg Loss: 0.0096 \n",
      "Epoch:  111|steps:  120|Train Avg Loss: 0.0093 \n",
      "Epoch:  112|steps:   10|Train Avg Loss: 0.0088 \n",
      "Epoch:  112|steps:   20|Train Avg Loss: 0.0094 \n",
      "Epoch:  112|steps:   30|Train Avg Loss: 0.0100 \n",
      "Epoch:  112|steps:   40|Train Avg Loss: 0.0093 \n",
      "Epoch:  112|steps:   50|Train Avg Loss: 0.0095 \n",
      "Epoch:  112|steps:   60|Train Avg Loss: 0.0094 \n",
      "Epoch:  112|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  112|steps:   80|Train Avg Loss: 0.0098 \n",
      "Epoch:  112|steps:   90|Train Avg Loss: 0.0099 \n",
      "Epoch:  112|steps:  100|Train Avg Loss: 0.0098 \n",
      "Epoch:  112|steps:  110|Train Avg Loss: 0.0099 \n",
      "Epoch:  112|steps:  120|Train Avg Loss: 0.0098 \n",
      "Epoch:  113|steps:   10|Train Avg Loss: 0.0106 \n",
      "Epoch:  113|steps:   20|Train Avg Loss: 0.0097 \n",
      "Epoch:  113|steps:   30|Train Avg Loss: 0.0093 \n",
      "Epoch:  113|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  113|steps:   50|Train Avg Loss: 0.0092 \n",
      "Epoch:  113|steps:   60|Train Avg Loss: 0.0098 \n",
      "Epoch:  113|steps:   70|Train Avg Loss: 0.0093 \n",
      "Epoch:  113|steps:   80|Train Avg Loss: 0.0097 \n",
      "Epoch:  113|steps:   90|Train Avg Loss: 0.0098 \n",
      "Epoch:  113|steps:  100|Train Avg Loss: 0.0095 \n",
      "Epoch:  113|steps:  110|Train Avg Loss: 0.0094 \n",
      "Epoch:  113|steps:  120|Train Avg Loss: 0.0103 \n",
      "Epoch:  114|steps:   10|Train Avg Loss: 0.0091 \n",
      "Epoch:  114|steps:   20|Train Avg Loss: 0.0099 \n",
      "Epoch:  114|steps:   30|Train Avg Loss: 0.0096 \n",
      "Epoch:  114|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  114|steps:   50|Train Avg Loss: 0.0091 \n",
      "Epoch:  114|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  114|steps:   70|Train Avg Loss: 0.0098 \n",
      "Epoch:  114|steps:   80|Train Avg Loss: 0.0106 \n",
      "Epoch:  114|steps:   90|Train Avg Loss: 0.0091 \n",
      "Epoch:  114|steps:  100|Train Avg Loss: 0.0093 \n",
      "Epoch:  114|steps:  110|Train Avg Loss: 0.0105 \n",
      "Epoch:  114|steps:  120|Train Avg Loss: 0.0100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  115|steps:   10|Train Avg Loss: 0.0093 \n",
      "Epoch:  115|steps:   20|Train Avg Loss: 0.0094 \n",
      "Epoch:  115|steps:   30|Train Avg Loss: 0.0104 \n",
      "Epoch:  115|steps:   40|Train Avg Loss: 0.0093 \n",
      "Epoch:  115|steps:   50|Train Avg Loss: 0.0091 \n",
      "Epoch:  115|steps:   60|Train Avg Loss: 0.0094 \n",
      "Epoch:  115|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  115|steps:   80|Train Avg Loss: 0.0092 \n",
      "Epoch:  115|steps:   90|Train Avg Loss: 0.0096 \n",
      "Epoch:  115|steps:  100|Train Avg Loss: 0.0099 \n",
      "Epoch:  115|steps:  110|Train Avg Loss: 0.0098 \n",
      "Epoch:  115|steps:  120|Train Avg Loss: 0.0099 \n",
      "Epoch:  116|steps:   10|Train Avg Loss: 0.0094 \n",
      "Epoch:  116|steps:   20|Train Avg Loss: 0.0101 \n",
      "Epoch:  116|steps:   30|Train Avg Loss: 0.0107 \n",
      "Epoch:  116|steps:   40|Train Avg Loss: 0.0089 \n",
      "Epoch:  116|steps:   50|Train Avg Loss: 0.0089 \n",
      "Epoch:  116|steps:   60|Train Avg Loss: 0.0098 \n",
      "Epoch:  116|steps:   70|Train Avg Loss: 0.0093 \n",
      "Epoch:  116|steps:   80|Train Avg Loss: 0.0092 \n",
      "Epoch:  116|steps:   90|Train Avg Loss: 0.0091 \n",
      "Epoch:  116|steps:  100|Train Avg Loss: 0.0095 \n",
      "Epoch:  116|steps:  110|Train Avg Loss: 0.0099 \n",
      "Epoch:  116|steps:  120|Train Avg Loss: 0.0095 \n",
      "Epoch:  117|steps:   10|Train Avg Loss: 0.0091 \n",
      "Epoch:  117|steps:   20|Train Avg Loss: 0.0096 \n",
      "Epoch:  117|steps:   30|Train Avg Loss: 0.0099 \n",
      "Epoch:  117|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  117|steps:   50|Train Avg Loss: 0.0097 \n",
      "Epoch:  117|steps:   60|Train Avg Loss: 0.0098 \n",
      "Epoch:  117|steps:   70|Train Avg Loss: 0.0094 \n",
      "Epoch:  117|steps:   80|Train Avg Loss: 0.0096 \n",
      "Epoch:  117|steps:   90|Train Avg Loss: 0.0090 \n",
      "Epoch:  117|steps:  100|Train Avg Loss: 0.0090 \n",
      "Epoch:  117|steps:  110|Train Avg Loss: 0.0097 \n",
      "Epoch:  117|steps:  120|Train Avg Loss: 0.0099 \n",
      "Epoch:  118|steps:   10|Train Avg Loss: 0.0100 \n",
      "Epoch:  118|steps:   20|Train Avg Loss: 0.0090 \n",
      "Epoch:  118|steps:   30|Train Avg Loss: 0.0091 \n",
      "Epoch:  118|steps:   40|Train Avg Loss: 0.0095 \n",
      "Epoch:  118|steps:   50|Train Avg Loss: 0.0104 \n",
      "Epoch:  118|steps:   60|Train Avg Loss: 0.0084 \n",
      "Epoch:  118|steps:   70|Train Avg Loss: 0.0094 \n",
      "Epoch:  118|steps:   80|Train Avg Loss: 0.0093 \n",
      "Epoch:  118|steps:   90|Train Avg Loss: 0.0095 \n",
      "Epoch:  118|steps:  100|Train Avg Loss: 0.0090 \n",
      "Epoch:  118|steps:  110|Train Avg Loss: 0.0091 \n",
      "Epoch:  118|steps:  120|Train Avg Loss: 0.0097 \n",
      "Epoch:  119|steps:   10|Train Avg Loss: 0.0094 \n",
      "Epoch:  119|steps:   20|Train Avg Loss: 0.0092 \n",
      "Epoch:  119|steps:   30|Train Avg Loss: 0.0088 \n",
      "Epoch:  119|steps:   40|Train Avg Loss: 0.0090 \n",
      "Epoch:  119|steps:   50|Train Avg Loss: 0.0098 \n",
      "Epoch:  119|steps:   60|Train Avg Loss: 0.0095 \n",
      "Epoch:  119|steps:   70|Train Avg Loss: 0.0092 \n",
      "Epoch:  119|steps:   80|Train Avg Loss: 0.0090 \n",
      "Epoch:  119|steps:   90|Train Avg Loss: 0.0100 \n",
      "Epoch:  119|steps:  100|Train Avg Loss: 0.0095 \n",
      "Epoch:  119|steps:  110|Train Avg Loss: 0.0094 \n",
      "Epoch:  119|steps:  120|Train Avg Loss: 0.0097 \n",
      "Epoch:  120|steps:   10|Train Avg Loss: 0.0089 \n",
      "Epoch:  120|steps:   20|Train Avg Loss: 0.0087 \n",
      "Epoch:  120|steps:   30|Train Avg Loss: 0.0096 \n",
      "Epoch:  120|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  120|steps:   50|Train Avg Loss: 0.0097 \n",
      "Epoch:  120|steps:   60|Train Avg Loss: 0.0092 \n",
      "Epoch:  120|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  120|steps:   80|Train Avg Loss: 0.0093 \n",
      "Epoch:  120|steps:   90|Train Avg Loss: 0.0095 \n",
      "Epoch:  120|steps:  100|Train Avg Loss: 0.0097 \n",
      "Epoch:  120|steps:  110|Train Avg Loss: 0.0101 \n",
      "Epoch:  120|steps:  120|Train Avg Loss: 0.0092 \n",
      "Epoch:  121|steps:   10|Train Avg Loss: 0.0095 \n",
      "Epoch:  121|steps:   20|Train Avg Loss: 0.0096 \n",
      "Epoch:  121|steps:   30|Train Avg Loss: 0.0088 \n",
      "Epoch:  121|steps:   40|Train Avg Loss: 0.0090 \n",
      "Epoch:  121|steps:   50|Train Avg Loss: 0.0106 \n",
      "Epoch:  121|steps:   60|Train Avg Loss: 0.0097 \n",
      "Epoch:  121|steps:   70|Train Avg Loss: 0.0090 \n",
      "Epoch:  121|steps:   80|Train Avg Loss: 0.0090 \n",
      "Epoch:  121|steps:   90|Train Avg Loss: 0.0097 \n",
      "Epoch:  121|steps:  100|Train Avg Loss: 0.0087 \n",
      "Epoch:  121|steps:  110|Train Avg Loss: 0.0088 \n",
      "Epoch:  121|steps:  120|Train Avg Loss: 0.0097 \n",
      "Epoch:  122|steps:   10|Train Avg Loss: 0.0088 \n",
      "Epoch:  122|steps:   20|Train Avg Loss: 0.0096 \n",
      "Epoch:  122|steps:   30|Train Avg Loss: 0.0088 \n",
      "Epoch:  122|steps:   40|Train Avg Loss: 0.0096 \n",
      "Epoch:  122|steps:   50|Train Avg Loss: 0.0085 \n",
      "Epoch:  122|steps:   60|Train Avg Loss: 0.0094 \n",
      "Epoch:  122|steps:   70|Train Avg Loss: 0.0099 \n",
      "Epoch:  122|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  122|steps:   90|Train Avg Loss: 0.0093 \n",
      "Epoch:  122|steps:  100|Train Avg Loss: 0.0087 \n",
      "Epoch:  122|steps:  110|Train Avg Loss: 0.0099 \n",
      "Epoch:  122|steps:  120|Train Avg Loss: 0.0097 \n",
      "Epoch:  123|steps:   10|Train Avg Loss: 0.0086 \n",
      "Epoch:  123|steps:   20|Train Avg Loss: 0.0094 \n",
      "Epoch:  123|steps:   30|Train Avg Loss: 0.0092 \n",
      "Epoch:  123|steps:   40|Train Avg Loss: 0.0095 \n",
      "Epoch:  123|steps:   50|Train Avg Loss: 0.0087 \n",
      "Epoch:  123|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  123|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  123|steps:   80|Train Avg Loss: 0.0094 \n",
      "Epoch:  123|steps:   90|Train Avg Loss: 0.0090 \n",
      "Epoch:  123|steps:  100|Train Avg Loss: 0.0095 \n",
      "Epoch:  123|steps:  110|Train Avg Loss: 0.0096 \n",
      "Epoch:  123|steps:  120|Train Avg Loss: 0.0094 \n",
      "Epoch:  124|steps:   10|Train Avg Loss: 0.0093 \n",
      "Epoch:  124|steps:   20|Train Avg Loss: 0.0090 \n",
      "Epoch:  124|steps:   30|Train Avg Loss: 0.0087 \n",
      "Epoch:  124|steps:   40|Train Avg Loss: 0.0093 \n",
      "Epoch:  124|steps:   50|Train Avg Loss: 0.0097 \n",
      "Epoch:  124|steps:   60|Train Avg Loss: 0.0095 \n",
      "Epoch:  124|steps:   70|Train Avg Loss: 0.0089 \n",
      "Epoch:  124|steps:   80|Train Avg Loss: 0.0092 \n",
      "Epoch:  124|steps:   90|Train Avg Loss: 0.0096 \n",
      "Epoch:  124|steps:  100|Train Avg Loss: 0.0096 \n",
      "Epoch:  124|steps:  110|Train Avg Loss: 0.0086 \n",
      "Epoch:  124|steps:  120|Train Avg Loss: 0.0091 \n",
      "Epoch:  125|steps:   10|Train Avg Loss: 0.0094 \n",
      "Epoch:  125|steps:   20|Train Avg Loss: 0.0087 \n",
      "Epoch:  125|steps:   30|Train Avg Loss: 0.0094 \n",
      "Epoch:  125|steps:   40|Train Avg Loss: 0.0098 \n",
      "Epoch:  125|steps:   50|Train Avg Loss: 0.0090 \n",
      "Epoch:  125|steps:   60|Train Avg Loss: 0.0085 \n",
      "Epoch:  125|steps:   70|Train Avg Loss: 0.0088 \n",
      "Epoch:  125|steps:   80|Train Avg Loss: 0.0102 \n",
      "Epoch:  125|steps:   90|Train Avg Loss: 0.0087 \n",
      "Epoch:  125|steps:  100|Train Avg Loss: 0.0087 \n",
      "Epoch:  125|steps:  110|Train Avg Loss: 0.0094 \n",
      "Epoch:  125|steps:  120|Train Avg Loss: 0.0096 \n",
      "Epoch:  126|steps:   10|Train Avg Loss: 0.0086 \n",
      "Epoch:  126|steps:   20|Train Avg Loss: 0.0089 \n",
      "Epoch:  126|steps:   30|Train Avg Loss: 0.0088 \n",
      "Epoch:  126|steps:   40|Train Avg Loss: 0.0093 \n",
      "Epoch:  126|steps:   50|Train Avg Loss: 0.0091 \n",
      "Epoch:  126|steps:   60|Train Avg Loss: 0.0097 \n",
      "Epoch:  126|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  126|steps:   80|Train Avg Loss: 0.0101 \n",
      "Epoch:  126|steps:   90|Train Avg Loss: 0.0084 \n",
      "Epoch:  126|steps:  100|Train Avg Loss: 0.0097 \n",
      "Epoch:  126|steps:  110|Train Avg Loss: 0.0091 \n",
      "Epoch:  126|steps:  120|Train Avg Loss: 0.0086 \n",
      "Epoch:  127|steps:   10|Train Avg Loss: 0.0085 \n",
      "Epoch:  127|steps:   20|Train Avg Loss: 0.0084 \n",
      "Epoch:  127|steps:   30|Train Avg Loss: 0.0094 \n",
      "Epoch:  127|steps:   40|Train Avg Loss: 0.0093 \n",
      "Epoch:  127|steps:   50|Train Avg Loss: 0.0086 \n",
      "Epoch:  127|steps:   60|Train Avg Loss: 0.0091 \n",
      "Epoch:  127|steps:   70|Train Avg Loss: 0.0089 \n",
      "Epoch:  127|steps:   80|Train Avg Loss: 0.0094 \n",
      "Epoch:  127|steps:   90|Train Avg Loss: 0.0091 \n",
      "Epoch:  127|steps:  100|Train Avg Loss: 0.0097 \n",
      "Epoch:  127|steps:  110|Train Avg Loss: 0.0096 \n",
      "Epoch:  127|steps:  120|Train Avg Loss: 0.0097 \n",
      "Epoch:  128|steps:   10|Train Avg Loss: 0.0088 \n",
      "Epoch:  128|steps:   20|Train Avg Loss: 0.0092 \n",
      "Epoch:  128|steps:   30|Train Avg Loss: 0.0095 \n",
      "Epoch:  128|steps:   40|Train Avg Loss: 0.0094 \n",
      "Epoch:  128|steps:   50|Train Avg Loss: 0.0088 \n",
      "Epoch:  128|steps:   60|Train Avg Loss: 0.0086 \n",
      "Epoch:  128|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  128|steps:   80|Train Avg Loss: 0.0097 \n",
      "Epoch:  128|steps:   90|Train Avg Loss: 0.0085 \n",
      "Epoch:  128|steps:  100|Train Avg Loss: 0.0083 \n",
      "Epoch:  128|steps:  110|Train Avg Loss: 0.0090 \n",
      "Epoch:  128|steps:  120|Train Avg Loss: 0.0095 \n",
      "Epoch:  129|steps:   10|Train Avg Loss: 0.0096 \n",
      "Epoch:  129|steps:   20|Train Avg Loss: 0.0092 \n",
      "Epoch:  129|steps:   30|Train Avg Loss: 0.0085 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  129|steps:   40|Train Avg Loss: 0.0086 \n",
      "Epoch:  129|steps:   50|Train Avg Loss: 0.0089 \n",
      "Epoch:  129|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  129|steps:   70|Train Avg Loss: 0.0089 \n",
      "Epoch:  129|steps:   80|Train Avg Loss: 0.0086 \n",
      "Epoch:  129|steps:   90|Train Avg Loss: 0.0091 \n",
      "Epoch:  129|steps:  100|Train Avg Loss: 0.0092 \n",
      "Epoch:  129|steps:  110|Train Avg Loss: 0.0086 \n",
      "Epoch:  129|steps:  120|Train Avg Loss: 0.0098 \n",
      "Epoch:  130|steps:   10|Train Avg Loss: 0.0087 \n",
      "Epoch:  130|steps:   20|Train Avg Loss: 0.0081 \n",
      "Epoch:  130|steps:   30|Train Avg Loss: 0.0090 \n",
      "Epoch:  130|steps:   40|Train Avg Loss: 0.0087 \n",
      "Epoch:  130|steps:   50|Train Avg Loss: 0.0094 \n",
      "Epoch:  130|steps:   60|Train Avg Loss: 0.0096 \n",
      "Epoch:  130|steps:   70|Train Avg Loss: 0.0090 \n",
      "Epoch:  130|steps:   80|Train Avg Loss: 0.0094 \n",
      "Epoch:  130|steps:   90|Train Avg Loss: 0.0089 \n",
      "Epoch:  130|steps:  100|Train Avg Loss: 0.0090 \n",
      "Epoch:  130|steps:  110|Train Avg Loss: 0.0091 \n",
      "Epoch:  130|steps:  120|Train Avg Loss: 0.0095 \n",
      "Epoch:  131|steps:   10|Train Avg Loss: 0.0091 \n",
      "Epoch:  131|steps:   20|Train Avg Loss: 0.0091 \n",
      "Epoch:  131|steps:   30|Train Avg Loss: 0.0084 \n",
      "Epoch:  131|steps:   40|Train Avg Loss: 0.0086 \n",
      "Epoch:  131|steps:   50|Train Avg Loss: 0.0088 \n",
      "Epoch:  131|steps:   60|Train Avg Loss: 0.0090 \n",
      "Epoch:  131|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  131|steps:   80|Train Avg Loss: 0.0089 \n",
      "Epoch:  131|steps:   90|Train Avg Loss: 0.0095 \n",
      "Epoch:  131|steps:  100|Train Avg Loss: 0.0094 \n",
      "Epoch:  131|steps:  110|Train Avg Loss: 0.0090 \n",
      "Epoch:  131|steps:  120|Train Avg Loss: 0.0088 \n",
      "Epoch:  132|steps:   10|Train Avg Loss: 0.0086 \n",
      "Epoch:  132|steps:   20|Train Avg Loss: 0.0092 \n",
      "Epoch:  132|steps:   30|Train Avg Loss: 0.0092 \n",
      "Epoch:  132|steps:   40|Train Avg Loss: 0.0087 \n",
      "Epoch:  132|steps:   50|Train Avg Loss: 0.0087 \n",
      "Epoch:  132|steps:   60|Train Avg Loss: 0.0092 \n",
      "Epoch:  132|steps:   70|Train Avg Loss: 0.0092 \n",
      "Epoch:  132|steps:   80|Train Avg Loss: 0.0085 \n",
      "Epoch:  132|steps:   90|Train Avg Loss: 0.0089 \n",
      "Epoch:  132|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  132|steps:  110|Train Avg Loss: 0.0091 \n",
      "Epoch:  132|steps:  120|Train Avg Loss: 0.0095 \n",
      "Epoch:  133|steps:   10|Train Avg Loss: 0.0085 \n",
      "Epoch:  133|steps:   20|Train Avg Loss: 0.0089 \n",
      "Epoch:  133|steps:   30|Train Avg Loss: 0.0090 \n",
      "Epoch:  133|steps:   40|Train Avg Loss: 0.0090 \n",
      "Epoch:  133|steps:   50|Train Avg Loss: 0.0086 \n",
      "Epoch:  133|steps:   60|Train Avg Loss: 0.0095 \n",
      "Epoch:  133|steps:   70|Train Avg Loss: 0.0082 \n",
      "Epoch:  133|steps:   80|Train Avg Loss: 0.0084 \n",
      "Epoch:  133|steps:   90|Train Avg Loss: 0.0084 \n",
      "Epoch:  133|steps:  100|Train Avg Loss: 0.0100 \n",
      "Epoch:  133|steps:  110|Train Avg Loss: 0.0095 \n",
      "Epoch:  133|steps:  120|Train Avg Loss: 0.0092 \n",
      "Epoch:  134|steps:   10|Train Avg Loss: 0.0092 \n",
      "Epoch:  134|steps:   20|Train Avg Loss: 0.0091 \n",
      "Epoch:  134|steps:   30|Train Avg Loss: 0.0087 \n",
      "Epoch:  134|steps:   40|Train Avg Loss: 0.0097 \n",
      "Epoch:  134|steps:   50|Train Avg Loss: 0.0090 \n",
      "Epoch:  134|steps:   60|Train Avg Loss: 0.0091 \n",
      "Epoch:  134|steps:   70|Train Avg Loss: 0.0088 \n",
      "Epoch:  134|steps:   80|Train Avg Loss: 0.0084 \n",
      "Epoch:  134|steps:   90|Train Avg Loss: 0.0088 \n",
      "Epoch:  134|steps:  100|Train Avg Loss: 0.0094 \n",
      "Epoch:  134|steps:  110|Train Avg Loss: 0.0086 \n",
      "Epoch:  134|steps:  120|Train Avg Loss: 0.0090 \n",
      "Epoch:  135|steps:   10|Train Avg Loss: 0.0085 \n",
      "Epoch:  135|steps:   20|Train Avg Loss: 0.0093 \n",
      "Epoch:  135|steps:   30|Train Avg Loss: 0.0085 \n",
      "Epoch:  135|steps:   40|Train Avg Loss: 0.0094 \n",
      "Epoch:  135|steps:   50|Train Avg Loss: 0.0089 \n",
      "Epoch:  135|steps:   60|Train Avg Loss: 0.0087 \n",
      "Epoch:  135|steps:   70|Train Avg Loss: 0.0092 \n",
      "Epoch:  135|steps:   80|Train Avg Loss: 0.0089 \n",
      "Epoch:  135|steps:   90|Train Avg Loss: 0.0094 \n",
      "Epoch:  135|steps:  100|Train Avg Loss: 0.0081 \n",
      "Epoch:  135|steps:  110|Train Avg Loss: 0.0099 \n",
      "Epoch:  135|steps:  120|Train Avg Loss: 0.0091 \n",
      "Epoch:  136|steps:   10|Train Avg Loss: 0.0082 \n",
      "Epoch:  136|steps:   20|Train Avg Loss: 0.0095 \n",
      "Epoch:  136|steps:   30|Train Avg Loss: 0.0088 \n",
      "Epoch:  136|steps:   40|Train Avg Loss: 0.0090 \n",
      "Epoch:  136|steps:   50|Train Avg Loss: 0.0089 \n",
      "Epoch:  136|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  136|steps:   70|Train Avg Loss: 0.0086 \n",
      "Epoch:  136|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  136|steps:   90|Train Avg Loss: 0.0096 \n",
      "Epoch:  136|steps:  100|Train Avg Loss: 0.0088 \n",
      "Epoch:  136|steps:  110|Train Avg Loss: 0.0086 \n",
      "Epoch:  136|steps:  120|Train Avg Loss: 0.0094 \n",
      "Epoch:  137|steps:   10|Train Avg Loss: 0.0091 \n",
      "Epoch:  137|steps:   20|Train Avg Loss: 0.0087 \n",
      "Epoch:  137|steps:   30|Train Avg Loss: 0.0080 \n",
      "Epoch:  137|steps:   40|Train Avg Loss: 0.0086 \n",
      "Epoch:  137|steps:   50|Train Avg Loss: 0.0088 \n",
      "Epoch:  137|steps:   60|Train Avg Loss: 0.0085 \n",
      "Epoch:  137|steps:   70|Train Avg Loss: 0.0095 \n",
      "Epoch:  137|steps:   80|Train Avg Loss: 0.0090 \n",
      "Epoch:  137|steps:   90|Train Avg Loss: 0.0087 \n",
      "Epoch:  137|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  137|steps:  110|Train Avg Loss: 0.0093 \n",
      "Epoch:  137|steps:  120|Train Avg Loss: 0.0093 \n",
      "Epoch:  138|steps:   10|Train Avg Loss: 0.0085 \n",
      "Epoch:  138|steps:   20|Train Avg Loss: 0.0087 \n",
      "Epoch:  138|steps:   30|Train Avg Loss: 0.0099 \n",
      "Epoch:  138|steps:   40|Train Avg Loss: 0.0085 \n",
      "Epoch:  138|steps:   50|Train Avg Loss: 0.0089 \n",
      "Epoch:  138|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  138|steps:   70|Train Avg Loss: 0.0086 \n",
      "Epoch:  138|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  138|steps:   90|Train Avg Loss: 0.0085 \n",
      "Epoch:  138|steps:  100|Train Avg Loss: 0.0086 \n",
      "Epoch:  138|steps:  110|Train Avg Loss: 0.0091 \n",
      "Epoch:  138|steps:  120|Train Avg Loss: 0.0086 \n",
      "Epoch:  139|steps:   10|Train Avg Loss: 0.0085 \n",
      "Epoch:  139|steps:   20|Train Avg Loss: 0.0099 \n",
      "Epoch:  139|steps:   30|Train Avg Loss: 0.0084 \n",
      "Epoch:  139|steps:   40|Train Avg Loss: 0.0091 \n",
      "Epoch:  139|steps:   50|Train Avg Loss: 0.0086 \n",
      "Epoch:  139|steps:   60|Train Avg Loss: 0.0085 \n",
      "Epoch:  139|steps:   70|Train Avg Loss: 0.0084 \n",
      "Epoch:  139|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  139|steps:   90|Train Avg Loss: 0.0096 \n",
      "Epoch:  139|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  139|steps:  110|Train Avg Loss: 0.0086 \n",
      "Epoch:  139|steps:  120|Train Avg Loss: 0.0084 \n",
      "Epoch:  140|steps:   10|Train Avg Loss: 0.0085 \n",
      "Epoch:  140|steps:   20|Train Avg Loss: 0.0078 \n",
      "Epoch:  140|steps:   30|Train Avg Loss: 0.0085 \n",
      "Epoch:  140|steps:   40|Train Avg Loss: 0.0089 \n",
      "Epoch:  140|steps:   50|Train Avg Loss: 0.0087 \n",
      "Epoch:  140|steps:   60|Train Avg Loss: 0.0092 \n",
      "Epoch:  140|steps:   70|Train Avg Loss: 0.0093 \n",
      "Epoch:  140|steps:   80|Train Avg Loss: 0.0094 \n",
      "Epoch:  140|steps:   90|Train Avg Loss: 0.0089 \n",
      "Epoch:  140|steps:  100|Train Avg Loss: 0.0094 \n",
      "Epoch:  140|steps:  110|Train Avg Loss: 0.0079 \n",
      "Epoch:  140|steps:  120|Train Avg Loss: 0.0087 \n",
      "Epoch:  141|steps:   10|Train Avg Loss: 0.0094 \n",
      "Epoch:  141|steps:   20|Train Avg Loss: 0.0093 \n",
      "Epoch:  141|steps:   30|Train Avg Loss: 0.0088 \n",
      "Epoch:  141|steps:   40|Train Avg Loss: 0.0096 \n",
      "Epoch:  141|steps:   50|Train Avg Loss: 0.0086 \n",
      "Epoch:  141|steps:   60|Train Avg Loss: 0.0082 \n",
      "Epoch:  141|steps:   70|Train Avg Loss: 0.0083 \n",
      "Epoch:  141|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  141|steps:   90|Train Avg Loss: 0.0087 \n",
      "Epoch:  141|steps:  100|Train Avg Loss: 0.0088 \n",
      "Epoch:  141|steps:  110|Train Avg Loss: 0.0083 \n",
      "Epoch:  141|steps:  120|Train Avg Loss: 0.0082 \n",
      "Epoch:  142|steps:   10|Train Avg Loss: 0.0089 \n",
      "Epoch:  142|steps:   20|Train Avg Loss: 0.0085 \n",
      "Epoch:  142|steps:   30|Train Avg Loss: 0.0081 \n",
      "Epoch:  142|steps:   40|Train Avg Loss: 0.0083 \n",
      "Epoch:  142|steps:   50|Train Avg Loss: 0.0089 \n",
      "Epoch:  142|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  142|steps:   70|Train Avg Loss: 0.0097 \n",
      "Epoch:  142|steps:   80|Train Avg Loss: 0.0084 \n",
      "Epoch:  142|steps:   90|Train Avg Loss: 0.0089 \n",
      "Epoch:  142|steps:  100|Train Avg Loss: 0.0084 \n",
      "Epoch:  142|steps:  110|Train Avg Loss: 0.0092 \n",
      "Epoch:  142|steps:  120|Train Avg Loss: 0.0088 \n",
      "Epoch:  143|steps:   10|Train Avg Loss: 0.0084 \n",
      "Epoch:  143|steps:   20|Train Avg Loss: 0.0091 \n",
      "Epoch:  143|steps:   30|Train Avg Loss: 0.0083 \n",
      "Epoch:  143|steps:   40|Train Avg Loss: 0.0082 \n",
      "Epoch:  143|steps:   50|Train Avg Loss: 0.0087 \n",
      "Epoch:  143|steps:   60|Train Avg Loss: 0.0087 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  143|steps:   70|Train Avg Loss: 0.0092 \n",
      "Epoch:  143|steps:   80|Train Avg Loss: 0.0095 \n",
      "Epoch:  143|steps:   90|Train Avg Loss: 0.0087 \n",
      "Epoch:  143|steps:  100|Train Avg Loss: 0.0087 \n",
      "Epoch:  143|steps:  110|Train Avg Loss: 0.0085 \n",
      "Epoch:  143|steps:  120|Train Avg Loss: 0.0090 \n",
      "Epoch:  144|steps:   10|Train Avg Loss: 0.0087 \n",
      "Epoch:  144|steps:   20|Train Avg Loss: 0.0096 \n",
      "Epoch:  144|steps:   30|Train Avg Loss: 0.0085 \n",
      "Epoch:  144|steps:   40|Train Avg Loss: 0.0085 \n",
      "Epoch:  144|steps:   50|Train Avg Loss: 0.0083 \n",
      "Epoch:  144|steps:   60|Train Avg Loss: 0.0088 \n",
      "Epoch:  144|steps:   70|Train Avg Loss: 0.0086 \n",
      "Epoch:  144|steps:   80|Train Avg Loss: 0.0085 \n",
      "Epoch:  144|steps:   90|Train Avg Loss: 0.0084 \n",
      "Epoch:  144|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  144|steps:  110|Train Avg Loss: 0.0088 \n",
      "Epoch:  144|steps:  120|Train Avg Loss: 0.0095 \n",
      "Epoch:  145|steps:   10|Train Avg Loss: 0.0084 \n",
      "Epoch:  145|steps:   20|Train Avg Loss: 0.0087 \n",
      "Epoch:  145|steps:   30|Train Avg Loss: 0.0086 \n",
      "Epoch:  145|steps:   40|Train Avg Loss: 0.0075 \n",
      "Epoch:  145|steps:   50|Train Avg Loss: 0.0086 \n",
      "Epoch:  145|steps:   60|Train Avg Loss: 0.0083 \n",
      "Epoch:  145|steps:   70|Train Avg Loss: 0.0085 \n",
      "Epoch:  145|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  145|steps:   90|Train Avg Loss: 0.0091 \n",
      "Epoch:  145|steps:  100|Train Avg Loss: 0.0089 \n",
      "Epoch:  145|steps:  110|Train Avg Loss: 0.0084 \n",
      "Epoch:  145|steps:  120|Train Avg Loss: 0.0093 \n",
      "Epoch:  146|steps:   10|Train Avg Loss: 0.0087 \n",
      "Epoch:  146|steps:   20|Train Avg Loss: 0.0088 \n",
      "Epoch:  146|steps:   30|Train Avg Loss: 0.0082 \n",
      "Epoch:  146|steps:   40|Train Avg Loss: 0.0091 \n",
      "Epoch:  146|steps:   50|Train Avg Loss: 0.0088 \n",
      "Epoch:  146|steps:   60|Train Avg Loss: 0.0089 \n",
      "Epoch:  146|steps:   70|Train Avg Loss: 0.0086 \n",
      "Epoch:  146|steps:   80|Train Avg Loss: 0.0086 \n",
      "Epoch:  146|steps:   90|Train Avg Loss: 0.0084 \n",
      "Epoch:  146|steps:  100|Train Avg Loss: 0.0092 \n",
      "Epoch:  146|steps:  110|Train Avg Loss: 0.0088 \n",
      "Epoch:  146|steps:  120|Train Avg Loss: 0.0081 \n",
      "Epoch:  147|steps:   10|Train Avg Loss: 0.0086 \n",
      "Epoch:  147|steps:   20|Train Avg Loss: 0.0084 \n",
      "Epoch:  147|steps:   30|Train Avg Loss: 0.0080 \n",
      "Epoch:  147|steps:   40|Train Avg Loss: 0.0091 \n",
      "Epoch:  147|steps:   50|Train Avg Loss: 0.0087 \n",
      "Epoch:  147|steps:   60|Train Avg Loss: 0.0090 \n",
      "Epoch:  147|steps:   70|Train Avg Loss: 0.0079 \n",
      "Epoch:  147|steps:   80|Train Avg Loss: 0.0082 \n",
      "Epoch:  147|steps:   90|Train Avg Loss: 0.0085 \n",
      "Epoch:  147|steps:  100|Train Avg Loss: 0.0087 \n",
      "Epoch:  147|steps:  110|Train Avg Loss: 0.0091 \n",
      "Epoch:  147|steps:  120|Train Avg Loss: 0.0088 \n",
      "Epoch:  148|steps:   10|Train Avg Loss: 0.0099 \n",
      "Epoch:  148|steps:   20|Train Avg Loss: 0.0083 \n",
      "Epoch:  148|steps:   30|Train Avg Loss: 0.0083 \n",
      "Epoch:  148|steps:   40|Train Avg Loss: 0.0082 \n",
      "Epoch:  148|steps:   50|Train Avg Loss: 0.0087 \n",
      "Epoch:  148|steps:   60|Train Avg Loss: 0.0085 \n",
      "Epoch:  148|steps:   70|Train Avg Loss: 0.0087 \n",
      "Epoch:  148|steps:   80|Train Avg Loss: 0.0084 \n",
      "Epoch:  148|steps:   90|Train Avg Loss: 0.0087 \n",
      "Epoch:  148|steps:  100|Train Avg Loss: 0.0084 \n",
      "Epoch:  148|steps:  110|Train Avg Loss: 0.0090 \n",
      "Epoch:  148|steps:  120|Train Avg Loss: 0.0095 \n",
      "Epoch:  149|steps:   10|Train Avg Loss: 0.0084 \n",
      "Epoch:  149|steps:   20|Train Avg Loss: 0.0088 \n",
      "Epoch:  149|steps:   30|Train Avg Loss: 0.0082 \n",
      "Epoch:  149|steps:   40|Train Avg Loss: 0.0089 \n",
      "Epoch:  149|steps:   50|Train Avg Loss: 0.0083 \n",
      "Epoch:  149|steps:   60|Train Avg Loss: 0.0093 \n",
      "Epoch:  149|steps:   70|Train Avg Loss: 0.0089 \n",
      "Epoch:  149|steps:   80|Train Avg Loss: 0.0087 \n",
      "Epoch:  149|steps:   90|Train Avg Loss: 0.0080 \n",
      "Epoch:  149|steps:  100|Train Avg Loss: 0.0079 \n",
      "Epoch:  149|steps:  110|Train Avg Loss: 0.0086 \n",
      "Epoch:  149|steps:  120|Train Avg Loss: 0.0089 \n",
      "Epoch:  150|steps:   10|Train Avg Loss: 0.0090 \n",
      "Epoch:  150|steps:   20|Train Avg Loss: 0.0087 \n",
      "Epoch:  150|steps:   30|Train Avg Loss: 0.0086 \n",
      "Epoch:  150|steps:   40|Train Avg Loss: 0.0080 \n",
      "Epoch:  150|steps:   50|Train Avg Loss: 0.0088 \n",
      "Epoch:  150|steps:   60|Train Avg Loss: 0.0089 \n",
      "Epoch:  150|steps:   70|Train Avg Loss: 0.0088 \n",
      "Epoch:  150|steps:   80|Train Avg Loss: 0.0085 \n",
      "Epoch:  150|steps:   90|Train Avg Loss: 0.0089 \n",
      "Epoch:  150|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  150|steps:  110|Train Avg Loss: 0.0084 \n",
      "Epoch:  150|steps:  120|Train Avg Loss: 0.0078 \n",
      "Epoch:  151|steps:   10|Train Avg Loss: 0.0083 \n",
      "Epoch:  151|steps:   20|Train Avg Loss: 0.0091 \n",
      "Epoch:  151|steps:   30|Train Avg Loss: 0.0082 \n",
      "Epoch:  151|steps:   40|Train Avg Loss: 0.0089 \n",
      "Epoch:  151|steps:   50|Train Avg Loss: 0.0079 \n",
      "Epoch:  151|steps:   60|Train Avg Loss: 0.0084 \n",
      "Epoch:  151|steps:   70|Train Avg Loss: 0.0087 \n",
      "Epoch:  151|steps:   80|Train Avg Loss: 0.0085 \n",
      "Epoch:  151|steps:   90|Train Avg Loss: 0.0085 \n",
      "Epoch:  151|steps:  100|Train Avg Loss: 0.0083 \n",
      "Epoch:  151|steps:  110|Train Avg Loss: 0.0088 \n",
      "Epoch:  151|steps:  120|Train Avg Loss: 0.0088 \n",
      "Epoch:  152|steps:   10|Train Avg Loss: 0.0084 \n",
      "Epoch:  152|steps:   20|Train Avg Loss: 0.0081 \n",
      "Epoch:  152|steps:   30|Train Avg Loss: 0.0077 \n",
      "Epoch:  152|steps:   40|Train Avg Loss: 0.0080 \n",
      "Epoch:  152|steps:   50|Train Avg Loss: 0.0081 \n",
      "Epoch:  152|steps:   60|Train Avg Loss: 0.0086 \n",
      "Epoch:  152|steps:   70|Train Avg Loss: 0.0086 \n",
      "Epoch:  152|steps:   80|Train Avg Loss: 0.0087 \n",
      "Epoch:  152|steps:   90|Train Avg Loss: 0.0089 \n",
      "Epoch:  152|steps:  100|Train Avg Loss: 0.0086 \n",
      "Epoch:  152|steps:  110|Train Avg Loss: 0.0090 \n",
      "Epoch:  152|steps:  120|Train Avg Loss: 0.0090 \n",
      "Epoch:  153|steps:   10|Train Avg Loss: 0.0083 \n",
      "Epoch:  153|steps:   20|Train Avg Loss: 0.0095 \n",
      "Epoch:  153|steps:   30|Train Avg Loss: 0.0086 \n",
      "Epoch:  153|steps:   40|Train Avg Loss: 0.0087 \n",
      "Epoch:  153|steps:   50|Train Avg Loss: 0.0087 \n",
      "Epoch:  153|steps:   60|Train Avg Loss: 0.0082 \n",
      "Epoch:  153|steps:   70|Train Avg Loss: 0.0083 \n",
      "Epoch:  153|steps:   80|Train Avg Loss: 0.0081 \n",
      "Epoch:  153|steps:   90|Train Avg Loss: 0.0086 \n",
      "Epoch:  153|steps:  100|Train Avg Loss: 0.0076 \n",
      "Epoch:  153|steps:  110|Train Avg Loss: 0.0084 \n",
      "Epoch:  153|steps:  120|Train Avg Loss: 0.0086 \n",
      "Epoch:  154|steps:   10|Train Avg Loss: 0.0081 \n",
      "Epoch:  154|steps:   20|Train Avg Loss: 0.0084 \n",
      "Epoch:  154|steps:   30|Train Avg Loss: 0.0090 \n",
      "Epoch:  154|steps:   40|Train Avg Loss: 0.0082 \n",
      "Epoch:  154|steps:   50|Train Avg Loss: 0.0081 \n",
      "Epoch:  154|steps:   60|Train Avg Loss: 0.0086 \n",
      "Epoch:  154|steps:   70|Train Avg Loss: 0.0084 \n",
      "Epoch:  154|steps:   80|Train Avg Loss: 0.0085 \n",
      "Epoch:  154|steps:   90|Train Avg Loss: 0.0086 \n",
      "Epoch:  154|steps:  100|Train Avg Loss: 0.0092 \n",
      "Epoch:  154|steps:  110|Train Avg Loss: 0.0093 \n",
      "Epoch:  154|steps:  120|Train Avg Loss: 0.0083 \n",
      "Epoch:  155|steps:   10|Train Avg Loss: 0.0082 \n",
      "Epoch:  155|steps:   20|Train Avg Loss: 0.0085 \n",
      "Epoch:  155|steps:   30|Train Avg Loss: 0.0081 \n",
      "Epoch:  155|steps:   40|Train Avg Loss: 0.0086 \n",
      "Epoch:  155|steps:   50|Train Avg Loss: 0.0082 \n",
      "Epoch:  155|steps:   60|Train Avg Loss: 0.0087 \n",
      "Epoch:  155|steps:   70|Train Avg Loss: 0.0073 \n",
      "Epoch:  155|steps:   80|Train Avg Loss: 0.0091 \n",
      "Epoch:  155|steps:   90|Train Avg Loss: 0.0079 \n",
      "Epoch:  155|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  155|steps:  110|Train Avg Loss: 0.0082 \n",
      "Epoch:  155|steps:  120|Train Avg Loss: 0.0090 \n",
      "Epoch:  156|steps:   10|Train Avg Loss: 0.0086 \n",
      "Epoch:  156|steps:   20|Train Avg Loss: 0.0082 \n",
      "Epoch:  156|steps:   30|Train Avg Loss: 0.0089 \n",
      "Epoch:  156|steps:   40|Train Avg Loss: 0.0089 \n",
      "Epoch:  156|steps:   50|Train Avg Loss: 0.0085 \n",
      "Epoch:  156|steps:   60|Train Avg Loss: 0.0084 \n",
      "Epoch:  156|steps:   70|Train Avg Loss: 0.0088 \n",
      "Epoch:  156|steps:   80|Train Avg Loss: 0.0079 \n",
      "Epoch:  156|steps:   90|Train Avg Loss: 0.0083 \n",
      "Epoch:  156|steps:  100|Train Avg Loss: 0.0088 \n",
      "Epoch:  156|steps:  110|Train Avg Loss: 0.0083 \n",
      "Epoch:  156|steps:  120|Train Avg Loss: 0.0080 \n",
      "Epoch:  157|steps:   10|Train Avg Loss: 0.0086 \n",
      "Epoch:  157|steps:   20|Train Avg Loss: 0.0085 \n",
      "Epoch:  157|steps:   30|Train Avg Loss: 0.0090 \n",
      "Epoch:  157|steps:   40|Train Avg Loss: 0.0087 \n",
      "Epoch:  157|steps:   50|Train Avg Loss: 0.0094 \n",
      "Epoch:  157|steps:   60|Train Avg Loss: 0.0082 \n",
      "Epoch:  157|steps:   70|Train Avg Loss: 0.0087 \n",
      "Epoch:  157|steps:   80|Train Avg Loss: 0.0086 \n",
      "Epoch:  157|steps:   90|Train Avg Loss: 0.0080 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  157|steps:  100|Train Avg Loss: 0.0081 \n",
      "Epoch:  157|steps:  110|Train Avg Loss: 0.0079 \n",
      "Epoch:  157|steps:  120|Train Avg Loss: 0.0083 \n",
      "Epoch:  158|steps:   10|Train Avg Loss: 0.0088 \n",
      "Epoch:  158|steps:   20|Train Avg Loss: 0.0085 \n",
      "Epoch:  158|steps:   30|Train Avg Loss: 0.0083 \n",
      "Epoch:  158|steps:   40|Train Avg Loss: 0.0086 \n",
      "Epoch:  158|steps:   50|Train Avg Loss: 0.0076 \n",
      "Epoch:  158|steps:   60|Train Avg Loss: 0.0085 \n",
      "Epoch:  158|steps:   70|Train Avg Loss: 0.0087 \n",
      "Epoch:  158|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  158|steps:   90|Train Avg Loss: 0.0077 \n",
      "Epoch:  158|steps:  100|Train Avg Loss: 0.0084 \n",
      "Epoch:  158|steps:  110|Train Avg Loss: 0.0088 \n",
      "Epoch:  158|steps:  120|Train Avg Loss: 0.0087 \n",
      "Epoch:  159|steps:   10|Train Avg Loss: 0.0084 \n",
      "Epoch:  159|steps:   20|Train Avg Loss: 0.0088 \n",
      "Epoch:  159|steps:   30|Train Avg Loss: 0.0083 \n",
      "Epoch:  159|steps:   40|Train Avg Loss: 0.0078 \n",
      "Epoch:  159|steps:   50|Train Avg Loss: 0.0090 \n",
      "Epoch:  159|steps:   60|Train Avg Loss: 0.0081 \n",
      "Epoch:  159|steps:   70|Train Avg Loss: 0.0083 \n",
      "Epoch:  159|steps:   80|Train Avg Loss: 0.0087 \n",
      "Epoch:  159|steps:   90|Train Avg Loss: 0.0084 \n",
      "Epoch:  159|steps:  100|Train Avg Loss: 0.0074 \n",
      "Epoch:  159|steps:  110|Train Avg Loss: 0.0077 \n",
      "Epoch:  159|steps:  120|Train Avg Loss: 0.0088 \n",
      "Epoch:  160|steps:   10|Train Avg Loss: 0.0080 \n",
      "Epoch:  160|steps:   20|Train Avg Loss: 0.0083 \n",
      "Epoch:  160|steps:   30|Train Avg Loss: 0.0085 \n",
      "Epoch:  160|steps:   40|Train Avg Loss: 0.0080 \n",
      "Epoch:  160|steps:   50|Train Avg Loss: 0.0083 \n",
      "Epoch:  160|steps:   60|Train Avg Loss: 0.0088 \n",
      "Epoch:  160|steps:   70|Train Avg Loss: 0.0085 \n",
      "Epoch:  160|steps:   80|Train Avg Loss: 0.0083 \n",
      "Epoch:  160|steps:   90|Train Avg Loss: 0.0085 \n",
      "Epoch:  160|steps:  100|Train Avg Loss: 0.0086 \n",
      "Epoch:  160|steps:  110|Train Avg Loss: 0.0078 \n",
      "Epoch:  160|steps:  120|Train Avg Loss: 0.0085 \n",
      "Epoch:  161|steps:   10|Train Avg Loss: 0.0083 \n",
      "Epoch:  161|steps:   20|Train Avg Loss: 0.0084 \n",
      "Epoch:  161|steps:   30|Train Avg Loss: 0.0085 \n",
      "Epoch:  161|steps:   40|Train Avg Loss: 0.0082 \n",
      "Epoch:  161|steps:   50|Train Avg Loss: 0.0079 \n",
      "Epoch:  161|steps:   60|Train Avg Loss: 0.0082 \n",
      "Epoch:  161|steps:   70|Train Avg Loss: 0.0082 \n",
      "Epoch:  161|steps:   80|Train Avg Loss: 0.0082 \n",
      "Epoch:  161|steps:   90|Train Avg Loss: 0.0082 \n",
      "Epoch:  161|steps:  100|Train Avg Loss: 0.0084 \n",
      "Epoch:  161|steps:  110|Train Avg Loss: 0.0078 \n",
      "Epoch:  161|steps:  120|Train Avg Loss: 0.0092 \n",
      "Epoch:  162|steps:   10|Train Avg Loss: 0.0085 \n",
      "Epoch:  162|steps:   20|Train Avg Loss: 0.0085 \n",
      "Epoch:  162|steps:   30|Train Avg Loss: 0.0077 \n",
      "Epoch:  162|steps:   40|Train Avg Loss: 0.0084 \n",
      "Epoch:  162|steps:   50|Train Avg Loss: 0.0082 \n",
      "Epoch:  162|steps:   60|Train Avg Loss: 0.0082 \n",
      "Epoch:  162|steps:   70|Train Avg Loss: 0.0085 \n",
      "Epoch:  162|steps:   80|Train Avg Loss: 0.0084 \n",
      "Epoch:  162|steps:   90|Train Avg Loss: 0.0079 \n",
      "Epoch:  162|steps:  100|Train Avg Loss: 0.0081 \n",
      "Epoch:  162|steps:  110|Train Avg Loss: 0.0083 \n",
      "Epoch:  162|steps:  120|Train Avg Loss: 0.0084 \n",
      "Epoch:  163|steps:   10|Train Avg Loss: 0.0082 \n",
      "Epoch:  163|steps:   20|Train Avg Loss: 0.0083 \n",
      "Epoch:  163|steps:   30|Train Avg Loss: 0.0082 \n",
      "Epoch:  163|steps:   40|Train Avg Loss: 0.0083 \n",
      "Epoch:  163|steps:   50|Train Avg Loss: 0.0077 \n",
      "Epoch:  163|steps:   60|Train Avg Loss: 0.0080 \n",
      "Epoch:  163|steps:   70|Train Avg Loss: 0.0089 \n",
      "Epoch:  163|steps:   80|Train Avg Loss: 0.0082 \n",
      "Epoch:  163|steps:   90|Train Avg Loss: 0.0087 \n",
      "Epoch:  163|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  163|steps:  110|Train Avg Loss: 0.0080 \n",
      "Epoch:  163|steps:  120|Train Avg Loss: 0.0078 \n",
      "Epoch:  164|steps:   10|Train Avg Loss: 0.0077 \n",
      "Epoch:  164|steps:   20|Train Avg Loss: 0.0079 \n",
      "Epoch:  164|steps:   30|Train Avg Loss: 0.0082 \n",
      "Epoch:  164|steps:   40|Train Avg Loss: 0.0083 \n",
      "Epoch:  164|steps:   50|Train Avg Loss: 0.0092 \n",
      "Epoch:  164|steps:   60|Train Avg Loss: 0.0086 \n",
      "Epoch:  164|steps:   70|Train Avg Loss: 0.0080 \n",
      "Epoch:  164|steps:   80|Train Avg Loss: 0.0083 \n",
      "Epoch:  164|steps:   90|Train Avg Loss: 0.0083 \n",
      "Epoch:  164|steps:  100|Train Avg Loss: 0.0078 \n",
      "Epoch:  164|steps:  110|Train Avg Loss: 0.0082 \n",
      "Epoch:  164|steps:  120|Train Avg Loss: 0.0088 \n",
      "Epoch:  165|steps:   10|Train Avg Loss: 0.0086 \n",
      "Epoch:  165|steps:   20|Train Avg Loss: 0.0079 \n",
      "Epoch:  165|steps:   30|Train Avg Loss: 0.0081 \n",
      "Epoch:  165|steps:   40|Train Avg Loss: 0.0081 \n",
      "Epoch:  165|steps:   50|Train Avg Loss: 0.0085 \n",
      "Epoch:  165|steps:   60|Train Avg Loss: 0.0087 \n",
      "Epoch:  165|steps:   70|Train Avg Loss: 0.0080 \n",
      "Epoch:  165|steps:   80|Train Avg Loss: 0.0074 \n",
      "Epoch:  165|steps:   90|Train Avg Loss: 0.0082 \n",
      "Epoch:  165|steps:  100|Train Avg Loss: 0.0082 \n",
      "Epoch:  165|steps:  110|Train Avg Loss: 0.0081 \n",
      "Epoch:  165|steps:  120|Train Avg Loss: 0.0091 \n",
      "Epoch:  166|steps:   10|Train Avg Loss: 0.0082 \n",
      "Epoch:  166|steps:   20|Train Avg Loss: 0.0082 \n",
      "Epoch:  166|steps:   30|Train Avg Loss: 0.0081 \n",
      "Epoch:  166|steps:   40|Train Avg Loss: 0.0077 \n",
      "Epoch:  166|steps:   50|Train Avg Loss: 0.0089 \n",
      "Epoch:  166|steps:   60|Train Avg Loss: 0.0088 \n",
      "Epoch:  166|steps:   70|Train Avg Loss: 0.0073 \n",
      "Epoch:  166|steps:   80|Train Avg Loss: 0.0088 \n",
      "Epoch:  166|steps:   90|Train Avg Loss: 0.0084 \n",
      "Epoch:  166|steps:  100|Train Avg Loss: 0.0085 \n",
      "Epoch:  166|steps:  110|Train Avg Loss: 0.0085 \n",
      "Epoch:  166|steps:  120|Train Avg Loss: 0.0074 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2be6b27d2b3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloss_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTIME_STEP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINPUT_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#reshape the features to (batch,time_step*input_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    857\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,TIME_STEP, INPUT_SIZE)\n",
    "        #reshape the features to (batch,time_step*input_size)\n",
    "        \n",
    "        \n",
    "        # start trainnig \n",
    "        z,output = model(inputs)\n",
    "        \n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,inputs)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 10 steps\n",
    "        if((step+1) %10 == 0):\n",
    "            # average of loss in 10 steps\n",
    "            avg = loss_total / 10\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f \"\n",
    "                  %(epoch+1,step+1,avg))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    #scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0811,  0.2375, -0.6506, -0.3543, -0.1627, -0.0674],\n",
      "        [ 0.1540,  0.2474, -0.6236, -0.2743, -0.0916,  0.0114],\n",
      "        [ 0.9432,  0.2942, -0.4706, -0.0462,  0.1378,  0.4162],\n",
      "        [-0.1474,  0.0843, -0.7388, -0.2641,  0.1527,  0.2257],\n",
      "        [ 0.0354,  0.1134, -0.6670, -0.3303,  0.1571,  0.1343],\n",
      "        [-0.0841,  0.0503, -0.7455, -0.4100,  0.0874,  0.0150],\n",
      "        [-0.6698, -0.1504, -0.9216, -0.5972, -0.1499, -0.3528]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[ 0.2578,  0.3713, -0.5138, -0.4369, -0.1382, -0.0536],\n",
      "        [ 0.1088,  0.4648, -0.5502, -0.2568, -0.1485, -0.0226],\n",
      "        [ 0.9896,  0.1469, -0.4896, -0.0537,  0.1033,  0.4391],\n",
      "        [-0.2229, -0.1644, -0.6897, -0.1680,  0.0689,  0.1624],\n",
      "        [ 0.1004,  0.0531, -0.7409, -0.2888,  0.1045,  0.1336],\n",
      "        [ 0.0669, -0.0636, -0.7294, -0.3416,  0.1384,  0.0954],\n",
      "        [-0.7091, -0.1015, -0.9103, -0.5647, -0.0663, -0.3258]])\n"
     ]
    }
   ],
   "source": [
    "print(output[0])\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,len(LOSS),len(LOSS))\n",
    "y = np.array(LOSS)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,_ = model(train_features)\n",
    "x_train = x_train.view(-1,6)\n",
    "x_train = x_train.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = train_labels.detach().numpy()\n",
    "y_train = y_train.reshape(len(y_train),)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,_ = model(test_features)\n",
    "x_test = x_test.view(-1,6)\n",
    "x_test = x_test.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276,)\n"
     ]
    }
   ],
   "source": [
    "y_test = test_labels.detach().numpy()\n",
    "y_test = y_test.reshape(len(y_test),)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(x_train,columns = [\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049793</td>\n",
       "      <td>0.898055</td>\n",
       "      <td>-0.409099</td>\n",
       "      <td>0.075851</td>\n",
       "      <td>-0.738460</td>\n",
       "      <td>-0.521373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.105515</td>\n",
       "      <td>0.757293</td>\n",
       "      <td>-0.506611</td>\n",
       "      <td>-0.522477</td>\n",
       "      <td>-0.668335</td>\n",
       "      <td>-0.700161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035078</td>\n",
       "      <td>0.105294</td>\n",
       "      <td>0.359197</td>\n",
       "      <td>-0.673533</td>\n",
       "      <td>-0.696296</td>\n",
       "      <td>-0.799454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.872810</td>\n",
       "      <td>0.290386</td>\n",
       "      <td>0.537954</td>\n",
       "      <td>-0.175134</td>\n",
       "      <td>-0.290019</td>\n",
       "      <td>-0.956010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.928975</td>\n",
       "      <td>0.712894</td>\n",
       "      <td>0.818719</td>\n",
       "      <td>-0.616418</td>\n",
       "      <td>-0.516517</td>\n",
       "      <td>-0.832278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.258112</td>\n",
       "      <td>0.342796</td>\n",
       "      <td>0.208885</td>\n",
       "      <td>0.089367</td>\n",
       "      <td>-0.561022</td>\n",
       "      <td>-0.038190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.154862</td>\n",
       "      <td>0.482284</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.011129</td>\n",
       "      <td>-0.491039</td>\n",
       "      <td>-0.029575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-0.198089</td>\n",
       "      <td>0.260621</td>\n",
       "      <td>0.124842</td>\n",
       "      <td>-0.013335</td>\n",
       "      <td>-0.461794</td>\n",
       "      <td>-0.143586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-0.117219</td>\n",
       "      <td>0.328620</td>\n",
       "      <td>0.227355</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>-0.407998</td>\n",
       "      <td>-0.186344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.212116</td>\n",
       "      <td>0.178159</td>\n",
       "      <td>0.289786</td>\n",
       "      <td>0.365845</td>\n",
       "      <td>-0.501343</td>\n",
       "      <td>-0.285700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1        f2        f3        f4        f5        f6  label\n",
       "0     0.049793  0.898055 -0.409099  0.075851 -0.738460 -0.521373      0\n",
       "1    -0.105515  0.757293 -0.506611 -0.522477 -0.668335 -0.700161      1\n",
       "2    -0.035078  0.105294  0.359197 -0.673533 -0.696296 -0.799454      0\n",
       "3    -0.872810  0.290386  0.537954 -0.175134 -0.290019 -0.956010      0\n",
       "4    -0.928975  0.712894  0.818719 -0.616418 -0.516517 -0.832278      1\n",
       "...        ...       ...       ...       ...       ...       ...    ...\n",
       "1995 -0.258112  0.342796  0.208885  0.089367 -0.561022 -0.038190      1\n",
       "1996 -0.154862  0.482284  0.194690  0.011129 -0.491039 -0.029575      1\n",
       "1997 -0.198089  0.260621  0.124842 -0.013335 -0.461794 -0.143586      1\n",
       "1998 -0.117219  0.328620  0.227355  0.018442 -0.407998 -0.186344      1\n",
       "1999 -0.212116  0.178159  0.289786  0.365845 -0.501343 -0.285700      1\n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(x_test,columns = [\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\"])\n",
    "test_df[\"label\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.585452</td>\n",
       "      <td>0.537005</td>\n",
       "      <td>0.889799</td>\n",
       "      <td>-0.108686</td>\n",
       "      <td>-0.637157</td>\n",
       "      <td>-0.161149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.471562</td>\n",
       "      <td>0.359512</td>\n",
       "      <td>0.275298</td>\n",
       "      <td>-0.381588</td>\n",
       "      <td>-0.860028</td>\n",
       "      <td>0.363237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.321655</td>\n",
       "      <td>0.436228</td>\n",
       "      <td>0.123557</td>\n",
       "      <td>-0.312269</td>\n",
       "      <td>-0.460502</td>\n",
       "      <td>-0.176002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.487490</td>\n",
       "      <td>0.300810</td>\n",
       "      <td>0.496499</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>-0.365455</td>\n",
       "      <td>-0.231464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.361425</td>\n",
       "      <td>0.506481</td>\n",
       "      <td>0.370880</td>\n",
       "      <td>-0.208517</td>\n",
       "      <td>-0.538276</td>\n",
       "      <td>0.115013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>-0.200019</td>\n",
       "      <td>0.308827</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.102653</td>\n",
       "      <td>-0.300474</td>\n",
       "      <td>-0.100670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>-0.237955</td>\n",
       "      <td>0.445253</td>\n",
       "      <td>0.580180</td>\n",
       "      <td>-0.045984</td>\n",
       "      <td>-0.564651</td>\n",
       "      <td>0.284254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>-0.086763</td>\n",
       "      <td>0.472739</td>\n",
       "      <td>0.059499</td>\n",
       "      <td>0.140871</td>\n",
       "      <td>-0.463131</td>\n",
       "      <td>0.242721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-0.249495</td>\n",
       "      <td>0.243281</td>\n",
       "      <td>0.210798</td>\n",
       "      <td>-0.093395</td>\n",
       "      <td>-0.358696</td>\n",
       "      <td>-0.029523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.258901</td>\n",
       "      <td>0.405626</td>\n",
       "      <td>0.064267</td>\n",
       "      <td>0.012619</td>\n",
       "      <td>-0.433631</td>\n",
       "      <td>-0.109848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2        f3        f4        f5        f6  label\n",
       "0   -0.585452  0.537005  0.889799 -0.108686 -0.637157 -0.161149      0\n",
       "1   -0.471562  0.359512  0.275298 -0.381588 -0.860028  0.363237      1\n",
       "2   -0.321655  0.436228  0.123557 -0.312269 -0.460502 -0.176002      0\n",
       "3   -0.487490  0.300810  0.496499  0.005580 -0.365455 -0.231464      1\n",
       "4   -0.361425  0.506481  0.370880 -0.208517 -0.538276  0.115013      1\n",
       "..        ...       ...       ...       ...       ...       ...    ...\n",
       "271 -0.200019  0.308827  0.035897  0.102653 -0.300474 -0.100670      1\n",
       "272 -0.237955  0.445253  0.580180 -0.045984 -0.564651  0.284254      0\n",
       "273 -0.086763  0.472739  0.059499  0.140871 -0.463131  0.242721      1\n",
       "274 -0.249495  0.243281  0.210798 -0.093395 -0.358696 -0.029523      1\n",
       "275 -0.258901  0.405626  0.064267  0.012619 -0.433631 -0.109848      1\n",
       "\n",
       "[276 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df[[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\"]].values\n",
    "y_train = train_df[[\"label\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df[[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\"]].values\n",
    "y_test = test_df[[\"label\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 10000\n",
    "BATCH_SIZE = 16\n",
    "INPUT_SIZE = 6\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini-Batch\n",
    "torch_dataset = Data.TensorDataset(x_train,y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN,self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(6,32)\n",
    "        self.hidden2 = torch.nn.Linear(32,16)\n",
    "        self.hidden3 = torch.nn.Linear(16,3)\n",
    "    def forward(self,x):\n",
    "        h1_out = self.hidden1(x)\n",
    "        h2_out = self.hidden2(h1_out)\n",
    "        h3_out = self.hidden3(h2_out)\n",
    "        return h3_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (hidden1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (hidden2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (hidden3): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ann = ANN()\n",
    "print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.0000)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.9,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   10|Train Avg Loss: 1.1483 \n",
      "Epoch:    1|steps:   20|Train Avg Loss: 1.1472 \n",
      "Epoch:    1|steps:   30|Train Avg Loss: 1.1525 \n",
      "Epoch:    1|steps:   40|Train Avg Loss: 1.1497 \n",
      "Epoch:    1|steps:   50|Train Avg Loss: 1.1522 \n",
      "Epoch:    1|steps:   60|Train Avg Loss: 1.1439 \n",
      "Epoch:    1|steps:   70|Train Avg Loss: 1.1462 \n",
      "Epoch:    1|steps:   80|Train Avg Loss: 1.1501 \n",
      "Epoch:    1|steps:   90|Train Avg Loss: 1.1422 \n",
      "Epoch:    1|steps:  100|Train Avg Loss: 1.1453 \n",
      "Epoch:    1|steps:  110|Train Avg Loss: 1.1452 \n",
      "Epoch:    1|steps:  120|Train Avg Loss: 1.1445 \n",
      "Epoch:    2|steps:   10|Train Avg Loss: 1.1510 \n",
      "Epoch:    2|steps:   20|Train Avg Loss: 1.1487 \n",
      "Epoch:    2|steps:   30|Train Avg Loss: 1.1465 \n",
      "Epoch:    2|steps:   40|Train Avg Loss: 1.1447 \n",
      "Epoch:    2|steps:   50|Train Avg Loss: 1.1492 \n",
      "Epoch:    2|steps:   60|Train Avg Loss: 1.1450 \n",
      "Epoch:    2|steps:   70|Train Avg Loss: 1.1480 \n",
      "Epoch:    2|steps:   80|Train Avg Loss: 1.1465 \n",
      "Epoch:    2|steps:   90|Train Avg Loss: 1.1476 \n",
      "Epoch:    2|steps:  100|Train Avg Loss: 1.1444 \n",
      "Epoch:    2|steps:  110|Train Avg Loss: 1.1457 \n",
      "Epoch:    2|steps:  120|Train Avg Loss: 1.1487 \n",
      "Epoch:    3|steps:   10|Train Avg Loss: 1.1465 \n",
      "Epoch:    3|steps:   20|Train Avg Loss: 1.1481 \n",
      "Epoch:    3|steps:   30|Train Avg Loss: 1.1462 \n",
      "Epoch:    3|steps:   40|Train Avg Loss: 1.1466 \n",
      "Epoch:    3|steps:   50|Train Avg Loss: 1.1502 \n",
      "Epoch:    3|steps:   60|Train Avg Loss: 1.1477 \n",
      "Epoch:    3|steps:   70|Train Avg Loss: 1.1448 \n",
      "Epoch:    3|steps:   80|Train Avg Loss: 1.1456 \n",
      "Epoch:    3|steps:   90|Train Avg Loss: 1.1473 \n",
      "Epoch:    3|steps:  100|Train Avg Loss: 1.1479 \n",
      "Epoch:    3|steps:  110|Train Avg Loss: 1.1507 \n",
      "Epoch:    3|steps:  120|Train Avg Loss: 1.1479 \n",
      "Epoch:    4|steps:   10|Train Avg Loss: 1.1451 \n",
      "Epoch:    4|steps:   20|Train Avg Loss: 1.1508 \n",
      "Epoch:    4|steps:   30|Train Avg Loss: 1.1457 \n",
      "Epoch:    4|steps:   40|Train Avg Loss: 1.1479 \n",
      "Epoch:    4|steps:   50|Train Avg Loss: 1.1468 \n",
      "Epoch:    4|steps:   60|Train Avg Loss: 1.1499 \n",
      "Epoch:    4|steps:   70|Train Avg Loss: 1.1445 \n",
      "Epoch:    4|steps:   80|Train Avg Loss: 1.1441 \n",
      "Epoch:    4|steps:   90|Train Avg Loss: 1.1503 \n",
      "Epoch:    4|steps:  100|Train Avg Loss: 1.1491 \n",
      "Epoch:    4|steps:  110|Train Avg Loss: 1.1470 \n",
      "Epoch:    4|steps:  120|Train Avg Loss: 1.1450 \n",
      "Epoch:    5|steps:   10|Train Avg Loss: 1.1427 \n",
      "Epoch:    5|steps:   20|Train Avg Loss: 1.1477 \n",
      "Epoch:    5|steps:   30|Train Avg Loss: 1.1490 \n",
      "Epoch:    5|steps:   40|Train Avg Loss: 1.1512 \n",
      "Epoch:    5|steps:   50|Train Avg Loss: 1.1437 \n",
      "Epoch:    5|steps:   60|Train Avg Loss: 1.1491 \n",
      "Epoch:    5|steps:   70|Train Avg Loss: 1.1449 \n",
      "Epoch:    5|steps:   80|Train Avg Loss: 1.1489 \n",
      "Epoch:    5|steps:   90|Train Avg Loss: 1.1480 \n",
      "Epoch:    5|steps:  100|Train Avg Loss: 1.1476 \n",
      "Epoch:    5|steps:  110|Train Avg Loss: 1.1436 \n",
      "Epoch:    5|steps:  120|Train Avg Loss: 1.1481 \n",
      "Epoch:    6|steps:   10|Train Avg Loss: 1.1462 \n",
      "Epoch:    6|steps:   20|Train Avg Loss: 1.1479 \n",
      "Epoch:    6|steps:   30|Train Avg Loss: 1.1465 \n",
      "Epoch:    6|steps:   40|Train Avg Loss: 1.1449 \n",
      "Epoch:    6|steps:   50|Train Avg Loss: 1.1453 \n",
      "Epoch:    6|steps:   60|Train Avg Loss: 1.1484 \n",
      "Epoch:    6|steps:   70|Train Avg Loss: 1.1471 \n",
      "Epoch:    6|steps:   80|Train Avg Loss: 1.1465 \n",
      "Epoch:    6|steps:   90|Train Avg Loss: 1.1457 \n",
      "Epoch:    6|steps:  100|Train Avg Loss: 1.1503 \n",
      "Epoch:    6|steps:  110|Train Avg Loss: 1.1520 \n",
      "Epoch:    6|steps:  120|Train Avg Loss: 1.1489 \n",
      "Epoch:    7|steps:   10|Train Avg Loss: 1.1496 \n",
      "Epoch:    7|steps:   20|Train Avg Loss: 1.1447 \n",
      "Epoch:    7|steps:   30|Train Avg Loss: 1.1482 \n",
      "Epoch:    7|steps:   40|Train Avg Loss: 1.1420 \n",
      "Epoch:    7|steps:   50|Train Avg Loss: 1.1490 \n",
      "Epoch:    7|steps:   60|Train Avg Loss: 1.1462 \n",
      "Epoch:    7|steps:   70|Train Avg Loss: 1.1489 \n",
      "Epoch:    7|steps:   80|Train Avg Loss: 1.1469 \n",
      "Epoch:    7|steps:   90|Train Avg Loss: 1.1517 \n",
      "Epoch:    7|steps:  100|Train Avg Loss: 1.1448 \n",
      "Epoch:    7|steps:  110|Train Avg Loss: 1.1461 \n",
      "Epoch:    7|steps:  120|Train Avg Loss: 1.1479 \n",
      "Epoch:    8|steps:   10|Train Avg Loss: 1.1470 \n",
      "Epoch:    8|steps:   20|Train Avg Loss: 1.1435 \n",
      "Epoch:    8|steps:   30|Train Avg Loss: 1.1483 \n",
      "Epoch:    8|steps:   40|Train Avg Loss: 1.1474 \n",
      "Epoch:    8|steps:   50|Train Avg Loss: 1.1480 \n",
      "Epoch:    8|steps:   60|Train Avg Loss: 1.1470 \n",
      "Epoch:    8|steps:   70|Train Avg Loss: 1.1494 \n",
      "Epoch:    8|steps:   80|Train Avg Loss: 1.1489 \n",
      "Epoch:    8|steps:   90|Train Avg Loss: 1.1465 \n",
      "Epoch:    8|steps:  100|Train Avg Loss: 1.1440 \n",
      "Epoch:    8|steps:  110|Train Avg Loss: 1.1465 \n",
      "Epoch:    8|steps:  120|Train Avg Loss: 1.1492 \n",
      "Epoch:    9|steps:   10|Train Avg Loss: 1.1479 \n",
      "Epoch:    9|steps:   20|Train Avg Loss: 1.1467 \n",
      "Epoch:    9|steps:   30|Train Avg Loss: 1.1476 \n",
      "Epoch:    9|steps:   40|Train Avg Loss: 1.1463 \n",
      "Epoch:    9|steps:   50|Train Avg Loss: 1.1440 \n",
      "Epoch:    9|steps:   60|Train Avg Loss: 1.1507 \n",
      "Epoch:    9|steps:   70|Train Avg Loss: 1.1466 \n",
      "Epoch:    9|steps:   80|Train Avg Loss: 1.1451 \n",
      "Epoch:    9|steps:   90|Train Avg Loss: 1.1477 \n",
      "Epoch:    9|steps:  100|Train Avg Loss: 1.1502 \n",
      "Epoch:    9|steps:  110|Train Avg Loss: 1.1465 \n",
      "Epoch:    9|steps:  120|Train Avg Loss: 1.1493 \n",
      "Epoch:   10|steps:   10|Train Avg Loss: 1.1472 \n",
      "Epoch:   10|steps:   20|Train Avg Loss: 1.1461 \n",
      "Epoch:   10|steps:   30|Train Avg Loss: 1.1488 \n",
      "Epoch:   10|steps:   40|Train Avg Loss: 1.1454 \n",
      "Epoch:   10|steps:   50|Train Avg Loss: 1.1469 \n",
      "Epoch:   10|steps:   60|Train Avg Loss: 1.1509 \n",
      "Epoch:   10|steps:   70|Train Avg Loss: 1.1433 \n",
      "Epoch:   10|steps:   80|Train Avg Loss: 1.1464 \n",
      "Epoch:   10|steps:   90|Train Avg Loss: 1.1512 \n",
      "Epoch:   10|steps:  100|Train Avg Loss: 1.1447 \n",
      "Epoch:   10|steps:  110|Train Avg Loss: 1.1476 \n",
      "Epoch:   10|steps:  120|Train Avg Loss: 1.1508 \n",
      "Epoch:   11|steps:   10|Train Avg Loss: 1.1470 \n",
      "Epoch:   11|steps:   20|Train Avg Loss: 1.1438 \n",
      "Epoch:   11|steps:   30|Train Avg Loss: 1.1468 \n",
      "Epoch:   11|steps:   40|Train Avg Loss: 1.1498 \n",
      "Epoch:   11|steps:   50|Train Avg Loss: 1.1482 \n",
      "Epoch:   11|steps:   60|Train Avg Loss: 1.1495 \n",
      "Epoch:   11|steps:   70|Train Avg Loss: 1.1476 \n",
      "Epoch:   11|steps:   80|Train Avg Loss: 1.1472 \n",
      "Epoch:   11|steps:   90|Train Avg Loss: 1.1451 \n",
      "Epoch:   11|steps:  100|Train Avg Loss: 1.1472 \n",
      "Epoch:   11|steps:  110|Train Avg Loss: 1.1458 \n",
      "Epoch:   11|steps:  120|Train Avg Loss: 1.1503 \n",
      "Epoch:   12|steps:   10|Train Avg Loss: 1.1483 \n",
      "Epoch:   12|steps:   20|Train Avg Loss: 1.1467 \n",
      "Epoch:   12|steps:   30|Train Avg Loss: 1.1438 \n",
      "Epoch:   12|steps:   40|Train Avg Loss: 1.1489 \n",
      "Epoch:   12|steps:   50|Train Avg Loss: 1.1491 \n",
      "Epoch:   12|steps:   60|Train Avg Loss: 1.1466 \n",
      "Epoch:   12|steps:   70|Train Avg Loss: 1.1471 \n",
      "Epoch:   12|steps:   80|Train Avg Loss: 1.1502 \n",
      "Epoch:   12|steps:   90|Train Avg Loss: 1.1438 \n",
      "Epoch:   12|steps:  100|Train Avg Loss: 1.1475 \n",
      "Epoch:   12|steps:  110|Train Avg Loss: 1.1458 \n",
      "Epoch:   12|steps:  120|Train Avg Loss: 1.1506 \n",
      "Epoch:   13|steps:   10|Train Avg Loss: 1.1494 \n",
      "Epoch:   13|steps:   20|Train Avg Loss: 1.1478 \n",
      "Epoch:   13|steps:   30|Train Avg Loss: 1.1528 \n",
      "Epoch:   13|steps:   40|Train Avg Loss: 1.1439 \n",
      "Epoch:   13|steps:   50|Train Avg Loss: 1.1514 \n",
      "Epoch:   13|steps:   60|Train Avg Loss: 1.1481 \n",
      "Epoch:   13|steps:   70|Train Avg Loss: 1.1463 \n",
      "Epoch:   13|steps:   80|Train Avg Loss: 1.1451 \n",
      "Epoch:   13|steps:   90|Train Avg Loss: 1.1488 \n",
      "Epoch:   13|steps:  100|Train Avg Loss: 1.1449 \n",
      "Epoch:   13|steps:  110|Train Avg Loss: 1.1477 \n",
      "Epoch:   13|steps:  120|Train Avg Loss: 1.1452 \n",
      "Epoch:   14|steps:   10|Train Avg Loss: 1.1475 \n",
      "Epoch:   14|steps:   20|Train Avg Loss: 1.1470 \n",
      "Epoch:   14|steps:   30|Train Avg Loss: 1.1496 \n",
      "Epoch:   14|steps:   40|Train Avg Loss: 1.1514 \n",
      "Epoch:   14|steps:   50|Train Avg Loss: 1.1490 \n",
      "Epoch:   14|steps:   60|Train Avg Loss: 1.1430 \n",
      "Epoch:   14|steps:   70|Train Avg Loss: 1.1464 \n",
      "Epoch:   14|steps:   80|Train Avg Loss: 1.1446 \n",
      "Epoch:   14|steps:   90|Train Avg Loss: 1.1477 \n",
      "Epoch:   14|steps:  100|Train Avg Loss: 1.1436 \n",
      "Epoch:   14|steps:  110|Train Avg Loss: 1.1486 \n",
      "Epoch:   14|steps:  120|Train Avg Loss: 1.1504 \n",
      "Epoch:   15|steps:   10|Train Avg Loss: 1.1500 \n",
      "Epoch:   15|steps:   20|Train Avg Loss: 1.1457 \n",
      "Epoch:   15|steps:   30|Train Avg Loss: 1.1519 \n",
      "Epoch:   15|steps:   40|Train Avg Loss: 1.1442 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   15|steps:   50|Train Avg Loss: 1.1478 \n",
      "Epoch:   15|steps:   60|Train Avg Loss: 1.1461 \n",
      "Epoch:   15|steps:   70|Train Avg Loss: 1.1461 \n",
      "Epoch:   15|steps:   80|Train Avg Loss: 1.1461 \n",
      "Epoch:   15|steps:   90|Train Avg Loss: 1.1493 \n",
      "Epoch:   15|steps:  100|Train Avg Loss: 1.1478 \n",
      "Epoch:   15|steps:  110|Train Avg Loss: 1.1464 \n",
      "Epoch:   15|steps:  120|Train Avg Loss: 1.1447 \n",
      "Epoch:   16|steps:   10|Train Avg Loss: 1.1486 \n",
      "Epoch:   16|steps:   20|Train Avg Loss: 1.1454 \n",
      "Epoch:   16|steps:   30|Train Avg Loss: 1.1487 \n",
      "Epoch:   16|steps:   40|Train Avg Loss: 1.1479 \n",
      "Epoch:   16|steps:   50|Train Avg Loss: 1.1427 \n",
      "Epoch:   16|steps:   60|Train Avg Loss: 1.1481 \n",
      "Epoch:   16|steps:   70|Train Avg Loss: 1.1452 \n",
      "Epoch:   16|steps:   80|Train Avg Loss: 1.1471 \n",
      "Epoch:   16|steps:   90|Train Avg Loss: 1.1449 \n",
      "Epoch:   16|steps:  100|Train Avg Loss: 1.1477 \n",
      "Epoch:   16|steps:  110|Train Avg Loss: 1.1509 \n",
      "Epoch:   16|steps:  120|Train Avg Loss: 1.1459 \n",
      "Epoch:   17|steps:   10|Train Avg Loss: 1.1442 \n",
      "Epoch:   17|steps:   20|Train Avg Loss: 1.1447 \n",
      "Epoch:   17|steps:   30|Train Avg Loss: 1.1511 \n",
      "Epoch:   17|steps:   40|Train Avg Loss: 1.1463 \n",
      "Epoch:   17|steps:   50|Train Avg Loss: 1.1441 \n",
      "Epoch:   17|steps:   60|Train Avg Loss: 1.1489 \n",
      "Epoch:   17|steps:   70|Train Avg Loss: 1.1483 \n",
      "Epoch:   17|steps:   80|Train Avg Loss: 1.1451 \n",
      "Epoch:   17|steps:   90|Train Avg Loss: 1.1447 \n",
      "Epoch:   17|steps:  100|Train Avg Loss: 1.1487 \n",
      "Epoch:   17|steps:  110|Train Avg Loss: 1.1475 \n",
      "Epoch:   17|steps:  120|Train Avg Loss: 1.1517 \n",
      "Epoch:   18|steps:   10|Train Avg Loss: 1.1465 \n",
      "Epoch:   18|steps:   20|Train Avg Loss: 1.1480 \n",
      "Epoch:   18|steps:   30|Train Avg Loss: 1.1487 \n",
      "Epoch:   18|steps:   40|Train Avg Loss: 1.1427 \n",
      "Epoch:   18|steps:   50|Train Avg Loss: 1.1477 \n",
      "Epoch:   18|steps:   60|Train Avg Loss: 1.1449 \n",
      "Epoch:   18|steps:   70|Train Avg Loss: 1.1452 \n",
      "Epoch:   18|steps:   80|Train Avg Loss: 1.1475 \n",
      "Epoch:   18|steps:   90|Train Avg Loss: 1.1492 \n",
      "Epoch:   18|steps:  100|Train Avg Loss: 1.1513 \n",
      "Epoch:   18|steps:  110|Train Avg Loss: 1.1500 \n",
      "Epoch:   18|steps:  120|Train Avg Loss: 1.1483 \n",
      "Epoch:   19|steps:   10|Train Avg Loss: 1.1491 \n",
      "Epoch:   19|steps:   20|Train Avg Loss: 1.1476 \n",
      "Epoch:   19|steps:   30|Train Avg Loss: 1.1481 \n",
      "Epoch:   19|steps:   40|Train Avg Loss: 1.1471 \n",
      "Epoch:   19|steps:   50|Train Avg Loss: 1.1435 \n",
      "Epoch:   19|steps:   60|Train Avg Loss: 1.1444 \n",
      "Epoch:   19|steps:   70|Train Avg Loss: 1.1506 \n",
      "Epoch:   19|steps:   80|Train Avg Loss: 1.1508 \n",
      "Epoch:   19|steps:   90|Train Avg Loss: 1.1449 \n",
      "Epoch:   19|steps:  100|Train Avg Loss: 1.1461 \n",
      "Epoch:   19|steps:  110|Train Avg Loss: 1.1484 \n",
      "Epoch:   19|steps:  120|Train Avg Loss: 1.1471 \n",
      "Epoch:   20|steps:   10|Train Avg Loss: 1.1508 \n",
      "Epoch:   20|steps:   20|Train Avg Loss: 1.1489 \n",
      "Epoch:   20|steps:   30|Train Avg Loss: 1.1499 \n",
      "Epoch:   20|steps:   40|Train Avg Loss: 1.1450 \n",
      "Epoch:   20|steps:   50|Train Avg Loss: 1.1473 \n",
      "Epoch:   20|steps:   60|Train Avg Loss: 1.1464 \n",
      "Epoch:   20|steps:   70|Train Avg Loss: 1.1455 \n",
      "Epoch:   20|steps:   80|Train Avg Loss: 1.1487 \n",
      "Epoch:   20|steps:   90|Train Avg Loss: 1.1443 \n",
      "Epoch:   20|steps:  100|Train Avg Loss: 1.1501 \n",
      "Epoch:   20|steps:  110|Train Avg Loss: 1.1447 \n",
      "Epoch:   20|steps:  120|Train Avg Loss: 1.1472 \n",
      "Epoch:   21|steps:   10|Train Avg Loss: 1.1467 \n",
      "Epoch:   21|steps:   20|Train Avg Loss: 1.1482 \n",
      "Epoch:   21|steps:   30|Train Avg Loss: 1.1487 \n",
      "Epoch:   21|steps:   40|Train Avg Loss: 1.1498 \n",
      "Epoch:   21|steps:   50|Train Avg Loss: 1.1483 \n",
      "Epoch:   21|steps:   60|Train Avg Loss: 1.1487 \n",
      "Epoch:   21|steps:   70|Train Avg Loss: 1.1449 \n",
      "Epoch:   21|steps:   80|Train Avg Loss: 1.1442 \n",
      "Epoch:   21|steps:   90|Train Avg Loss: 1.1469 \n",
      "Epoch:   21|steps:  100|Train Avg Loss: 1.1497 \n",
      "Epoch:   21|steps:  110|Train Avg Loss: 1.1471 \n",
      "Epoch:   21|steps:  120|Train Avg Loss: 1.1459 \n",
      "Epoch:   22|steps:   10|Train Avg Loss: 1.1464 \n",
      "Epoch:   22|steps:   20|Train Avg Loss: 1.1471 \n",
      "Epoch:   22|steps:   30|Train Avg Loss: 1.1471 \n",
      "Epoch:   22|steps:   40|Train Avg Loss: 1.1477 \n",
      "Epoch:   22|steps:   50|Train Avg Loss: 1.1498 \n",
      "Epoch:   22|steps:   60|Train Avg Loss: 1.1460 \n",
      "Epoch:   22|steps:   70|Train Avg Loss: 1.1458 \n",
      "Epoch:   22|steps:   80|Train Avg Loss: 1.1497 \n",
      "Epoch:   22|steps:   90|Train Avg Loss: 1.1453 \n",
      "Epoch:   22|steps:  100|Train Avg Loss: 1.1448 \n",
      "Epoch:   22|steps:  110|Train Avg Loss: 1.1487 \n",
      "Epoch:   22|steps:  120|Train Avg Loss: 1.1492 \n",
      "Epoch:   23|steps:   10|Train Avg Loss: 1.1516 \n",
      "Epoch:   23|steps:   20|Train Avg Loss: 1.1489 \n",
      "Epoch:   23|steps:   30|Train Avg Loss: 1.1473 \n",
      "Epoch:   23|steps:   40|Train Avg Loss: 1.1478 \n",
      "Epoch:   23|steps:   50|Train Avg Loss: 1.1468 \n",
      "Epoch:   23|steps:   60|Train Avg Loss: 1.1505 \n",
      "Epoch:   23|steps:   70|Train Avg Loss: 1.1443 \n",
      "Epoch:   23|steps:   80|Train Avg Loss: 1.1464 \n",
      "Epoch:   23|steps:   90|Train Avg Loss: 1.1470 \n",
      "Epoch:   23|steps:  100|Train Avg Loss: 1.1427 \n",
      "Epoch:   23|steps:  110|Train Avg Loss: 1.1491 \n",
      "Epoch:   23|steps:  120|Train Avg Loss: 1.1444 \n",
      "Epoch:   24|steps:   10|Train Avg Loss: 1.1461 \n",
      "Epoch:   24|steps:   20|Train Avg Loss: 1.1485 \n",
      "Epoch:   24|steps:   30|Train Avg Loss: 1.1504 \n",
      "Epoch:   24|steps:   40|Train Avg Loss: 1.1434 \n",
      "Epoch:   24|steps:   50|Train Avg Loss: 1.1474 \n",
      "Epoch:   24|steps:   60|Train Avg Loss: 1.1485 \n",
      "Epoch:   24|steps:   70|Train Avg Loss: 1.1469 \n",
      "Epoch:   24|steps:   80|Train Avg Loss: 1.1481 \n",
      "Epoch:   24|steps:   90|Train Avg Loss: 1.1505 \n",
      "Epoch:   24|steps:  100|Train Avg Loss: 1.1458 \n",
      "Epoch:   24|steps:  110|Train Avg Loss: 1.1507 \n",
      "Epoch:   24|steps:  120|Train Avg Loss: 1.1429 \n",
      "Epoch:   25|steps:   10|Train Avg Loss: 1.1472 \n",
      "Epoch:   25|steps:   20|Train Avg Loss: 1.1520 \n",
      "Epoch:   25|steps:   30|Train Avg Loss: 1.1472 \n",
      "Epoch:   25|steps:   40|Train Avg Loss: 1.1441 \n",
      "Epoch:   25|steps:   50|Train Avg Loss: 1.1463 \n",
      "Epoch:   25|steps:   60|Train Avg Loss: 1.1472 \n",
      "Epoch:   25|steps:   70|Train Avg Loss: 1.1482 \n",
      "Epoch:   25|steps:   80|Train Avg Loss: 1.1462 \n",
      "Epoch:   25|steps:   90|Train Avg Loss: 1.1464 \n",
      "Epoch:   25|steps:  100|Train Avg Loss: 1.1480 \n",
      "Epoch:   25|steps:  110|Train Avg Loss: 1.1469 \n",
      "Epoch:   25|steps:  120|Train Avg Loss: 1.1465 \n",
      "Epoch:   26|steps:   10|Train Avg Loss: 1.1489 \n",
      "Epoch:   26|steps:   20|Train Avg Loss: 1.1468 \n",
      "Epoch:   26|steps:   30|Train Avg Loss: 1.1477 \n",
      "Epoch:   26|steps:   40|Train Avg Loss: 1.1455 \n",
      "Epoch:   26|steps:   50|Train Avg Loss: 1.1522 \n",
      "Epoch:   26|steps:   60|Train Avg Loss: 1.1486 \n",
      "Epoch:   26|steps:   70|Train Avg Loss: 1.1463 \n",
      "Epoch:   26|steps:   80|Train Avg Loss: 1.1478 \n",
      "Epoch:   26|steps:   90|Train Avg Loss: 1.1449 \n",
      "Epoch:   26|steps:  100|Train Avg Loss: 1.1477 \n",
      "Epoch:   26|steps:  110|Train Avg Loss: 1.1437 \n",
      "Epoch:   26|steps:  120|Train Avg Loss: 1.1478 \n",
      "Epoch:   27|steps:   10|Train Avg Loss: 1.1477 \n",
      "Epoch:   27|steps:   20|Train Avg Loss: 1.1499 \n",
      "Epoch:   27|steps:   30|Train Avg Loss: 1.1471 \n",
      "Epoch:   27|steps:   40|Train Avg Loss: 1.1501 \n",
      "Epoch:   27|steps:   50|Train Avg Loss: 1.1488 \n",
      "Epoch:   27|steps:   60|Train Avg Loss: 1.1439 \n",
      "Epoch:   27|steps:   70|Train Avg Loss: 1.1412 \n",
      "Epoch:   27|steps:   80|Train Avg Loss: 1.1494 \n",
      "Epoch:   27|steps:   90|Train Avg Loss: 1.1460 \n",
      "Epoch:   27|steps:  100|Train Avg Loss: 1.1473 \n",
      "Epoch:   27|steps:  110|Train Avg Loss: 1.1471 \n",
      "Epoch:   27|steps:  120|Train Avg Loss: 1.1490 \n",
      "Epoch:   28|steps:   10|Train Avg Loss: 1.1479 \n",
      "Epoch:   28|steps:   20|Train Avg Loss: 1.1432 \n",
      "Epoch:   28|steps:   30|Train Avg Loss: 1.1527 \n",
      "Epoch:   28|steps:   40|Train Avg Loss: 1.1480 \n",
      "Epoch:   28|steps:   50|Train Avg Loss: 1.1471 \n",
      "Epoch:   28|steps:   60|Train Avg Loss: 1.1519 \n",
      "Epoch:   28|steps:   70|Train Avg Loss: 1.1464 \n",
      "Epoch:   28|steps:   80|Train Avg Loss: 1.1465 \n",
      "Epoch:   28|steps:   90|Train Avg Loss: 1.1484 \n",
      "Epoch:   28|steps:  100|Train Avg Loss: 1.1468 \n",
      "Epoch:   28|steps:  110|Train Avg Loss: 1.1437 \n",
      "Epoch:   28|steps:  120|Train Avg Loss: 1.1437 \n",
      "Epoch:   29|steps:   10|Train Avg Loss: 1.1470 \n",
      "Epoch:   29|steps:   20|Train Avg Loss: 1.1469 \n",
      "Epoch:   29|steps:   30|Train Avg Loss: 1.1489 \n",
      "Epoch:   29|steps:   40|Train Avg Loss: 1.1460 \n",
      "Epoch:   29|steps:   50|Train Avg Loss: 1.1519 \n",
      "Epoch:   29|steps:   60|Train Avg Loss: 1.1520 \n",
      "Epoch:   29|steps:   70|Train Avg Loss: 1.1470 \n",
      "Epoch:   29|steps:   80|Train Avg Loss: 1.1449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   29|steps:   90|Train Avg Loss: 1.1452 \n",
      "Epoch:   29|steps:  100|Train Avg Loss: 1.1449 \n",
      "Epoch:   29|steps:  110|Train Avg Loss: 1.1450 \n",
      "Epoch:   29|steps:  120|Train Avg Loss: 1.1512 \n",
      "Epoch:   30|steps:   10|Train Avg Loss: 1.1465 \n",
      "Epoch:   30|steps:   20|Train Avg Loss: 1.1498 \n",
      "Epoch:   30|steps:   30|Train Avg Loss: 1.1471 \n",
      "Epoch:   30|steps:   40|Train Avg Loss: 1.1446 \n",
      "Epoch:   30|steps:   50|Train Avg Loss: 1.1490 \n",
      "Epoch:   30|steps:   60|Train Avg Loss: 1.1486 \n",
      "Epoch:   30|steps:   70|Train Avg Loss: 1.1467 \n",
      "Epoch:   30|steps:   80|Train Avg Loss: 1.1504 \n",
      "Epoch:   30|steps:   90|Train Avg Loss: 1.1514 \n",
      "Epoch:   30|steps:  100|Train Avg Loss: 1.1442 \n",
      "Epoch:   30|steps:  110|Train Avg Loss: 1.1426 \n",
      "Epoch:   30|steps:  120|Train Avg Loss: 1.1446 \n",
      "Epoch:   31|steps:   10|Train Avg Loss: 1.1497 \n",
      "Epoch:   31|steps:   20|Train Avg Loss: 1.1407 \n",
      "Epoch:   31|steps:   30|Train Avg Loss: 1.1479 \n",
      "Epoch:   31|steps:   40|Train Avg Loss: 1.1478 \n",
      "Epoch:   31|steps:   50|Train Avg Loss: 1.1447 \n",
      "Epoch:   31|steps:   60|Train Avg Loss: 1.1474 \n",
      "Epoch:   31|steps:   70|Train Avg Loss: 1.1488 \n",
      "Epoch:   31|steps:   80|Train Avg Loss: 1.1466 \n",
      "Epoch:   31|steps:   90|Train Avg Loss: 1.1496 \n",
      "Epoch:   31|steps:  100|Train Avg Loss: 1.1459 \n",
      "Epoch:   31|steps:  110|Train Avg Loss: 1.1497 \n",
      "Epoch:   31|steps:  120|Train Avg Loss: 1.1498 \n",
      "Epoch:   32|steps:   10|Train Avg Loss: 1.1487 \n",
      "Epoch:   32|steps:   20|Train Avg Loss: 1.1503 \n",
      "Epoch:   32|steps:   30|Train Avg Loss: 1.1484 \n",
      "Epoch:   32|steps:   40|Train Avg Loss: 1.1490 \n",
      "Epoch:   32|steps:   50|Train Avg Loss: 1.1420 \n",
      "Epoch:   32|steps:   60|Train Avg Loss: 1.1482 \n",
      "Epoch:   32|steps:   70|Train Avg Loss: 1.1443 \n",
      "Epoch:   32|steps:   80|Train Avg Loss: 1.1479 \n",
      "Epoch:   32|steps:   90|Train Avg Loss: 1.1477 \n",
      "Epoch:   32|steps:  100|Train Avg Loss: 1.1462 \n",
      "Epoch:   32|steps:  110|Train Avg Loss: 1.1506 \n",
      "Epoch:   32|steps:  120|Train Avg Loss: 1.1469 \n",
      "Epoch:   33|steps:   10|Train Avg Loss: 1.1430 \n",
      "Epoch:   33|steps:   20|Train Avg Loss: 1.1424 \n",
      "Epoch:   33|steps:   30|Train Avg Loss: 1.1521 \n",
      "Epoch:   33|steps:   40|Train Avg Loss: 1.1488 \n",
      "Epoch:   33|steps:   50|Train Avg Loss: 1.1444 \n",
      "Epoch:   33|steps:   60|Train Avg Loss: 1.1480 \n",
      "Epoch:   33|steps:   70|Train Avg Loss: 1.1476 \n",
      "Epoch:   33|steps:   80|Train Avg Loss: 1.1471 \n",
      "Epoch:   33|steps:   90|Train Avg Loss: 1.1462 \n",
      "Epoch:   33|steps:  100|Train Avg Loss: 1.1488 \n",
      "Epoch:   33|steps:  110|Train Avg Loss: 1.1467 \n",
      "Epoch:   33|steps:  120|Train Avg Loss: 1.1521 \n",
      "Epoch:   34|steps:   10|Train Avg Loss: 1.1491 \n",
      "Epoch:   34|steps:   20|Train Avg Loss: 1.1472 \n",
      "Epoch:   34|steps:   30|Train Avg Loss: 1.1444 \n",
      "Epoch:   34|steps:   40|Train Avg Loss: 1.1472 \n",
      "Epoch:   34|steps:   50|Train Avg Loss: 1.1467 \n",
      "Epoch:   34|steps:   60|Train Avg Loss: 1.1489 \n",
      "Epoch:   34|steps:   70|Train Avg Loss: 1.1482 \n",
      "Epoch:   34|steps:   80|Train Avg Loss: 1.1450 \n",
      "Epoch:   34|steps:   90|Train Avg Loss: 1.1499 \n",
      "Epoch:   34|steps:  100|Train Avg Loss: 1.1463 \n",
      "Epoch:   34|steps:  110|Train Avg Loss: 1.1472 \n",
      "Epoch:   34|steps:  120|Train Avg Loss: 1.1463 \n",
      "Epoch:   35|steps:   10|Train Avg Loss: 1.1500 \n",
      "Epoch:   35|steps:   20|Train Avg Loss: 1.1488 \n",
      "Epoch:   35|steps:   30|Train Avg Loss: 1.1464 \n",
      "Epoch:   35|steps:   40|Train Avg Loss: 1.1502 \n",
      "Epoch:   35|steps:   50|Train Avg Loss: 1.1459 \n",
      "Epoch:   35|steps:   60|Train Avg Loss: 1.1502 \n",
      "Epoch:   35|steps:   70|Train Avg Loss: 1.1461 \n",
      "Epoch:   35|steps:   80|Train Avg Loss: 1.1440 \n",
      "Epoch:   35|steps:   90|Train Avg Loss: 1.1455 \n",
      "Epoch:   35|steps:  100|Train Avg Loss: 1.1439 \n",
      "Epoch:   35|steps:  110|Train Avg Loss: 1.1476 \n",
      "Epoch:   35|steps:  120|Train Avg Loss: 1.1495 \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "       \n",
    "         # start trainnig \n",
    "        output = ann(inputs)\n",
    "        \n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,targets)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 10 steps\n",
    "        if((step+1) %10 == 0):\n",
    "            # average of loss in 10 steps\n",
    "            avg = loss_total / 10\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f \"%(epoch+1,step+1,avg))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    #scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
