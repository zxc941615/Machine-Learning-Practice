{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "stock_path = \"C:/Users/acer/Desktop/LAB/lab_2.csv\"\n",
    "stock_df = pd.read_csv(stock_path)\n",
    "stock_df=stock_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 12)\n",
      "(238, 12)\n"
     ]
    }
   ],
   "source": [
    "train_set = stock_df[:2200]\n",
    "test_set = stock_df[2200:]\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K value</th>\n",
       "      <th>D value</th>\n",
       "      <th>MACD</th>\n",
       "      <th>William</th>\n",
       "      <th>SMA</th>\n",
       "      <th>WMA</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>RSI</th>\n",
       "      <th>A/D 0</th>\n",
       "      <th>CCI</th>\n",
       "      <th>Close r/f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.478459</td>\n",
       "      <td>0.494744</td>\n",
       "      <td>0.515782</td>\n",
       "      <td>0.515950</td>\n",
       "      <td>0.425581</td>\n",
       "      <td>0.426475</td>\n",
       "      <td>0.464374</td>\n",
       "      <td>0.492458</td>\n",
       "      <td>0.445859</td>\n",
       "      <td>0.463808</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.242761</td>\n",
       "      <td>0.231248</td>\n",
       "      <td>0.128572</td>\n",
       "      <td>0.292113</td>\n",
       "      <td>0.297391</td>\n",
       "      <td>0.296803</td>\n",
       "      <td>0.101128</td>\n",
       "      <td>0.251304</td>\n",
       "      <td>0.278231</td>\n",
       "      <td>0.159410</td>\n",
       "      <td>0.964113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.272646</td>\n",
       "      <td>0.306148</td>\n",
       "      <td>0.453617</td>\n",
       "      <td>0.258398</td>\n",
       "      <td>0.180373</td>\n",
       "      <td>0.181665</td>\n",
       "      <td>0.419330</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>0.208334</td>\n",
       "      <td>0.351748</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.469031</td>\n",
       "      <td>0.482729</td>\n",
       "      <td>0.512733</td>\n",
       "      <td>0.520003</td>\n",
       "      <td>0.364709</td>\n",
       "      <td>0.364416</td>\n",
       "      <td>0.461561</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.415185</td>\n",
       "      <td>0.467025</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.691137</td>\n",
       "      <td>0.682273</td>\n",
       "      <td>0.585561</td>\n",
       "      <td>0.777773</td>\n",
       "      <td>0.712705</td>\n",
       "      <td>0.712135</td>\n",
       "      <td>0.514771</td>\n",
       "      <td>0.687486</td>\n",
       "      <td>0.687456</td>\n",
       "      <td>0.572115</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          K value       D value         MACD      William          SMA  \\\n",
       "count  2200.000000  2200.000000  2200.000000  2200.000000  2200.000000   \n",
       "mean      0.478459     0.494744     0.515782     0.515950     0.425581   \n",
       "std       0.242761     0.231248     0.128572     0.292113     0.297391   \n",
       "min       0.004943     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.272646     0.306148     0.453617     0.258398     0.180373   \n",
       "50%       0.469031     0.482729     0.512733     0.520003     0.364709   \n",
       "75%       0.691137     0.682273     0.585561     0.777773     0.712705   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               WMA     Momentum          RSI        A/D 0          CCI  \\\n",
       "count  2200.000000  2200.000000  2200.000000  2200.000000  2200.000000   \n",
       "mean      0.426475     0.464374     0.492458     0.445859     0.463808   \n",
       "std       0.296803     0.101128     0.251304     0.278231     0.159410   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.181665     0.419330     0.292431     0.208334     0.351748   \n",
       "50%       0.364416     0.461561     0.500000     0.415185     0.467025   \n",
       "75%       0.712135     0.514771     0.687486     0.687456     0.572115   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         Close r/f  \n",
       "count  2200.000000  \n",
       "mean      1.000000  \n",
       "std       0.964113  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       1.000000  \n",
       "75%       2.000000  \n",
       "max       2.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every features in range(0 , 1)\n",
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Rise: 46.4545\n",
      "Train set Flat: 7.0909\n",
      "Train set Fall: 46.4545\n",
      "Test set Rise: 49.5798\n",
      "Test set Flat: 2.5210\n",
      "Test set Fall: 47.8992\n"
     ]
    }
   ],
   "source": [
    "# print the number of rise , flat , fall data\n",
    "print(\"Train set Rise: %.4f\"%(len(train_set[train_set[\"Close r/f\"] == 2])*100 / len(train_set)))\n",
    "print(\"Train set Flat: %.4f\"%(len(train_set[train_set[\"Close r/f\"] == 1])*100 / len(train_set)))\n",
    "print(\"Train set Fall: %.4f\"%(len(train_set[train_set[\"Close r/f\"] == 0])*100 / len(train_set)))\n",
    "print(\"Test set Rise: %.4f\"%(len(test_set[test_set[\"Close r/f\"] == 2])*100 / len(test_set)))\n",
    "print(\"Test set Flat: %.4f\"%(len(test_set[test_set[\"Close r/f\"] == 1])*100 / len(test_set)))\n",
    "print(\"Test set Fall: %.4f\"%(len(test_set[test_set[\"Close r/f\"] == 0])*100 / len(test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of feature:  torch.Size([2170, 30, 10])\n",
      "size of label:  torch.Size([2170])\n"
     ]
    }
   ],
   "source": [
    "# declear training features data\n",
    "features = []\n",
    "for i in range(30,len(train_set)):\n",
    "    x = train_set[i-30:i][[\"K value \",\"D value\",\"William\",\"MACD\",\"SMA\",\"WMA\",\"Momentum\",\"RSI\",\"A/D 0\",\"CCI\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = torch.FloatTensor(features)\n",
    "print(\"size of feature: \",features.size())\n",
    "\n",
    "# declear trainging labels data\n",
    "labels = []\n",
    "for i in range(31,len(train_set)+1):\n",
    "    x = train_set[i-1:i][\"Close r/f\"]\n",
    "    labels.append(x.tolist())\n",
    "labels = torch.LongTensor(labels).view(-1)\n",
    "print(\"size of label: \",labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test features:  torch.Size([208, 30, 10])\n",
      "size of test labels:  torch.Size([208])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#delcear testing features and labels\n",
    "\n",
    "test_features = []\n",
    "for i in range(30,len(test_set)):\n",
    "    x = test_set[i-30:i][[\"K value \",\"D value\",\"William\",\"MACD\",\"SMA\",\"WMA\",\"Momentum\",\"RSI\",\"A/D 0\",\"CCI\"]].values\n",
    "    test_features.append(x.tolist())\n",
    "test_features = torch.FloatTensor(test_features)\n",
    "print(\"size of test features: \",test_features.size())\n",
    "\n",
    "test_labels = []\n",
    "for i in range(31,len(test_set)+1):\n",
    "    x = test_set[i-1:i][\"Close r/f\"]\n",
    "    test_labels.append(x.tolist())\n",
    "test_labels = torch.FloatTensor(test_labels).view(-1)\n",
    "print(\"size of test labels: \",test_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAE(features,labels,test_features,test_labels):\n",
    "    #hy parameter\n",
    "    EPOCH = 20\n",
    "    BATCH_SIZE = 64\n",
    "    TIME_STEP = 30\n",
    "    INPUT_SIZE = 10\n",
    "    LR = 0.001\n",
    "    \n",
    "    #Mini-Batch\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(features,labels)\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset = torch_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        num_workers = 2,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    class autoencoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(autoencoder, self).__init__()\n",
    "            self.encoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(TIME_STEP * INPUT_SIZE, 64),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Linear(64, 12), \n",
    "                torch.nn.ReLU(True), \n",
    "                torch.nn.Linear(12, 3))\n",
    "            self.decoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(3, 12),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Linear(12, 64),\n",
    "                torch.nn.ReLU(True), \n",
    "                torch.nn.Linear(64, TIME_STEP * INPUT_SIZE), \n",
    "                torch.nn.Tanh())\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.encoder(x)\n",
    "            x = self.decoder(x)\n",
    "            return x\n",
    "    model = autoencoder()\n",
    "    print(model)\n",
    "    \n",
    "    # define optimizer and loss function \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.0005)\n",
    "    # adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.9,min_lr=0.0001)\n",
    "    # crossentroy loss \n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    '''\n",
    "    Training \n",
    "    '''\n",
    "    LOSS = []\n",
    "    TEST_LOSS = []\n",
    "    for epoch in range(EPOCH):\n",
    "        loss_total = 0\n",
    "        for step,(inputs,targets) in enumerate(train_loader):\n",
    "            inputs = inputs.view(-1,TIME_STEP * INPUT_SIZE)\n",
    "            #reshape the features to (batch,time_step*input_size)\n",
    "            inputs_n = inputs.view(-1,TIME_STEP * INPUT_SIZE)\n",
    "\n",
    "\n",
    "            inputs_n =  inputs_n + torch.randn(TIME_STEP * INPUT_SIZE)*0.5\n",
    "            # Clip the images to be between 0 and 1\n",
    "            inputs_n = np.clip( inputs_n, 0., 1.)\n",
    "\n",
    "\n",
    "            # start trainnig \n",
    "            output = model(inputs_n)\n",
    "            # calculate loss  (cross entroy)\n",
    "            loss = loss_func(output,inputs)\n",
    "            # clear the gradients of all optimized variables(from last training)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # sum of loss\n",
    "            loss_total = loss_total + loss\n",
    "\n",
    "            # print training info every 10 steps\n",
    "            if((step+1) %10 == 0):\n",
    "                # average of loss in 10 steps\n",
    "                avg = loss_total / 10\n",
    "                LOSS.append(avg.tolist())\n",
    "\n",
    "                # test data\n",
    "                test_features = torch.Tensor(test_features).view(-1,30*10)\n",
    "                out = model(test_features)\n",
    "                test_loss = loss_func(out,test_features)\n",
    "                TEST_LOSS.append(test_loss.tolist())\n",
    "                # print the epoch , steps , average loss , accuracy \n",
    "                print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f |Test Loss: %4f\"\n",
    "                      %(epoch+1,step+1,avg,test_loss))\n",
    "\n",
    "                # inital variable\n",
    "                loss_total = 0\n",
    "        # updata learning rate\n",
    "        #scheduler.step(loss)\n",
    "        \n",
    "    x = np.linspace(1,60,60)\n",
    "    LOSS = np.array(LOSS)\n",
    "    TEST_LOSS = np.array(TEST_LOSS)\n",
    "    plt.plot(x,LOSS)\n",
    "    plt.xlabel(\"steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Denoise autoencoder\")\n",
    "    plt.plot(x,TEST_LOSS)\n",
    "    plt.show()\n",
    "\n",
    "    features = features.view(-1,TIME_STEP*INPUT_SIZE)\n",
    "    features = model(features)\n",
    "    features = features.detach().view(-1,TIME_STEP,INPUT_SIZE)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=12, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=12, out_features=3, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=12, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=300, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Epoch:    1|steps:   10|Train Avg Loss: 0.2630 |Test Loss: 0.207820\n",
      "Epoch:    1|steps:   20|Train Avg Loss: 0.1920 |Test Loss: 0.117046\n",
      "Epoch:    1|steps:   30|Train Avg Loss: 0.1028 |Test Loss: 0.063802\n",
      "Epoch:    2|steps:   10|Train Avg Loss: 0.0596 |Test Loss: 0.042570\n",
      "Epoch:    2|steps:   20|Train Avg Loss: 0.0562 |Test Loss: 0.041447\n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.0557 |Test Loss: 0.041378\n",
      "Epoch:    3|steps:   10|Train Avg Loss: 0.0544 |Test Loss: 0.040948\n",
      "Epoch:    3|steps:   20|Train Avg Loss: 0.0530 |Test Loss: 0.040935\n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.0550 |Test Loss: 0.040980\n",
      "Epoch:    4|steps:   10|Train Avg Loss: 0.0527 |Test Loss: 0.040595\n",
      "Epoch:    4|steps:   20|Train Avg Loss: 0.0537 |Test Loss: 0.040889\n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.0530 |Test Loss: 0.040769\n",
      "Epoch:    5|steps:   10|Train Avg Loss: 0.0527 |Test Loss: 0.040941\n",
      "Epoch:    5|steps:   20|Train Avg Loss: 0.0527 |Test Loss: 0.041894\n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.0505 |Test Loss: 0.040716\n",
      "Epoch:    6|steps:   10|Train Avg Loss: 0.0514 |Test Loss: 0.040484\n",
      "Epoch:    6|steps:   20|Train Avg Loss: 0.0499 |Test Loss: 0.040770\n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.0502 |Test Loss: 0.044303\n",
      "Epoch:    7|steps:   10|Train Avg Loss: 0.0487 |Test Loss: 0.041282\n",
      "Epoch:    7|steps:   20|Train Avg Loss: 0.0476 |Test Loss: 0.040639\n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.0482 |Test Loss: 0.040286\n",
      "Epoch:    8|steps:   10|Train Avg Loss: 0.0453 |Test Loss: 0.041762\n",
      "Epoch:    8|steps:   20|Train Avg Loss: 0.0438 |Test Loss: 0.040641\n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.0445 |Test Loss: 0.040821\n",
      "Epoch:    9|steps:   10|Train Avg Loss: 0.0416 |Test Loss: 0.041119\n",
      "Epoch:    9|steps:   20|Train Avg Loss: 0.0410 |Test Loss: 0.044262\n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.0421 |Test Loss: 0.040329\n",
      "Epoch:   10|steps:   10|Train Avg Loss: 0.0405 |Test Loss: 0.040033\n",
      "Epoch:   10|steps:   20|Train Avg Loss: 0.0417 |Test Loss: 0.044003\n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.0406 |Test Loss: 0.040784\n",
      "Epoch:   11|steps:   10|Train Avg Loss: 0.0413 |Test Loss: 0.041073\n",
      "Epoch:   11|steps:   20|Train Avg Loss: 0.0403 |Test Loss: 0.040100\n",
      "Epoch:   11|steps:   30|Train Avg Loss: 0.0408 |Test Loss: 0.040032\n",
      "Epoch:   12|steps:   10|Train Avg Loss: 0.0401 |Test Loss: 0.040191\n",
      "Epoch:   12|steps:   20|Train Avg Loss: 0.0423 |Test Loss: 0.043788\n",
      "Epoch:   12|steps:   30|Train Avg Loss: 0.0403 |Test Loss: 0.040001\n",
      "Epoch:   13|steps:   10|Train Avg Loss: 0.0402 |Test Loss: 0.041307\n",
      "Epoch:   13|steps:   20|Train Avg Loss: 0.0404 |Test Loss: 0.040828\n",
      "Epoch:   13|steps:   30|Train Avg Loss: 0.0406 |Test Loss: 0.040938\n",
      "Epoch:   14|steps:   10|Train Avg Loss: 0.0407 |Test Loss: 0.042453\n",
      "Epoch:   14|steps:   20|Train Avg Loss: 0.0402 |Test Loss: 0.039799\n",
      "Epoch:   14|steps:   30|Train Avg Loss: 0.0404 |Test Loss: 0.040931\n",
      "Epoch:   15|steps:   10|Train Avg Loss: 0.0403 |Test Loss: 0.042166\n",
      "Epoch:   15|steps:   20|Train Avg Loss: 0.0411 |Test Loss: 0.040400\n",
      "Epoch:   15|steps:   30|Train Avg Loss: 0.0406 |Test Loss: 0.039832\n",
      "Epoch:   16|steps:   10|Train Avg Loss: 0.0400 |Test Loss: 0.042002\n",
      "Epoch:   16|steps:   20|Train Avg Loss: 0.0399 |Test Loss: 0.041413\n",
      "Epoch:   16|steps:   30|Train Avg Loss: 0.0408 |Test Loss: 0.039841\n",
      "Epoch:   17|steps:   10|Train Avg Loss: 0.0403 |Test Loss: 0.041651\n",
      "Epoch:   17|steps:   20|Train Avg Loss: 0.0410 |Test Loss: 0.042625\n",
      "Epoch:   17|steps:   30|Train Avg Loss: 0.0395 |Test Loss: 0.040176\n",
      "Epoch:   18|steps:   10|Train Avg Loss: 0.0397 |Test Loss: 0.041796\n",
      "Epoch:   18|steps:   20|Train Avg Loss: 0.0412 |Test Loss: 0.039582\n",
      "Epoch:   18|steps:   30|Train Avg Loss: 0.0410 |Test Loss: 0.039903\n",
      "Epoch:   19|steps:   10|Train Avg Loss: 0.0418 |Test Loss: 0.041855\n",
      "Epoch:   19|steps:   20|Train Avg Loss: 0.0396 |Test Loss: 0.039645\n",
      "Epoch:   19|steps:   30|Train Avg Loss: 0.0403 |Test Loss: 0.042994\n",
      "Epoch:   20|steps:   10|Train Avg Loss: 0.0413 |Test Loss: 0.042904\n",
      "Epoch:   20|steps:   20|Train Avg Loss: 0.0405 |Test Loss: 0.040659\n",
      "Epoch:   20|steps:   30|Train Avg Loss: 0.0395 |Test Loss: 0.040207\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxcV3338c9vRhqNtpGsxYvkfUkcJ9hO4mwkkDhAEiAltKVAWAoUmqZPaaGUltBSCm15aJ+mkFJoQwohZQlLCaGhhCaQQEISJ8QOjrN73yKvsq19m5nf88e9ksejkT2yPR5L832/XvOaudvcc0bSfHXOvfdcc3dERESyRYpdABEROT0pIEREJCcFhIiI5KSAEBGRnBQQIiKSkwJCRERyUkBISTCzd5rZ/cUux0RmZleY2c5il0NOHQWEnDJmttXM+sysy8wOmdljZnajmRX899Ddv+XuVxV6P/nSl61MBAoIOdV+w91rgTnAPwAfA75a3CJJoZlZWbHLIOOngJCicPcOd78HeBvwHjM7B8DMKszsZjPbbmZ7zOxWM6sMl11hZjvN7M/MbK+Z7TKz9w2/p5nVmdnXzWyfmW0zs08Mt07M7L1m9kj42szs8+F7dJjZunz2n83MFpjZg2bWbmb7zexbZlafsdzNbGHG9B1m9vdmVg38BGgxs+7w0RLu+xYzawsft5hZRcb215rZ2ozW19KMZVvN7KNhXTrM7LtmFs9Yfl24baeZbTKza8L5LWZ2j5kdMLONZvb7GdtUhmU+aGbPAxdk1b/FzO4KP+8tZvYnGcs+ZWbfN7Nvmlkn8N48fi3kNKOAkKJy918BO4FXhbP+ETgDWA4sBFqBT2ZsMh2oC+e/H/iSmU0Jl/1ruGw+cDnwu8D7GO0q4NXhfuoJQqo9z/1nMuCzQAtwFjAL+FQede4BXg+0uXtN+GgD/gq4ONz3MuBC4BMAZnYecDvwB0Aj8GXgnswAAd4KXAPMA5YSfimb2YXA14E/D+v7amBruM23CT7/FuAtwP81s9eEy/4GWBA+rgbeM1LxIHh/BDwdfkavAT5sZldnlOc64PvhPr91rM9FTkPurocep+RB8KX02hzzHyf4cjSgB1iQsewSYEv4+gqgDyjLWL6X4Es1CgwASzKW/QHwi/D1e4FHwtdXAuvD7SIZ6x91/3nU783ArzOmHViYMX0H8PcZddmZtf0m4A0Z01cDW8PX/w78Xdb6LwGXZ3y278pY9v+AW8PXXwY+n6O8s4AUUJsx77PAHeHrzcA1GctuGC4zcBGwPev9Pg58LXz9KeDhYv/O6XFiD/ULyumgFTgANANVwBozG15mBF/+w9rdPZkx3QvUAE1ADNiWsWxb+N5HcPcHzeyLwJeA2WZ2N/BRIJ7H/keY2VTgCwStn1qCFvnBvGqcW0uO8reEr+cQdMX9ccbyWMZygN0Zr3szls0C7h1jfwfcvStrnysylu/IWjZsDkEX2aGMeVHglxnTmdvKBKQuJikqM7uA4Ev8EWA/QQvhbHevDx917l6Tx1vtB4YIvriGzQZezrWyu3/B3c8HziboUvrz49j/ZwlaCUvdPQG8iyBQhvUSBM6w6ZlFyPF+bTnK3xa+3gF8JqNc9e5e5e7fHqNsmXYQdBPl2l+DmdVm7XP4M9tFEC6ZyzLfc0tWeWrd/Q0Z62io6AlOASFFYWYJM7sW+A7wTXd/xt3TwH8Anw//O8fMWrP6tXNy9xTwPeAzZlZrZnOAjwDfzLHvC8zsIjMrJ+hS6gdSx7H/WqAbOGRmrQQhk2kt8A4zi4YHhS/PWLYHaDSzuox53wY+YWbNZtZEcOxjuPz/AdwYltvMrNrM3pj15T6WrwLvM7PXmFkkrNNid98BPAZ81szi4UHv93P4eMH3gI+b2RQzmwlktl5+BXSa2cfCg9lRMzsnDHyZJBQQcqr9yMy6CP4D/Svgcxx5IPljwEbg8fDsl58BZ+b53n9M8IW/maBFcifBgd1sCYIv3IME3SbtwM3Hsf9PA+cBHcCPgR9kLf8Q8BvAIeCdwA+HF7j7iwSBsDk8K6kF+HtgNbAOeAZ4KpyHu68Gfh/4YljujeR5ZpAHJwK8D/h8WNaHONxSuR6YS9CauBv4G3f/aUb9tgFbgPuBb2S8Zyqs2/Jw+X7gKwQnCcgkYe5qBYqIyGhqQYiISE4KCBERyUkBISIiOSkgREQkp0l1oVxTU5PPnTu32MUQEZkw1qxZs9/dm3Mtm1QBMXfuXFavXl3sYoiITBhmtm2sZepiEhGRnBQQIiKSkwJCRERyUkCIiEhOCggREclJASEiIjkpIEREJKeSDwh3518f2MBD6/cVuygiIqeVkg8IM+O2hzfz8xf3FrsoIiKnlZIPCICGmhgHegaLXQwRkdOKAgJoqFZAiIhkU0AAjdUx9ncPFLsYIiKnFQUEakGIiOSigAAaqis42DuI7s8tInKYAoKgi2ko5XT2J4tdFBGR04YCgqCLCVA3k4hIBgUEwWmuAAd6dKBaRGRYQQPCzK4xs5fMbKOZ3ZRj+TvNbF34eMzMlmUs22pmz5jZWjMr6G3iGsMWRHu3WhAiIsMKdstRM4sCXwJeB+wEnjSze9z9+YzVtgCXu/tBM3s9cBtwUcbyle6+v1BlHKYuJhGR0QrZgrgQ2Ojum919EPgOcF3mCu7+mLsfDCcfB2YWsDxjaqyuAKBdASEiMqKQAdEK7MiY3hnOG8v7gZ9kTDtwv5mtMbMbxtrIzG4ws9VmtnrfvuMbcK8yFqWyPKoWhIhIhoJ1MQGWY17OCw3MbCVBQFyWMftSd28zs6nAT83sRXd/eNQbut9G0DXFihUrjvtCBl0sJyJypEK2IHYCszKmZwJt2SuZ2VLgK8B17t4+PN/d28LnvcDdBF1WBdNYE1MXk4hIhkIGxJPAIjObZ2Yx4O3APZkrmNls4AfAu919fcb8ajOrHX4NXAU8W8Cyhi0IneYqIjKsYF1M7p40sw8C9wFR4HZ3f87MbgyX3wp8EmgE/s3MAJLuvgKYBtwdzisD7nT3/y1UWSE4UL1+d1chdyEiMqEU8hgE7n4vcG/WvFszXn8A+ECO7TYDy7LnF9JwF5O7EwaTiEhJ05XUoYbqGAPJNL2DqWIXRUTktKCACOliORGRIykgQiPDbSggREQABcSIwy0InckkIgIKiBEjw21owD4REUABMeLwkN8KCBERUECMqI5FiZVFFBAiIiEFRMjMaKzWcBsiIsMUEBk0YJ+IyGEKiAwNakGIiIxQQGRo1IB9IiIjFBAZGqordJqriEhIAZGhsSZG72CK/iGNxyQiooDI0KDhNkRERiggMowMt6FuJhERBUSmwwP26UC1iIgCIoOG/BYROUwBkWF4wD4FhIiIAuIIicoyyiKmg9QiIiggjmBmTKmO6SC1iAgKiFE0YJ+ISEABkaWxRsNtiIiAAmKUhuoKHaQWEUEBMYq6mEREAgqILA3VMbr6kwwm08UuiohIUSkgsgxfLHewV60IESltCogsI8Nt6FRXESlxCogsGm5DRCSggMjSWKMB+0REQAExSoPGYxIRARQQ4A7f+h1Y/TUA6ivLiZgCQkSkrNgFKDoz2PErmDIXgEjEmFKlayFERNSCAIjXQX/HyGSDBuwTEVFAAFBZD32HRiYbqmPqYhKRkqeAgFEtiMaamM5iEpGSV9CAMLNrzOwlM9toZjflWP5OM1sXPh4zs2X5bntSxeuhXy0IEZFMBQsIM4sCXwJeDywBrjezJVmrbQEud/elwN8Bt41j25Nn1DGICg71DZFMaTwmESldhWxBXAhsdPfN7j4IfAe4LnMFd3/M3Q+Gk48DM/Pd9qSK1x1xDKKxOoY7HOwdKtguRUROd4UMiFZgR8b0znDeWN4P/GS825rZDWa22sxW79u37/hKWlkPyT5IBscdNNyGiEhhA8JyzPOcK5qtJAiIj413W3e/zd1XuPuK5ubm4yoo8frgub8TyBiwTweqRaSEFTIgdgKzMqZnAm3ZK5nZUuArwHXu3j6ebU+akYAIupkaatSCEBEpZEA8CSwys3lmFgPeDtyTuYKZzQZ+ALzb3dePZ9uTKl4XPIcHqtXFJCJSwKE23D1pZh8E7gOiwO3u/pyZ3RguvxX4JNAI/JuZASTD7qKc2xaqrFSGLYjwQPWUKt0TQkSkoGMxufu9wL1Z827NeP0B4AP5blswIy2IICDKoxHqKsvVghCRkqYrqWHUMQgIrqZWQIhIKVNAwKhjEBB0Mx3qU0CISOlSQACUxyFaccTFcol4GR19ulBOREqXAmJYZf0RLYi6ynIFhIiUNAXEsKzxmOoqy+nQUBsiUsIUEMOyRnStqyynayBJOp3zAm4RkUlPATEsqwWRqCzHHboGkkUslIhI8SgghmXdVS5RWQ5Ap45DiEiJUkAMy3EMAtCBahEpWQqIYfHwLCYPjjnUqQUhIiVOATEsXgeegsFuABJxtSBEpLQpIIYNX00dHoeoq1JAiEhpU0AMGx7RNTwOMdLF1K+AEJHSpIAYljWia3UsSjRiakGISMlSQAyLH9mCMDONxyQiJU0BMSzHiK7BeEy6UE5ESpMCYljWXeUgCAid5ioipUoBMawiETxnDbehLiYRKVUKiGGRKFTUHTFgX0ItCBEpYQqITLmG/FZAiEiJUkBkiteNPgbRP4S7hvwWkdKjgMiU465yQymnbyhVxEKJiBSHAiJTPOsYhMZjEpESpoDIFB/dggAFhIiUJgVEphzHIAA6dbGciJQgBUSmynoY6oFU0GJQC0JESpkCItPIcBudACQqywAFhIiUJgVEppEB+8J7QqgFISIlTAGRKWvI79q4bjsqIqUrr4Aws2ozi4SvzzCzN5lZeWGLVgRZd5WLRoxaDfktIiUq3xbEw0DczFqBB4D3AXcUqlBFk3VXOQiuhVALQkRKUb4BYe7eC/wW8K/u/pvAksIVq0iyupjg8HAbIiKlJu+AMLNLgHcCPw7nlRWmSEUUH92C0IB9IlKq8g2IDwMfB+529+fMbD7w88IVq0jKKyFSPupiOQWEiJSivFoB7v4Q8BBAeLB6v7v/SSELVhRmowbsS1TqILWIlKZ8z2K608wSZlYNPA+8ZGZ/nsd215jZS2a20cxuyrF8sZmtMrMBM/to1rKtZvaMma01s9X5VuiEZQ3YF9x2VENtiEjpybeLaYm7dwJvBu4FZgPvPtoGZhYFvgS8nuCA9vVmln1g+wDwJ8DNY7zNSndf7u4r8iznicsxYF/fUIrBZPqUFUFE5HSQb0CUh9c9vBn4b3cfAo51F50LgY3uvtndB4HvANdlruDue939SeD06cPJuqtcQldTi0iJyjcgvgxsBaqBh81sDtB5jG1agR0Z0zvDefly4H4zW2NmN4y1kpndYGarzWz1vn37xvH2YxhjRFcFhIiUmrwCwt2/4O6t7v4GD2wDVh5jM8v1VuMo26Xufh5BF9Ufmdmrxyjbbe6+wt1XNDc3j+PtxzDqIHU43IauhRCREpPvQeo6M/vc8H/qZvbPBK2Jo9kJzMqYngm05Vswd28Ln/cCdxN0WRXe8EHq8D7UakGISKnKt4vpdqALeGv46AS+doxtngQWmdk8M4sBbwfuyWdn4dhPtcOvgauAZ/Ms64mJ10M6CUO9wOHbjmq4DREpNfleDb3A3X87Y/rTZrb2aBu4e9LMPgjcB0SB28OL7G4Ml99qZtOB1UACSJvZhwnOeGoC7jaz4TLe6e7/O56KHbfMAfti1WpBiEjJyjcg+szsMnd/BMDMLgX6jrWRu99LcFps5rxbM17vJuh6ytYJLMuzbCdX5oB9da0Ztx1VQIhIack3IG4Evm5m4b/XHATeU5giFVnWgH2xsgiV5VG1IESk5OQ71MbTwDIzS4TTnWF30LpCFq4ocgzYp+E2RKQUjeuOcu7eGV5RDfCRApSn+EZaEBrRVURK24nccjTXdQ4T33ALok/jMYlIaTuRgBjPRW8Th1oQIiLAMY5BmFkXuYPAgMqClKjYomUQqzliRNdEvJwX+rqKWCgRkVPvqAHh7rWnqiCnlfjo4TZ0mquIlJoT6WKavHIM2Nc1kCSVnpy9aiIiuSggcqkcfU8IgC4N2CciJUQBkUvWXeV0TwgRKUUKiFxy3FUO0KmuIlJSFBC56KZBIiIKiJzidTDYBamgxaCAEJFSpIDIZXhE14FgVJFEZXA2sAJCREqJAiKXrBFd63TbUREpQQqIXLJGdK0sj1IeNbUgRKSkKCByybyrHGBmGo9JREqOAiKXyhz3hIgrIESktCggcsk6BgEaj0lESo8CIpccd5WrU0CISIlRQOQSqwaLjrpYTl1MIlJKFBC5mI0asE/3pRaRUqOAGEvWgH11leV09idx15DfIlIaFBBjqWyA3vaRybrKclJpp2cwVcRCiYicOgqIsSRmQOeukUmNxyQipUYBMZZEK3S2HZ6MhwHRq4AQkdKggBhLoiUY0bU/GLBP4zGJSKlRQIwl0Ro8h60I3VVOREqNAmIsiZbgufNlQMcgRKT0KCDGMhIQR7YgdDW1iJQKBcRYamcEz2FA1FaUYaaAEJHSoYAYS1kFVDePdDFFIqYRXUWkpCggjibRcsSprhqPSURKiQLiaLKvhagso7M/WcQCiYicOgqIo0m0jnQxgVoQIlJaChoQZnaNmb1kZhvN7KYcyxeb2SozGzCzj45n21Mi0RIM2DfYAwQBcah3sChFERE51QoWEGYWBb4EvB5YAlxvZkuyVjsA/Alw83FsW3gjF8sFYzJNS8TZ3dGvEV1FpCQUsgVxIbDR3Te7+yDwHeC6zBXcfa+7Pwlk99scc9tTIutiudkNVfQMpmjvUStCRCa/QgZEK7AjY3pnOO+kbmtmN5jZajNbvW/fvuMq6JiyAmJOYxUA2w/0ntz9iIichgoZEJZjXr59M3lv6+63ufsKd1/R3Nycd+HykqMFAbC9XQEhIpNfIQNiJzArY3om0DbGuidz25OnvDK4cVB4quvMKUFAbFNAiEgJKGRAPAksMrN5ZhYD3g7ccwq2PbkyroWIl0eZnoiri0lESkJZod7Y3ZNm9kHgPiAK3O7uz5nZjeHyW81sOrAaSABpM/swsMTdO3NtW6iyHlWi5YhrIWY3VrH9QE9RiiIicioVLCAA3P1e4N6sebdmvN5N0H2U17ZFkWiBl1ePTM5pqOLhDSf5YLiIyGlIV1IfS6IVetthqB8IDlTv6RygfyhV5IKJiBSWAuJYhs9k6gqOQ8wOT3XdoeMQIjLJKSCOJevGQcOnuupMJhGZ7BQQx5J1b+o5jdUAbFMLQkQmOQXEsSSG7ywXnMk0paqcmooydTGJyKSngDiWilqoqBtpQZgZsxuq2NauU11FZHJTQOQj685ycxqrdLGciEx6Coh8ZF8s11DFjoN9pNMa9ltEJi8FRD6yWhCzG6sYTKbZ3dlfxEKJiBSWAiIfiVbo3gvJ4D4QI6O6qptJRCYxBUQ+Ei2AQ/duAOY0BKe6athvEZnMFBD5yLoWYkZ9nGjE1IIQkUlNAZGPrBsHlUcjtNZX6mI5EZnUFBD5GA6IjsNnMulUVxGZ7BQQ+YjXQXn1EWcyzWqoYrsulhORSUwBkQ+zUddCzGmo4mDvEJ39Q0UsmIhI4Sgg8lXXeuS1EMOnuupMJhGZpBQQ+Uq0jrpYDnQthIhMXgqIfCVagusgUklAF8uJyOSngMhXogU8Dd17AKiNl9NQHdONg0Rk0lJA5CvrYjkIWhHbD+hMJhGZnBQQ+cq6WA6GA0ItCBGZnBQQ+crRgpjTWEXboX6GUukiFUpEpHAUEPmqnAJl8SNaELMaqkilnbZDfUUsmIhIYSgg8jVysVxGCyI8k0kHqkVkMlJAjIeuhRCREqKAGI/62dC+AdLBMYdptXFiZREFhIhMSgqI8Zj7Kuhthz3PABCJGLMbqtimQftEZBJSQIzHgpXB86YHR2YFp7rqILWITD4KiPGonQ5Tzx4VENvae3Qmk4hMOgqI8Vp4JWx/HAaDbqWrz55OKu285p8f4osPbqB/KFXkAoqInBwKiPFacCWkBmHrowBcsqCRn33kcq44s5mb71/P6z7/EPc/txt3L3JBRUROTFmxCzDhzL4kuGBu04NwxlVAcMHcv7/rfB7ZsJ9P/+g5bvjGGi6e38ArWutorq0IHjVxpiYqmNdUTXlUuSwipz8FxHiVV8KcS484DjHsskVN3PuhV/GNVdv4+qqt/Hr7IQaSRw7DUVkeZdmsOs6fM4Xz50zh3FlTmFIdO+ou93cP8JNndzOUTPOm5S001VSczBqJiORkk6krZMWKFb569erC7+ixL8L9fwV/+hzUzRxzNXeneyDJvq4B9ncP0naoj6d3HuKpbQd5rq2TZDr47BdOreGCuVNYMaeBC+Y2MKuhks7+JPc9t5sfPd3Goxv3E65KWcR4zVlTedsFs3j1ombKwtaIu7Oro58Ne7vZ09HP1EQFM6dU0lpfRWUsWvCPREQmJjNb4+4rci0raAvCzK4B/gWIAl9x93/IWm7h8jcAvcB73f2pcNlWoAtIAcmxKlAUC64Mnjc9COf97pirmRm18XJq4+XMbw7mvfncYNC/vsEU63YeYvW2g6zZdpAfr9vFt3+1A4Dm2go6eocYTKWZ3VDF/7liIW9a3kLE4Hurd3LXmp3c99wepifiXDivgW3tPWzc203PYO4D5A3VMVrq49RUlFEVK6MyFqWqPEpVLEpZNEI0YkTMiFgQQEta6rhy8VRiZeoKEyllBWtBmFkUWA+8DtgJPAlc7+7PZ6zzBuCPCQLiIuBf3P2icNlWYIW77893n6esBeEOnzsLZl8Mv3PHSXnLdNpZv7eLJ7ceZM3WAzTWVPAby1pYNrOOIEcPG0ymefDFPXz3yR28uLuL+c3VLGyuYeG0WhZNrWFGXZy9XQO8fLCPlw/1sfNgH7s7+ugZSNE7lKR3IEXvYIrewSSptJN2SLmTTjspd9xhSlU51y1v5S3nz+TslsRIGZKpNNsO9LJhTzdDqTQXz2+kuVZdXiITVbFaEBcCG919c1iI7wDXAc9nrHMd8HUPUupxM6s3sxnuvquA5TpxZkEr4qV7IZ2CyIl34UQixuLpCRZPT/Dui+ccdd1YWYRrzpnBNefMGHOdOY3VXDB3/OVIptL8cuN+vr9mJ3c+sZ07HtvK4um1LGiuYePebjbv72YodeQ/FYun13LpwiYuW9jE0pl1DKWc7oEhugdSdPcnGUqnWdpaR6OOnYhMKIUMiFZgR8b0ToJWwrHWaQV2AQ7cb2YOfNndbytgWcdvwZWw9lvQthZmnl/s0pw0ZdEIK8+cysozp9LRO8SP1rVx11M7ebatg0VTa1i5eCoLp9awaGoNAI9taufRjfv5xuPb+OojW4763oun1/LKBU1curCRC+c1UBsvPxVVEpHjVMiAsBzzsvuzjrbOpe7eZmZTgZ+a2Yvu/vConZjdANwAMHv27BMp7/jMXwlYcBxiEgVEprqqct518RzedZQWzbJZ9fzhFQvoH0qxZttBXtzdRWV5lJp4GTUVUWoqynF3Vm87yGOb9vOtJ7Zx+6NbiEaMBc3VLJmRYElLgiUz6lg8oxZ32N3Rz+7OfnZ39LGro595TdW8+dxWnR4scooV8hjEJcCn3P3qcPrjAO7+2Yx1vgz8wt2/HU6/BFyR3cVkZp8Cut395qPt85Qdgxj25cuhvAp+7yenbp8TXP9Qiqe2H+TxTe0819bJ87s62dXRP+b6EYO0w8wplXxw5UJ++/yZCgqRk6hYxyCeBBaZ2TzgZeDtwDuy1rkH+GB4fOIioMPdd5lZNRBx967w9VXA3xawrMdnwZXw2BegvxPiiWKXZkKIl0d55YImXrmgaWTegZ5BXtjVyQu7OimPRpheF2dGXZzpdXEaqyt4eP0+bvnZem76wTN88ecbFRQip0hBr4MIz1K6heA019vd/TNmdiOAu98anub6ReAagtNc3+fuq81sPnB3+DZlwJ3u/plj7e+UtyC2PgJ3vBHeficsfuOp2+9Ek05D5MS+zN2dX7wUBMXTOztoqI5x8fwGLpnfyCULGlnQXHPE2V79Qyn2dPbTO5hi8fTaUWeCjZJKQlTXjUrpOVoLQhfKnYjkIPzjXFh+Pbzxn0/dfieK7r1w/1/Duu/CWb8BK/8Spp51Qm85HBQ/WtfG45vaaQu7p5prKzhzWi37uwfY09nPwd6hkW3OaU3wwZWLuGrJNCKRrKAY6IbH/w0e/QLMuhDe9IWjXvw4SmoIHrkFtj0Cr/oozHvVCdVvXNzh2btg7wtw6YfUipXjooAopDvfBrvWwVV/B9OXQuOCk3La64SWTsGTX4UH/x6GemHJdbD+PhjshqVvhStugob5J7wbd2f7gV5WbWpn1eZ2tu7vobk2zvS6CmbUVTItEad/KMVXfrmZre29nDGthj9auZBrl7YQ9SQ89Z/wi3+Enr3BSQc7noBIOVzzWVj+Dtp7BoOLCmNjtCz2vgh3/wHsWgvxOujvgDNeD6/7NDSfecL1O6r9G+HHH4EtDwWfRaIVu/aWkfHBTqrOtqC1HK+Hha8df2twsAc6dkLjwuP720glwdNQdvQhaY7KPSjDvheDx6EdsPA1sPB1J9y6PSHpFKy9Ex7+p+D3JxKFSFn4iELLeXDuu4N70RToe0UBUUjP3Q0/+ANIDQTT5dUw/RxoXhyM22TR4Bdw+Icer4PKBqicAlUNwetoeLrnSDeIAR78UbiHj3TQBRKvD94j1y+Le/CFPNQfLI/GgveOlAXvnU4H5UyGj6FeOLgF9r0U/uG8BPvXB+/fcm7wmLEcZizL/7/THU8GX1y718H8K+ANN0PTIug9AI/eAk/cFoyGe+47g+WJ1uBRO/3w55BLKhl8EW95OPiy6jsYfM7Tlwblm3Y2xKpHfx6pQZJ9nfx83Wb+67EXaD9wgBW1B/mA30Xz0Mvsrj+PLed+jNjci+h8eT0LV/0Fs7rW8ks7n4/0/R4d0UZWzJ3C5Wc0c/mZzZw5rRbzNEOP/ivRX3yGwUg1353+pzzKebyDe7l019cpS/Xh572HyBU34bEqOg4dYte+/exr38+hzi4SMxawYN5CWqdUjW7RjBTdc3eLDXyvcawAAA1VSURBVPWTfPhm7NFb6PcYnx18G8+nZ/NP8a+ywHdwcMGbqf+tf8aqDx/jYbAH2n4Ne54PPqPqJqhqgupGqGoEDNJDwZdVaij4Hdn1NGx+KPi82zccfq/GRfDKD8LSt0N5PPfPad+L8PKaw4+9zwe/v1WNsOhqUouupq3xlezoiVBVUUZTTYymmgri5Rm/0+7BtmvvDFpJ/YeCMtfOCH5XaqcHgTP/iuB3IPtL3j34fXnxXtj88yDMB7sOL45WYKkBmDIXLvgALH9n8PcIQc/Ajsdh4wNB/WumwvJ3wBmvZ5Ay1u44xO7OfirLo8EjFiFeHmUwmWZbey9te/ZQu+1nnNH+ALOGttI27QqmXf5+Zp114ZFl3PYY/ORjwd9K6wpoPR/SSZLJIbp6++nu6WLavlXEBg+Rrm0hcu47g3I2zMv5O3O8FBCFlhoK/ih2rQv+sHavC75oU0nwFKSTwR9fOsnoM32PU0UdVNYFI8sOdAf/nQ92B3+IuUTKwv2PoXJKEGpNi4Iv37anoWP74eXVzcEZW+VVQfCVVwWhM9gT7HegO3g90BH8EV/9GTj7tzJCL9S1G375OVjztSAoRhjUTAu+ROIJqEgEQRVPwMFtsH1VsB+A5rOgphl2Pwt9B8LNI1DbEnzRJfuDP/JkP2N93ptsNv8w9DZ+mlxO5tnW5RHno4kH+b2Bb5CKxnmm8Wo2HIqwrTtCD3HK4jW8xR7gnOTz3JdawV8OvR+qm2mqqWDz/m5qUx38cdndvCv6M8pt7HuDdHgVm5hFe9U8BqacQZ9V0TGQ5lC/c6g/RUd/mooymFFlTKs2miuhKZ5m7rbv0ziwkx+mXsl/VL6fK1e8gnlN1dy/bhtLNn2VP4z8kG6r4cnWd9OU2kNL1zNM7dlAhPHfp6SXOGsjZ7Mm8grWRJeyMNLGO4Z+yPzkRrrLprB2xlvZ03QxdZ3raeh6kandLzGtbyPlHvxce6O1vFy1hLbqJRwon860A7/inJ4nSNDNgJfxRPosNnkL+7yevdTTVdYIVQ1cXvYcrxl4gGmD20lGKtg783Uk6xcQ69tLrG8vFX17Ke/dTaxvHwA90XqeiS3jF6lz2DaQ4HXlT/Pq9K9oSu8nTYSd1eewuWw+LyRbWd03jTW90+ihkuviT3FDxQOcMfAM6WgFdvZvYv0dsPWXwe9apAxvvYBk+2bKe/fQFUlwd/JSvjP0Kp73uURIU0MftfRSY32caTu4Nvo4l0fWUWFDHIg2saN8Lmf1/ZqYpdhUtpD2RW9h8QVXEXv8X4iv/2/6K6ez5owP8XjVlWzY28P6PV1sbe8ZGXstxhCvjazhrdGHeHV0HRGcvbFZpOINRGuaqKqfRnV9M5aYARf/4bh/xqCAOH24w0Bn8AXceyD4cus7FATM8BeZe/DaIocfWPBFmxoM1u8/dPg52Q+xWqiogVhN8FxWGYbSUBBS6aFgH9EYlFUEoVIWC57r5wTBUN00+su8Z39wIWDbr6HzZRjqC1sofcHDU8F/pMP7jdVAogVW/B5U1B79sxjoho4dwft2tgWPjp3BZ9PfETwGOoPn6ubgfuBzLwuea5oPf1YdO4NA3rUODm0L6xjPqGdFUJbMMlbWw4zlpIlwoHeQ3R397O3qZ2ptnEXTaqgoi8L+DfA/fxrUf7CbzKDpsWp+seCjRJddz9JZ9cyoi2NmDCbTbNzbzfO7Otmz5Tlmtv2EysoqqmvrSNRNYUp9PXU11XS8vJ6+l58jduAlGns3U+Pdef8KbfYZ3Dvrz1j66jdz6cImohktkI7eIVateohFj3+cBUPr6fEKnvaFrEkv4qn0Ip5Nz6U2OsS0sh6mRrtosi4aI93EohEsWk6krJxoWfDcHp/L9srFuIXdaxaMH9bePcCszjX8Zu9dXMavR/bd6VW8aPPYEJnPxugCnmUhW9LTSRNcnQ8wc0oVCxrjXBLbyPl9q5h5cBUVPW2UDY2u/9N2Ft8duowfJS+ii6qcn0UzB7ks8iwrY89xaeRZGtPBPwuDVsG6+Pk8ZBfxv4NLeXmwiul1cVrrK5k5pYqZUyqZUhXjmZcP8dimdioPvMjvRu/nN8se5YDVs8qW84gv45fJxRxIxjFP86rIM7yv6lEuSz1BmQ+RLosTSY4+PTtZPQ2WvJmyV/wWzLwQIhH27XmZDT/7GlM33cXC9GYA+jzGl1PX8uXktfQRJ2Iwt7GaM6bVcsb0WhZPD4bNGUo52w/0sK29l0O7tzDv5R/R2L2e+FAHDdbFFOuigS66onU0/PWmY5+MkYMCQuREpNNBMA63lKqbgpA5GdyDIB7qDULd00Fr01NBqy8M9XQkRns/VFQnSFQeoy8+nQrCsm72EWdmjdltdZxSu58ntW8DZS1LiTTMHf0PRr4Ge6F7N3TtCY4HTTsHGheQTjvtPUGA9wwmw95Wx4G0O1WxMuY1VTOlqjxoA+57MfhHY/YlEMsdKrnsPBgcx1q9ZT+DaaOiLEJFWdBtFCuLML+5mlcuaGJaIh78Y/fsXXBwa/CPR0UifK4Nukpbzx/zmIa7s+mZVexa+1N2tV5FYtpcpiWC07mbaypGRmbOR2f/EBv2dLN+Txfrd3eSGuzjb99yQd7bZ1JAiIhITkcLCF1pJCIiOSkgREQkJwWEiIjkpIAQEZGcFBAiIpKTAkJERHJSQIiISE4KCBERyWlSXShnZvuAbXms2gTsL3BxTpXJVBeYXPWZTHUB1ed0diJ1mePuzbkWTKqAyJeZrR7rysGJZjLVBSZXfSZTXUD1OZ0Vqi7qYhIRkZwUECIiklOpBsRtxS7ASTSZ6gKTqz6TqS6g+pzOClKXkjwGISIix1aqLQgRETkGBYSIiORUUgFhZteY2UtmttHMbip2ecbLzG43s71m9mzGvAYz+6mZbQifpxSzjPkys1lm9nMze8HMnjOzD4XzJ2p94mb2KzN7OqzPp8P5E7I+AGYWNbNfm9n/hNMTuS5bzewZM1trZqvDeRO5PvVm9n0zezH8G7qkEPUpmYAwsyjwJeD1wBLgejNbUtxSjdsdwDVZ824CHnD3RcAD4fREkAT+zN3PAi4G/ij8eUzU+gwAV7r7MmA5cI2ZXczErQ/Ah4AXMqYncl0AVrr78ozrBSZyff4F+F93XwwsI/g5nfz6uHtJPIBLgPsypj8OfLzY5TqOeswFns2YfgmYEb6eAbxU7DIeZ73+G3jdZKgPUAU8BVw0UesDzAy/ZK4E/iecNyHrEpZ3K9CUNW9C1gdIAFsITzIqZH1KpgUBtAI7MqZ3hvMmumnuvgsgfJ5a5PKMm5nNBc4FnmAC1yfsklkL7AV+6u4TuT63AH8BpDPmTdS6ADhwv5mtMbMbwnkTtT7zgX3A18IuwK+YWTUFqE8pBYTlmKdzfIvMzGqAu4APu3tnsctzItw95e7LCf77vtDMzil2mY6HmV0L7HX3NcUuy0l0qbufR9DF/Edm9upiF+gElAHnAf/u7ucCPRSoe6yUAmInMCtjeibQVqSynEx7zGwGQPi8t8jlyZuZlROEw7fc/Qfh7Albn2Hufgj4BcHxoolYn0uBN5nZVuA7wJVm9k0mZl0AcPe28HkvcDdwIRO3PjuBnWELFeD7BIFx0utTSgHxJLDIzOaZWQx4O3BPkct0MtwDvCd8/R6CvvzTnpkZ8FXgBXf/XMaiiVqfZjOrD19XAq8FXmQC1sfdP+7uM919LsHfyYPu/i4mYF0AzKzazGqHXwNXAc8yQevj7ruBHWZ2ZjjrNcDzFKA+JXUltZm9gaBvNQrc7u6fKXKRxsXMvg1cQTC07x7gb4AfAt8DZgPbgd9x9wPFKmO+zOwy4JfAMxzu5/5LguMQE7E+S4H/JPjdigDfc/e/NbNGJmB9hpnZFcBH3f3aiVoXM5tP0GqAoHvmTnf/zEStD4CZLQe+AsSAzcD7CH/vOIn1KamAEBGR/JVSF5OIiIyDAkJERHJSQIiISE4KCBERyUkBISIiOSkgRE4CM/uwmVUVuxwiJ5NOcxU5CcKrjle4+/5il0XkZFELQmScwitzfxze++FZM/sboAX4uZn9PFznKjNbZWZPmdl/hWNODd+X4B/De0f8yswWhvN/J3yvp83s4eLVTuQwBYTI+F0DtLn7Mnc/h+Dq/DaC+w2sNLMm4BPAa8MB4lYDH8nYvtPdLwS+GG4L8Engag/uJ/GmU1URkaNRQIiM3zPAa8OWwKvcvSNr+cUEN6V6NBz++z3AnIzl3854viR8/Shwh5n9PsFwHSJFV1bsAohMNO6+3szOB94AfNbM7s9axQjuB3H9WG+R/drdbzSzi4A3AmvNbLm7t5/ssouMh1oQIuNkZi1Ar7t/E7iZYKjlLqA2XOVx4NKM4wtVZnZGxlu8LeN5VbjOAnd/wt0/CeznyKHpRYpCLQiR8XsF8E9mlgaGgD8k6Cr6iZntCo9DvBf4tplVhNt8Algfvq4wsycI/kEbbmX8k5ktImh9PAA8fWqqIjI2neYqcgrpdFiZSNTFJCIiOakFISIiOakFISIiOSkgREQkJwWEiIjkpIAQEZGcFBAiIpLT/wdXnZlesTsBawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DAE_features = DAE(features.detach(),labels.detach(),test_features.detach(),test_labels.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 10000\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEP = 30\n",
    "INPUT_SIZE = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(DAE_features,labels)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define NN architecture\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        # define lstm layer\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=128,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        # dropout layer\n",
    "        #self.dropout = torch.nn.Dropout(p = 0.0)\n",
    "        # Dense layer\n",
    "        #self.hidden  =torch.nn.Linear(64,16)\n",
    "        self.out = torch.nn.Linear(128,3)\n",
    "        self.activate = torch.nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,(h_n,h_c) = self.lstm(x,None)\n",
    "        #hidden_out = self.hidden(lstm_out[:,-1,:])\n",
    "        # only need last output of lstm layer\n",
    "        out = self.out(lstm_out[:,-1,:])\n",
    "        #out = self.activate(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(10, 128, batch_first=True)\n",
      "  (out): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (activate): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.0000)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.9,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   30|Train Avg Loss: 0.9558 |Train set Accuracy: 0.3125 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    1|steps:   60|Train Avg Loss: 0.9153 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.9078 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    2|steps:   60|Train Avg Loss: 0.9019 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.8950 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    3|steps:   60|Train Avg Loss: 0.9145 |Train set Accuracy: 0.6250 |Test set Accuracy: 0.5385|lr = 0.00100\n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.8981 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    4|steps:   60|Train Avg Loss: 0.9091 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.8907 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:    5|steps:   60|Train Avg Loss: 0.9096 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.9111 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:    6|steps:   60|Train Avg Loss: 0.8988 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.8999 |Train set Accuracy: 0.3125 |Test set Accuracy: 0.5288|lr = 0.00100\n",
      "Epoch:    7|steps:   60|Train Avg Loss: 0.9068 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.9201 |Train set Accuracy: 0.3438 |Test set Accuracy: 0.4712|lr = 0.00100\n",
      "Epoch:    8|steps:   60|Train Avg Loss: 0.8865 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.9017 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:    9|steps:   60|Train Avg Loss: 0.9014 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.9004 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:   10|steps:   60|Train Avg Loss: 0.9052 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:   11|steps:   30|Train Avg Loss: 0.9001 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:   11|steps:   60|Train Avg Loss: 0.9030 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.5337|lr = 0.00100\n",
      "Epoch:   12|steps:   30|Train Avg Loss: 0.9022 |Train set Accuracy: 0.6250 |Test set Accuracy: 0.4760|lr = 0.00100\n",
      "Epoch:   12|steps:   60|Train Avg Loss: 0.8989 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:   13|steps:   30|Train Avg Loss: 0.8900 |Train set Accuracy: 0.3750 |Test set Accuracy: 0.5385|lr = 0.00100\n",
      "Epoch:   13|steps:   60|Train Avg Loss: 0.9130 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4808|lr = 0.00100\n",
      "Epoch:   14|steps:   30|Train Avg Loss: 0.8904 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.5529|lr = 0.00100\n",
      "Epoch:   14|steps:   60|Train Avg Loss: 0.9128 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.5000|lr = 0.00100\n",
      "Epoch:   15|steps:   30|Train Avg Loss: 0.9047 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.5385|lr = 0.00100\n",
      "Epoch:   15|steps:   60|Train Avg Loss: 0.8971 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4952|lr = 0.00100\n",
      "Epoch:   16|steps:   30|Train Avg Loss: 0.8980 |Train set Accuracy: 0.5625 |Test set Accuracy: 0.5048|lr = 0.00100\n",
      "Epoch:   16|steps:   60|Train Avg Loss: 0.8998 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.5288|lr = 0.00100\n",
      "Epoch:   17|steps:   30|Train Avg Loss: 0.8981 |Train set Accuracy: 0.3438 |Test set Accuracy: 0.5096|lr = 0.00090\n",
      "Epoch:   17|steps:   60|Train Avg Loss: 0.9096 |Train set Accuracy: 0.3438 |Test set Accuracy: 0.5385|lr = 0.00090\n",
      "Epoch:   18|steps:   30|Train Avg Loss: 0.9121 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   18|steps:   60|Train Avg Loss: 0.8990 |Train set Accuracy: 0.6250 |Test set Accuracy: 0.5192|lr = 0.00090\n",
      "Epoch:   19|steps:   30|Train Avg Loss: 0.8914 |Train set Accuracy: 0.5625 |Test set Accuracy: 0.4760|lr = 0.00090\n",
      "Epoch:   19|steps:   60|Train Avg Loss: 0.9047 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.5096|lr = 0.00090\n",
      "Epoch:   20|steps:   30|Train Avg Loss: 0.8894 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   20|steps:   60|Train Avg Loss: 0.9051 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   21|steps:   30|Train Avg Loss: 0.8620 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4760|lr = 0.00090\n",
      "Epoch:   21|steps:   60|Train Avg Loss: 0.9295 |Train set Accuracy: 0.3125 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   22|steps:   30|Train Avg Loss: 0.9048 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   22|steps:   60|Train Avg Loss: 0.8933 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   23|steps:   30|Train Avg Loss: 0.9013 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   23|steps:   60|Train Avg Loss: 0.9023 |Train set Accuracy: 0.3750 |Test set Accuracy: 0.5577|lr = 0.00090\n",
      "Epoch:   24|steps:   30|Train Avg Loss: 0.9018 |Train set Accuracy: 0.3125 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   24|steps:   60|Train Avg Loss: 0.8933 |Train set Accuracy: 0.6562 |Test set Accuracy: 0.5144|lr = 0.00090\n",
      "Epoch:   25|steps:   30|Train Avg Loss: 0.9249 |Train set Accuracy: 0.5938 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   25|steps:   60|Train Avg Loss: 0.8665 |Train set Accuracy: 0.6875 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   26|steps:   30|Train Avg Loss: 0.8930 |Train set Accuracy: 0.5938 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   26|steps:   60|Train Avg Loss: 0.9153 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.5385|lr = 0.00090\n",
      "Epoch:   27|steps:   30|Train Avg Loss: 0.8874 |Train set Accuracy: 0.6562 |Test set Accuracy: 0.5000|lr = 0.00090\n",
      "Epoch:   27|steps:   60|Train Avg Loss: 0.9113 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   28|steps:   30|Train Avg Loss: 0.9156 |Train set Accuracy: 0.3750 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   28|steps:   60|Train Avg Loss: 0.8766 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   29|steps:   30|Train Avg Loss: 0.9231 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   29|steps:   60|Train Avg Loss: 0.8851 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   30|steps:   30|Train Avg Loss: 0.8822 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.5577|lr = 0.00090\n",
      "Epoch:   30|steps:   60|Train Avg Loss: 0.9118 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.5192|lr = 0.00090\n",
      "Epoch:   31|steps:   30|Train Avg Loss: 0.8996 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.5000|lr = 0.00090\n",
      "Epoch:   31|steps:   60|Train Avg Loss: 0.9035 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   32|steps:   30|Train Avg Loss: 0.8790 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   32|steps:   60|Train Avg Loss: 0.9210 |Train set Accuracy: 0.3438 |Test set Accuracy: 0.5433|lr = 0.00090\n",
      "Epoch:   33|steps:   30|Train Avg Loss: 0.9025 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   33|steps:   60|Train Avg Loss: 0.8890 |Train set Accuracy: 0.3750 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   34|steps:   30|Train Avg Loss: 0.8991 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   34|steps:   60|Train Avg Loss: 0.8974 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   35|steps:   30|Train Avg Loss: 0.9175 |Train set Accuracy: 0.6562 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   35|steps:   60|Train Avg Loss: 0.8943 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   36|steps:   30|Train Avg Loss: 0.8934 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   36|steps:   60|Train Avg Loss: 0.9067 |Train set Accuracy: 0.6250 |Test set Accuracy: 0.5192|lr = 0.00090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   37|steps:   30|Train Avg Loss: 0.8969 |Train set Accuracy: 0.2812 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   37|steps:   60|Train Avg Loss: 0.9124 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.5288|lr = 0.00090\n",
      "Epoch:   38|steps:   30|Train Avg Loss: 0.9023 |Train set Accuracy: 0.2812 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   38|steps:   60|Train Avg Loss: 0.8885 |Train set Accuracy: 0.5625 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   39|steps:   30|Train Avg Loss: 0.9032 |Train set Accuracy: 0.3125 |Test set Accuracy: 0.5048|lr = 0.00090\n",
      "Epoch:   39|steps:   60|Train Avg Loss: 0.8997 |Train set Accuracy: 0.3438 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   40|steps:   30|Train Avg Loss: 0.9056 |Train set Accuracy: 0.5625 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   40|steps:   60|Train Avg Loss: 0.8823 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   41|steps:   30|Train Avg Loss: 0.9056 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   41|steps:   60|Train Avg Loss: 0.8834 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.5481|lr = 0.00090\n",
      "Epoch:   42|steps:   30|Train Avg Loss: 0.8974 |Train set Accuracy: 0.5938 |Test set Accuracy: 0.5192|lr = 0.00090\n",
      "Epoch:   42|steps:   60|Train Avg Loss: 0.9023 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.5096|lr = 0.00090\n",
      "Epoch:   43|steps:   30|Train Avg Loss: 0.9060 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4712|lr = 0.00090\n",
      "Epoch:   43|steps:   60|Train Avg Loss: 0.8913 |Train set Accuracy: 0.3750 |Test set Accuracy: 0.5000|lr = 0.00090\n",
      "Epoch:   44|steps:   30|Train Avg Loss: 0.9176 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.5240|lr = 0.00090\n",
      "Epoch:   44|steps:   60|Train Avg Loss: 0.8852 |Train set Accuracy: 0.5312 |Test set Accuracy: 0.4808|lr = 0.00090\n",
      "Epoch:   45|steps:   30|Train Avg Loss: 0.9124 |Train set Accuracy: 0.6562 |Test set Accuracy: 0.5288|lr = 0.00090\n",
      "Epoch:   45|steps:   60|Train Avg Loss: 0.8945 |Train set Accuracy: 0.5938 |Test set Accuracy: 0.4952|lr = 0.00090\n",
      "Epoch:   46|steps:   30|Train Avg Loss: 0.8930 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4760|lr = 0.00081\n",
      "Epoch:   46|steps:   60|Train Avg Loss: 0.8988 |Train set Accuracy: 0.4375 |Test set Accuracy: 0.5433|lr = 0.00081\n",
      "Epoch:   47|steps:   30|Train Avg Loss: 0.8740 |Train set Accuracy: 0.4688 |Test set Accuracy: 0.5385|lr = 0.00081\n",
      "Epoch:   47|steps:   60|Train Avg Loss: 0.9351 |Train set Accuracy: 0.4062 |Test set Accuracy: 0.4952|lr = 0.00081\n",
      "Epoch:   48|steps:   30|Train Avg Loss: 0.8886 |Train set Accuracy: 0.5000 |Test set Accuracy: 0.4760|lr = 0.00081\n",
      "Epoch:   48|steps:   60|Train Avg Loss: 0.9056 |Train set Accuracy: 0.3438 |Test set Accuracy: 0.4952|lr = 0.00081\n",
      "Epoch:   49|steps:   30|Train Avg Loss: 0.8923 |Train set Accuracy: 0.3125 |Test set Accuracy: 0.5625|lr = 0.00081\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "ACC = []\n",
    "TRAIN_ACC = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,TIME_STEP,INPUT_SIZE)\n",
    "        # start trainnig \n",
    "        output = model(inputs)\n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,targets)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 30 steps\n",
    "        if((step+1) %30 == 0):\n",
    "            # average of loss in 30 steps\n",
    "            avg = loss_total / 30\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # calculate the accuracy of training \n",
    "            pred_train_y = torch.max(output, 1)[1].data.numpy()\n",
    "            train_accuracy = float((pred_train_y == targets.numpy()).astype(int).sum()) / float(targets.numpy().size)\n",
    "            TRAIN_ACC.append(train_accuracy)\n",
    "            \n",
    "            # calculate the accuracy of using testing data as inputs\n",
    "            test_output = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "            pred_test_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            test_accuracy = float((pred_test_y == test_labels.numpy()).astype(int).sum()) / float(test_labels.numpy().size)\n",
    "            ACC.append(test_accuracy)\n",
    "            \n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f |Train set Accuracy: %.4f |Test set Accuracy: %.4f|lr = %.5f\"\n",
    "                  %(epoch+1,step+1,avg,train_accuracy,test_accuracy,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
