{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler() # define min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "stock_path = \"C:/Users/acer/Desktop/LAB/lab_2.csv\"\n",
    "stock_df = pd.read_csv(stock_path)\n",
    "stock_df=stock_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 12)\n",
      "(238, 12)\n"
     ]
    }
   ],
   "source": [
    "train_set = stock_df[:2200]\n",
    "test_set = stock_df[2200:]\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K value</th>\n",
       "      <th>D value</th>\n",
       "      <th>MACD</th>\n",
       "      <th>William</th>\n",
       "      <th>SMA</th>\n",
       "      <th>WMA</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>RSI</th>\n",
       "      <th>A/D 0</th>\n",
       "      <th>CCI</th>\n",
       "      <th>Close r/f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.478459</td>\n",
       "      <td>0.494744</td>\n",
       "      <td>0.515782</td>\n",
       "      <td>0.515950</td>\n",
       "      <td>0.425581</td>\n",
       "      <td>0.426475</td>\n",
       "      <td>0.464374</td>\n",
       "      <td>0.492458</td>\n",
       "      <td>0.445859</td>\n",
       "      <td>0.463808</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.242761</td>\n",
       "      <td>0.231248</td>\n",
       "      <td>0.128572</td>\n",
       "      <td>0.292113</td>\n",
       "      <td>0.297391</td>\n",
       "      <td>0.296803</td>\n",
       "      <td>0.101128</td>\n",
       "      <td>0.251304</td>\n",
       "      <td>0.278231</td>\n",
       "      <td>0.159410</td>\n",
       "      <td>0.964113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.272646</td>\n",
       "      <td>0.306148</td>\n",
       "      <td>0.453617</td>\n",
       "      <td>0.258398</td>\n",
       "      <td>0.180373</td>\n",
       "      <td>0.181665</td>\n",
       "      <td>0.419330</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>0.208334</td>\n",
       "      <td>0.351748</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.469031</td>\n",
       "      <td>0.482729</td>\n",
       "      <td>0.512733</td>\n",
       "      <td>0.520003</td>\n",
       "      <td>0.364709</td>\n",
       "      <td>0.364416</td>\n",
       "      <td>0.461561</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.415185</td>\n",
       "      <td>0.467025</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.691137</td>\n",
       "      <td>0.682273</td>\n",
       "      <td>0.585561</td>\n",
       "      <td>0.777773</td>\n",
       "      <td>0.712705</td>\n",
       "      <td>0.712135</td>\n",
       "      <td>0.514771</td>\n",
       "      <td>0.687486</td>\n",
       "      <td>0.687456</td>\n",
       "      <td>0.572115</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          K value       D value         MACD      William          SMA  \\\n",
       "count  2200.000000  2200.000000  2200.000000  2200.000000  2200.000000   \n",
       "mean      0.478459     0.494744     0.515782     0.515950     0.425581   \n",
       "std       0.242761     0.231248     0.128572     0.292113     0.297391   \n",
       "min       0.004943     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.272646     0.306148     0.453617     0.258398     0.180373   \n",
       "50%       0.469031     0.482729     0.512733     0.520003     0.364709   \n",
       "75%       0.691137     0.682273     0.585561     0.777773     0.712705   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               WMA     Momentum          RSI        A/D 0          CCI  \\\n",
       "count  2200.000000  2200.000000  2200.000000  2200.000000  2200.000000   \n",
       "mean      0.426475     0.464374     0.492458     0.445859     0.463808   \n",
       "std       0.296803     0.101128     0.251304     0.278231     0.159410   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.181665     0.419330     0.292431     0.208334     0.351748   \n",
       "50%       0.364416     0.461561     0.500000     0.415185     0.467025   \n",
       "75%       0.712135     0.514771     0.687486     0.687456     0.572115   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         Close r/f  \n",
       "count  2200.000000  \n",
       "mean      1.000000  \n",
       "std       0.964113  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       1.000000  \n",
       "75%       2.000000  \n",
       "max       2.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every features in range(0 , 1)\n",
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Rise: 46.4545\n",
      "Train set Flat: 7.0909\n",
      "Train set Fall: 46.4545\n",
      "Test set Rise: 49.5798\n",
      "Test set Flat: 2.5210\n",
      "Test set Fall: 47.8992\n"
     ]
    }
   ],
   "source": [
    "# print the number of rise , flat , fall data\n",
    "print(\"Train set Rise: %.4f\"%(len(train_set[train_set[\"Close r/f\"] == 2])*100 / len(train_set)))\n",
    "print(\"Train set Flat: %.4f\"%(len(train_set[train_set[\"Close r/f\"] == 1])*100 / len(train_set)))\n",
    "print(\"Train set Fall: %.4f\"%(len(train_set[train_set[\"Close r/f\"] == 0])*100 / len(train_set)))\n",
    "print(\"Test set Rise: %.4f\"%(len(test_set[test_set[\"Close r/f\"] == 2])*100 / len(test_set)))\n",
    "print(\"Test set Flat: %.4f\"%(len(test_set[test_set[\"Close r/f\"] == 1])*100 / len(test_set)))\n",
    "print(\"Test set Fall: %.4f\"%(len(test_set[test_set[\"Close r/f\"] == 0])*100 / len(test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2170, 30, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declear training features data\n",
    "features = []\n",
    "for i in range(30,len(train_set)):\n",
    "    x = train_set[i-30:i][[\"K value \",\"D value\",\"William\",\"MACD\",\"SMA\",\"WMA\",\"Momentum\",\"RSI\",\"A/D 0\",\"CCI\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = torch.FloatTensor(features)\n",
    "features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2170])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declear trainging labels data\n",
    "labels = []\n",
    "for i in range(31,len(train_set)+1):\n",
    "    x = train_set[i-1:i][\"Close r/f\"]\n",
    "    labels.append(x.tolist())\n",
    "labels = torch.LongTensor(labels).view(-1)\n",
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 30\n",
    "INPUT_SIZE = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(features,labels)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(TIME_STEP * INPUT_SIZE, 64),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Linear(64, 12), \n",
    "            torch.nn.ReLU(True), \n",
    "            torch.nn.Linear(12, 3))\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 12),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Linear(12, 64),\n",
    "            torch.nn.ReLU(True), \n",
    "            torch.nn.Linear(64, TIME_STEP * INPUT_SIZE), \n",
    "            torch.nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=12, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=12, out_features=3, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=12, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=300, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.0005)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.9,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#delcear testing features and labels\n",
    "\n",
    "test_features = []\n",
    "for i in range(30,len(test_set)):\n",
    "    x = test_set[i-30:i][[\"K value \",\"D value\",\"William\",\"MACD\",\"SMA\",\"WMA\",\"Momentum\",\"RSI\",\"A/D 0\",\"CCI\"]].values\n",
    "    test_features.append(x.tolist())\n",
    "test_features = torch.FloatTensor(test_features)\n",
    "test_labels = []\n",
    "for i in range(31,len(test_set)+1):\n",
    "    x = test_set[i-1:i][\"Close r/f\"]\n",
    "    test_labels.append(x.tolist())\n",
    "test_labels = torch.FloatTensor(test_labels).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   10|Train Avg Loss: 0.2609 |Test Loss: 0.207552\n",
      "Epoch:    1|steps:   20|Train Avg Loss: 0.2035 |Test Loss: 0.149740\n",
      "Epoch:    1|steps:   30|Train Avg Loss: 0.1376 |Test Loss: 0.079029\n",
      "Epoch:    2|steps:   10|Train Avg Loss: 0.0712 |Test Loss: 0.046990\n",
      "Epoch:    2|steps:   20|Train Avg Loss: 0.0598 |Test Loss: 0.042341\n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.0560 |Test Loss: 0.041498\n",
      "Epoch:    3|steps:   10|Train Avg Loss: 0.0540 |Test Loss: 0.040982\n",
      "Epoch:    3|steps:   20|Train Avg Loss: 0.0557 |Test Loss: 0.040988\n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.0541 |Test Loss: 0.040789\n",
      "Epoch:    4|steps:   10|Train Avg Loss: 0.0533 |Test Loss: 0.040778\n",
      "Epoch:    4|steps:   20|Train Avg Loss: 0.0541 |Test Loss: 0.040745\n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.0518 |Test Loss: 0.040784\n",
      "Epoch:    5|steps:   10|Train Avg Loss: 0.0516 |Test Loss: 0.041018\n",
      "Epoch:    5|steps:   20|Train Avg Loss: 0.0508 |Test Loss: 0.040868\n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.0498 |Test Loss: 0.040707\n",
      "Epoch:    6|steps:   10|Train Avg Loss: 0.0480 |Test Loss: 0.040484\n",
      "Epoch:    6|steps:   20|Train Avg Loss: 0.0445 |Test Loss: 0.040201\n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.0418 |Test Loss: 0.041995\n",
      "Epoch:    7|steps:   10|Train Avg Loss: 0.0414 |Test Loss: 0.041653\n",
      "Epoch:    7|steps:   20|Train Avg Loss: 0.0419 |Test Loss: 0.042540\n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.0414 |Test Loss: 0.041242\n",
      "Epoch:    8|steps:   10|Train Avg Loss: 0.0421 |Test Loss: 0.039963\n",
      "Epoch:    8|steps:   20|Train Avg Loss: 0.0413 |Test Loss: 0.040620\n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.0409 |Test Loss: 0.041733\n",
      "Epoch:    9|steps:   10|Train Avg Loss: 0.0423 |Test Loss: 0.044788\n",
      "Epoch:    9|steps:   20|Train Avg Loss: 0.0406 |Test Loss: 0.040085\n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.0403 |Test Loss: 0.041190\n",
      "Epoch:   10|steps:   10|Train Avg Loss: 0.0404 |Test Loss: 0.040131\n",
      "Epoch:   10|steps:   20|Train Avg Loss: 0.0402 |Test Loss: 0.041145\n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.0400 |Test Loss: 0.040808\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "TEST_LOSS = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,TIME_STEP * INPUT_SIZE)\n",
    "        #reshape the features to (batch,time_step*input_size)\n",
    "        inputs_n = inputs.view(-1,TIME_STEP * INPUT_SIZE)\n",
    "       \n",
    "    \n",
    "        inputs_n =  inputs_n + torch.randn(TIME_STEP * INPUT_SIZE)*0.5\n",
    "        # Clip the images to be between 0 and 1\n",
    "        inputs_n = np.clip( inputs_n, 0., 1.)\n",
    "\n",
    "        \n",
    "        # start trainnig \n",
    "        output = model(inputs_n)\n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,inputs)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 10 steps\n",
    "        if((step+1) %10 == 0):\n",
    "            # average of loss in 10 steps\n",
    "            avg = loss_total / 10\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # test data\n",
    "            test_features = torch.Tensor(test_features).view(-1,30*10)\n",
    "            out = model(test_features)\n",
    "            test_loss = loss_func(out,test_features)\n",
    "            TEST_LOSS.append(test_loss.tolist())\n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f |Test Loss: %4f\"\n",
    "                  %(epoch+1,step+1,avg,test_loss))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    #scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,30,30)\n",
    "y = np.array(LOSS)\n",
    "plt.plot(x,y)\n",
    "y = np.array(TEST_LOSS)\n",
    "plt.plot(x,y)\n",
    "plt.title(\"DAE Loss\")\n",
    "plt.xlabel(\"EPOCH\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#delcear testing features and labels\n",
    "\n",
    "test_features = []\n",
    "for i in range(30,len(test_set)):\n",
    "    x = test_set[i-30:i][[\"K value \",\"D value\",\"William\",\"MACD\",\"SMA\",\"WMA\",\"Momentum\",\"RSI\",\"A/D 0\",\"CCI\"]].values\n",
    "    test_features.append(x.tolist())\n",
    "test_features = torch.FloatTensor(test_features)\n",
    "test_labels = []\n",
    "for i in range(31,len(test_set)+1):\n",
    "    x = test_set[i-1:i][\"Close r/f\"]\n",
    "    test_labels.append(x.tolist())\n",
    "test_labels = torch.FloatTensor(test_labels).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.view(-1,300)\n",
    "features = model(features)\n",
    "features = features.view(-1,30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 50000\n",
    "BATCH_SIZE = 16\n",
    "TIME_STEP = 30\n",
    "INPUT_SIZE = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(features,labels)\n",
    "train_loader2 = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define NN architecture\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        # define lstm layer\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=64,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        # dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p = 0.3)\n",
    "        # Dense layer\n",
    "        self.hidden1 = torch.nn.Linear(64, 16)\n",
    "        self.out = torch.nn.Linear(16,3)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,(h_n,h_c) = self.lstm(x,None)\n",
    "        # only need last output of lstm layer\n",
    "        lstm_out = self.dropout(lstm_out[:,-1,:])\n",
    "        h1_out = self.hidden1(lstm_out)\n",
    "        h1_out = self.dropout(h1_out) \n",
    "        out = self.out(h1_out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(10, 64, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (hidden1): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.0005)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.9,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-cafef13e3db1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTIME_STEP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# start trainnig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\multiprocessing\\reductions.py\u001b[0m in \u001b[0;36mreduce_tensor\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         raise RuntimeError(\"Cowardly refusing to serialize non-leaf tensor which requires_grad, \"\n\u001b[0m\u001b[0;32m    137\u001b[0m                            \u001b[1;34m\"since autograd does not support crossing process boundaries.  \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                            \u001b[1;34m\"If you just want to transfer the data, call detach() on the tensor \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue)."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "ACC = []\n",
    "TRAIN_ACC = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader2):\n",
    "        inputs = inputs.view(-1,TIME_STEP,INPUT_SIZE)\n",
    "        # start trainnig \n",
    "        output = model(inputs)\n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,targets)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 30 steps\n",
    "        if((step+1) %30 == 0):\n",
    "            # average of loss in 30 steps\n",
    "            avg = loss_total / 30\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # calculate the accuracy of training \n",
    "            pred_train_y = torch.max(output, 1)[1].data.numpy()\n",
    "            train_accuracy = float((pred_train_y == targets.numpy()).astype(int).sum()) / float(targets.numpy().size)\n",
    "            TRAIN_ACC.append(train_accuracy)\n",
    "            \n",
    "            # calculate the accuracy of using testing data as inputs\n",
    "            test_output = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "            pred_test_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            test_accuracy = float((pred_test_y == test_labels.numpy()).astype(int).sum()) / float(test_labels.numpy().size)\n",
    "            ACC.append(test_accuracy)\n",
    "            \n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f |Train set Accuracy: %.4f |Test set Accuracy: %.4f|lr = %.5f\"\n",
    "                  %(epoch+1,step+1,avg,train_accuracy,test_accuracy,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
