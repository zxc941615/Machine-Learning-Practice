{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "stock_path = \"C:/Users/USER/Desktop/Lab/lab4_2.csv\"\n",
    "stock_df = pd.read_csv(stock_path)\n",
    "stock_df = stock_df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_Change(%)</th>\n",
       "      <th>High_Change(%)</th>\n",
       "      <th>Low_Change(%)</th>\n",
       "      <th>Close_Change(%)</th>\n",
       "      <th>K r/f</th>\n",
       "      <th>D r/f</th>\n",
       "      <th>MACD r/f</th>\n",
       "      <th>SMA r/f</th>\n",
       "      <th>RISE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010/1/21</td>\n",
       "      <td>-0.166353</td>\n",
       "      <td>-2.097008</td>\n",
       "      <td>-4.047132</td>\n",
       "      <td>-6.219438</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010/1/22</td>\n",
       "      <td>-5.666917</td>\n",
       "      <td>-0.658867</td>\n",
       "      <td>-0.527480</td>\n",
       "      <td>3.141455</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010/1/25</td>\n",
       "      <td>4.417183</td>\n",
       "      <td>-1.990007</td>\n",
       "      <td>-2.826685</td>\n",
       "      <td>-6.937428</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010/1/26</td>\n",
       "      <td>-6.937428</td>\n",
       "      <td>-6.260734</td>\n",
       "      <td>-6.909228</td>\n",
       "      <td>-6.909228</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010/1/27</td>\n",
       "      <td>-6.909228</td>\n",
       "      <td>-4.873456</td>\n",
       "      <td>-3.906296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>2019/12/25</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.863558</td>\n",
       "      <td>-0.530034</td>\n",
       "      <td>0.706717</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>2019/12/26</td>\n",
       "      <td>2.473502</td>\n",
       "      <td>3.135887</td>\n",
       "      <td>2.309057</td>\n",
       "      <td>2.280700</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>2019/12/27</td>\n",
       "      <td>1.034479</td>\n",
       "      <td>0.675671</td>\n",
       "      <td>1.388896</td>\n",
       "      <td>1.886798</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>2019/12/30</td>\n",
       "      <td>5.460753</td>\n",
       "      <td>9.563767</td>\n",
       "      <td>5.821912</td>\n",
       "      <td>9.932661</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>2019/12/31</td>\n",
       "      <td>5.663437</td>\n",
       "      <td>3.369061</td>\n",
       "      <td>4.368934</td>\n",
       "      <td>3.369061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2438 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Open_Change(%)  High_Change(%)  Low_Change(%)  \\\n",
       "1      2010/1/21       -0.166353       -2.097008      -4.047132   \n",
       "2      2010/1/22       -5.666917       -0.658867      -0.527480   \n",
       "3      2010/1/25        4.417183       -1.990007      -2.826685   \n",
       "4      2010/1/26       -6.937428       -6.260734      -6.909228   \n",
       "5      2010/1/27       -6.909228       -4.873456      -3.906296   \n",
       "...          ...             ...             ...            ...   \n",
       "2434  2019/12/25       -0.701758       -0.863558      -0.530034   \n",
       "2435  2019/12/26        2.473502        3.135887       2.309057   \n",
       "2436  2019/12/27        1.034479        0.675671       1.388896   \n",
       "2437  2019/12/30        5.460753        9.563767       5.821912   \n",
       "2438  2019/12/31        5.663437        3.369061       4.368934   \n",
       "\n",
       "      Close_Change(%)  K r/f  D r/f  MACD r/f  SMA r/f  RISE  \n",
       "1           -6.219438     -1     -1        -1       -1    -1  \n",
       "2            3.141455     -1     -1        -1       -1     1  \n",
       "3           -6.937428     -1     -1        -1       -1    -1  \n",
       "4           -6.909228     -1     -1        -1       -1    -1  \n",
       "5            0.000000     -1     -1        -1       -1     0  \n",
       "...               ...    ...    ...       ...      ...   ...  \n",
       "2434         0.706717      1      1         1        1     1  \n",
       "2435         2.280700      1      1         1        1     1  \n",
       "2436         1.886798      1      1         1        1     1  \n",
       "2437         9.932661      1      1         1        1     1  \n",
       "2438         3.369061      1      1         1        1     1  \n",
       "\n",
       "[2438 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler(feature_range = (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = scaler.fit_transform(stock_df[[\"Open_Change(%)\",\"High_Change(%)\",\"Low_Change(%)\",\"Close_Change(%)\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04530549 -0.09189919 -0.52636024 -0.62124465]\n",
      " [-0.4447902   0.04276637 -0.24415337  0.31657508]\n",
      " [ 0.28757901 -0.08187981 -0.42850435 -0.69317645]\n",
      " ...\n",
      " [ 0.04190633  0.16773068 -0.09049776  0.19087739]\n",
      " [ 0.36336948  1.          0.26494293  0.99695111]\n",
      " [ 0.37808966  0.41993612  0.14844263  0.33937774]]\n"
     ]
    }
   ],
   "source": [
    "print(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = pd.DataFrame(minmax_df,columns = [\"Open_Change(%)\",\"High_Change(%)\",\"Low_Change(%)\",\"Close_Change(%)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df[\"K r/f\"] = stock_df[\"K r/f\"].values\n",
    "minmax_df[\"D r/f\"] = stock_df[\"D r/f\"].values\n",
    "minmax_df[\"MACD r/f\"] = stock_df[\"MACD r/f\"].values\n",
    "minmax_df[\"SMA r/f\"] = stock_df[\"SMA r/f\"].values\n",
    "minmax_df[\"RISE\"] = stock_df[\"RISE\"].values\n",
    "minmax_df[\"Date\"] = stock_df[\"Date\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_Change(%)</th>\n",
       "      <th>High_Change(%)</th>\n",
       "      <th>Low_Change(%)</th>\n",
       "      <th>Close_Change(%)</th>\n",
       "      <th>K r/f</th>\n",
       "      <th>D r/f</th>\n",
       "      <th>MACD r/f</th>\n",
       "      <th>SMA r/f</th>\n",
       "      <th>RISE</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045305</td>\n",
       "      <td>-0.091899</td>\n",
       "      <td>-0.526360</td>\n",
       "      <td>-0.621245</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2010/1/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.444790</td>\n",
       "      <td>0.042766</td>\n",
       "      <td>-0.244153</td>\n",
       "      <td>0.316575</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010/1/22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287579</td>\n",
       "      <td>-0.081880</td>\n",
       "      <td>-0.428504</td>\n",
       "      <td>-0.693176</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2010/1/25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.537063</td>\n",
       "      <td>-0.481785</td>\n",
       "      <td>-0.755844</td>\n",
       "      <td>-0.690351</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2010/1/26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.535014</td>\n",
       "      <td>-0.351882</td>\n",
       "      <td>-0.515068</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2010/1/27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.179564</td>\n",
       "      <td>0.459829</td>\n",
       "      <td>0.254462</td>\n",
       "      <td>0.686732</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010/1/28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.214816</td>\n",
       "      <td>0.549530</td>\n",
       "      <td>-0.016832</td>\n",
       "      <td>0.239940</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010/1/29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.166680</td>\n",
       "      <td>-0.026281</td>\n",
       "      <td>-0.081281</td>\n",
       "      <td>-0.087620</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2010/2/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.031609</td>\n",
       "      <td>0.104462</td>\n",
       "      <td>-0.454291</td>\n",
       "      <td>-0.557745</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2010/2/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.431706</td>\n",
       "      <td>0.021611</td>\n",
       "      <td>-0.079206</td>\n",
       "      <td>0.690164</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010/2/3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Open_Change(%)  High_Change(%)  Low_Change(%)  Close_Change(%)  K r/f  \\\n",
       "0       -0.045305       -0.091899      -0.526360        -0.621245     -1   \n",
       "1       -0.444790        0.042766      -0.244153         0.316575     -1   \n",
       "2        0.287579       -0.081880      -0.428504        -0.693176     -1   \n",
       "3       -0.537063       -0.481785      -0.755844        -0.690351     -1   \n",
       "4       -0.535014       -0.351882      -0.515068         0.001849     -1   \n",
       "5        0.179564        0.459829       0.254462         0.686732      1   \n",
       "6        0.214816        0.549530      -0.016832         0.239940      1   \n",
       "7        0.166680       -0.026281      -0.081281        -0.087620      1   \n",
       "8        0.031609        0.104462      -0.454291        -0.557745     -1   \n",
       "9       -0.431706        0.021611      -0.079206         0.690164      1   \n",
       "\n",
       "   D r/f  MACD r/f  SMA r/f  RISE       Date  \n",
       "0     -1        -1       -1    -1  2010/1/21  \n",
       "1     -1        -1       -1     1  2010/1/22  \n",
       "2     -1        -1       -1    -1  2010/1/25  \n",
       "3     -1        -1       -1    -1  2010/1/26  \n",
       "4     -1        -1       -1     0  2010/1/27  \n",
       "5     -1         1       -1     1  2010/1/28  \n",
       "6     -1         1       -1     1  2010/1/29  \n",
       "7      1         1       -1    -1   2010/2/1  \n",
       "8     -1        -1       -1    -1   2010/2/2  \n",
       "9      1         1        1     1   2010/2/3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2438"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 10)\n",
      "(238, 10)\n"
     ]
    }
   ],
   "source": [
    "train_set = minmax_df[:2200]\n",
    "test_set = minmax_df[2200:]\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 100000\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEP = 7\n",
    "INPUT_SIZE = 8\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of feature:  torch.Size([2431, 7, 8])\n",
      "size of label:  torch.Size([2431, 1])\n"
     ]
    }
   ],
   "source": [
    "# declear training features data\n",
    "features = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i-TIME_STEP:i][[\"Open_Change(%)\",\"High_Change(%)\",\"Low_Change(%)\",\"Close_Change(%)\",\"K r/f\",\"D r/f\",\"MACD r/f\",\"SMA r/f\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = torch.FloatTensor(features)\n",
    "print(\"size of feature: \",features.size())\n",
    "\n",
    "# declear trainging labels data\n",
    "labels = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i:i+1][\"RISE\"]\n",
    "    labels.append(x.tolist())\n",
    "labels = torch.FloatTensor(labels)\n",
    "print(\"size of label: \",labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random select 90% training set index and 10% testing set index \n",
    "\n",
    "x = np.linspace(0,2430,2431).tolist()\n",
    "for i in range(0,2431):\n",
    "    x[i] = int(x[i])\n",
    "\n",
    "x = np.random.shuffle(x)\n",
    "training, test = x[:2200], x[2200:]\n",
    "test.sort()\n",
    "training.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features[training]\n",
    "train_labels = labels[training]\n",
    "test_features = features[test]\n",
    "test_labels = labels[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(train_features,train_labels)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define NN architecture\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        # define lstm layer\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=128,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        self.lstm2 = torch.nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=64,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        # dropout layer\n",
    "        self.Relu =  torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p = 0.5)\n",
    "        self.out = torch.nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm1_out,(h_n,h_c) = self.lstm(x,None)\n",
    "        lstm1_out = self.dropout(lstm1_out)\n",
    "        lstm2_out,_ = self.lstm2(lstm1_out,None)\n",
    "        lstm2_out = self.Relu(lstm2_out[:,-1,:])\n",
    "        out = self.out(lstm2_out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(8, 128, batch_first=True)\n",
      "  (lstm2): LSTM(128, 64, batch_first=True)\n",
      "  (Relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.001)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.98,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   30|Train Avg Loss: 0.9315 |Test Loss: 0.9315|lr = 0.01000\n",
      "Epoch:    1|steps:   60|Train Avg Loss: 0.9373 |Test Loss: 0.9774|lr = 0.01000\n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.9343 |Test Loss: 0.9539|lr = 0.01000\n",
      "Epoch:    2|steps:   60|Train Avg Loss: 0.9295 |Test Loss: 0.9339|lr = 0.01000\n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.9326 |Test Loss: 0.9365|lr = 0.01000\n",
      "Epoch:    3|steps:   60|Train Avg Loss: 0.9358 |Test Loss: 0.9730|lr = 0.01000\n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.9250 |Test Loss: 0.9691|lr = 0.01000\n",
      "Epoch:    4|steps:   60|Train Avg Loss: 0.9354 |Test Loss: 0.9452|lr = 0.01000\n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.9344 |Test Loss: 0.9637|lr = 0.01000\n",
      "Epoch:    5|steps:   60|Train Avg Loss: 0.9292 |Test Loss: 0.9442|lr = 0.01000\n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.9279 |Test Loss: 0.9620|lr = 0.01000\n",
      "Epoch:    6|steps:   60|Train Avg Loss: 0.9412 |Test Loss: 0.9757|lr = 0.01000\n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.9285 |Test Loss: 0.9609|lr = 0.01000\n",
      "Epoch:    7|steps:   60|Train Avg Loss: 0.9410 |Test Loss: 0.9422|lr = 0.01000\n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.9297 |Test Loss: 0.9568|lr = 0.01000\n",
      "Epoch:    8|steps:   60|Train Avg Loss: 0.9345 |Test Loss: 0.9622|lr = 0.01000\n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.9401 |Test Loss: 0.9653|lr = 0.01000\n",
      "Epoch:    9|steps:   60|Train Avg Loss: 0.9327 |Test Loss: 0.9393|lr = 0.01000\n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.9385 |Test Loss: 0.9706|lr = 0.01000\n",
      "Epoch:   10|steps:   60|Train Avg Loss: 0.9245 |Test Loss: 0.9763|lr = 0.01000\n",
      "Epoch:   11|steps:   30|Train Avg Loss: 0.9395 |Test Loss: 0.9487|lr = 0.01000\n",
      "Epoch:   11|steps:   60|Train Avg Loss: 0.9234 |Test Loss: 0.9537|lr = 0.01000\n",
      "Epoch:   12|steps:   30|Train Avg Loss: 0.9280 |Test Loss: 0.9586|lr = 0.01000\n",
      "Epoch:   12|steps:   60|Train Avg Loss: 0.9364 |Test Loss: 0.9376|lr = 0.01000\n",
      "Epoch:   13|steps:   30|Train Avg Loss: 0.9279 |Test Loss: 0.9440|lr = 0.01000\n",
      "Epoch:   13|steps:   60|Train Avg Loss: 0.9330 |Test Loss: 0.9644|lr = 0.01000\n",
      "Epoch:   14|steps:   30|Train Avg Loss: 0.9240 |Test Loss: 0.9571|lr = 0.01000\n",
      "Epoch:   14|steps:   60|Train Avg Loss: 0.9372 |Test Loss: 0.9914|lr = 0.01000\n",
      "Epoch:   15|steps:   30|Train Avg Loss: 0.9356 |Test Loss: 0.9689|lr = 0.01000\n",
      "Epoch:   15|steps:   60|Train Avg Loss: 0.9303 |Test Loss: 0.9522|lr = 0.01000\n",
      "Epoch:   16|steps:   30|Train Avg Loss: 0.9330 |Test Loss: 0.9789|lr = 0.01000\n",
      "Epoch:   16|steps:   60|Train Avg Loss: 0.9265 |Test Loss: 0.9596|lr = 0.01000\n",
      "Epoch:   17|steps:   30|Train Avg Loss: 0.9286 |Test Loss: 0.9751|lr = 0.01000\n",
      "Epoch:   17|steps:   60|Train Avg Loss: 0.9295 |Test Loss: 0.9386|lr = 0.01000\n",
      "Epoch:   18|steps:   30|Train Avg Loss: 0.9338 |Test Loss: 0.9699|lr = 0.01000\n",
      "Epoch:   18|steps:   60|Train Avg Loss: 0.9346 |Test Loss: 0.9595|lr = 0.01000\n",
      "Epoch:   19|steps:   30|Train Avg Loss: 0.9423 |Test Loss: 0.9802|lr = 0.00980\n",
      "Epoch:   19|steps:   60|Train Avg Loss: 0.9298 |Test Loss: 0.9527|lr = 0.00980\n",
      "Epoch:   20|steps:   30|Train Avg Loss: 0.9493 |Test Loss: 0.9516|lr = 0.00980\n",
      "Epoch:   20|steps:   60|Train Avg Loss: 0.9275 |Test Loss: 0.9570|lr = 0.00980\n",
      "Epoch:   21|steps:   30|Train Avg Loss: 0.9350 |Test Loss: 0.9709|lr = 0.00980\n",
      "Epoch:   21|steps:   60|Train Avg Loss: 0.9301 |Test Loss: 0.9561|lr = 0.00980\n",
      "Epoch:   22|steps:   30|Train Avg Loss: 0.9268 |Test Loss: 0.9502|lr = 0.00980\n",
      "Epoch:   22|steps:   60|Train Avg Loss: 0.9398 |Test Loss: 0.9649|lr = 0.00980\n",
      "Epoch:   23|steps:   30|Train Avg Loss: 0.9366 |Test Loss: 0.9593|lr = 0.00980\n",
      "Epoch:   23|steps:   60|Train Avg Loss: 0.9344 |Test Loss: 0.9543|lr = 0.00980\n",
      "Epoch:   24|steps:   30|Train Avg Loss: 0.9294 |Test Loss: 0.9470|lr = 0.00980\n",
      "Epoch:   24|steps:   60|Train Avg Loss: 0.9305 |Test Loss: 0.9672|lr = 0.00980\n",
      "Epoch:   25|steps:   30|Train Avg Loss: 0.9322 |Test Loss: 0.9610|lr = 0.00980\n",
      "Epoch:   25|steps:   60|Train Avg Loss: 0.9342 |Test Loss: 0.9544|lr = 0.00980\n",
      "Epoch:   26|steps:   30|Train Avg Loss: 0.9355 |Test Loss: 0.9725|lr = 0.00980\n",
      "Epoch:   26|steps:   60|Train Avg Loss: 0.9266 |Test Loss: 0.9552|lr = 0.00980\n",
      "Epoch:   27|steps:   30|Train Avg Loss: 0.9371 |Test Loss: 0.9437|lr = 0.00980\n",
      "Epoch:   27|steps:   60|Train Avg Loss: 0.9277 |Test Loss: 0.9804|lr = 0.00980\n",
      "Epoch:   28|steps:   30|Train Avg Loss: 0.9389 |Test Loss: 0.9605|lr = 0.00980\n",
      "Epoch:   28|steps:   60|Train Avg Loss: 0.9246 |Test Loss: 0.9310|lr = 0.00980\n",
      "Epoch:   29|steps:   30|Train Avg Loss: 0.9428 |Test Loss: 0.9618|lr = 0.00980\n",
      "Epoch:   29|steps:   60|Train Avg Loss: 0.9243 |Test Loss: 0.9717|lr = 0.00980\n",
      "Epoch:   30|steps:   30|Train Avg Loss: 0.9324 |Test Loss: 0.9447|lr = 0.00960\n",
      "Epoch:   30|steps:   60|Train Avg Loss: 0.9274 |Test Loss: 0.9758|lr = 0.00960\n",
      "Epoch:   31|steps:   30|Train Avg Loss: 0.9282 |Test Loss: 0.9827|lr = 0.00960\n",
      "Epoch:   31|steps:   60|Train Avg Loss: 0.9388 |Test Loss: 0.9697|lr = 0.00960\n",
      "Epoch:   32|steps:   30|Train Avg Loss: 0.9491 |Test Loss: 0.9425|lr = 0.00960\n",
      "Epoch:   32|steps:   60|Train Avg Loss: 0.9322 |Test Loss: 0.9395|lr = 0.00960\n",
      "Epoch:   33|steps:   30|Train Avg Loss: 0.9301 |Test Loss: 1.0025|lr = 0.00960\n",
      "Epoch:   33|steps:   60|Train Avg Loss: 0.9419 |Test Loss: 0.9648|lr = 0.00960\n",
      "Epoch:   34|steps:   30|Train Avg Loss: 0.9273 |Test Loss: 0.9501|lr = 0.00960\n",
      "Epoch:   34|steps:   60|Train Avg Loss: 0.9400 |Test Loss: 0.9669|lr = 0.00960\n",
      "Epoch:   35|steps:   30|Train Avg Loss: 0.9313 |Test Loss: 0.9622|lr = 0.00960\n",
      "Epoch:   35|steps:   60|Train Avg Loss: 0.9407 |Test Loss: 0.9626|lr = 0.00960\n",
      "Epoch:   36|steps:   30|Train Avg Loss: 0.9352 |Test Loss: 0.9712|lr = 0.00960\n",
      "Epoch:   36|steps:   60|Train Avg Loss: 0.9333 |Test Loss: 0.9540|lr = 0.00960\n",
      "Epoch:   37|steps:   30|Train Avg Loss: 0.9271 |Test Loss: 0.9740|lr = 0.00960\n",
      "Epoch:   37|steps:   60|Train Avg Loss: 0.9367 |Test Loss: 0.9410|lr = 0.00960\n",
      "Epoch:   38|steps:   30|Train Avg Loss: 0.9370 |Test Loss: 0.9396|lr = 0.00960\n",
      "Epoch:   38|steps:   60|Train Avg Loss: 0.9371 |Test Loss: 0.9922|lr = 0.00960\n",
      "Epoch:   39|steps:   30|Train Avg Loss: 0.9290 |Test Loss: 0.9777|lr = 0.00960\n",
      "Epoch:   39|steps:   60|Train Avg Loss: 0.9389 |Test Loss: 0.9747|lr = 0.00960\n",
      "Epoch:   40|steps:   30|Train Avg Loss: 0.9324 |Test Loss: 0.9523|lr = 0.00960\n",
      "Epoch:   40|steps:   60|Train Avg Loss: 0.9374 |Test Loss: 0.9387|lr = 0.00960\n",
      "Epoch:   41|steps:   30|Train Avg Loss: 0.9313 |Test Loss: 0.9634|lr = 0.00941\n",
      "Epoch:   41|steps:   60|Train Avg Loss: 0.9309 |Test Loss: 0.9657|lr = 0.00941\n",
      "Epoch:   42|steps:   30|Train Avg Loss: 0.9336 |Test Loss: 0.9493|lr = 0.00941\n",
      "Epoch:   42|steps:   60|Train Avg Loss: 0.9312 |Test Loss: 0.9547|lr = 0.00941\n",
      "Epoch:   43|steps:   30|Train Avg Loss: 0.9361 |Test Loss: 0.9472|lr = 0.00941\n",
      "Epoch:   43|steps:   60|Train Avg Loss: 0.9308 |Test Loss: 0.9649|lr = 0.00941\n",
      "Epoch:   44|steps:   30|Train Avg Loss: 0.9358 |Test Loss: 0.9484|lr = 0.00941\n",
      "Epoch:   44|steps:   60|Train Avg Loss: 0.9251 |Test Loss: 0.9643|lr = 0.00941\n",
      "Epoch:   45|steps:   30|Train Avg Loss: 0.9424 |Test Loss: 0.9471|lr = 0.00941\n",
      "Epoch:   45|steps:   60|Train Avg Loss: 0.9207 |Test Loss: 1.0725|lr = 0.00941\n",
      "Epoch:   46|steps:   30|Train Avg Loss: 0.9268 |Test Loss: 0.9678|lr = 0.00941\n",
      "Epoch:   46|steps:   60|Train Avg Loss: 0.9430 |Test Loss: 1.0461|lr = 0.00941\n",
      "Epoch:   47|steps:   30|Train Avg Loss: 0.9449 |Test Loss: 0.9663|lr = 0.00941\n",
      "Epoch:   47|steps:   60|Train Avg Loss: 0.9479 |Test Loss: 0.9744|lr = 0.00941\n",
      "Epoch:   48|steps:   30|Train Avg Loss: 0.9350 |Test Loss: 1.0145|lr = 0.00941\n",
      "Epoch:   48|steps:   60|Train Avg Loss: 0.9377 |Test Loss: 0.9493|lr = 0.00941\n",
      "Epoch:   49|steps:   30|Train Avg Loss: 0.9379 |Test Loss: 0.9806|lr = 0.00941\n",
      "Epoch:   49|steps:   60|Train Avg Loss: 0.9354 |Test Loss: 0.9578|lr = 0.00941\n",
      "Epoch:   50|steps:   30|Train Avg Loss: 0.9290 |Test Loss: 0.9590|lr = 0.00941\n",
      "Epoch:   50|steps:   60|Train Avg Loss: 0.9327 |Test Loss: 0.9622|lr = 0.00941\n",
      "Epoch:   51|steps:   30|Train Avg Loss: 0.9280 |Test Loss: 0.9605|lr = 0.00941\n",
      "Epoch:   51|steps:   60|Train Avg Loss: 0.9367 |Test Loss: 0.9434|lr = 0.00941\n",
      "Epoch:   52|steps:   30|Train Avg Loss: 0.9366 |Test Loss: 0.9816|lr = 0.00922\n",
      "Epoch:   52|steps:   60|Train Avg Loss: 0.9339 |Test Loss: 0.9720|lr = 0.00922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   53|steps:   30|Train Avg Loss: 0.9226 |Test Loss: 0.9641|lr = 0.00922\n",
      "Epoch:   53|steps:   60|Train Avg Loss: 0.9402 |Test Loss: 0.9493|lr = 0.00922\n",
      "Epoch:   54|steps:   30|Train Avg Loss: 0.9300 |Test Loss: 0.9500|lr = 0.00922\n",
      "Epoch:   54|steps:   60|Train Avg Loss: 0.9324 |Test Loss: 0.9832|lr = 0.00922\n",
      "Epoch:   55|steps:   30|Train Avg Loss: 0.9238 |Test Loss: 0.9586|lr = 0.00922\n",
      "Epoch:   55|steps:   60|Train Avg Loss: 0.9356 |Test Loss: 0.9568|lr = 0.00922\n",
      "Epoch:   56|steps:   30|Train Avg Loss: 0.9348 |Test Loss: 0.9574|lr = 0.00922\n",
      "Epoch:   56|steps:   60|Train Avg Loss: 0.9423 |Test Loss: 0.9650|lr = 0.00922\n",
      "Epoch:   57|steps:   30|Train Avg Loss: 0.9373 |Test Loss: 0.9462|lr = 0.00922\n",
      "Epoch:   57|steps:   60|Train Avg Loss: 0.9315 |Test Loss: 0.9774|lr = 0.00922\n",
      "Epoch:   58|steps:   30|Train Avg Loss: 0.9416 |Test Loss: 0.9485|lr = 0.00922\n",
      "Epoch:   58|steps:   60|Train Avg Loss: 0.9237 |Test Loss: 1.0302|lr = 0.00922\n",
      "Epoch:   59|steps:   30|Train Avg Loss: 0.9367 |Test Loss: 1.0314|lr = 0.00922\n",
      "Epoch:   59|steps:   60|Train Avg Loss: 0.9336 |Test Loss: 0.9709|lr = 0.00922\n",
      "Epoch:   60|steps:   30|Train Avg Loss: 0.9310 |Test Loss: 0.9569|lr = 0.00922\n",
      "Epoch:   60|steps:   60|Train Avg Loss: 0.9380 |Test Loss: 0.9644|lr = 0.00922\n",
      "Epoch:   61|steps:   30|Train Avg Loss: 0.9341 |Test Loss: 0.9553|lr = 0.00922\n",
      "Epoch:   61|steps:   60|Train Avg Loss: 0.9360 |Test Loss: 0.9778|lr = 0.00922\n",
      "Epoch:   62|steps:   30|Train Avg Loss: 0.9277 |Test Loss: 0.9620|lr = 0.00922\n",
      "Epoch:   62|steps:   60|Train Avg Loss: 0.9380 |Test Loss: 0.9623|lr = 0.00922\n",
      "Epoch:   63|steps:   30|Train Avg Loss: 0.9358 |Test Loss: 0.9586|lr = 0.00904\n",
      "Epoch:   63|steps:   60|Train Avg Loss: 0.9306 |Test Loss: 0.9875|lr = 0.00904\n",
      "Epoch:   64|steps:   30|Train Avg Loss: 0.9264 |Test Loss: 0.9530|lr = 0.00904\n",
      "Epoch:   64|steps:   60|Train Avg Loss: 0.9395 |Test Loss: 0.9559|lr = 0.00904\n",
      "Epoch:   65|steps:   30|Train Avg Loss: 0.9448 |Test Loss: 0.9566|lr = 0.00904\n",
      "Epoch:   65|steps:   60|Train Avg Loss: 0.9254 |Test Loss: 0.9565|lr = 0.00904\n",
      "Epoch:   66|steps:   30|Train Avg Loss: 0.9289 |Test Loss: 0.9757|lr = 0.00904\n",
      "Epoch:   66|steps:   60|Train Avg Loss: 0.9409 |Test Loss: 0.9631|lr = 0.00904\n",
      "Epoch:   67|steps:   30|Train Avg Loss: 0.9305 |Test Loss: 0.9564|lr = 0.00904\n",
      "Epoch:   67|steps:   60|Train Avg Loss: 0.9343 |Test Loss: 0.9487|lr = 0.00904\n",
      "Epoch:   68|steps:   30|Train Avg Loss: 0.9396 |Test Loss: 0.9568|lr = 0.00904\n",
      "Epoch:   68|steps:   60|Train Avg Loss: 0.9330 |Test Loss: 0.9629|lr = 0.00904\n",
      "Epoch:   69|steps:   30|Train Avg Loss: 0.9305 |Test Loss: 0.9525|lr = 0.00904\n",
      "Epoch:   69|steps:   60|Train Avg Loss: 0.9338 |Test Loss: 0.9526|lr = 0.00904\n",
      "Epoch:   70|steps:   30|Train Avg Loss: 0.9286 |Test Loss: 0.9575|lr = 0.00904\n",
      "Epoch:   70|steps:   60|Train Avg Loss: 0.9393 |Test Loss: 0.9729|lr = 0.00904\n",
      "Epoch:   71|steps:   30|Train Avg Loss: 0.9353 |Test Loss: 0.9531|lr = 0.00904\n",
      "Epoch:   71|steps:   60|Train Avg Loss: 0.9321 |Test Loss: 0.9485|lr = 0.00904\n",
      "Epoch:   72|steps:   30|Train Avg Loss: 0.9316 |Test Loss: 0.9606|lr = 0.00904\n",
      "Epoch:   72|steps:   60|Train Avg Loss: 0.9317 |Test Loss: 0.9616|lr = 0.00904\n",
      "Epoch:   73|steps:   30|Train Avg Loss: 0.9360 |Test Loss: 0.9558|lr = 0.00904\n",
      "Epoch:   73|steps:   60|Train Avg Loss: 0.9347 |Test Loss: 0.9583|lr = 0.00904\n",
      "Epoch:   74|steps:   30|Train Avg Loss: 0.9241 |Test Loss: 0.9624|lr = 0.00886\n",
      "Epoch:   74|steps:   60|Train Avg Loss: 0.9434 |Test Loss: 0.9582|lr = 0.00886\n",
      "Epoch:   75|steps:   30|Train Avg Loss: 0.9292 |Test Loss: 0.9576|lr = 0.00886\n",
      "Epoch:   75|steps:   60|Train Avg Loss: 0.9367 |Test Loss: 0.9614|lr = 0.00886\n",
      "Epoch:   76|steps:   30|Train Avg Loss: 0.9337 |Test Loss: 0.9551|lr = 0.00886\n",
      "Epoch:   76|steps:   60|Train Avg Loss: 0.9348 |Test Loss: 0.9686|lr = 0.00886\n",
      "Epoch:   77|steps:   30|Train Avg Loss: 0.9263 |Test Loss: 0.9572|lr = 0.00886\n",
      "Epoch:   77|steps:   60|Train Avg Loss: 0.9427 |Test Loss: 0.9589|lr = 0.00886\n",
      "Epoch:   78|steps:   30|Train Avg Loss: 0.9302 |Test Loss: 0.9628|lr = 0.00886\n",
      "Epoch:   78|steps:   60|Train Avg Loss: 0.9299 |Test Loss: 0.9681|lr = 0.00886\n",
      "Epoch:   79|steps:   30|Train Avg Loss: 0.9321 |Test Loss: 0.9539|lr = 0.00886\n",
      "Epoch:   79|steps:   60|Train Avg Loss: 0.9290 |Test Loss: 0.9656|lr = 0.00886\n",
      "Epoch:   80|steps:   30|Train Avg Loss: 0.9310 |Test Loss: 0.9529|lr = 0.00886\n",
      "Epoch:   80|steps:   60|Train Avg Loss: 0.9346 |Test Loss: 0.9634|lr = 0.00886\n",
      "Epoch:   81|steps:   30|Train Avg Loss: 0.9355 |Test Loss: 0.9607|lr = 0.00886\n",
      "Epoch:   81|steps:   60|Train Avg Loss: 0.9377 |Test Loss: 0.9621|lr = 0.00886\n",
      "Epoch:   82|steps:   30|Train Avg Loss: 0.9326 |Test Loss: 0.9554|lr = 0.00886\n",
      "Epoch:   82|steps:   60|Train Avg Loss: 0.9209 |Test Loss: 0.9562|lr = 0.00886\n",
      "Epoch:   83|steps:   30|Train Avg Loss: 0.9419 |Test Loss: 0.9607|lr = 0.00886\n",
      "Epoch:   83|steps:   60|Train Avg Loss: 0.9311 |Test Loss: 0.9607|lr = 0.00886\n",
      "Epoch:   84|steps:   30|Train Avg Loss: 0.9373 |Test Loss: 0.9658|lr = 0.00886\n",
      "Epoch:   84|steps:   60|Train Avg Loss: 0.9288 |Test Loss: 0.9499|lr = 0.00886\n",
      "Epoch:   85|steps:   30|Train Avg Loss: 0.9302 |Test Loss: 0.9527|lr = 0.00868\n",
      "Epoch:   85|steps:   60|Train Avg Loss: 0.9360 |Test Loss: 0.9618|lr = 0.00868\n",
      "Epoch:   86|steps:   30|Train Avg Loss: 0.9364 |Test Loss: 0.9668|lr = 0.00868\n",
      "Epoch:   86|steps:   60|Train Avg Loss: 0.9317 |Test Loss: 0.9528|lr = 0.00868\n",
      "Epoch:   87|steps:   30|Train Avg Loss: 0.9313 |Test Loss: 0.9615|lr = 0.00868\n",
      "Epoch:   87|steps:   60|Train Avg Loss: 0.9388 |Test Loss: 0.9588|lr = 0.00868\n",
      "Epoch:   88|steps:   30|Train Avg Loss: 0.9186 |Test Loss: 0.9673|lr = 0.00868\n",
      "Epoch:   88|steps:   60|Train Avg Loss: 0.9458 |Test Loss: 0.9498|lr = 0.00868\n",
      "Epoch:   89|steps:   30|Train Avg Loss: 0.9293 |Test Loss: 0.9481|lr = 0.00868\n",
      "Epoch:   89|steps:   60|Train Avg Loss: 0.9330 |Test Loss: 0.9647|lr = 0.00868\n",
      "Epoch:   90|steps:   30|Train Avg Loss: 0.9289 |Test Loss: 0.9565|lr = 0.00868\n",
      "Epoch:   90|steps:   60|Train Avg Loss: 0.9347 |Test Loss: 0.9537|lr = 0.00868\n",
      "Epoch:   91|steps:   30|Train Avg Loss: 0.9282 |Test Loss: 0.9663|lr = 0.00868\n",
      "Epoch:   91|steps:   60|Train Avg Loss: 0.9366 |Test Loss: 0.9566|lr = 0.00868\n",
      "Epoch:   92|steps:   30|Train Avg Loss: 0.9389 |Test Loss: 0.9562|lr = 0.00868\n",
      "Epoch:   92|steps:   60|Train Avg Loss: 0.9195 |Test Loss: 0.9645|lr = 0.00868\n",
      "Epoch:   93|steps:   30|Train Avg Loss: 0.9296 |Test Loss: 0.9500|lr = 0.00868\n",
      "Epoch:   93|steps:   60|Train Avg Loss: 0.9336 |Test Loss: 0.9548|lr = 0.00868\n",
      "Epoch:   94|steps:   30|Train Avg Loss: 0.9362 |Test Loss: 0.9617|lr = 0.00868\n",
      "Epoch:   94|steps:   60|Train Avg Loss: 0.9317 |Test Loss: 0.9612|lr = 0.00868\n",
      "Epoch:   95|steps:   30|Train Avg Loss: 0.9335 |Test Loss: 0.9586|lr = 0.00868\n",
      "Epoch:   95|steps:   60|Train Avg Loss: 0.9329 |Test Loss: 0.9624|lr = 0.00868\n",
      "Epoch:   96|steps:   30|Train Avg Loss: 0.9333 |Test Loss: 0.9565|lr = 0.00851\n",
      "Epoch:   96|steps:   60|Train Avg Loss: 0.9256 |Test Loss: 0.9586|lr = 0.00851\n",
      "Epoch:   97|steps:   30|Train Avg Loss: 0.9299 |Test Loss: 0.9567|lr = 0.00851\n",
      "Epoch:   97|steps:   60|Train Avg Loss: 0.9371 |Test Loss: 0.9608|lr = 0.00851\n",
      "Epoch:   98|steps:   30|Train Avg Loss: 0.9303 |Test Loss: 0.9587|lr = 0.00851\n",
      "Epoch:   98|steps:   60|Train Avg Loss: 0.9337 |Test Loss: 0.9620|lr = 0.00851\n",
      "Epoch:   99|steps:   30|Train Avg Loss: 0.9365 |Test Loss: 0.9680|lr = 0.00851\n",
      "Epoch:   99|steps:   60|Train Avg Loss: 0.9289 |Test Loss: 0.9574|lr = 0.00851\n",
      "Epoch:  100|steps:   30|Train Avg Loss: 0.9359 |Test Loss: 0.9535|lr = 0.00851\n",
      "Epoch:  100|steps:   60|Train Avg Loss: 0.9338 |Test Loss: 0.9477|lr = 0.00851\n",
      "Epoch:  101|steps:   30|Train Avg Loss: 0.9331 |Test Loss: 0.9728|lr = 0.00851\n",
      "Epoch:  101|steps:   60|Train Avg Loss: 0.9263 |Test Loss: 0.9635|lr = 0.00851\n",
      "Epoch:  102|steps:   30|Train Avg Loss: 0.9355 |Test Loss: 0.9556|lr = 0.00851\n",
      "Epoch:  102|steps:   60|Train Avg Loss: 0.9327 |Test Loss: 0.9544|lr = 0.00851\n",
      "Epoch:  103|steps:   30|Train Avg Loss: 0.9266 |Test Loss: 0.9618|lr = 0.00851\n",
      "Epoch:  103|steps:   60|Train Avg Loss: 0.9375 |Test Loss: 0.9531|lr = 0.00851\n",
      "Epoch:  104|steps:   30|Train Avg Loss: 0.9302 |Test Loss: 0.9571|lr = 0.00851\n",
      "Epoch:  104|steps:   60|Train Avg Loss: 0.9378 |Test Loss: 0.9624|lr = 0.00851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  105|steps:   30|Train Avg Loss: 0.9285 |Test Loss: 0.9688|lr = 0.00851\n",
      "Epoch:  105|steps:   60|Train Avg Loss: 0.9340 |Test Loss: 0.9495|lr = 0.00851\n",
      "Epoch:  106|steps:   30|Train Avg Loss: 0.9318 |Test Loss: 0.9511|lr = 0.00851\n",
      "Epoch:  106|steps:   60|Train Avg Loss: 0.9327 |Test Loss: 0.9613|lr = 0.00851\n",
      "Epoch:  107|steps:   30|Train Avg Loss: 0.9303 |Test Loss: 0.9578|lr = 0.00834\n",
      "Epoch:  107|steps:   60|Train Avg Loss: 0.9272 |Test Loss: 0.9622|lr = 0.00834\n",
      "Epoch:  108|steps:   30|Train Avg Loss: 0.9435 |Test Loss: 0.9545|lr = 0.00834\n",
      "Epoch:  108|steps:   60|Train Avg Loss: 0.9244 |Test Loss: 0.9460|lr = 0.00834\n",
      "Epoch:  109|steps:   30|Train Avg Loss: 0.9337 |Test Loss: 0.9652|lr = 0.00834\n",
      "Epoch:  109|steps:   60|Train Avg Loss: 0.9339 |Test Loss: 0.9584|lr = 0.00834\n",
      "Epoch:  110|steps:   30|Train Avg Loss: 0.9347 |Test Loss: 0.9632|lr = 0.00834\n",
      "Epoch:  110|steps:   60|Train Avg Loss: 0.9246 |Test Loss: 0.9545|lr = 0.00834\n",
      "Epoch:  111|steps:   30|Train Avg Loss: 0.9223 |Test Loss: 0.9601|lr = 0.00834\n",
      "Epoch:  111|steps:   60|Train Avg Loss: 0.9427 |Test Loss: 0.9611|lr = 0.00834\n",
      "Epoch:  112|steps:   30|Train Avg Loss: 0.9295 |Test Loss: 0.9629|lr = 0.00834\n",
      "Epoch:  112|steps:   60|Train Avg Loss: 0.9349 |Test Loss: 0.9563|lr = 0.00834\n",
      "Epoch:  113|steps:   30|Train Avg Loss: 0.9320 |Test Loss: 0.9568|lr = 0.00834\n",
      "Epoch:  113|steps:   60|Train Avg Loss: 0.9308 |Test Loss: 0.9540|lr = 0.00834\n",
      "Epoch:  114|steps:   30|Train Avg Loss: 0.9217 |Test Loss: 0.9647|lr = 0.00834\n",
      "Epoch:  114|steps:   60|Train Avg Loss: 0.9352 |Test Loss: 0.9650|lr = 0.00834\n",
      "Epoch:  115|steps:   30|Train Avg Loss: 0.9298 |Test Loss: 0.9559|lr = 0.00834\n",
      "Epoch:  115|steps:   60|Train Avg Loss: 0.9331 |Test Loss: 0.9564|lr = 0.00834\n",
      "Epoch:  116|steps:   30|Train Avg Loss: 0.9324 |Test Loss: 0.9487|lr = 0.00834\n",
      "Epoch:  116|steps:   60|Train Avg Loss: 0.9376 |Test Loss: 0.9764|lr = 0.00834\n",
      "Epoch:  117|steps:   30|Train Avg Loss: 0.9299 |Test Loss: 0.9664|lr = 0.00834\n",
      "Epoch:  117|steps:   60|Train Avg Loss: 0.9386 |Test Loss: 0.9575|lr = 0.00834\n",
      "Epoch:  118|steps:   30|Train Avg Loss: 0.9391 |Test Loss: 0.9579|lr = 0.00817\n",
      "Epoch:  118|steps:   60|Train Avg Loss: 0.9266 |Test Loss: 0.9491|lr = 0.00817\n",
      "Epoch:  119|steps:   30|Train Avg Loss: 0.9317 |Test Loss: 0.9681|lr = 0.00817\n",
      "Epoch:  119|steps:   60|Train Avg Loss: 0.9373 |Test Loss: 0.9584|lr = 0.00817\n",
      "Epoch:  120|steps:   30|Train Avg Loss: 0.9261 |Test Loss: 0.9576|lr = 0.00817\n",
      "Epoch:  120|steps:   60|Train Avg Loss: 0.9352 |Test Loss: 0.9594|lr = 0.00817\n",
      "Epoch:  121|steps:   30|Train Avg Loss: 0.9328 |Test Loss: 0.9652|lr = 0.00817\n",
      "Epoch:  121|steps:   60|Train Avg Loss: 0.9340 |Test Loss: 0.9547|lr = 0.00817\n",
      "Epoch:  122|steps:   30|Train Avg Loss: 0.9303 |Test Loss: 0.9570|lr = 0.00817\n",
      "Epoch:  122|steps:   60|Train Avg Loss: 0.9395 |Test Loss: 0.9624|lr = 0.00817\n",
      "Epoch:  123|steps:   30|Train Avg Loss: 0.9262 |Test Loss: 0.9702|lr = 0.00817\n",
      "Epoch:  123|steps:   60|Train Avg Loss: 0.9359 |Test Loss: 0.9534|lr = 0.00817\n",
      "Epoch:  124|steps:   30|Train Avg Loss: 0.9325 |Test Loss: 0.9444|lr = 0.00817\n",
      "Epoch:  124|steps:   60|Train Avg Loss: 0.9415 |Test Loss: 0.9622|lr = 0.00817\n",
      "Epoch:  125|steps:   30|Train Avg Loss: 0.9318 |Test Loss: 0.9640|lr = 0.00817\n",
      "Epoch:  125|steps:   60|Train Avg Loss: 0.9292 |Test Loss: 0.9535|lr = 0.00817\n",
      "Epoch:  126|steps:   30|Train Avg Loss: 0.9237 |Test Loss: 0.9529|lr = 0.00817\n",
      "Epoch:  126|steps:   60|Train Avg Loss: 0.9401 |Test Loss: 0.9609|lr = 0.00817\n",
      "Epoch:  127|steps:   30|Train Avg Loss: 0.9448 |Test Loss: 0.9692|lr = 0.00817\n",
      "Epoch:  127|steps:   60|Train Avg Loss: 0.9215 |Test Loss: 0.9577|lr = 0.00817\n",
      "Epoch:  128|steps:   30|Train Avg Loss: 0.9351 |Test Loss: 0.9683|lr = 0.00817\n",
      "Epoch:  128|steps:   60|Train Avg Loss: 0.9278 |Test Loss: 0.9604|lr = 0.00817\n",
      "Epoch:  129|steps:   30|Train Avg Loss: 0.9245 |Test Loss: 0.9586|lr = 0.00801\n",
      "Epoch:  129|steps:   60|Train Avg Loss: 0.9328 |Test Loss: 0.9614|lr = 0.00801\n",
      "Epoch:  130|steps:   30|Train Avg Loss: 0.9292 |Test Loss: 0.9545|lr = 0.00801\n",
      "Epoch:  130|steps:   60|Train Avg Loss: 0.9343 |Test Loss: 0.9598|lr = 0.00801\n",
      "Epoch:  131|steps:   30|Train Avg Loss: 0.9189 |Test Loss: 0.9657|lr = 0.00801\n",
      "Epoch:  131|steps:   60|Train Avg Loss: 0.9438 |Test Loss: 0.9485|lr = 0.00801\n",
      "Epoch:  132|steps:   30|Train Avg Loss: 0.9328 |Test Loss: 0.9642|lr = 0.00801\n",
      "Epoch:  132|steps:   60|Train Avg Loss: 0.9297 |Test Loss: 0.9582|lr = 0.00801\n",
      "Epoch:  133|steps:   30|Train Avg Loss: 0.9371 |Test Loss: 0.9606|lr = 0.00801\n",
      "Epoch:  133|steps:   60|Train Avg Loss: 0.9315 |Test Loss: 0.9503|lr = 0.00801\n",
      "Epoch:  134|steps:   30|Train Avg Loss: 0.9280 |Test Loss: 0.9538|lr = 0.00801\n",
      "Epoch:  134|steps:   60|Train Avg Loss: 0.9427 |Test Loss: 0.9584|lr = 0.00801\n",
      "Epoch:  135|steps:   30|Train Avg Loss: 0.9327 |Test Loss: 0.9636|lr = 0.00801\n",
      "Epoch:  135|steps:   60|Train Avg Loss: 0.9261 |Test Loss: 0.9606|lr = 0.00801\n",
      "Epoch:  136|steps:   30|Train Avg Loss: 0.9389 |Test Loss: 0.9508|lr = 0.00801\n",
      "Epoch:  136|steps:   60|Train Avg Loss: 0.9311 |Test Loss: 0.9315|lr = 0.00801\n",
      "Epoch:  137|steps:   30|Train Avg Loss: 0.9513 |Test Loss: 0.9733|lr = 0.00801\n",
      "Epoch:  137|steps:   60|Train Avg Loss: 0.9352 |Test Loss: 0.9747|lr = 0.00801\n",
      "Epoch:  138|steps:   30|Train Avg Loss: 0.9269 |Test Loss: 0.9829|lr = 0.00801\n",
      "Epoch:  138|steps:   60|Train Avg Loss: 0.9442 |Test Loss: 0.9515|lr = 0.00801\n",
      "Epoch:  139|steps:   30|Train Avg Loss: 0.9263 |Test Loss: 0.9568|lr = 0.00801\n",
      "Epoch:  139|steps:   60|Train Avg Loss: 0.9411 |Test Loss: 0.9606|lr = 0.00801\n",
      "Epoch:  140|steps:   30|Train Avg Loss: 0.9336 |Test Loss: 0.9616|lr = 0.00785\n",
      "Epoch:  140|steps:   60|Train Avg Loss: 0.9302 |Test Loss: 0.9563|lr = 0.00785\n",
      "Epoch:  141|steps:   30|Train Avg Loss: 0.9377 |Test Loss: 0.9495|lr = 0.00785\n",
      "Epoch:  141|steps:   60|Train Avg Loss: 0.9308 |Test Loss: 0.9696|lr = 0.00785\n",
      "Epoch:  142|steps:   30|Train Avg Loss: 0.9386 |Test Loss: 0.9668|lr = 0.00785\n",
      "Epoch:  142|steps:   60|Train Avg Loss: 0.9334 |Test Loss: 0.9506|lr = 0.00785\n",
      "Epoch:  143|steps:   30|Train Avg Loss: 0.9421 |Test Loss: 0.9478|lr = 0.00785\n",
      "Epoch:  143|steps:   60|Train Avg Loss: 0.9335 |Test Loss: 0.9542|lr = 0.00785\n",
      "Epoch:  144|steps:   30|Train Avg Loss: 0.9323 |Test Loss: 0.9508|lr = 0.00785\n",
      "Epoch:  144|steps:   60|Train Avg Loss: 0.9350 |Test Loss: 0.9617|lr = 0.00785\n",
      "Epoch:  145|steps:   30|Train Avg Loss: 0.9416 |Test Loss: 0.9567|lr = 0.00785\n",
      "Epoch:  145|steps:   60|Train Avg Loss: 0.9254 |Test Loss: 0.9412|lr = 0.00785\n",
      "Epoch:  146|steps:   30|Train Avg Loss: 0.9405 |Test Loss: 0.9801|lr = 0.00785\n",
      "Epoch:  146|steps:   60|Train Avg Loss: 0.9342 |Test Loss: 0.9505|lr = 0.00785\n",
      "Epoch:  147|steps:   30|Train Avg Loss: 0.9390 |Test Loss: 0.9628|lr = 0.00785\n",
      "Epoch:  147|steps:   60|Train Avg Loss: 0.9264 |Test Loss: 1.0006|lr = 0.00785\n",
      "Epoch:  148|steps:   30|Train Avg Loss: 0.9440 |Test Loss: 0.9687|lr = 0.00785\n",
      "Epoch:  148|steps:   60|Train Avg Loss: 0.9434 |Test Loss: 0.9482|lr = 0.00785\n",
      "Epoch:  149|steps:   30|Train Avg Loss: 0.9574 |Test Loss: 0.9272|lr = 0.00785\n",
      "Epoch:  149|steps:   60|Train Avg Loss: 0.9407 |Test Loss: 0.9917|lr = 0.00785\n",
      "Epoch:  150|steps:   30|Train Avg Loss: 0.9386 |Test Loss: 0.9388|lr = 0.00785\n",
      "Epoch:  150|steps:   60|Train Avg Loss: 0.9370 |Test Loss: 0.9747|lr = 0.00785\n",
      "Epoch:  151|steps:   30|Train Avg Loss: 0.9433 |Test Loss: 0.9440|lr = 0.00769\n",
      "Epoch:  151|steps:   60|Train Avg Loss: 0.9324 |Test Loss: 0.9499|lr = 0.00769\n",
      "Epoch:  152|steps:   30|Train Avg Loss: 0.9323 |Test Loss: 0.9592|lr = 0.00769\n",
      "Epoch:  152|steps:   60|Train Avg Loss: 0.9403 |Test Loss: 0.9501|lr = 0.00769\n",
      "Epoch:  153|steps:   30|Train Avg Loss: 0.9376 |Test Loss: 0.9531|lr = 0.00769\n",
      "Epoch:  153|steps:   60|Train Avg Loss: 0.9312 |Test Loss: 0.9447|lr = 0.00769\n",
      "Epoch:  154|steps:   30|Train Avg Loss: 0.9373 |Test Loss: 0.9462|lr = 0.00769\n",
      "Epoch:  154|steps:   60|Train Avg Loss: 0.9402 |Test Loss: 0.9725|lr = 0.00769\n",
      "Epoch:  155|steps:   30|Train Avg Loss: 0.9449 |Test Loss: 0.9405|lr = 0.00769\n",
      "Epoch:  155|steps:   60|Train Avg Loss: 0.9364 |Test Loss: 0.9527|lr = 0.00769\n",
      "Epoch:  156|steps:   30|Train Avg Loss: 0.9491 |Test Loss: 0.9603|lr = 0.00769\n",
      "Epoch:  156|steps:   60|Train Avg Loss: 0.9351 |Test Loss: 0.9636|lr = 0.00769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  157|steps:   30|Train Avg Loss: 0.9445 |Test Loss: 0.9555|lr = 0.00769\n",
      "Epoch:  157|steps:   60|Train Avg Loss: 0.9293 |Test Loss: 0.9529|lr = 0.00769\n",
      "Epoch:  158|steps:   30|Train Avg Loss: 0.9308 |Test Loss: 0.9497|lr = 0.00769\n",
      "Epoch:  158|steps:   60|Train Avg Loss: 0.9378 |Test Loss: 0.9570|lr = 0.00769\n",
      "Epoch:  159|steps:   30|Train Avg Loss: 0.9245 |Test Loss: 0.9599|lr = 0.00769\n",
      "Epoch:  159|steps:   60|Train Avg Loss: 0.9337 |Test Loss: 0.9506|lr = 0.00769\n",
      "Epoch:  160|steps:   30|Train Avg Loss: 0.9262 |Test Loss: 0.9501|lr = 0.00769\n",
      "Epoch:  160|steps:   60|Train Avg Loss: 0.9462 |Test Loss: 0.9336|lr = 0.00769\n",
      "Epoch:  161|steps:   30|Train Avg Loss: 0.9463 |Test Loss: 0.9923|lr = 0.00769\n",
      "Epoch:  161|steps:   60|Train Avg Loss: 0.9276 |Test Loss: 0.9525|lr = 0.00769\n",
      "Epoch:  162|steps:   30|Train Avg Loss: 0.9317 |Test Loss: 0.9703|lr = 0.00754\n",
      "Epoch:  162|steps:   60|Train Avg Loss: 0.9364 |Test Loss: 0.9468|lr = 0.00754\n",
      "Epoch:  163|steps:   30|Train Avg Loss: 0.9359 |Test Loss: 0.9448|lr = 0.00754\n",
      "Epoch:  163|steps:   60|Train Avg Loss: 0.9243 |Test Loss: 0.9740|lr = 0.00754\n",
      "Epoch:  164|steps:   30|Train Avg Loss: 0.9288 |Test Loss: 0.9583|lr = 0.00754\n",
      "Epoch:  164|steps:   60|Train Avg Loss: 0.9357 |Test Loss: 0.9489|lr = 0.00754\n",
      "Epoch:  165|steps:   30|Train Avg Loss: 0.9295 |Test Loss: 0.9658|lr = 0.00754\n",
      "Epoch:  165|steps:   60|Train Avg Loss: 0.9354 |Test Loss: 0.9586|lr = 0.00754\n",
      "Epoch:  166|steps:   30|Train Avg Loss: 0.9324 |Test Loss: 0.9626|lr = 0.00754\n",
      "Epoch:  166|steps:   60|Train Avg Loss: 0.9356 |Test Loss: 0.9623|lr = 0.00754\n",
      "Epoch:  167|steps:   30|Train Avg Loss: 0.9373 |Test Loss: 0.9521|lr = 0.00754\n",
      "Epoch:  167|steps:   60|Train Avg Loss: 0.9392 |Test Loss: 0.9658|lr = 0.00754\n",
      "Epoch:  168|steps:   30|Train Avg Loss: 0.9572 |Test Loss: 1.0105|lr = 0.00754\n",
      "Epoch:  168|steps:   60|Train Avg Loss: 0.9429 |Test Loss: 0.9269|lr = 0.00754\n",
      "Epoch:  169|steps:   30|Train Avg Loss: 0.9394 |Test Loss: 0.9368|lr = 0.00754\n",
      "Epoch:  169|steps:   60|Train Avg Loss: 0.9630 |Test Loss: 0.9863|lr = 0.00754\n",
      "Epoch:  170|steps:   30|Train Avg Loss: 0.9300 |Test Loss: 1.0079|lr = 0.00754\n",
      "Epoch:  170|steps:   60|Train Avg Loss: 0.9485 |Test Loss: 0.9327|lr = 0.00754\n",
      "Epoch:  171|steps:   30|Train Avg Loss: 0.9390 |Test Loss: 1.0338|lr = 0.00754\n",
      "Epoch:  171|steps:   60|Train Avg Loss: 0.9415 |Test Loss: 0.9435|lr = 0.00754\n",
      "Epoch:  172|steps:   30|Train Avg Loss: 0.9554 |Test Loss: 0.9410|lr = 0.00754\n",
      "Epoch:  172|steps:   60|Train Avg Loss: 0.9254 |Test Loss: 0.9448|lr = 0.00754\n",
      "Epoch:  173|steps:   30|Train Avg Loss: 0.9396 |Test Loss: 0.9345|lr = 0.00739\n",
      "Epoch:  173|steps:   60|Train Avg Loss: 0.9330 |Test Loss: 0.9562|lr = 0.00739\n",
      "Epoch:  174|steps:   30|Train Avg Loss: 0.9337 |Test Loss: 0.9631|lr = 0.00739\n",
      "Epoch:  174|steps:   60|Train Avg Loss: 0.9413 |Test Loss: 0.9552|lr = 0.00739\n",
      "Epoch:  175|steps:   30|Train Avg Loss: 0.9303 |Test Loss: 0.9555|lr = 0.00739\n",
      "Epoch:  175|steps:   60|Train Avg Loss: 0.9413 |Test Loss: 0.9527|lr = 0.00739\n",
      "Epoch:  176|steps:   30|Train Avg Loss: 0.9368 |Test Loss: 0.9588|lr = 0.00739\n",
      "Epoch:  176|steps:   60|Train Avg Loss: 0.9241 |Test Loss: 0.9697|lr = 0.00739\n",
      "Epoch:  177|steps:   30|Train Avg Loss: 0.9370 |Test Loss: 0.9548|lr = 0.00739\n",
      "Epoch:  177|steps:   60|Train Avg Loss: 0.9312 |Test Loss: 0.9573|lr = 0.00739\n",
      "Epoch:  178|steps:   30|Train Avg Loss: 0.9350 |Test Loss: 0.9616|lr = 0.00739\n",
      "Epoch:  178|steps:   60|Train Avg Loss: 0.9343 |Test Loss: 0.9694|lr = 0.00739\n",
      "Epoch:  179|steps:   30|Train Avg Loss: 0.9347 |Test Loss: 0.9638|lr = 0.00739\n",
      "Epoch:  179|steps:   60|Train Avg Loss: 0.9311 |Test Loss: 0.9540|lr = 0.00739\n",
      "Epoch:  180|steps:   30|Train Avg Loss: 0.9370 |Test Loss: 0.9528|lr = 0.00739\n",
      "Epoch:  180|steps:   60|Train Avg Loss: 0.9283 |Test Loss: 0.9638|lr = 0.00739\n",
      "Epoch:  181|steps:   30|Train Avg Loss: 0.9304 |Test Loss: 0.9581|lr = 0.00739\n",
      "Epoch:  181|steps:   60|Train Avg Loss: 0.9354 |Test Loss: 0.9540|lr = 0.00739\n",
      "Epoch:  182|steps:   30|Train Avg Loss: 0.9243 |Test Loss: 0.9653|lr = 0.00739\n",
      "Epoch:  182|steps:   60|Train Avg Loss: 0.9387 |Test Loss: 0.9506|lr = 0.00739\n",
      "Epoch:  183|steps:   30|Train Avg Loss: 0.9364 |Test Loss: 0.9700|lr = 0.00739\n",
      "Epoch:  183|steps:   60|Train Avg Loss: 0.9286 |Test Loss: 0.9509|lr = 0.00739\n",
      "Epoch:  184|steps:   30|Train Avg Loss: 0.9265 |Test Loss: 0.9733|lr = 0.00724\n",
      "Epoch:  184|steps:   60|Train Avg Loss: 0.9384 |Test Loss: 0.9565|lr = 0.00724\n",
      "Epoch:  185|steps:   30|Train Avg Loss: 0.9330 |Test Loss: 0.9553|lr = 0.00724\n",
      "Epoch:  185|steps:   60|Train Avg Loss: 0.9323 |Test Loss: 0.9575|lr = 0.00724\n",
      "Epoch:  186|steps:   30|Train Avg Loss: 0.9346 |Test Loss: 0.9791|lr = 0.00724\n",
      "Epoch:  186|steps:   60|Train Avg Loss: 0.9287 |Test Loss: 0.9586|lr = 0.00724\n",
      "Epoch:  187|steps:   30|Train Avg Loss: 0.9292 |Test Loss: 0.9760|lr = 0.00724\n",
      "Epoch:  187|steps:   60|Train Avg Loss: 0.9426 |Test Loss: 0.9502|lr = 0.00724\n",
      "Epoch:  188|steps:   30|Train Avg Loss: 0.9338 |Test Loss: 0.9673|lr = 0.00724\n",
      "Epoch:  188|steps:   60|Train Avg Loss: 0.9363 |Test Loss: 0.9575|lr = 0.00724\n",
      "Epoch:  189|steps:   30|Train Avg Loss: 0.9270 |Test Loss: 0.9585|lr = 0.00724\n",
      "Epoch:  189|steps:   60|Train Avg Loss: 0.9382 |Test Loss: 0.9536|lr = 0.00724\n",
      "Epoch:  190|steps:   30|Train Avg Loss: 0.9352 |Test Loss: 0.9644|lr = 0.00724\n",
      "Epoch:  190|steps:   60|Train Avg Loss: 0.9222 |Test Loss: 0.9572|lr = 0.00724\n",
      "Epoch:  191|steps:   30|Train Avg Loss: 0.9241 |Test Loss: 0.9643|lr = 0.00724\n",
      "Epoch:  191|steps:   60|Train Avg Loss: 0.9357 |Test Loss: 0.9548|lr = 0.00724\n",
      "Epoch:  192|steps:   30|Train Avg Loss: 0.9411 |Test Loss: 0.9668|lr = 0.00724\n",
      "Epoch:  192|steps:   60|Train Avg Loss: 0.9299 |Test Loss: 0.9664|lr = 0.00724\n",
      "Epoch:  193|steps:   30|Train Avg Loss: 0.9282 |Test Loss: 0.9574|lr = 0.00724\n",
      "Epoch:  193|steps:   60|Train Avg Loss: 0.9415 |Test Loss: 0.9561|lr = 0.00724\n",
      "Epoch:  194|steps:   30|Train Avg Loss: 0.9393 |Test Loss: 0.9653|lr = 0.00724\n",
      "Epoch:  194|steps:   60|Train Avg Loss: 0.9197 |Test Loss: 0.9729|lr = 0.00724\n",
      "Epoch:  195|steps:   30|Train Avg Loss: 0.9327 |Test Loss: 0.9566|lr = 0.00709\n",
      "Epoch:  195|steps:   60|Train Avg Loss: 0.9220 |Test Loss: 0.9556|lr = 0.00709\n",
      "Epoch:  196|steps:   30|Train Avg Loss: 0.9508 |Test Loss: 0.9641|lr = 0.00709\n",
      "Epoch:  196|steps:   60|Train Avg Loss: 0.9189 |Test Loss: 0.9741|lr = 0.00709\n",
      "Epoch:  197|steps:   30|Train Avg Loss: 0.9297 |Test Loss: 0.9549|lr = 0.00709\n",
      "Epoch:  197|steps:   60|Train Avg Loss: 0.9333 |Test Loss: 0.9680|lr = 0.00709\n",
      "Epoch:  198|steps:   30|Train Avg Loss: 0.9262 |Test Loss: 0.9655|lr = 0.00709\n",
      "Epoch:  198|steps:   60|Train Avg Loss: 0.9344 |Test Loss: 0.9571|lr = 0.00709\n",
      "Epoch:  199|steps:   30|Train Avg Loss: 0.9285 |Test Loss: 0.9723|lr = 0.00709\n",
      "Epoch:  199|steps:   60|Train Avg Loss: 0.9375 |Test Loss: 0.9549|lr = 0.00709\n",
      "Epoch:  200|steps:   30|Train Avg Loss: 0.9259 |Test Loss: 0.9747|lr = 0.00709\n",
      "Epoch:  200|steps:   60|Train Avg Loss: 0.9307 |Test Loss: 0.9375|lr = 0.00709\n",
      "Epoch:  201|steps:   30|Train Avg Loss: 0.9300 |Test Loss: 0.9761|lr = 0.00709\n",
      "Epoch:  201|steps:   60|Train Avg Loss: 0.9277 |Test Loss: 0.9274|lr = 0.00709\n",
      "Epoch:  202|steps:   30|Train Avg Loss: 0.9325 |Test Loss: 0.9441|lr = 0.00709\n",
      "Epoch:  202|steps:   60|Train Avg Loss: 0.9325 |Test Loss: 0.9259|lr = 0.00709\n",
      "Epoch:  203|steps:   30|Train Avg Loss: 0.9393 |Test Loss: 0.9378|lr = 0.00709\n",
      "Epoch:  203|steps:   60|Train Avg Loss: 0.9194 |Test Loss: 1.0090|lr = 0.00709\n",
      "Epoch:  204|steps:   30|Train Avg Loss: 0.9349 |Test Loss: 0.9624|lr = 0.00709\n",
      "Epoch:  204|steps:   60|Train Avg Loss: 0.9285 |Test Loss: 0.9430|lr = 0.00709\n",
      "Epoch:  205|steps:   30|Train Avg Loss: 0.9328 |Test Loss: 0.9667|lr = 0.00709\n",
      "Epoch:  205|steps:   60|Train Avg Loss: 0.9243 |Test Loss: 0.9735|lr = 0.00709\n",
      "Epoch:  206|steps:   30|Train Avg Loss: 0.9289 |Test Loss: 0.9630|lr = 0.00695\n",
      "Epoch:  206|steps:   60|Train Avg Loss: 0.9340 |Test Loss: 0.9529|lr = 0.00695\n",
      "Epoch:  207|steps:   30|Train Avg Loss: 0.9270 |Test Loss: 0.9657|lr = 0.00695\n",
      "Epoch:  207|steps:   60|Train Avg Loss: 0.9328 |Test Loss: 0.9599|lr = 0.00695\n",
      "Epoch:  208|steps:   30|Train Avg Loss: 0.9211 |Test Loss: 0.9581|lr = 0.00695\n",
      "Epoch:  208|steps:   60|Train Avg Loss: 0.9356 |Test Loss: 0.9686|lr = 0.00695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  209|steps:   30|Train Avg Loss: 0.9230 |Test Loss: 0.9766|lr = 0.00695\n",
      "Epoch:  209|steps:   60|Train Avg Loss: 0.9402 |Test Loss: 0.9613|lr = 0.00695\n",
      "Epoch:  210|steps:   30|Train Avg Loss: 0.9268 |Test Loss: 0.9712|lr = 0.00695\n",
      "Epoch:  210|steps:   60|Train Avg Loss: 0.9303 |Test Loss: 0.9559|lr = 0.00695\n",
      "Epoch:  211|steps:   30|Train Avg Loss: 0.9157 |Test Loss: 0.9751|lr = 0.00695\n",
      "Epoch:  211|steps:   60|Train Avg Loss: 0.9357 |Test Loss: 0.9517|lr = 0.00695\n",
      "Epoch:  212|steps:   30|Train Avg Loss: 0.9262 |Test Loss: 0.9631|lr = 0.00695\n",
      "Epoch:  212|steps:   60|Train Avg Loss: 0.9374 |Test Loss: 0.9606|lr = 0.00695\n",
      "Epoch:  213|steps:   30|Train Avg Loss: 0.9271 |Test Loss: 0.9520|lr = 0.00695\n",
      "Epoch:  213|steps:   60|Train Avg Loss: 0.9406 |Test Loss: 0.9566|lr = 0.00695\n",
      "Epoch:  214|steps:   30|Train Avg Loss: 0.9433 |Test Loss: 0.9415|lr = 0.00695\n",
      "Epoch:  214|steps:   60|Train Avg Loss: 0.9278 |Test Loss: 0.9990|lr = 0.00695\n",
      "Epoch:  215|steps:   30|Train Avg Loss: 0.9312 |Test Loss: 0.9575|lr = 0.00695\n",
      "Epoch:  215|steps:   60|Train Avg Loss: 0.9304 |Test Loss: 0.9632|lr = 0.00695\n",
      "Epoch:  216|steps:   30|Train Avg Loss: 0.9370 |Test Loss: 0.9668|lr = 0.00695\n",
      "Epoch:  216|steps:   60|Train Avg Loss: 0.9319 |Test Loss: 0.9705|lr = 0.00695\n",
      "Epoch:  217|steps:   30|Train Avg Loss: 0.9176 |Test Loss: 0.9563|lr = 0.00681\n",
      "Epoch:  217|steps:   60|Train Avg Loss: 0.9413 |Test Loss: 0.9565|lr = 0.00681\n",
      "Epoch:  218|steps:   30|Train Avg Loss: 0.9296 |Test Loss: 0.9569|lr = 0.00681\n",
      "Epoch:  218|steps:   60|Train Avg Loss: 0.9303 |Test Loss: 0.9570|lr = 0.00681\n",
      "Epoch:  219|steps:   30|Train Avg Loss: 0.9356 |Test Loss: 0.9794|lr = 0.00681\n",
      "Epoch:  219|steps:   60|Train Avg Loss: 0.9307 |Test Loss: 0.9548|lr = 0.00681\n",
      "Epoch:  220|steps:   30|Train Avg Loss: 0.9202 |Test Loss: 0.9830|lr = 0.00681\n",
      "Epoch:  220|steps:   60|Train Avg Loss: 0.9510 |Test Loss: 0.9487|lr = 0.00681\n",
      "Epoch:  221|steps:   30|Train Avg Loss: 0.9420 |Test Loss: 0.9514|lr = 0.00681\n",
      "Epoch:  221|steps:   60|Train Avg Loss: 0.9228 |Test Loss: 0.9446|lr = 0.00681\n",
      "Epoch:  222|steps:   30|Train Avg Loss: 0.9309 |Test Loss: 0.9644|lr = 0.00681\n",
      "Epoch:  222|steps:   60|Train Avg Loss: 0.9338 |Test Loss: 0.9763|lr = 0.00681\n",
      "Epoch:  223|steps:   30|Train Avg Loss: 0.9235 |Test Loss: 0.9596|lr = 0.00681\n",
      "Epoch:  223|steps:   60|Train Avg Loss: 0.9367 |Test Loss: 0.9538|lr = 0.00681\n",
      "Epoch:  224|steps:   30|Train Avg Loss: 0.9382 |Test Loss: 0.9477|lr = 0.00681\n",
      "Epoch:  224|steps:   60|Train Avg Loss: 0.9167 |Test Loss: 0.9673|lr = 0.00681\n",
      "Epoch:  225|steps:   30|Train Avg Loss: 0.9298 |Test Loss: 0.9846|lr = 0.00681\n",
      "Epoch:  225|steps:   60|Train Avg Loss: 0.9421 |Test Loss: 0.9533|lr = 0.00681\n",
      "Epoch:  226|steps:   30|Train Avg Loss: 0.9417 |Test Loss: 0.9826|lr = 0.00681\n",
      "Epoch:  226|steps:   60|Train Avg Loss: 0.9344 |Test Loss: 0.9739|lr = 0.00681\n",
      "Epoch:  227|steps:   30|Train Avg Loss: 0.9297 |Test Loss: 0.9524|lr = 0.00681\n",
      "Epoch:  227|steps:   60|Train Avg Loss: 0.9317 |Test Loss: 0.9520|lr = 0.00681\n",
      "Epoch:  228|steps:   30|Train Avg Loss: 0.9444 |Test Loss: 0.9378|lr = 0.00668\n",
      "Epoch:  228|steps:   60|Train Avg Loss: 0.9214 |Test Loss: 0.9836|lr = 0.00668\n",
      "Epoch:  229|steps:   30|Train Avg Loss: 0.9417 |Test Loss: 0.9496|lr = 0.00668\n",
      "Epoch:  229|steps:   60|Train Avg Loss: 0.9145 |Test Loss: 1.0173|lr = 0.00668\n",
      "Epoch:  230|steps:   30|Train Avg Loss: 0.9267 |Test Loss: 0.9271|lr = 0.00668\n",
      "Epoch:  230|steps:   60|Train Avg Loss: 0.9377 |Test Loss: 0.9561|lr = 0.00668\n",
      "Epoch:  231|steps:   30|Train Avg Loss: 0.9272 |Test Loss: 0.9642|lr = 0.00668\n",
      "Epoch:  231|steps:   60|Train Avg Loss: 0.9357 |Test Loss: 0.9756|lr = 0.00668\n",
      "Epoch:  232|steps:   30|Train Avg Loss: 0.9319 |Test Loss: 0.9450|lr = 0.00668\n",
      "Epoch:  232|steps:   60|Train Avg Loss: 0.9334 |Test Loss: 0.9804|lr = 0.00668\n",
      "Epoch:  233|steps:   30|Train Avg Loss: 0.9273 |Test Loss: 0.9628|lr = 0.00668\n",
      "Epoch:  233|steps:   60|Train Avg Loss: 0.9206 |Test Loss: 0.9749|lr = 0.00668\n",
      "Epoch:  234|steps:   30|Train Avg Loss: 0.9322 |Test Loss: 0.9695|lr = 0.00668\n",
      "Epoch:  234|steps:   60|Train Avg Loss: 0.9303 |Test Loss: 0.9532|lr = 0.00668\n",
      "Epoch:  235|steps:   30|Train Avg Loss: 0.9338 |Test Loss: 0.9672|lr = 0.00668\n",
      "Epoch:  235|steps:   60|Train Avg Loss: 0.9267 |Test Loss: 0.9675|lr = 0.00668\n",
      "Epoch:  236|steps:   30|Train Avg Loss: 0.9275 |Test Loss: 0.9480|lr = 0.00668\n",
      "Epoch:  236|steps:   60|Train Avg Loss: 0.9232 |Test Loss: 0.9653|lr = 0.00668\n",
      "Epoch:  237|steps:   30|Train Avg Loss: 0.9363 |Test Loss: 0.9726|lr = 0.00668\n",
      "Epoch:  237|steps:   60|Train Avg Loss: 0.9217 |Test Loss: 0.9831|lr = 0.00668\n",
      "Epoch:  238|steps:   30|Train Avg Loss: 0.9287 |Test Loss: 0.9703|lr = 0.00668\n",
      "Epoch:  238|steps:   60|Train Avg Loss: 0.9180 |Test Loss: 0.9966|lr = 0.00668\n",
      "Epoch:  239|steps:   30|Train Avg Loss: 0.9116 |Test Loss: 0.9782|lr = 0.00654\n",
      "Epoch:  239|steps:   60|Train Avg Loss: 0.9286 |Test Loss: 0.9666|lr = 0.00654\n",
      "Epoch:  240|steps:   30|Train Avg Loss: 0.9129 |Test Loss: 0.9898|lr = 0.00654\n",
      "Epoch:  240|steps:   60|Train Avg Loss: 0.9315 |Test Loss: 0.9417|lr = 0.00654\n",
      "Epoch:  241|steps:   30|Train Avg Loss: 0.9149 |Test Loss: 0.9743|lr = 0.00654\n",
      "Epoch:  241|steps:   60|Train Avg Loss: 0.9195 |Test Loss: 0.9700|lr = 0.00654\n",
      "Epoch:  242|steps:   30|Train Avg Loss: 0.9129 |Test Loss: 0.9656|lr = 0.00654\n",
      "Epoch:  242|steps:   60|Train Avg Loss: 0.9206 |Test Loss: 0.9716|lr = 0.00654\n",
      "Epoch:  243|steps:   30|Train Avg Loss: 0.9292 |Test Loss: 0.9459|lr = 0.00654\n",
      "Epoch:  243|steps:   60|Train Avg Loss: 0.9318 |Test Loss: 0.9527|lr = 0.00654\n",
      "Epoch:  244|steps:   30|Train Avg Loss: 0.9316 |Test Loss: 0.9817|lr = 0.00654\n",
      "Epoch:  244|steps:   60|Train Avg Loss: 0.9009 |Test Loss: 0.9842|lr = 0.00654\n",
      "Epoch:  245|steps:   30|Train Avg Loss: 0.9066 |Test Loss: 0.9913|lr = 0.00654\n",
      "Epoch:  245|steps:   60|Train Avg Loss: 0.9197 |Test Loss: 0.9660|lr = 0.00654\n",
      "Epoch:  246|steps:   30|Train Avg Loss: 0.8976 |Test Loss: 0.9875|lr = 0.00654\n",
      "Epoch:  246|steps:   60|Train Avg Loss: 0.9298 |Test Loss: 0.9581|lr = 0.00654\n",
      "Epoch:  247|steps:   30|Train Avg Loss: 0.9065 |Test Loss: 0.9604|lr = 0.00654\n",
      "Epoch:  247|steps:   60|Train Avg Loss: 0.9128 |Test Loss: 1.0067|lr = 0.00654\n",
      "Epoch:  248|steps:   30|Train Avg Loss: 0.9240 |Test Loss: 0.9832|lr = 0.00654\n",
      "Epoch:  248|steps:   60|Train Avg Loss: 0.9085 |Test Loss: 0.9985|lr = 0.00654\n",
      "Epoch:  249|steps:   30|Train Avg Loss: 0.9090 |Test Loss: 0.9960|lr = 0.00654\n",
      "Epoch:  249|steps:   60|Train Avg Loss: 0.9225 |Test Loss: 0.9723|lr = 0.00654\n",
      "Epoch:  250|steps:   30|Train Avg Loss: 0.9223 |Test Loss: 0.9754|lr = 0.00641\n",
      "Epoch:  250|steps:   60|Train Avg Loss: 0.9104 |Test Loss: 0.9950|lr = 0.00641\n",
      "Epoch:  251|steps:   30|Train Avg Loss: 0.9175 |Test Loss: 0.9659|lr = 0.00641\n",
      "Epoch:  251|steps:   60|Train Avg Loss: 0.9142 |Test Loss: 0.9963|lr = 0.00641\n",
      "Epoch:  252|steps:   30|Train Avg Loss: 0.9214 |Test Loss: 0.9752|lr = 0.00641\n",
      "Epoch:  252|steps:   60|Train Avg Loss: 0.9222 |Test Loss: 0.9603|lr = 0.00641\n",
      "Epoch:  253|steps:   30|Train Avg Loss: 0.9001 |Test Loss: 0.9569|lr = 0.00641\n",
      "Epoch:  253|steps:   60|Train Avg Loss: 0.9246 |Test Loss: 0.9589|lr = 0.00641\n",
      "Epoch:  254|steps:   30|Train Avg Loss: 0.8838 |Test Loss: 0.9764|lr = 0.00641\n",
      "Epoch:  254|steps:   60|Train Avg Loss: 0.9138 |Test Loss: 0.9794|lr = 0.00641\n",
      "Epoch:  255|steps:   30|Train Avg Loss: 0.8873 |Test Loss: 0.9941|lr = 0.00641\n",
      "Epoch:  255|steps:   60|Train Avg Loss: 0.8890 |Test Loss: 0.9778|lr = 0.00641\n",
      "Epoch:  256|steps:   30|Train Avg Loss: 0.9069 |Test Loss: 0.9526|lr = 0.00641\n",
      "Epoch:  256|steps:   60|Train Avg Loss: 0.8898 |Test Loss: 0.9801|lr = 0.00641\n",
      "Epoch:  257|steps:   30|Train Avg Loss: 0.9122 |Test Loss: 0.9684|lr = 0.00641\n",
      "Epoch:  257|steps:   60|Train Avg Loss: 0.9062 |Test Loss: 1.0103|lr = 0.00641\n",
      "Epoch:  258|steps:   30|Train Avg Loss: 0.8912 |Test Loss: 1.0044|lr = 0.00641\n",
      "Epoch:  258|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 1.0078|lr = 0.00641\n",
      "Epoch:  259|steps:   30|Train Avg Loss: 0.9154 |Test Loss: 1.0231|lr = 0.00641\n",
      "Epoch:  259|steps:   60|Train Avg Loss: 0.8897 |Test Loss: 1.0040|lr = 0.00641\n",
      "Epoch:  260|steps:   30|Train Avg Loss: 0.9085 |Test Loss: 1.0114|lr = 0.00641\n",
      "Epoch:  260|steps:   60|Train Avg Loss: 0.8833 |Test Loss: 0.9874|lr = 0.00641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  261|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.9593|lr = 0.00628\n",
      "Epoch:  261|steps:   60|Train Avg Loss: 0.8994 |Test Loss: 0.9993|lr = 0.00628\n",
      "Epoch:  262|steps:   30|Train Avg Loss: 0.8725 |Test Loss: 1.0054|lr = 0.00628\n",
      "Epoch:  262|steps:   60|Train Avg Loss: 0.8756 |Test Loss: 1.0146|lr = 0.00628\n",
      "Epoch:  263|steps:   30|Train Avg Loss: 0.8700 |Test Loss: 1.0052|lr = 0.00628\n",
      "Epoch:  263|steps:   60|Train Avg Loss: 0.8915 |Test Loss: 1.0216|lr = 0.00628\n",
      "Epoch:  264|steps:   30|Train Avg Loss: 0.8766 |Test Loss: 1.0575|lr = 0.00628\n",
      "Epoch:  264|steps:   60|Train Avg Loss: 0.8629 |Test Loss: 0.9876|lr = 0.00628\n",
      "Epoch:  265|steps:   30|Train Avg Loss: 0.8692 |Test Loss: 1.0201|lr = 0.00628\n",
      "Epoch:  265|steps:   60|Train Avg Loss: 0.8716 |Test Loss: 1.0265|lr = 0.00628\n",
      "Epoch:  266|steps:   30|Train Avg Loss: 0.8562 |Test Loss: 1.0374|lr = 0.00628\n",
      "Epoch:  266|steps:   60|Train Avg Loss: 0.8712 |Test Loss: 1.0404|lr = 0.00628\n",
      "Epoch:  267|steps:   30|Train Avg Loss: 0.8536 |Test Loss: 1.0272|lr = 0.00628\n",
      "Epoch:  267|steps:   60|Train Avg Loss: 0.8295 |Test Loss: 1.0550|lr = 0.00628\n",
      "Epoch:  268|steps:   30|Train Avg Loss: 0.8270 |Test Loss: 1.0260|lr = 0.00628\n",
      "Epoch:  268|steps:   60|Train Avg Loss: 0.8850 |Test Loss: 1.0545|lr = 0.00628\n",
      "Epoch:  269|steps:   30|Train Avg Loss: 0.8230 |Test Loss: 1.0197|lr = 0.00628\n",
      "Epoch:  269|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 1.0203|lr = 0.00628\n",
      "Epoch:  270|steps:   30|Train Avg Loss: 0.8415 |Test Loss: 1.0768|lr = 0.00628\n",
      "Epoch:  270|steps:   60|Train Avg Loss: 0.8191 |Test Loss: 1.0286|lr = 0.00628\n",
      "Epoch:  271|steps:   30|Train Avg Loss: 0.8403 |Test Loss: 0.9949|lr = 0.00628\n",
      "Epoch:  271|steps:   60|Train Avg Loss: 0.8102 |Test Loss: 1.0521|lr = 0.00628\n",
      "Epoch:  272|steps:   30|Train Avg Loss: 0.7971 |Test Loss: 1.0623|lr = 0.00628\n",
      "Epoch:  272|steps:   60|Train Avg Loss: 0.8495 |Test Loss: 1.0614|lr = 0.00628\n",
      "Epoch:  273|steps:   30|Train Avg Loss: 0.7874 |Test Loss: 1.1475|lr = 0.00628\n",
      "Epoch:  273|steps:   60|Train Avg Loss: 0.7779 |Test Loss: 1.1040|lr = 0.00628\n",
      "Epoch:  274|steps:   30|Train Avg Loss: 0.8080 |Test Loss: 1.0673|lr = 0.00628\n",
      "Epoch:  274|steps:   60|Train Avg Loss: 0.7964 |Test Loss: 1.0741|lr = 0.00628\n",
      "Epoch:  275|steps:   30|Train Avg Loss: 0.7647 |Test Loss: 1.1050|lr = 0.00628\n",
      "Epoch:  275|steps:   60|Train Avg Loss: 0.8018 |Test Loss: 1.1006|lr = 0.00628\n",
      "Epoch:  276|steps:   30|Train Avg Loss: 0.8036 |Test Loss: 1.0317|lr = 0.00628\n",
      "Epoch:  276|steps:   60|Train Avg Loss: 0.7709 |Test Loss: 1.0607|lr = 0.00628\n",
      "Epoch:  277|steps:   30|Train Avg Loss: 0.7151 |Test Loss: 1.1269|lr = 0.00628\n",
      "Epoch:  277|steps:   60|Train Avg Loss: 0.7868 |Test Loss: 1.0805|lr = 0.00628\n",
      "Epoch:  278|steps:   30|Train Avg Loss: 0.7398 |Test Loss: 1.1000|lr = 0.00628\n",
      "Epoch:  278|steps:   60|Train Avg Loss: 0.7277 |Test Loss: 1.1972|lr = 0.00628\n",
      "Epoch:  279|steps:   30|Train Avg Loss: 0.6875 |Test Loss: 1.1733|lr = 0.00628\n",
      "Epoch:  279|steps:   60|Train Avg Loss: 0.7514 |Test Loss: 1.1762|lr = 0.00628\n",
      "Epoch:  280|steps:   30|Train Avg Loss: 0.7161 |Test Loss: 1.1568|lr = 0.00628\n",
      "Epoch:  280|steps:   60|Train Avg Loss: 0.6904 |Test Loss: 1.1374|lr = 0.00628\n",
      "Epoch:  281|steps:   30|Train Avg Loss: 0.6973 |Test Loss: 1.1585|lr = 0.00628\n",
      "Epoch:  281|steps:   60|Train Avg Loss: 0.7136 |Test Loss: 1.1666|lr = 0.00628\n",
      "Epoch:  282|steps:   30|Train Avg Loss: 0.6618 |Test Loss: 1.1845|lr = 0.00616\n",
      "Epoch:  282|steps:   60|Train Avg Loss: 0.6812 |Test Loss: 1.2156|lr = 0.00616\n",
      "Epoch:  283|steps:   30|Train Avg Loss: 0.6398 |Test Loss: 1.1689|lr = 0.00616\n",
      "Epoch:  283|steps:   60|Train Avg Loss: 0.6676 |Test Loss: 1.1002|lr = 0.00616\n",
      "Epoch:  284|steps:   30|Train Avg Loss: 0.6345 |Test Loss: 1.2041|lr = 0.00616\n",
      "Epoch:  284|steps:   60|Train Avg Loss: 0.6428 |Test Loss: 1.2983|lr = 0.00616\n",
      "Epoch:  285|steps:   30|Train Avg Loss: 0.6029 |Test Loss: 1.2661|lr = 0.00616\n",
      "Epoch:  285|steps:   60|Train Avg Loss: 0.6359 |Test Loss: 1.2305|lr = 0.00616\n",
      "Epoch:  286|steps:   30|Train Avg Loss: 0.5570 |Test Loss: 1.3938|lr = 0.00616\n",
      "Epoch:  286|steps:   60|Train Avg Loss: 0.5896 |Test Loss: 1.3198|lr = 0.00616\n",
      "Epoch:  287|steps:   30|Train Avg Loss: 0.5452 |Test Loss: 1.2929|lr = 0.00616\n",
      "Epoch:  287|steps:   60|Train Avg Loss: 0.5754 |Test Loss: 1.2976|lr = 0.00616\n",
      "Epoch:  288|steps:   30|Train Avg Loss: 0.5030 |Test Loss: 1.3229|lr = 0.00616\n",
      "Epoch:  288|steps:   60|Train Avg Loss: 0.5769 |Test Loss: 1.2158|lr = 0.00616\n",
      "Epoch:  289|steps:   30|Train Avg Loss: 0.4905 |Test Loss: 1.3069|lr = 0.00616\n",
      "Epoch:  289|steps:   60|Train Avg Loss: 0.5141 |Test Loss: 1.2683|lr = 0.00616\n",
      "Epoch:  290|steps:   30|Train Avg Loss: 0.4439 |Test Loss: 1.3101|lr = 0.00616\n",
      "Epoch:  290|steps:   60|Train Avg Loss: 0.5208 |Test Loss: 1.3957|lr = 0.00616\n",
      "Epoch:  291|steps:   30|Train Avg Loss: 0.4638 |Test Loss: 1.2973|lr = 0.00616\n",
      "Epoch:  291|steps:   60|Train Avg Loss: 0.5301 |Test Loss: 1.2480|lr = 0.00616\n",
      "Epoch:  292|steps:   30|Train Avg Loss: 0.4383 |Test Loss: 1.4930|lr = 0.00616\n",
      "Epoch:  292|steps:   60|Train Avg Loss: 0.4559 |Test Loss: 1.3689|lr = 0.00616\n",
      "Epoch:  293|steps:   30|Train Avg Loss: 0.4363 |Test Loss: 1.4019|lr = 0.00616\n",
      "Epoch:  293|steps:   60|Train Avg Loss: 0.4248 |Test Loss: 1.3026|lr = 0.00616\n",
      "Epoch:  294|steps:   30|Train Avg Loss: 0.3399 |Test Loss: 1.3979|lr = 0.00616\n",
      "Epoch:  294|steps:   60|Train Avg Loss: 0.4024 |Test Loss: 1.4207|lr = 0.00616\n",
      "Epoch:  295|steps:   30|Train Avg Loss: 0.3927 |Test Loss: 1.4706|lr = 0.00616\n",
      "Epoch:  295|steps:   60|Train Avg Loss: 0.3568 |Test Loss: 1.4725|lr = 0.00616\n",
      "Epoch:  296|steps:   30|Train Avg Loss: 0.3474 |Test Loss: 1.4793|lr = 0.00616\n",
      "Epoch:  296|steps:   60|Train Avg Loss: 0.3361 |Test Loss: 1.3785|lr = 0.00616\n",
      "Epoch:  297|steps:   30|Train Avg Loss: 0.2943 |Test Loss: 1.5208|lr = 0.00616\n",
      "Epoch:  297|steps:   60|Train Avg Loss: 0.3631 |Test Loss: 1.4929|lr = 0.00616\n",
      "Epoch:  298|steps:   30|Train Avg Loss: 0.2930 |Test Loss: 1.6246|lr = 0.00616\n",
      "Epoch:  298|steps:   60|Train Avg Loss: 0.3477 |Test Loss: 1.4178|lr = 0.00616\n",
      "Epoch:  299|steps:   30|Train Avg Loss: 0.2795 |Test Loss: 1.5256|lr = 0.00616\n",
      "Epoch:  299|steps:   60|Train Avg Loss: 0.3144 |Test Loss: 1.4649|lr = 0.00616\n",
      "Epoch:  300|steps:   30|Train Avg Loss: 0.3004 |Test Loss: 1.3591|lr = 0.00616\n",
      "Epoch:  300|steps:   60|Train Avg Loss: 0.3308 |Test Loss: 1.4275|lr = 0.00616\n",
      "Epoch:  301|steps:   30|Train Avg Loss: 0.2927 |Test Loss: 1.4383|lr = 0.00616\n",
      "Epoch:  301|steps:   60|Train Avg Loss: 0.3065 |Test Loss: 1.5059|lr = 0.00616\n",
      "Epoch:  302|steps:   30|Train Avg Loss: 0.2894 |Test Loss: 1.5399|lr = 0.00616\n",
      "Epoch:  302|steps:   60|Train Avg Loss: 0.2757 |Test Loss: 1.5281|lr = 0.00616\n",
      "Epoch:  303|steps:   30|Train Avg Loss: 0.2386 |Test Loss: 1.5375|lr = 0.00616\n",
      "Epoch:  303|steps:   60|Train Avg Loss: 0.2730 |Test Loss: 1.4841|lr = 0.00616\n",
      "Epoch:  304|steps:   30|Train Avg Loss: 0.2220 |Test Loss: 1.4940|lr = 0.00616\n",
      "Epoch:  304|steps:   60|Train Avg Loss: 0.2530 |Test Loss: 1.4688|lr = 0.00616\n",
      "Epoch:  305|steps:   30|Train Avg Loss: 0.2145 |Test Loss: 1.6576|lr = 0.00616\n",
      "Epoch:  305|steps:   60|Train Avg Loss: 0.2269 |Test Loss: 1.5026|lr = 0.00616\n",
      "Epoch:  306|steps:   30|Train Avg Loss: 0.1813 |Test Loss: 1.6323|lr = 0.00616\n",
      "Epoch:  306|steps:   60|Train Avg Loss: 0.2249 |Test Loss: 1.6826|lr = 0.00616\n",
      "Epoch:  307|steps:   30|Train Avg Loss: 0.2332 |Test Loss: 1.5392|lr = 0.00616\n",
      "Epoch:  307|steps:   60|Train Avg Loss: 0.2809 |Test Loss: 1.4907|lr = 0.00616\n",
      "Epoch:  308|steps:   30|Train Avg Loss: 0.2493 |Test Loss: 1.4919|lr = 0.00616\n",
      "Epoch:  308|steps:   60|Train Avg Loss: 0.2672 |Test Loss: 1.5723|lr = 0.00616\n",
      "Epoch:  309|steps:   30|Train Avg Loss: 0.2312 |Test Loss: 1.5581|lr = 0.00616\n",
      "Epoch:  309|steps:   60|Train Avg Loss: 0.2988 |Test Loss: 1.4930|lr = 0.00616\n",
      "Epoch:  310|steps:   30|Train Avg Loss: 0.3256 |Test Loss: 1.5400|lr = 0.00616\n",
      "Epoch:  310|steps:   60|Train Avg Loss: 0.2967 |Test Loss: 1.5759|lr = 0.00616\n",
      "Epoch:  311|steps:   30|Train Avg Loss: 0.2417 |Test Loss: 1.4201|lr = 0.00616\n",
      "Epoch:  311|steps:   60|Train Avg Loss: 0.2487 |Test Loss: 1.4775|lr = 0.00616\n",
      "Epoch:  312|steps:   30|Train Avg Loss: 0.2087 |Test Loss: 1.5373|lr = 0.00616\n",
      "Epoch:  312|steps:   60|Train Avg Loss: 0.1877 |Test Loss: 1.5389|lr = 0.00616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  313|steps:   30|Train Avg Loss: 0.1956 |Test Loss: 1.5371|lr = 0.00616\n",
      "Epoch:  313|steps:   60|Train Avg Loss: 0.1755 |Test Loss: 1.5475|lr = 0.00616\n",
      "Epoch:  314|steps:   30|Train Avg Loss: 0.1680 |Test Loss: 1.5365|lr = 0.00616\n",
      "Epoch:  314|steps:   60|Train Avg Loss: 0.1907 |Test Loss: 1.5075|lr = 0.00616\n",
      "Epoch:  315|steps:   30|Train Avg Loss: 0.1752 |Test Loss: 1.5601|lr = 0.00616\n",
      "Epoch:  315|steps:   60|Train Avg Loss: 0.1719 |Test Loss: 1.4977|lr = 0.00616\n",
      "Epoch:  316|steps:   30|Train Avg Loss: 0.1630 |Test Loss: 1.6340|lr = 0.00616\n",
      "Epoch:  316|steps:   60|Train Avg Loss: 0.1576 |Test Loss: 1.5504|lr = 0.00616\n",
      "Epoch:  317|steps:   30|Train Avg Loss: 0.1546 |Test Loss: 1.5092|lr = 0.00616\n",
      "Epoch:  317|steps:   60|Train Avg Loss: 0.2244 |Test Loss: 1.4668|lr = 0.00616\n",
      "Epoch:  318|steps:   30|Train Avg Loss: 0.1439 |Test Loss: 1.5589|lr = 0.00616\n",
      "Epoch:  318|steps:   60|Train Avg Loss: 0.1458 |Test Loss: 1.5884|lr = 0.00616\n",
      "Epoch:  319|steps:   30|Train Avg Loss: 0.1569 |Test Loss: 1.5351|lr = 0.00603\n",
      "Epoch:  319|steps:   60|Train Avg Loss: 0.1586 |Test Loss: 1.5744|lr = 0.00603\n",
      "Epoch:  320|steps:   30|Train Avg Loss: 0.1638 |Test Loss: 1.4759|lr = 0.00603\n",
      "Epoch:  320|steps:   60|Train Avg Loss: 0.1704 |Test Loss: 1.5879|lr = 0.00603\n",
      "Epoch:  321|steps:   30|Train Avg Loss: 0.1400 |Test Loss: 1.5496|lr = 0.00603\n",
      "Epoch:  321|steps:   60|Train Avg Loss: 0.1573 |Test Loss: 1.5301|lr = 0.00603\n",
      "Epoch:  322|steps:   30|Train Avg Loss: 0.1316 |Test Loss: 1.5624|lr = 0.00603\n",
      "Epoch:  322|steps:   60|Train Avg Loss: 0.1228 |Test Loss: 1.5654|lr = 0.00603\n",
      "Epoch:  323|steps:   30|Train Avg Loss: 0.1460 |Test Loss: 1.6052|lr = 0.00603\n",
      "Epoch:  323|steps:   60|Train Avg Loss: 0.1365 |Test Loss: 1.6203|lr = 0.00603\n",
      "Epoch:  324|steps:   30|Train Avg Loss: 0.1255 |Test Loss: 1.6975|lr = 0.00603\n",
      "Epoch:  324|steps:   60|Train Avg Loss: 0.1536 |Test Loss: 1.6296|lr = 0.00603\n",
      "Epoch:  325|steps:   30|Train Avg Loss: 0.1166 |Test Loss: 1.6155|lr = 0.00603\n",
      "Epoch:  325|steps:   60|Train Avg Loss: 0.1560 |Test Loss: 1.5679|lr = 0.00603\n",
      "Epoch:  326|steps:   30|Train Avg Loss: 0.1332 |Test Loss: 1.5871|lr = 0.00603\n",
      "Epoch:  326|steps:   60|Train Avg Loss: 0.1645 |Test Loss: 1.6851|lr = 0.00603\n",
      "Epoch:  327|steps:   30|Train Avg Loss: 0.1184 |Test Loss: 1.5925|lr = 0.00603\n",
      "Epoch:  327|steps:   60|Train Avg Loss: 0.1341 |Test Loss: 1.6641|lr = 0.00603\n",
      "Epoch:  328|steps:   30|Train Avg Loss: 0.1599 |Test Loss: 1.6088|lr = 0.00603\n",
      "Epoch:  328|steps:   60|Train Avg Loss: 0.1459 |Test Loss: 1.4624|lr = 0.00603\n",
      "Epoch:  329|steps:   30|Train Avg Loss: 0.1691 |Test Loss: 1.5767|lr = 0.00603\n",
      "Epoch:  329|steps:   60|Train Avg Loss: 0.1919 |Test Loss: 1.5149|lr = 0.00603\n",
      "Epoch:  330|steps:   30|Train Avg Loss: 0.1581 |Test Loss: 1.5996|lr = 0.00603\n",
      "Epoch:  330|steps:   60|Train Avg Loss: 0.1874 |Test Loss: 1.5338|lr = 0.00603\n",
      "Epoch:  331|steps:   30|Train Avg Loss: 0.1459 |Test Loss: 1.4497|lr = 0.00603\n",
      "Epoch:  331|steps:   60|Train Avg Loss: 0.1617 |Test Loss: 1.5239|lr = 0.00603\n",
      "Epoch:  332|steps:   30|Train Avg Loss: 0.1500 |Test Loss: 1.5270|lr = 0.00603\n",
      "Epoch:  332|steps:   60|Train Avg Loss: 0.1528 |Test Loss: 1.5241|lr = 0.00603\n",
      "Epoch:  333|steps:   30|Train Avg Loss: 0.1261 |Test Loss: 1.4907|lr = 0.00603\n",
      "Epoch:  333|steps:   60|Train Avg Loss: 0.1186 |Test Loss: 1.5444|lr = 0.00603\n",
      "Epoch:  334|steps:   30|Train Avg Loss: 0.0977 |Test Loss: 1.4448|lr = 0.00603\n",
      "Epoch:  334|steps:   60|Train Avg Loss: 0.1214 |Test Loss: 1.4926|lr = 0.00603\n",
      "Epoch:  335|steps:   30|Train Avg Loss: 0.1289 |Test Loss: 1.4822|lr = 0.00603\n",
      "Epoch:  335|steps:   60|Train Avg Loss: 0.1060 |Test Loss: 1.6489|lr = 0.00603\n",
      "Epoch:  336|steps:   30|Train Avg Loss: 0.0789 |Test Loss: 1.5500|lr = 0.00603\n",
      "Epoch:  336|steps:   60|Train Avg Loss: 0.1245 |Test Loss: 1.6003|lr = 0.00603\n",
      "Epoch:  337|steps:   30|Train Avg Loss: 0.1042 |Test Loss: 1.5081|lr = 0.00603\n",
      "Epoch:  337|steps:   60|Train Avg Loss: 0.1206 |Test Loss: 1.6816|lr = 0.00603\n",
      "Epoch:  338|steps:   30|Train Avg Loss: 0.1229 |Test Loss: 1.5356|lr = 0.00603\n",
      "Epoch:  338|steps:   60|Train Avg Loss: 0.1378 |Test Loss: 1.5504|lr = 0.00603\n",
      "Epoch:  339|steps:   30|Train Avg Loss: 0.1216 |Test Loss: 1.5228|lr = 0.00603\n",
      "Epoch:  339|steps:   60|Train Avg Loss: 0.1301 |Test Loss: 1.5098|lr = 0.00603\n",
      "Epoch:  340|steps:   30|Train Avg Loss: 0.1086 |Test Loss: 1.6303|lr = 0.00603\n",
      "Epoch:  340|steps:   60|Train Avg Loss: 0.1271 |Test Loss: 1.5693|lr = 0.00603\n",
      "Epoch:  341|steps:   30|Train Avg Loss: 0.1090 |Test Loss: 1.5606|lr = 0.00591\n",
      "Epoch:  341|steps:   60|Train Avg Loss: 0.1116 |Test Loss: 1.6260|lr = 0.00591\n",
      "Epoch:  342|steps:   30|Train Avg Loss: 0.1377 |Test Loss: 1.6022|lr = 0.00591\n",
      "Epoch:  342|steps:   60|Train Avg Loss: 0.1630 |Test Loss: 1.5536|lr = 0.00591\n",
      "Epoch:  343|steps:   30|Train Avg Loss: 0.1883 |Test Loss: 1.5586|lr = 0.00591\n",
      "Epoch:  343|steps:   60|Train Avg Loss: 0.1320 |Test Loss: 1.6458|lr = 0.00591\n",
      "Epoch:  344|steps:   30|Train Avg Loss: 0.1498 |Test Loss: 1.5905|lr = 0.00591\n",
      "Epoch:  344|steps:   60|Train Avg Loss: 0.1500 |Test Loss: 1.5532|lr = 0.00591\n",
      "Epoch:  345|steps:   30|Train Avg Loss: 0.1152 |Test Loss: 1.5521|lr = 0.00591\n",
      "Epoch:  345|steps:   60|Train Avg Loss: 0.1298 |Test Loss: 1.4933|lr = 0.00591\n",
      "Epoch:  346|steps:   30|Train Avg Loss: 0.1236 |Test Loss: 1.4510|lr = 0.00591\n",
      "Epoch:  346|steps:   60|Train Avg Loss: 0.1249 |Test Loss: 1.5364|lr = 0.00591\n",
      "Epoch:  347|steps:   30|Train Avg Loss: 0.0855 |Test Loss: 1.5110|lr = 0.00591\n",
      "Epoch:  347|steps:   60|Train Avg Loss: 0.1466 |Test Loss: 1.5120|lr = 0.00591\n",
      "Epoch:  348|steps:   30|Train Avg Loss: 0.1217 |Test Loss: 1.4145|lr = 0.00591\n",
      "Epoch:  348|steps:   60|Train Avg Loss: 0.1177 |Test Loss: 1.5425|lr = 0.00591\n",
      "Epoch:  349|steps:   30|Train Avg Loss: 0.0872 |Test Loss: 1.5591|lr = 0.00591\n",
      "Epoch:  349|steps:   60|Train Avg Loss: 0.1290 |Test Loss: 1.5137|lr = 0.00591\n",
      "Epoch:  350|steps:   30|Train Avg Loss: 0.1232 |Test Loss: 1.4035|lr = 0.00591\n",
      "Epoch:  350|steps:   60|Train Avg Loss: 0.1231 |Test Loss: 1.4508|lr = 0.00591\n",
      "Epoch:  351|steps:   30|Train Avg Loss: 0.0988 |Test Loss: 1.5561|lr = 0.00591\n",
      "Epoch:  351|steps:   60|Train Avg Loss: 0.1514 |Test Loss: 1.5156|lr = 0.00591\n",
      "Epoch:  352|steps:   30|Train Avg Loss: 0.1437 |Test Loss: 1.3782|lr = 0.00580\n",
      "Epoch:  352|steps:   60|Train Avg Loss: 0.1140 |Test Loss: 1.4956|lr = 0.00580\n",
      "Epoch:  353|steps:   30|Train Avg Loss: 0.1048 |Test Loss: 1.5418|lr = 0.00580\n",
      "Epoch:  353|steps:   60|Train Avg Loss: 0.1237 |Test Loss: 1.5591|lr = 0.00580\n",
      "Epoch:  354|steps:   30|Train Avg Loss: 0.1012 |Test Loss: 1.4579|lr = 0.00580\n",
      "Epoch:  354|steps:   60|Train Avg Loss: 0.1058 |Test Loss: 1.4978|lr = 0.00580\n",
      "Epoch:  355|steps:   30|Train Avg Loss: 0.1040 |Test Loss: 1.6097|lr = 0.00580\n",
      "Epoch:  355|steps:   60|Train Avg Loss: 0.1001 |Test Loss: 1.5711|lr = 0.00580\n",
      "Epoch:  356|steps:   30|Train Avg Loss: 0.0779 |Test Loss: 1.6099|lr = 0.00580\n",
      "Epoch:  356|steps:   60|Train Avg Loss: 0.0850 |Test Loss: 1.5772|lr = 0.00580\n",
      "Epoch:  357|steps:   30|Train Avg Loss: 0.1101 |Test Loss: 1.5767|lr = 0.00580\n",
      "Epoch:  357|steps:   60|Train Avg Loss: 0.1275 |Test Loss: 1.5391|lr = 0.00580\n",
      "Epoch:  358|steps:   30|Train Avg Loss: 0.1026 |Test Loss: 1.4920|lr = 0.00580\n",
      "Epoch:  358|steps:   60|Train Avg Loss: 0.0925 |Test Loss: 1.5740|lr = 0.00580\n",
      "Epoch:  359|steps:   30|Train Avg Loss: 0.1043 |Test Loss: 1.6881|lr = 0.00580\n",
      "Epoch:  359|steps:   60|Train Avg Loss: 0.1047 |Test Loss: 1.5708|lr = 0.00580\n",
      "Epoch:  360|steps:   30|Train Avg Loss: 0.0961 |Test Loss: 1.5765|lr = 0.00580\n",
      "Epoch:  360|steps:   60|Train Avg Loss: 0.1101 |Test Loss: 1.5239|lr = 0.00580\n",
      "Epoch:  361|steps:   30|Train Avg Loss: 0.0769 |Test Loss: 1.5335|lr = 0.00580\n",
      "Epoch:  361|steps:   60|Train Avg Loss: 0.0788 |Test Loss: 1.5164|lr = 0.00580\n",
      "Epoch:  362|steps:   30|Train Avg Loss: 0.0672 |Test Loss: 1.6218|lr = 0.00580\n",
      "Epoch:  362|steps:   60|Train Avg Loss: 0.0739 |Test Loss: 1.5429|lr = 0.00580\n",
      "Epoch:  363|steps:   30|Train Avg Loss: 0.0754 |Test Loss: 1.5693|lr = 0.00580\n",
      "Epoch:  363|steps:   60|Train Avg Loss: 0.0900 |Test Loss: 1.5465|lr = 0.00580\n",
      "Epoch:  364|steps:   30|Train Avg Loss: 0.0884 |Test Loss: 1.6243|lr = 0.00580\n",
      "Epoch:  364|steps:   60|Train Avg Loss: 0.0909 |Test Loss: 1.5713|lr = 0.00580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  365|steps:   30|Train Avg Loss: 0.0902 |Test Loss: 1.4873|lr = 0.00568\n",
      "Epoch:  365|steps:   60|Train Avg Loss: 0.0921 |Test Loss: 1.6400|lr = 0.00568\n",
      "Epoch:  366|steps:   30|Train Avg Loss: 0.0895 |Test Loss: 1.6299|lr = 0.00568\n",
      "Epoch:  366|steps:   60|Train Avg Loss: 0.0879 |Test Loss: 1.5743|lr = 0.00568\n",
      "Epoch:  367|steps:   30|Train Avg Loss: 0.0803 |Test Loss: 1.4942|lr = 0.00568\n",
      "Epoch:  367|steps:   60|Train Avg Loss: 0.0672 |Test Loss: 1.5843|lr = 0.00568\n",
      "Epoch:  368|steps:   30|Train Avg Loss: 0.0897 |Test Loss: 1.5766|lr = 0.00568\n",
      "Epoch:  368|steps:   60|Train Avg Loss: 0.1044 |Test Loss: 1.5345|lr = 0.00568\n",
      "Epoch:  369|steps:   30|Train Avg Loss: 0.1219 |Test Loss: 1.5269|lr = 0.00568\n",
      "Epoch:  369|steps:   60|Train Avg Loss: 0.1665 |Test Loss: 1.4996|lr = 0.00568\n",
      "Epoch:  370|steps:   30|Train Avg Loss: 0.1661 |Test Loss: 1.4974|lr = 0.00568\n",
      "Epoch:  370|steps:   60|Train Avg Loss: 0.1575 |Test Loss: 1.4131|lr = 0.00568\n",
      "Epoch:  371|steps:   30|Train Avg Loss: 0.1461 |Test Loss: 1.5364|lr = 0.00568\n",
      "Epoch:  371|steps:   60|Train Avg Loss: 0.1405 |Test Loss: 1.5812|lr = 0.00568\n",
      "Epoch:  372|steps:   30|Train Avg Loss: 0.1285 |Test Loss: 1.5371|lr = 0.00568\n",
      "Epoch:  372|steps:   60|Train Avg Loss: 0.1217 |Test Loss: 1.4965|lr = 0.00568\n",
      "Epoch:  373|steps:   30|Train Avg Loss: 0.0868 |Test Loss: 1.6043|lr = 0.00568\n",
      "Epoch:  373|steps:   60|Train Avg Loss: 0.1134 |Test Loss: 1.6690|lr = 0.00568\n",
      "Epoch:  374|steps:   30|Train Avg Loss: 0.1273 |Test Loss: 1.5379|lr = 0.00568\n",
      "Epoch:  374|steps:   60|Train Avg Loss: 0.1134 |Test Loss: 1.6368|lr = 0.00568\n",
      "Epoch:  375|steps:   30|Train Avg Loss: 0.0671 |Test Loss: 1.5278|lr = 0.00568\n",
      "Epoch:  375|steps:   60|Train Avg Loss: 0.0933 |Test Loss: 1.5524|lr = 0.00568\n",
      "Epoch:  376|steps:   30|Train Avg Loss: 0.0804 |Test Loss: 1.5047|lr = 0.00568\n",
      "Epoch:  376|steps:   60|Train Avg Loss: 0.1177 |Test Loss: 1.4810|lr = 0.00568\n",
      "Epoch:  377|steps:   30|Train Avg Loss: 0.1266 |Test Loss: 1.5374|lr = 0.00568\n",
      "Epoch:  377|steps:   60|Train Avg Loss: 0.1005 |Test Loss: 1.6306|lr = 0.00568\n",
      "Epoch:  378|steps:   30|Train Avg Loss: 0.0722 |Test Loss: 1.5992|lr = 0.00568\n",
      "Epoch:  378|steps:   60|Train Avg Loss: 0.1033 |Test Loss: 1.6326|lr = 0.00568\n",
      "Epoch:  379|steps:   30|Train Avg Loss: 0.0758 |Test Loss: 1.6567|lr = 0.00568\n",
      "Epoch:  379|steps:   60|Train Avg Loss: 0.0949 |Test Loss: 1.5314|lr = 0.00568\n",
      "Epoch:  380|steps:   30|Train Avg Loss: 0.0794 |Test Loss: 1.6238|lr = 0.00568\n",
      "Epoch:  380|steps:   60|Train Avg Loss: 0.1166 |Test Loss: 1.5243|lr = 0.00568\n",
      "Epoch:  381|steps:   30|Train Avg Loss: 0.1059 |Test Loss: 1.4128|lr = 0.00568\n",
      "Epoch:  381|steps:   60|Train Avg Loss: 0.0927 |Test Loss: 1.5333|lr = 0.00568\n",
      "Epoch:  382|steps:   30|Train Avg Loss: 0.1279 |Test Loss: 1.3909|lr = 0.00568\n",
      "Epoch:  382|steps:   60|Train Avg Loss: 0.1570 |Test Loss: 1.5186|lr = 0.00568\n",
      "Epoch:  383|steps:   30|Train Avg Loss: 0.1354 |Test Loss: 1.5583|lr = 0.00568\n",
      "Epoch:  383|steps:   60|Train Avg Loss: 0.1520 |Test Loss: 1.5303|lr = 0.00568\n",
      "Epoch:  384|steps:   30|Train Avg Loss: 0.1880 |Test Loss: 1.7217|lr = 0.00568\n",
      "Epoch:  384|steps:   60|Train Avg Loss: 0.1727 |Test Loss: 1.5060|lr = 0.00568\n",
      "Epoch:  385|steps:   30|Train Avg Loss: 0.1238 |Test Loss: 1.5024|lr = 0.00568\n",
      "Epoch:  385|steps:   60|Train Avg Loss: 0.0998 |Test Loss: 1.5361|lr = 0.00568\n",
      "Epoch:  386|steps:   30|Train Avg Loss: 0.0946 |Test Loss: 1.5867|lr = 0.00557\n",
      "Epoch:  386|steps:   60|Train Avg Loss: 0.0879 |Test Loss: 1.5884|lr = 0.00557\n",
      "Epoch:  387|steps:   30|Train Avg Loss: 0.0653 |Test Loss: 1.5938|lr = 0.00557\n",
      "Epoch:  387|steps:   60|Train Avg Loss: 0.0823 |Test Loss: 1.4918|lr = 0.00557\n",
      "Epoch:  388|steps:   30|Train Avg Loss: 0.0763 |Test Loss: 1.6109|lr = 0.00557\n",
      "Epoch:  388|steps:   60|Train Avg Loss: 0.0910 |Test Loss: 1.5542|lr = 0.00557\n",
      "Epoch:  389|steps:   30|Train Avg Loss: 0.1003 |Test Loss: 1.5979|lr = 0.00557\n",
      "Epoch:  389|steps:   60|Train Avg Loss: 0.0964 |Test Loss: 1.6303|lr = 0.00557\n",
      "Epoch:  390|steps:   30|Train Avg Loss: 0.0798 |Test Loss: 1.4666|lr = 0.00557\n",
      "Epoch:  390|steps:   60|Train Avg Loss: 0.0752 |Test Loss: 1.5252|lr = 0.00557\n",
      "Epoch:  391|steps:   30|Train Avg Loss: 0.0636 |Test Loss: 1.5200|lr = 0.00557\n",
      "Epoch:  391|steps:   60|Train Avg Loss: 0.0687 |Test Loss: 1.5857|lr = 0.00557\n",
      "Epoch:  392|steps:   30|Train Avg Loss: 0.0774 |Test Loss: 1.5483|lr = 0.00557\n",
      "Epoch:  392|steps:   60|Train Avg Loss: 0.0596 |Test Loss: 1.6183|lr = 0.00557\n",
      "Epoch:  393|steps:   30|Train Avg Loss: 0.1040 |Test Loss: 1.5353|lr = 0.00557\n",
      "Epoch:  393|steps:   60|Train Avg Loss: 0.0850 |Test Loss: 1.5959|lr = 0.00557\n",
      "Epoch:  394|steps:   30|Train Avg Loss: 0.0907 |Test Loss: 1.6062|lr = 0.00557\n",
      "Epoch:  394|steps:   60|Train Avg Loss: 0.0947 |Test Loss: 1.6430|lr = 0.00557\n",
      "Epoch:  395|steps:   30|Train Avg Loss: 0.0843 |Test Loss: 1.6195|lr = 0.00557\n",
      "Epoch:  395|steps:   60|Train Avg Loss: 0.1282 |Test Loss: 1.5975|lr = 0.00557\n",
      "Epoch:  396|steps:   30|Train Avg Loss: 0.1345 |Test Loss: 1.4990|lr = 0.00557\n",
      "Epoch:  396|steps:   60|Train Avg Loss: 0.1328 |Test Loss: 1.5260|lr = 0.00557\n",
      "Epoch:  397|steps:   30|Train Avg Loss: 0.1321 |Test Loss: 1.5638|lr = 0.00557\n",
      "Epoch:  397|steps:   60|Train Avg Loss: 0.1425 |Test Loss: 1.5600|lr = 0.00557\n",
      "Epoch:  398|steps:   30|Train Avg Loss: 0.0960 |Test Loss: 1.5905|lr = 0.00557\n",
      "Epoch:  398|steps:   60|Train Avg Loss: 0.1136 |Test Loss: 1.5543|lr = 0.00557\n",
      "Epoch:  399|steps:   30|Train Avg Loss: 0.1205 |Test Loss: 1.5965|lr = 0.00557\n",
      "Epoch:  399|steps:   60|Train Avg Loss: 0.0730 |Test Loss: 1.6139|lr = 0.00557\n",
      "Epoch:  400|steps:   30|Train Avg Loss: 0.0817 |Test Loss: 1.6367|lr = 0.00557\n",
      "Epoch:  400|steps:   60|Train Avg Loss: 0.1264 |Test Loss: 1.6158|lr = 0.00557\n",
      "Epoch:  401|steps:   30|Train Avg Loss: 0.1036 |Test Loss: 1.5202|lr = 0.00557\n",
      "Epoch:  401|steps:   60|Train Avg Loss: 0.1246 |Test Loss: 1.5849|lr = 0.00557\n",
      "Epoch:  402|steps:   30|Train Avg Loss: 0.0770 |Test Loss: 1.6377|lr = 0.00545\n",
      "Epoch:  402|steps:   60|Train Avg Loss: 0.0847 |Test Loss: 1.6030|lr = 0.00545\n",
      "Epoch:  403|steps:   30|Train Avg Loss: 0.0591 |Test Loss: 1.6124|lr = 0.00545\n",
      "Epoch:  403|steps:   60|Train Avg Loss: 0.0681 |Test Loss: 1.6516|lr = 0.00545\n",
      "Epoch:  404|steps:   30|Train Avg Loss: 0.0645 |Test Loss: 1.6604|lr = 0.00545\n",
      "Epoch:  404|steps:   60|Train Avg Loss: 0.0693 |Test Loss: 1.5830|lr = 0.00545\n",
      "Epoch:  405|steps:   30|Train Avg Loss: 0.0520 |Test Loss: 1.6182|lr = 0.00545\n",
      "Epoch:  405|steps:   60|Train Avg Loss: 0.0702 |Test Loss: 1.5391|lr = 0.00545\n",
      "Epoch:  406|steps:   30|Train Avg Loss: 0.0552 |Test Loss: 1.5571|lr = 0.00545\n",
      "Epoch:  406|steps:   60|Train Avg Loss: 0.0618 |Test Loss: 1.5062|lr = 0.00545\n",
      "Epoch:  407|steps:   30|Train Avg Loss: 0.0838 |Test Loss: 1.6090|lr = 0.00545\n",
      "Epoch:  407|steps:   60|Train Avg Loss: 0.0833 |Test Loss: 1.5195|lr = 0.00545\n",
      "Epoch:  408|steps:   30|Train Avg Loss: 0.0719 |Test Loss: 1.3902|lr = 0.00545\n",
      "Epoch:  408|steps:   60|Train Avg Loss: 0.1189 |Test Loss: 1.4543|lr = 0.00545\n",
      "Epoch:  409|steps:   30|Train Avg Loss: 0.1303 |Test Loss: 1.5970|lr = 0.00545\n",
      "Epoch:  409|steps:   60|Train Avg Loss: 0.1176 |Test Loss: 1.4838|lr = 0.00545\n",
      "Epoch:  410|steps:   30|Train Avg Loss: 0.1099 |Test Loss: 1.4826|lr = 0.00545\n",
      "Epoch:  410|steps:   60|Train Avg Loss: 0.1342 |Test Loss: 1.4269|lr = 0.00545\n",
      "Epoch:  411|steps:   30|Train Avg Loss: 0.1692 |Test Loss: 1.6334|lr = 0.00545\n",
      "Epoch:  411|steps:   60|Train Avg Loss: 0.1615 |Test Loss: 1.5135|lr = 0.00545\n",
      "Epoch:  412|steps:   30|Train Avg Loss: 0.0964 |Test Loss: 1.4931|lr = 0.00545\n",
      "Epoch:  412|steps:   60|Train Avg Loss: 0.1016 |Test Loss: 1.4869|lr = 0.00545\n",
      "Epoch:  413|steps:   30|Train Avg Loss: 0.0863 |Test Loss: 1.4287|lr = 0.00535\n",
      "Epoch:  413|steps:   60|Train Avg Loss: 0.0869 |Test Loss: 1.5216|lr = 0.00535\n",
      "Epoch:  414|steps:   30|Train Avg Loss: 0.0619 |Test Loss: 1.5416|lr = 0.00535\n",
      "Epoch:  414|steps:   60|Train Avg Loss: 0.0720 |Test Loss: 1.5679|lr = 0.00535\n",
      "Epoch:  415|steps:   30|Train Avg Loss: 0.0624 |Test Loss: 1.5392|lr = 0.00535\n",
      "Epoch:  415|steps:   60|Train Avg Loss: 0.0560 |Test Loss: 1.5286|lr = 0.00535\n",
      "Epoch:  416|steps:   30|Train Avg Loss: 0.0538 |Test Loss: 1.5896|lr = 0.00535\n",
      "Epoch:  416|steps:   60|Train Avg Loss: 0.0551 |Test Loss: 1.5019|lr = 0.00535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  417|steps:   30|Train Avg Loss: 0.0565 |Test Loss: 1.5430|lr = 0.00535\n",
      "Epoch:  417|steps:   60|Train Avg Loss: 0.0572 |Test Loss: 1.5783|lr = 0.00535\n",
      "Epoch:  418|steps:   30|Train Avg Loss: 0.0575 |Test Loss: 1.5705|lr = 0.00535\n",
      "Epoch:  418|steps:   60|Train Avg Loss: 0.0846 |Test Loss: 1.5986|lr = 0.00535\n",
      "Epoch:  419|steps:   30|Train Avg Loss: 0.0694 |Test Loss: 1.5739|lr = 0.00535\n",
      "Epoch:  419|steps:   60|Train Avg Loss: 0.0496 |Test Loss: 1.5853|lr = 0.00535\n",
      "Epoch:  420|steps:   30|Train Avg Loss: 0.0569 |Test Loss: 1.4556|lr = 0.00535\n",
      "Epoch:  420|steps:   60|Train Avg Loss: 0.1052 |Test Loss: 1.4876|lr = 0.00535\n",
      "Epoch:  421|steps:   30|Train Avg Loss: 0.0856 |Test Loss: 1.4906|lr = 0.00535\n",
      "Epoch:  421|steps:   60|Train Avg Loss: 0.1008 |Test Loss: 1.4960|lr = 0.00535\n",
      "Epoch:  422|steps:   30|Train Avg Loss: 0.0856 |Test Loss: 1.5563|lr = 0.00535\n",
      "Epoch:  422|steps:   60|Train Avg Loss: 0.0809 |Test Loss: 1.6555|lr = 0.00535\n",
      "Epoch:  423|steps:   30|Train Avg Loss: 0.0743 |Test Loss: 1.6634|lr = 0.00535\n",
      "Epoch:  423|steps:   60|Train Avg Loss: 0.1014 |Test Loss: 1.6892|lr = 0.00535\n",
      "Epoch:  424|steps:   30|Train Avg Loss: 0.1087 |Test Loss: 1.5898|lr = 0.00535\n",
      "Epoch:  424|steps:   60|Train Avg Loss: 0.1174 |Test Loss: 1.5876|lr = 0.00535\n",
      "Epoch:  425|steps:   30|Train Avg Loss: 0.1220 |Test Loss: 1.5334|lr = 0.00535\n",
      "Epoch:  425|steps:   60|Train Avg Loss: 0.0984 |Test Loss: 1.5380|lr = 0.00535\n",
      "Epoch:  426|steps:   30|Train Avg Loss: 0.0655 |Test Loss: 1.5564|lr = 0.00535\n",
      "Epoch:  426|steps:   60|Train Avg Loss: 0.0704 |Test Loss: 1.4924|lr = 0.00535\n",
      "Epoch:  427|steps:   30|Train Avg Loss: 0.0719 |Test Loss: 1.4946|lr = 0.00535\n",
      "Epoch:  427|steps:   60|Train Avg Loss: 0.0726 |Test Loss: 1.5598|lr = 0.00535\n",
      "Epoch:  428|steps:   30|Train Avg Loss: 0.1007 |Test Loss: 1.5399|lr = 0.00535\n",
      "Epoch:  428|steps:   60|Train Avg Loss: 0.0821 |Test Loss: 1.4754|lr = 0.00535\n",
      "Epoch:  429|steps:   30|Train Avg Loss: 0.0694 |Test Loss: 1.5573|lr = 0.00535\n",
      "Epoch:  429|steps:   60|Train Avg Loss: 0.0568 |Test Loss: 1.5397|lr = 0.00535\n",
      "Epoch:  430|steps:   30|Train Avg Loss: 0.0710 |Test Loss: 1.6407|lr = 0.00535\n",
      "Epoch:  430|steps:   60|Train Avg Loss: 0.0945 |Test Loss: 1.5501|lr = 0.00535\n",
      "Epoch:  431|steps:   30|Train Avg Loss: 0.0631 |Test Loss: 1.5755|lr = 0.00524\n",
      "Epoch:  431|steps:   60|Train Avg Loss: 0.0841 |Test Loss: 1.5030|lr = 0.00524\n",
      "Epoch:  432|steps:   30|Train Avg Loss: 0.0887 |Test Loss: 1.5565|lr = 0.00524\n",
      "Epoch:  432|steps:   60|Train Avg Loss: 0.0744 |Test Loss: 1.6766|lr = 0.00524\n",
      "Epoch:  433|steps:   30|Train Avg Loss: 0.0787 |Test Loss: 1.6958|lr = 0.00524\n",
      "Epoch:  433|steps:   60|Train Avg Loss: 0.0961 |Test Loss: 1.5886|lr = 0.00524\n",
      "Epoch:  434|steps:   30|Train Avg Loss: 0.0740 |Test Loss: 1.5915|lr = 0.00524\n",
      "Epoch:  434|steps:   60|Train Avg Loss: 0.0692 |Test Loss: 1.6130|lr = 0.00524\n",
      "Epoch:  435|steps:   30|Train Avg Loss: 0.0619 |Test Loss: 1.5822|lr = 0.00524\n",
      "Epoch:  435|steps:   60|Train Avg Loss: 0.0636 |Test Loss: 1.5445|lr = 0.00524\n",
      "Epoch:  436|steps:   30|Train Avg Loss: 0.0561 |Test Loss: 1.6307|lr = 0.00524\n",
      "Epoch:  436|steps:   60|Train Avg Loss: 0.0822 |Test Loss: 1.5212|lr = 0.00524\n",
      "Epoch:  437|steps:   30|Train Avg Loss: 0.0885 |Test Loss: 1.6424|lr = 0.00524\n",
      "Epoch:  437|steps:   60|Train Avg Loss: 0.1101 |Test Loss: 1.4507|lr = 0.00524\n",
      "Epoch:  438|steps:   30|Train Avg Loss: 0.0890 |Test Loss: 1.6217|lr = 0.00524\n",
      "Epoch:  438|steps:   60|Train Avg Loss: 0.0961 |Test Loss: 1.6076|lr = 0.00524\n",
      "Epoch:  439|steps:   30|Train Avg Loss: 0.0950 |Test Loss: 1.6424|lr = 0.00524\n",
      "Epoch:  439|steps:   60|Train Avg Loss: 0.1060 |Test Loss: 1.4956|lr = 0.00524\n",
      "Epoch:  440|steps:   30|Train Avg Loss: 0.1025 |Test Loss: 1.5327|lr = 0.00524\n",
      "Epoch:  440|steps:   60|Train Avg Loss: 0.0999 |Test Loss: 1.5902|lr = 0.00524\n",
      "Epoch:  441|steps:   30|Train Avg Loss: 0.0669 |Test Loss: 1.5786|lr = 0.00524\n",
      "Epoch:  441|steps:   60|Train Avg Loss: 0.0880 |Test Loss: 1.5796|lr = 0.00524\n",
      "Epoch:  442|steps:   30|Train Avg Loss: 0.1083 |Test Loss: 1.5635|lr = 0.00513\n",
      "Epoch:  442|steps:   60|Train Avg Loss: 0.0771 |Test Loss: 1.5110|lr = 0.00513\n",
      "Epoch:  443|steps:   30|Train Avg Loss: 0.0890 |Test Loss: 1.5078|lr = 0.00513\n",
      "Epoch:  443|steps:   60|Train Avg Loss: 0.1072 |Test Loss: 1.5740|lr = 0.00513\n",
      "Epoch:  444|steps:   30|Train Avg Loss: 0.1010 |Test Loss: 1.5383|lr = 0.00513\n",
      "Epoch:  444|steps:   60|Train Avg Loss: 0.0955 |Test Loss: 1.6895|lr = 0.00513\n",
      "Epoch:  445|steps:   30|Train Avg Loss: 0.0748 |Test Loss: 1.5304|lr = 0.00513\n",
      "Epoch:  445|steps:   60|Train Avg Loss: 0.0954 |Test Loss: 1.4788|lr = 0.00513\n",
      "Epoch:  446|steps:   30|Train Avg Loss: 0.0678 |Test Loss: 1.6442|lr = 0.00513\n",
      "Epoch:  446|steps:   60|Train Avg Loss: 0.0975 |Test Loss: 1.5566|lr = 0.00513\n",
      "Epoch:  447|steps:   30|Train Avg Loss: 0.0775 |Test Loss: 1.5198|lr = 0.00513\n",
      "Epoch:  447|steps:   60|Train Avg Loss: 0.0714 |Test Loss: 1.5751|lr = 0.00513\n",
      "Epoch:  448|steps:   30|Train Avg Loss: 0.0913 |Test Loss: 1.4772|lr = 0.00513\n",
      "Epoch:  448|steps:   60|Train Avg Loss: 0.0696 |Test Loss: 1.4886|lr = 0.00513\n",
      "Epoch:  449|steps:   30|Train Avg Loss: 0.0860 |Test Loss: 1.5017|lr = 0.00513\n",
      "Epoch:  449|steps:   60|Train Avg Loss: 0.0724 |Test Loss: 1.5369|lr = 0.00513\n",
      "Epoch:  450|steps:   30|Train Avg Loss: 0.0744 |Test Loss: 1.5269|lr = 0.00513\n",
      "Epoch:  450|steps:   60|Train Avg Loss: 0.0796 |Test Loss: 1.5149|lr = 0.00513\n",
      "Epoch:  451|steps:   30|Train Avg Loss: 0.0600 |Test Loss: 1.5854|lr = 0.00513\n",
      "Epoch:  451|steps:   60|Train Avg Loss: 0.0536 |Test Loss: 1.5494|lr = 0.00513\n",
      "Epoch:  452|steps:   30|Train Avg Loss: 0.0471 |Test Loss: 1.5948|lr = 0.00513\n",
      "Epoch:  452|steps:   60|Train Avg Loss: 0.0608 |Test Loss: 1.5144|lr = 0.00513\n",
      "Epoch:  453|steps:   30|Train Avg Loss: 0.0595 |Test Loss: 1.4585|lr = 0.00503\n",
      "Epoch:  453|steps:   60|Train Avg Loss: 0.0434 |Test Loss: 1.4663|lr = 0.00503\n",
      "Epoch:  454|steps:   30|Train Avg Loss: 0.0426 |Test Loss: 1.4998|lr = 0.00503\n",
      "Epoch:  454|steps:   60|Train Avg Loss: 0.0561 |Test Loss: 1.4997|lr = 0.00503\n",
      "Epoch:  455|steps:   30|Train Avg Loss: 0.0545 |Test Loss: 1.4993|lr = 0.00503\n",
      "Epoch:  455|steps:   60|Train Avg Loss: 0.0598 |Test Loss: 1.4349|lr = 0.00503\n",
      "Epoch:  456|steps:   30|Train Avg Loss: 0.0597 |Test Loss: 1.4704|lr = 0.00503\n",
      "Epoch:  456|steps:   60|Train Avg Loss: 0.0842 |Test Loss: 1.4313|lr = 0.00503\n",
      "Epoch:  457|steps:   30|Train Avg Loss: 0.0714 |Test Loss: 1.4637|lr = 0.00503\n",
      "Epoch:  457|steps:   60|Train Avg Loss: 0.0668 |Test Loss: 1.5056|lr = 0.00503\n",
      "Epoch:  458|steps:   30|Train Avg Loss: 0.0804 |Test Loss: 1.3241|lr = 0.00503\n",
      "Epoch:  458|steps:   60|Train Avg Loss: 0.1013 |Test Loss: 1.6087|lr = 0.00503\n",
      "Epoch:  459|steps:   30|Train Avg Loss: 0.0841 |Test Loss: 1.4573|lr = 0.00503\n",
      "Epoch:  459|steps:   60|Train Avg Loss: 0.1063 |Test Loss: 1.5115|lr = 0.00503\n",
      "Epoch:  460|steps:   30|Train Avg Loss: 0.0874 |Test Loss: 1.4485|lr = 0.00503\n",
      "Epoch:  460|steps:   60|Train Avg Loss: 0.0613 |Test Loss: 1.5569|lr = 0.00503\n",
      "Epoch:  461|steps:   30|Train Avg Loss: 0.0826 |Test Loss: 1.4856|lr = 0.00503\n",
      "Epoch:  461|steps:   60|Train Avg Loss: 0.0633 |Test Loss: 1.4960|lr = 0.00503\n",
      "Epoch:  462|steps:   30|Train Avg Loss: 0.0606 |Test Loss: 1.4286|lr = 0.00503\n",
      "Epoch:  462|steps:   60|Train Avg Loss: 0.0574 |Test Loss: 1.4382|lr = 0.00503\n",
      "Epoch:  463|steps:   30|Train Avg Loss: 0.0711 |Test Loss: 1.5528|lr = 0.00503\n",
      "Epoch:  463|steps:   60|Train Avg Loss: 0.0984 |Test Loss: 1.4591|lr = 0.00503\n",
      "Epoch:  464|steps:   30|Train Avg Loss: 0.0643 |Test Loss: 1.4889|lr = 0.00493\n",
      "Epoch:  464|steps:   60|Train Avg Loss: 0.0702 |Test Loss: 1.4372|lr = 0.00493\n",
      "Epoch:  465|steps:   30|Train Avg Loss: 0.0678 |Test Loss: 1.4489|lr = 0.00493\n",
      "Epoch:  465|steps:   60|Train Avg Loss: 0.0867 |Test Loss: 1.4900|lr = 0.00493\n",
      "Epoch:  466|steps:   30|Train Avg Loss: 0.0768 |Test Loss: 1.5146|lr = 0.00493\n",
      "Epoch:  466|steps:   60|Train Avg Loss: 0.0640 |Test Loss: 1.5631|lr = 0.00493\n",
      "Epoch:  467|steps:   30|Train Avg Loss: 0.0695 |Test Loss: 1.4324|lr = 0.00493\n",
      "Epoch:  467|steps:   60|Train Avg Loss: 0.0671 |Test Loss: 1.4793|lr = 0.00493\n",
      "Epoch:  468|steps:   30|Train Avg Loss: 0.0627 |Test Loss: 1.4958|lr = 0.00493\n",
      "Epoch:  468|steps:   60|Train Avg Loss: 0.0815 |Test Loss: 1.5133|lr = 0.00493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  469|steps:   30|Train Avg Loss: 0.0913 |Test Loss: 1.5097|lr = 0.00493\n",
      "Epoch:  469|steps:   60|Train Avg Loss: 0.0758 |Test Loss: 1.5553|lr = 0.00493\n",
      "Epoch:  470|steps:   30|Train Avg Loss: 0.0512 |Test Loss: 1.4793|lr = 0.00493\n",
      "Epoch:  470|steps:   60|Train Avg Loss: 0.0780 |Test Loss: 1.4956|lr = 0.00493\n",
      "Epoch:  471|steps:   30|Train Avg Loss: 0.0765 |Test Loss: 1.4462|lr = 0.00493\n",
      "Epoch:  471|steps:   60|Train Avg Loss: 0.1051 |Test Loss: 1.5655|lr = 0.00493\n",
      "Epoch:  472|steps:   30|Train Avg Loss: 0.0971 |Test Loss: 1.5029|lr = 0.00493\n",
      "Epoch:  472|steps:   60|Train Avg Loss: 0.0778 |Test Loss: 1.5569|lr = 0.00493\n",
      "Epoch:  473|steps:   30|Train Avg Loss: 0.0717 |Test Loss: 1.5634|lr = 0.00493\n",
      "Epoch:  473|steps:   60|Train Avg Loss: 0.0714 |Test Loss: 1.4900|lr = 0.00493\n",
      "Epoch:  474|steps:   30|Train Avg Loss: 0.0643 |Test Loss: 1.5389|lr = 0.00493\n",
      "Epoch:  474|steps:   60|Train Avg Loss: 0.0532 |Test Loss: 1.5176|lr = 0.00493\n",
      "Epoch:  475|steps:   30|Train Avg Loss: 0.0539 |Test Loss: 1.6003|lr = 0.00483\n",
      "Epoch:  475|steps:   60|Train Avg Loss: 0.0696 |Test Loss: 1.6350|lr = 0.00483\n",
      "Epoch:  476|steps:   30|Train Avg Loss: 0.0559 |Test Loss: 1.5856|lr = 0.00483\n",
      "Epoch:  476|steps:   60|Train Avg Loss: 0.0603 |Test Loss: 1.5750|lr = 0.00483\n",
      "Epoch:  477|steps:   30|Train Avg Loss: 0.0491 |Test Loss: 1.6014|lr = 0.00483\n",
      "Epoch:  477|steps:   60|Train Avg Loss: 0.0416 |Test Loss: 1.4971|lr = 0.00483\n",
      "Epoch:  478|steps:   30|Train Avg Loss: 0.0407 |Test Loss: 1.5918|lr = 0.00483\n",
      "Epoch:  478|steps:   60|Train Avg Loss: 0.0457 |Test Loss: 1.5554|lr = 0.00483\n",
      "Epoch:  479|steps:   30|Train Avg Loss: 0.0498 |Test Loss: 1.5298|lr = 0.00483\n",
      "Epoch:  479|steps:   60|Train Avg Loss: 0.0442 |Test Loss: 1.5770|lr = 0.00483\n",
      "Epoch:  480|steps:   30|Train Avg Loss: 0.0380 |Test Loss: 1.6118|lr = 0.00483\n",
      "Epoch:  480|steps:   60|Train Avg Loss: 0.0584 |Test Loss: 1.5372|lr = 0.00483\n",
      "Epoch:  481|steps:   30|Train Avg Loss: 0.0528 |Test Loss: 1.5140|lr = 0.00483\n",
      "Epoch:  481|steps:   60|Train Avg Loss: 0.0572 |Test Loss: 1.4109|lr = 0.00483\n",
      "Epoch:  482|steps:   30|Train Avg Loss: 0.0658 |Test Loss: 1.4817|lr = 0.00483\n",
      "Epoch:  482|steps:   60|Train Avg Loss: 0.0653 |Test Loss: 1.5078|lr = 0.00483\n",
      "Epoch:  483|steps:   30|Train Avg Loss: 0.0582 |Test Loss: 1.5677|lr = 0.00483\n",
      "Epoch:  483|steps:   60|Train Avg Loss: 0.0703 |Test Loss: 1.4513|lr = 0.00483\n",
      "Epoch:  484|steps:   30|Train Avg Loss: 0.1057 |Test Loss: 1.4612|lr = 0.00483\n",
      "Epoch:  484|steps:   60|Train Avg Loss: 0.0895 |Test Loss: 1.4437|lr = 0.00483\n",
      "Epoch:  485|steps:   30|Train Avg Loss: 0.0808 |Test Loss: 1.4678|lr = 0.00483\n",
      "Epoch:  485|steps:   60|Train Avg Loss: 0.1600 |Test Loss: 1.4424|lr = 0.00483\n",
      "Epoch:  486|steps:   30|Train Avg Loss: 0.1870 |Test Loss: 1.4435|lr = 0.00474\n",
      "Epoch:  486|steps:   60|Train Avg Loss: 0.1685 |Test Loss: 1.3905|lr = 0.00474\n",
      "Epoch:  487|steps:   30|Train Avg Loss: 0.1136 |Test Loss: 1.3993|lr = 0.00474\n",
      "Epoch:  487|steps:   60|Train Avg Loss: 0.1159 |Test Loss: 1.5372|lr = 0.00474\n",
      "Epoch:  488|steps:   30|Train Avg Loss: 0.1015 |Test Loss: 1.5189|lr = 0.00474\n",
      "Epoch:  488|steps:   60|Train Avg Loss: 0.0784 |Test Loss: 1.4977|lr = 0.00474\n",
      "Epoch:  489|steps:   30|Train Avg Loss: 0.0743 |Test Loss: 1.5080|lr = 0.00474\n",
      "Epoch:  489|steps:   60|Train Avg Loss: 0.0804 |Test Loss: 1.4704|lr = 0.00474\n",
      "Epoch:  490|steps:   30|Train Avg Loss: 0.0725 |Test Loss: 1.4024|lr = 0.00474\n",
      "Epoch:  490|steps:   60|Train Avg Loss: 0.0480 |Test Loss: 1.4415|lr = 0.00474\n",
      "Epoch:  491|steps:   30|Train Avg Loss: 0.0449 |Test Loss: 1.4608|lr = 0.00474\n",
      "Epoch:  491|steps:   60|Train Avg Loss: 0.0517 |Test Loss: 1.4939|lr = 0.00474\n",
      "Epoch:  492|steps:   30|Train Avg Loss: 0.0574 |Test Loss: 1.5140|lr = 0.00474\n",
      "Epoch:  492|steps:   60|Train Avg Loss: 0.0554 |Test Loss: 1.4852|lr = 0.00474\n",
      "Epoch:  493|steps:   30|Train Avg Loss: 0.0560 |Test Loss: 1.5133|lr = 0.00474\n",
      "Epoch:  493|steps:   60|Train Avg Loss: 0.0464 |Test Loss: 1.5588|lr = 0.00474\n",
      "Epoch:  494|steps:   30|Train Avg Loss: 0.0552 |Test Loss: 1.5224|lr = 0.00474\n",
      "Epoch:  494|steps:   60|Train Avg Loss: 0.0499 |Test Loss: 1.5046|lr = 0.00474\n",
      "Epoch:  495|steps:   30|Train Avg Loss: 0.0621 |Test Loss: 1.4977|lr = 0.00474\n",
      "Epoch:  495|steps:   60|Train Avg Loss: 0.0745 |Test Loss: 1.5071|lr = 0.00474\n",
      "Epoch:  496|steps:   30|Train Avg Loss: 0.0419 |Test Loss: 1.5785|lr = 0.00474\n",
      "Epoch:  496|steps:   60|Train Avg Loss: 0.0544 |Test Loss: 1.4799|lr = 0.00474\n",
      "Epoch:  497|steps:   30|Train Avg Loss: 0.0496 |Test Loss: 1.4425|lr = 0.00464\n",
      "Epoch:  497|steps:   60|Train Avg Loss: 0.0449 |Test Loss: 1.5133|lr = 0.00464\n",
      "Epoch:  498|steps:   30|Train Avg Loss: 0.0383 |Test Loss: 1.6136|lr = 0.00464\n",
      "Epoch:  498|steps:   60|Train Avg Loss: 0.0583 |Test Loss: 1.5252|lr = 0.00464\n",
      "Epoch:  499|steps:   30|Train Avg Loss: 0.0463 |Test Loss: 1.4747|lr = 0.00464\n",
      "Epoch:  499|steps:   60|Train Avg Loss: 0.0633 |Test Loss: 1.5053|lr = 0.00464\n",
      "Epoch:  500|steps:   30|Train Avg Loss: 0.0613 |Test Loss: 1.5116|lr = 0.00464\n",
      "Epoch:  500|steps:   60|Train Avg Loss: 0.0760 |Test Loss: 1.4614|lr = 0.00464\n",
      "Epoch:  501|steps:   30|Train Avg Loss: 0.0732 |Test Loss: 1.5650|lr = 0.00464\n",
      "Epoch:  501|steps:   60|Train Avg Loss: 0.0865 |Test Loss: 1.5341|lr = 0.00464\n",
      "Epoch:  502|steps:   30|Train Avg Loss: 0.0774 |Test Loss: 1.4227|lr = 0.00464\n",
      "Epoch:  502|steps:   60|Train Avg Loss: 0.0715 |Test Loss: 1.5567|lr = 0.00464\n",
      "Epoch:  503|steps:   30|Train Avg Loss: 0.0780 |Test Loss: 1.5801|lr = 0.00464\n",
      "Epoch:  503|steps:   60|Train Avg Loss: 0.0818 |Test Loss: 1.6622|lr = 0.00464\n",
      "Epoch:  504|steps:   30|Train Avg Loss: 0.0696 |Test Loss: 1.5784|lr = 0.00464\n",
      "Epoch:  504|steps:   60|Train Avg Loss: 0.0759 |Test Loss: 1.6530|lr = 0.00464\n",
      "Epoch:  505|steps:   30|Train Avg Loss: 0.0711 |Test Loss: 1.5495|lr = 0.00464\n",
      "Epoch:  505|steps:   60|Train Avg Loss: 0.0617 |Test Loss: 1.5494|lr = 0.00464\n",
      "Epoch:  506|steps:   30|Train Avg Loss: 0.0523 |Test Loss: 1.5544|lr = 0.00464\n",
      "Epoch:  506|steps:   60|Train Avg Loss: 0.0839 |Test Loss: 1.5895|lr = 0.00464\n",
      "Epoch:  507|steps:   30|Train Avg Loss: 0.0750 |Test Loss: 1.5402|lr = 0.00464\n",
      "Epoch:  507|steps:   60|Train Avg Loss: 0.0621 |Test Loss: 1.6579|lr = 0.00464\n",
      "Epoch:  508|steps:   30|Train Avg Loss: 0.1051 |Test Loss: 1.6704|lr = 0.00455\n",
      "Epoch:  508|steps:   60|Train Avg Loss: 0.0723 |Test Loss: 1.6635|lr = 0.00455\n",
      "Epoch:  509|steps:   30|Train Avg Loss: 0.0698 |Test Loss: 1.5887|lr = 0.00455\n",
      "Epoch:  509|steps:   60|Train Avg Loss: 0.1189 |Test Loss: 1.5216|lr = 0.00455\n",
      "Epoch:  510|steps:   30|Train Avg Loss: 0.1352 |Test Loss: 1.5934|lr = 0.00455\n",
      "Epoch:  510|steps:   60|Train Avg Loss: 0.0983 |Test Loss: 1.5377|lr = 0.00455\n",
      "Epoch:  511|steps:   30|Train Avg Loss: 0.0855 |Test Loss: 1.5065|lr = 0.00455\n",
      "Epoch:  511|steps:   60|Train Avg Loss: 0.0857 |Test Loss: 1.5852|lr = 0.00455\n",
      "Epoch:  512|steps:   30|Train Avg Loss: 0.0479 |Test Loss: 1.5597|lr = 0.00455\n",
      "Epoch:  512|steps:   60|Train Avg Loss: 0.0653 |Test Loss: 1.4856|lr = 0.00455\n",
      "Epoch:  513|steps:   30|Train Avg Loss: 0.0548 |Test Loss: 1.4884|lr = 0.00455\n",
      "Epoch:  513|steps:   60|Train Avg Loss: 0.0528 |Test Loss: 1.5441|lr = 0.00455\n",
      "Epoch:  514|steps:   30|Train Avg Loss: 0.0435 |Test Loss: 1.5540|lr = 0.00455\n",
      "Epoch:  514|steps:   60|Train Avg Loss: 0.0549 |Test Loss: 1.5517|lr = 0.00455\n",
      "Epoch:  515|steps:   30|Train Avg Loss: 0.0615 |Test Loss: 1.5653|lr = 0.00455\n",
      "Epoch:  515|steps:   60|Train Avg Loss: 0.0578 |Test Loss: 1.5179|lr = 0.00455\n",
      "Epoch:  516|steps:   30|Train Avg Loss: 0.0550 |Test Loss: 1.6394|lr = 0.00455\n",
      "Epoch:  516|steps:   60|Train Avg Loss: 0.0470 |Test Loss: 1.6098|lr = 0.00455\n",
      "Epoch:  517|steps:   30|Train Avg Loss: 0.0487 |Test Loss: 1.5346|lr = 0.00455\n",
      "Epoch:  517|steps:   60|Train Avg Loss: 0.0532 |Test Loss: 1.6493|lr = 0.00455\n",
      "Epoch:  518|steps:   30|Train Avg Loss: 0.0445 |Test Loss: 1.5901|lr = 0.00455\n",
      "Epoch:  518|steps:   60|Train Avg Loss: 0.0422 |Test Loss: 1.6286|lr = 0.00455\n",
      "Epoch:  519|steps:   30|Train Avg Loss: 0.0524 |Test Loss: 1.5126|lr = 0.00446\n",
      "Epoch:  519|steps:   60|Train Avg Loss: 0.0491 |Test Loss: 1.5724|lr = 0.00446\n",
      "Epoch:  520|steps:   30|Train Avg Loss: 0.0486 |Test Loss: 1.5392|lr = 0.00446\n",
      "Epoch:  520|steps:   60|Train Avg Loss: 0.0623 |Test Loss: 1.5271|lr = 0.00446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  521|steps:   30|Train Avg Loss: 0.0744 |Test Loss: 1.5273|lr = 0.00446\n",
      "Epoch:  521|steps:   60|Train Avg Loss: 0.0843 |Test Loss: 1.5004|lr = 0.00446\n",
      "Epoch:  522|steps:   30|Train Avg Loss: 0.0579 |Test Loss: 1.4580|lr = 0.00446\n",
      "Epoch:  522|steps:   60|Train Avg Loss: 0.0698 |Test Loss: 1.4791|lr = 0.00446\n",
      "Epoch:  523|steps:   30|Train Avg Loss: 0.0440 |Test Loss: 1.5771|lr = 0.00446\n",
      "Epoch:  523|steps:   60|Train Avg Loss: 0.0775 |Test Loss: 1.5576|lr = 0.00446\n",
      "Epoch:  524|steps:   30|Train Avg Loss: 0.0864 |Test Loss: 1.5050|lr = 0.00446\n",
      "Epoch:  524|steps:   60|Train Avg Loss: 0.0698 |Test Loss: 1.4965|lr = 0.00446\n",
      "Epoch:  525|steps:   30|Train Avg Loss: 0.0801 |Test Loss: 1.4846|lr = 0.00446\n",
      "Epoch:  525|steps:   60|Train Avg Loss: 0.1007 |Test Loss: 1.5197|lr = 0.00446\n",
      "Epoch:  526|steps:   30|Train Avg Loss: 0.0757 |Test Loss: 1.6493|lr = 0.00446\n",
      "Epoch:  526|steps:   60|Train Avg Loss: 0.0901 |Test Loss: 1.4570|lr = 0.00446\n",
      "Epoch:  527|steps:   30|Train Avg Loss: 0.0699 |Test Loss: 1.4173|lr = 0.00446\n",
      "Epoch:  527|steps:   60|Train Avg Loss: 0.0721 |Test Loss: 1.4369|lr = 0.00446\n",
      "Epoch:  528|steps:   30|Train Avg Loss: 0.0561 |Test Loss: 1.5324|lr = 0.00446\n",
      "Epoch:  528|steps:   60|Train Avg Loss: 0.0501 |Test Loss: 1.5137|lr = 0.00446\n",
      "Epoch:  529|steps:   30|Train Avg Loss: 0.0520 |Test Loss: 1.4981|lr = 0.00446\n",
      "Epoch:  529|steps:   60|Train Avg Loss: 0.0507 |Test Loss: 1.4901|lr = 0.00446\n",
      "Epoch:  530|steps:   30|Train Avg Loss: 0.0628 |Test Loss: 1.5029|lr = 0.00437\n",
      "Epoch:  530|steps:   60|Train Avg Loss: 0.0632 |Test Loss: 1.5460|lr = 0.00437\n",
      "Epoch:  531|steps:   30|Train Avg Loss: 0.0577 |Test Loss: 1.5216|lr = 0.00437\n",
      "Epoch:  531|steps:   60|Train Avg Loss: 0.0593 |Test Loss: 1.4908|lr = 0.00437\n",
      "Epoch:  532|steps:   30|Train Avg Loss: 0.0622 |Test Loss: 1.4968|lr = 0.00437\n",
      "Epoch:  532|steps:   60|Train Avg Loss: 0.0544 |Test Loss: 1.4231|lr = 0.00437\n",
      "Epoch:  533|steps:   30|Train Avg Loss: 0.0408 |Test Loss: 1.4835|lr = 0.00437\n",
      "Epoch:  533|steps:   60|Train Avg Loss: 0.0530 |Test Loss: 1.4460|lr = 0.00437\n",
      "Epoch:  534|steps:   30|Train Avg Loss: 0.0458 |Test Loss: 1.4631|lr = 0.00437\n",
      "Epoch:  534|steps:   60|Train Avg Loss: 0.0393 |Test Loss: 1.4696|lr = 0.00437\n",
      "Epoch:  535|steps:   30|Train Avg Loss: 0.0390 |Test Loss: 1.4891|lr = 0.00437\n",
      "Epoch:  535|steps:   60|Train Avg Loss: 0.0398 |Test Loss: 1.4805|lr = 0.00437\n",
      "Epoch:  536|steps:   30|Train Avg Loss: 0.0392 |Test Loss: 1.5903|lr = 0.00437\n",
      "Epoch:  536|steps:   60|Train Avg Loss: 0.0400 |Test Loss: 1.5294|lr = 0.00437\n",
      "Epoch:  537|steps:   30|Train Avg Loss: 0.0324 |Test Loss: 1.5120|lr = 0.00437\n",
      "Epoch:  537|steps:   60|Train Avg Loss: 0.0487 |Test Loss: 1.4084|lr = 0.00437\n",
      "Epoch:  538|steps:   30|Train Avg Loss: 0.0331 |Test Loss: 1.5533|lr = 0.00437\n",
      "Epoch:  538|steps:   60|Train Avg Loss: 0.0337 |Test Loss: 1.4897|lr = 0.00437\n",
      "Epoch:  539|steps:   30|Train Avg Loss: 0.0367 |Test Loss: 1.4398|lr = 0.00437\n",
      "Epoch:  539|steps:   60|Train Avg Loss: 0.0412 |Test Loss: 1.4827|lr = 0.00437\n",
      "Epoch:  540|steps:   30|Train Avg Loss: 0.0423 |Test Loss: 1.4456|lr = 0.00437\n",
      "Epoch:  540|steps:   60|Train Avg Loss: 0.0294 |Test Loss: 1.4730|lr = 0.00437\n",
      "Epoch:  541|steps:   30|Train Avg Loss: 0.0357 |Test Loss: 1.5355|lr = 0.00428\n",
      "Epoch:  541|steps:   60|Train Avg Loss: 0.0463 |Test Loss: 1.5203|lr = 0.00428\n",
      "Epoch:  542|steps:   30|Train Avg Loss: 0.0454 |Test Loss: 1.4925|lr = 0.00428\n",
      "Epoch:  542|steps:   60|Train Avg Loss: 0.0520 |Test Loss: 1.4689|lr = 0.00428\n",
      "Epoch:  543|steps:   30|Train Avg Loss: 0.0533 |Test Loss: 1.4822|lr = 0.00428\n",
      "Epoch:  543|steps:   60|Train Avg Loss: 0.0519 |Test Loss: 1.4020|lr = 0.00428\n",
      "Epoch:  544|steps:   30|Train Avg Loss: 0.0587 |Test Loss: 1.5120|lr = 0.00428\n",
      "Epoch:  544|steps:   60|Train Avg Loss: 0.0556 |Test Loss: 1.3852|lr = 0.00428\n",
      "Epoch:  545|steps:   30|Train Avg Loss: 0.0523 |Test Loss: 1.3815|lr = 0.00428\n",
      "Epoch:  545|steps:   60|Train Avg Loss: 0.0667 |Test Loss: 1.3922|lr = 0.00428\n",
      "Epoch:  546|steps:   30|Train Avg Loss: 0.0640 |Test Loss: 1.4535|lr = 0.00428\n",
      "Epoch:  546|steps:   60|Train Avg Loss: 0.0738 |Test Loss: 1.4927|lr = 0.00428\n",
      "Epoch:  547|steps:   30|Train Avg Loss: 0.0663 |Test Loss: 1.4755|lr = 0.00428\n",
      "Epoch:  547|steps:   60|Train Avg Loss: 0.0695 |Test Loss: 1.4450|lr = 0.00428\n",
      "Epoch:  548|steps:   30|Train Avg Loss: 0.0771 |Test Loss: 1.3666|lr = 0.00428\n",
      "Epoch:  548|steps:   60|Train Avg Loss: 0.0671 |Test Loss: 1.5451|lr = 0.00428\n",
      "Epoch:  549|steps:   30|Train Avg Loss: 0.0663 |Test Loss: 1.4826|lr = 0.00428\n",
      "Epoch:  549|steps:   60|Train Avg Loss: 0.0699 |Test Loss: 1.6065|lr = 0.00428\n",
      "Epoch:  550|steps:   30|Train Avg Loss: 0.0906 |Test Loss: 1.4950|lr = 0.00428\n",
      "Epoch:  550|steps:   60|Train Avg Loss: 0.0992 |Test Loss: 1.5247|lr = 0.00428\n",
      "Epoch:  551|steps:   30|Train Avg Loss: 0.0914 |Test Loss: 1.4274|lr = 0.00428\n",
      "Epoch:  551|steps:   60|Train Avg Loss: 0.0888 |Test Loss: 1.3945|lr = 0.00428\n",
      "Epoch:  552|steps:   30|Train Avg Loss: 0.0851 |Test Loss: 1.5485|lr = 0.00419\n",
      "Epoch:  552|steps:   60|Train Avg Loss: 0.0659 |Test Loss: 1.5059|lr = 0.00419\n",
      "Epoch:  553|steps:   30|Train Avg Loss: 0.0631 |Test Loss: 1.4570|lr = 0.00419\n",
      "Epoch:  553|steps:   60|Train Avg Loss: 0.0624 |Test Loss: 1.4357|lr = 0.00419\n",
      "Epoch:  554|steps:   30|Train Avg Loss: 0.0626 |Test Loss: 1.4500|lr = 0.00419\n",
      "Epoch:  554|steps:   60|Train Avg Loss: 0.0581 |Test Loss: 1.4562|lr = 0.00419\n",
      "Epoch:  555|steps:   30|Train Avg Loss: 0.0957 |Test Loss: 1.4891|lr = 0.00419\n",
      "Epoch:  555|steps:   60|Train Avg Loss: 0.0757 |Test Loss: 1.4928|lr = 0.00419\n",
      "Epoch:  556|steps:   30|Train Avg Loss: 0.0465 |Test Loss: 1.4759|lr = 0.00419\n",
      "Epoch:  556|steps:   60|Train Avg Loss: 0.0642 |Test Loss: 1.5283|lr = 0.00419\n",
      "Epoch:  557|steps:   30|Train Avg Loss: 0.0634 |Test Loss: 1.5197|lr = 0.00419\n",
      "Epoch:  557|steps:   60|Train Avg Loss: 0.0429 |Test Loss: 1.5080|lr = 0.00419\n",
      "Epoch:  558|steps:   30|Train Avg Loss: 0.0396 |Test Loss: 1.5041|lr = 0.00419\n",
      "Epoch:  558|steps:   60|Train Avg Loss: 0.0416 |Test Loss: 1.4915|lr = 0.00419\n",
      "Epoch:  559|steps:   30|Train Avg Loss: 0.0379 |Test Loss: 1.4738|lr = 0.00419\n",
      "Epoch:  559|steps:   60|Train Avg Loss: 0.0398 |Test Loss: 1.5402|lr = 0.00419\n",
      "Epoch:  560|steps:   30|Train Avg Loss: 0.0430 |Test Loss: 1.4998|lr = 0.00419\n",
      "Epoch:  560|steps:   60|Train Avg Loss: 0.0658 |Test Loss: 1.4634|lr = 0.00419\n",
      "Epoch:  561|steps:   30|Train Avg Loss: 0.0604 |Test Loss: 1.4701|lr = 0.00419\n",
      "Epoch:  561|steps:   60|Train Avg Loss: 0.0762 |Test Loss: 1.5125|lr = 0.00419\n",
      "Epoch:  562|steps:   30|Train Avg Loss: 0.1051 |Test Loss: 1.4957|lr = 0.00419\n",
      "Epoch:  562|steps:   60|Train Avg Loss: 0.0822 |Test Loss: 1.5401|lr = 0.00419\n",
      "Epoch:  563|steps:   30|Train Avg Loss: 0.0607 |Test Loss: 1.4713|lr = 0.00411\n",
      "Epoch:  563|steps:   60|Train Avg Loss: 0.0758 |Test Loss: 1.4372|lr = 0.00411\n",
      "Epoch:  564|steps:   30|Train Avg Loss: 0.0692 |Test Loss: 1.5262|lr = 0.00411\n",
      "Epoch:  564|steps:   60|Train Avg Loss: 0.0576 |Test Loss: 1.5606|lr = 0.00411\n",
      "Epoch:  565|steps:   30|Train Avg Loss: 0.0630 |Test Loss: 1.5825|lr = 0.00411\n",
      "Epoch:  565|steps:   60|Train Avg Loss: 0.0849 |Test Loss: 1.5185|lr = 0.00411\n",
      "Epoch:  566|steps:   30|Train Avg Loss: 0.0678 |Test Loss: 1.5592|lr = 0.00411\n",
      "Epoch:  566|steps:   60|Train Avg Loss: 0.0501 |Test Loss: 1.4716|lr = 0.00411\n",
      "Epoch:  567|steps:   30|Train Avg Loss: 0.0522 |Test Loss: 1.4786|lr = 0.00411\n",
      "Epoch:  567|steps:   60|Train Avg Loss: 0.0420 |Test Loss: 1.4964|lr = 0.00411\n",
      "Epoch:  568|steps:   30|Train Avg Loss: 0.0376 |Test Loss: 1.4784|lr = 0.00411\n",
      "Epoch:  568|steps:   60|Train Avg Loss: 0.0394 |Test Loss: 1.4365|lr = 0.00411\n",
      "Epoch:  569|steps:   30|Train Avg Loss: 0.0401 |Test Loss: 1.5234|lr = 0.00411\n",
      "Epoch:  569|steps:   60|Train Avg Loss: 0.0424 |Test Loss: 1.3846|lr = 0.00411\n",
      "Epoch:  570|steps:   30|Train Avg Loss: 0.0327 |Test Loss: 1.5283|lr = 0.00411\n",
      "Epoch:  570|steps:   60|Train Avg Loss: 0.0383 |Test Loss: 1.4083|lr = 0.00411\n",
      "Epoch:  571|steps:   30|Train Avg Loss: 0.0402 |Test Loss: 1.4695|lr = 0.00411\n",
      "Epoch:  571|steps:   60|Train Avg Loss: 0.0482 |Test Loss: 1.4329|lr = 0.00411\n",
      "Epoch:  572|steps:   30|Train Avg Loss: 0.0291 |Test Loss: 1.5370|lr = 0.00411\n",
      "Epoch:  572|steps:   60|Train Avg Loss: 0.0463 |Test Loss: 1.5375|lr = 0.00411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  573|steps:   30|Train Avg Loss: 0.0299 |Test Loss: 1.4254|lr = 0.00411\n",
      "Epoch:  573|steps:   60|Train Avg Loss: 0.0476 |Test Loss: 1.5004|lr = 0.00411\n",
      "Epoch:  574|steps:   30|Train Avg Loss: 0.0470 |Test Loss: 1.4252|lr = 0.00403\n",
      "Epoch:  574|steps:   60|Train Avg Loss: 0.0391 |Test Loss: 1.4384|lr = 0.00403\n",
      "Epoch:  575|steps:   30|Train Avg Loss: 0.0371 |Test Loss: 1.4680|lr = 0.00403\n",
      "Epoch:  575|steps:   60|Train Avg Loss: 0.0521 |Test Loss: 1.5420|lr = 0.00403\n",
      "Epoch:  576|steps:   30|Train Avg Loss: 0.0867 |Test Loss: 1.6301|lr = 0.00403\n",
      "Epoch:  576|steps:   60|Train Avg Loss: 0.0523 |Test Loss: 1.5030|lr = 0.00403\n",
      "Epoch:  577|steps:   30|Train Avg Loss: 0.0720 |Test Loss: 1.5310|lr = 0.00403\n",
      "Epoch:  577|steps:   60|Train Avg Loss: 0.0597 |Test Loss: 1.4950|lr = 0.00403\n",
      "Epoch:  578|steps:   30|Train Avg Loss: 0.0562 |Test Loss: 1.4990|lr = 0.00403\n",
      "Epoch:  578|steps:   60|Train Avg Loss: 0.0496 |Test Loss: 1.4701|lr = 0.00403\n",
      "Epoch:  579|steps:   30|Train Avg Loss: 0.0494 |Test Loss: 1.5168|lr = 0.00403\n",
      "Epoch:  579|steps:   60|Train Avg Loss: 0.0470 |Test Loss: 1.4859|lr = 0.00403\n",
      "Epoch:  580|steps:   30|Train Avg Loss: 0.0399 |Test Loss: 1.4767|lr = 0.00403\n",
      "Epoch:  580|steps:   60|Train Avg Loss: 0.0447 |Test Loss: 1.5309|lr = 0.00403\n",
      "Epoch:  581|steps:   30|Train Avg Loss: 0.0350 |Test Loss: 1.5257|lr = 0.00403\n",
      "Epoch:  581|steps:   60|Train Avg Loss: 0.0331 |Test Loss: 1.4987|lr = 0.00403\n",
      "Epoch:  582|steps:   30|Train Avg Loss: 0.0364 |Test Loss: 1.4879|lr = 0.00403\n",
      "Epoch:  582|steps:   60|Train Avg Loss: 0.0457 |Test Loss: 1.5073|lr = 0.00403\n",
      "Epoch:  583|steps:   30|Train Avg Loss: 0.0366 |Test Loss: 1.5184|lr = 0.00403\n",
      "Epoch:  583|steps:   60|Train Avg Loss: 0.0381 |Test Loss: 1.5709|lr = 0.00403\n",
      "Epoch:  584|steps:   30|Train Avg Loss: 0.0345 |Test Loss: 1.5129|lr = 0.00403\n",
      "Epoch:  584|steps:   60|Train Avg Loss: 0.0362 |Test Loss: 1.5013|lr = 0.00403\n",
      "Epoch:  585|steps:   30|Train Avg Loss: 0.0462 |Test Loss: 1.4713|lr = 0.00395\n",
      "Epoch:  585|steps:   60|Train Avg Loss: 0.0310 |Test Loss: 1.4472|lr = 0.00395\n",
      "Epoch:  586|steps:   30|Train Avg Loss: 0.0337 |Test Loss: 1.4634|lr = 0.00395\n",
      "Epoch:  586|steps:   60|Train Avg Loss: 0.0493 |Test Loss: 1.4511|lr = 0.00395\n",
      "Epoch:  587|steps:   30|Train Avg Loss: 0.0344 |Test Loss: 1.4425|lr = 0.00395\n",
      "Epoch:  587|steps:   60|Train Avg Loss: 0.0390 |Test Loss: 1.4975|lr = 0.00395\n",
      "Epoch:  588|steps:   30|Train Avg Loss: 0.0394 |Test Loss: 1.4509|lr = 0.00395\n",
      "Epoch:  588|steps:   60|Train Avg Loss: 0.0332 |Test Loss: 1.4511|lr = 0.00395\n",
      "Epoch:  589|steps:   30|Train Avg Loss: 0.0383 |Test Loss: 1.4635|lr = 0.00395\n",
      "Epoch:  589|steps:   60|Train Avg Loss: 0.0423 |Test Loss: 1.4990|lr = 0.00395\n",
      "Epoch:  590|steps:   30|Train Avg Loss: 0.0668 |Test Loss: 1.6755|lr = 0.00395\n",
      "Epoch:  590|steps:   60|Train Avg Loss: 0.1634 |Test Loss: 1.4285|lr = 0.00395\n",
      "Epoch:  591|steps:   30|Train Avg Loss: 0.0977 |Test Loss: 1.5140|lr = 0.00395\n",
      "Epoch:  591|steps:   60|Train Avg Loss: 0.1778 |Test Loss: 1.5175|lr = 0.00395\n",
      "Epoch:  592|steps:   30|Train Avg Loss: 0.1483 |Test Loss: 1.4032|lr = 0.00395\n",
      "Epoch:  592|steps:   60|Train Avg Loss: 0.1138 |Test Loss: 1.4051|lr = 0.00395\n",
      "Epoch:  593|steps:   30|Train Avg Loss: 0.0662 |Test Loss: 1.3968|lr = 0.00395\n",
      "Epoch:  593|steps:   60|Train Avg Loss: 0.0846 |Test Loss: 1.4554|lr = 0.00395\n",
      "Epoch:  594|steps:   30|Train Avg Loss: 0.0707 |Test Loss: 1.4121|lr = 0.00395\n",
      "Epoch:  594|steps:   60|Train Avg Loss: 0.0665 |Test Loss: 1.4298|lr = 0.00395\n",
      "Epoch:  595|steps:   30|Train Avg Loss: 0.0688 |Test Loss: 1.5053|lr = 0.00395\n",
      "Epoch:  595|steps:   60|Train Avg Loss: 0.0728 |Test Loss: 1.4392|lr = 0.00395\n",
      "Epoch:  596|steps:   30|Train Avg Loss: 0.0548 |Test Loss: 1.3805|lr = 0.00387\n",
      "Epoch:  596|steps:   60|Train Avg Loss: 0.0545 |Test Loss: 1.4368|lr = 0.00387\n",
      "Epoch:  597|steps:   30|Train Avg Loss: 0.0337 |Test Loss: 1.4027|lr = 0.00387\n",
      "Epoch:  597|steps:   60|Train Avg Loss: 0.0377 |Test Loss: 1.4384|lr = 0.00387\n",
      "Epoch:  598|steps:   30|Train Avg Loss: 0.0339 |Test Loss: 1.4644|lr = 0.00387\n",
      "Epoch:  598|steps:   60|Train Avg Loss: 0.0316 |Test Loss: 1.4491|lr = 0.00387\n",
      "Epoch:  599|steps:   30|Train Avg Loss: 0.0303 |Test Loss: 1.4563|lr = 0.00387\n",
      "Epoch:  599|steps:   60|Train Avg Loss: 0.0435 |Test Loss: 1.5139|lr = 0.00387\n",
      "Epoch:  600|steps:   30|Train Avg Loss: 0.0259 |Test Loss: 1.4534|lr = 0.00387\n",
      "Epoch:  600|steps:   60|Train Avg Loss: 0.0367 |Test Loss: 1.5165|lr = 0.00387\n",
      "Epoch:  601|steps:   30|Train Avg Loss: 0.0462 |Test Loss: 1.4455|lr = 0.00387\n",
      "Epoch:  601|steps:   60|Train Avg Loss: 0.0354 |Test Loss: 1.4876|lr = 0.00387\n",
      "Epoch:  602|steps:   30|Train Avg Loss: 0.0311 |Test Loss: 1.4373|lr = 0.00387\n",
      "Epoch:  602|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 1.4378|lr = 0.00387\n",
      "Epoch:  603|steps:   30|Train Avg Loss: 0.0281 |Test Loss: 1.4014|lr = 0.00387\n",
      "Epoch:  603|steps:   60|Train Avg Loss: 0.0326 |Test Loss: 1.4633|lr = 0.00387\n",
      "Epoch:  604|steps:   30|Train Avg Loss: 0.0331 |Test Loss: 1.4023|lr = 0.00387\n",
      "Epoch:  604|steps:   60|Train Avg Loss: 0.0416 |Test Loss: 1.3992|lr = 0.00387\n",
      "Epoch:  605|steps:   30|Train Avg Loss: 0.0429 |Test Loss: 1.4789|lr = 0.00387\n",
      "Epoch:  605|steps:   60|Train Avg Loss: 0.0438 |Test Loss: 1.4492|lr = 0.00387\n",
      "Epoch:  606|steps:   30|Train Avg Loss: 0.0379 |Test Loss: 1.3850|lr = 0.00387\n",
      "Epoch:  606|steps:   60|Train Avg Loss: 0.0348 |Test Loss: 1.4587|lr = 0.00387\n",
      "Epoch:  607|steps:   30|Train Avg Loss: 0.0367 |Test Loss: 1.5658|lr = 0.00379\n",
      "Epoch:  607|steps:   60|Train Avg Loss: 0.0252 |Test Loss: 1.4983|lr = 0.00379\n",
      "Epoch:  608|steps:   30|Train Avg Loss: 0.0388 |Test Loss: 1.5119|lr = 0.00379\n",
      "Epoch:  608|steps:   60|Train Avg Loss: 0.0382 |Test Loss: 1.5154|lr = 0.00379\n",
      "Epoch:  609|steps:   30|Train Avg Loss: 0.0363 |Test Loss: 1.5062|lr = 0.00379\n",
      "Epoch:  609|steps:   60|Train Avg Loss: 0.0323 |Test Loss: 1.4716|lr = 0.00379\n",
      "Epoch:  610|steps:   30|Train Avg Loss: 0.0251 |Test Loss: 1.4236|lr = 0.00379\n",
      "Epoch:  610|steps:   60|Train Avg Loss: 0.0330 |Test Loss: 1.4752|lr = 0.00379\n",
      "Epoch:  611|steps:   30|Train Avg Loss: 0.0306 |Test Loss: 1.4150|lr = 0.00379\n",
      "Epoch:  611|steps:   60|Train Avg Loss: 0.0394 |Test Loss: 1.4369|lr = 0.00379\n",
      "Epoch:  612|steps:   30|Train Avg Loss: 0.0528 |Test Loss: 1.5585|lr = 0.00379\n",
      "Epoch:  612|steps:   60|Train Avg Loss: 0.0894 |Test Loss: 1.4918|lr = 0.00379\n",
      "Epoch:  613|steps:   30|Train Avg Loss: 0.0935 |Test Loss: 1.4488|lr = 0.00379\n",
      "Epoch:  613|steps:   60|Train Avg Loss: 0.0645 |Test Loss: 1.5616|lr = 0.00379\n",
      "Epoch:  614|steps:   30|Train Avg Loss: 0.0771 |Test Loss: 1.5274|lr = 0.00379\n",
      "Epoch:  614|steps:   60|Train Avg Loss: 0.0818 |Test Loss: 1.5118|lr = 0.00379\n",
      "Epoch:  615|steps:   30|Train Avg Loss: 0.0683 |Test Loss: 1.4938|lr = 0.00379\n",
      "Epoch:  615|steps:   60|Train Avg Loss: 0.0657 |Test Loss: 1.5612|lr = 0.00379\n",
      "Epoch:  616|steps:   30|Train Avg Loss: 0.0807 |Test Loss: 1.4027|lr = 0.00379\n",
      "Epoch:  616|steps:   60|Train Avg Loss: 0.1013 |Test Loss: 1.5182|lr = 0.00379\n",
      "Epoch:  617|steps:   30|Train Avg Loss: 0.0989 |Test Loss: 1.3218|lr = 0.00379\n",
      "Epoch:  617|steps:   60|Train Avg Loss: 0.1276 |Test Loss: 1.4623|lr = 0.00379\n",
      "Epoch:  618|steps:   30|Train Avg Loss: 0.0941 |Test Loss: 1.5337|lr = 0.00372\n",
      "Epoch:  618|steps:   60|Train Avg Loss: 0.0743 |Test Loss: 1.4267|lr = 0.00372\n",
      "Epoch:  619|steps:   30|Train Avg Loss: 0.0568 |Test Loss: 1.3553|lr = 0.00372\n",
      "Epoch:  619|steps:   60|Train Avg Loss: 0.0477 |Test Loss: 1.4664|lr = 0.00372\n",
      "Epoch:  620|steps:   30|Train Avg Loss: 0.0379 |Test Loss: 1.3628|lr = 0.00372\n",
      "Epoch:  620|steps:   60|Train Avg Loss: 0.0275 |Test Loss: 1.4153|lr = 0.00372\n",
      "Epoch:  621|steps:   30|Train Avg Loss: 0.0371 |Test Loss: 1.3591|lr = 0.00372\n",
      "Epoch:  621|steps:   60|Train Avg Loss: 0.0304 |Test Loss: 1.4375|lr = 0.00372\n",
      "Epoch:  622|steps:   30|Train Avg Loss: 0.0315 |Test Loss: 1.3932|lr = 0.00372\n",
      "Epoch:  622|steps:   60|Train Avg Loss: 0.0359 |Test Loss: 1.4293|lr = 0.00372\n",
      "Epoch:  623|steps:   30|Train Avg Loss: 0.0290 |Test Loss: 1.4325|lr = 0.00372\n",
      "Epoch:  623|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 1.4643|lr = 0.00372\n",
      "Epoch:  624|steps:   30|Train Avg Loss: 0.0271 |Test Loss: 1.4476|lr = 0.00372\n",
      "Epoch:  624|steps:   60|Train Avg Loss: 0.0317 |Test Loss: 1.4804|lr = 0.00372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  625|steps:   30|Train Avg Loss: 0.0268 |Test Loss: 1.4586|lr = 0.00372\n",
      "Epoch:  625|steps:   60|Train Avg Loss: 0.0294 |Test Loss: 1.4641|lr = 0.00372\n",
      "Epoch:  626|steps:   30|Train Avg Loss: 0.0312 |Test Loss: 1.4849|lr = 0.00372\n",
      "Epoch:  626|steps:   60|Train Avg Loss: 0.0310 |Test Loss: 1.4832|lr = 0.00372\n",
      "Epoch:  627|steps:   30|Train Avg Loss: 0.0269 |Test Loss: 1.4639|lr = 0.00372\n",
      "Epoch:  627|steps:   60|Train Avg Loss: 0.0387 |Test Loss: 1.4731|lr = 0.00372\n",
      "Epoch:  628|steps:   30|Train Avg Loss: 0.0295 |Test Loss: 1.4592|lr = 0.00372\n",
      "Epoch:  628|steps:   60|Train Avg Loss: 0.0301 |Test Loss: 1.4925|lr = 0.00372\n",
      "Epoch:  629|steps:   30|Train Avg Loss: 0.0303 |Test Loss: 1.4297|lr = 0.00364\n",
      "Epoch:  629|steps:   60|Train Avg Loss: 0.0307 |Test Loss: 1.5190|lr = 0.00364\n",
      "Epoch:  630|steps:   30|Train Avg Loss: 0.0310 |Test Loss: 1.5793|lr = 0.00364\n",
      "Epoch:  630|steps:   60|Train Avg Loss: 0.0323 |Test Loss: 1.5157|lr = 0.00364\n",
      "Epoch:  631|steps:   30|Train Avg Loss: 0.0322 |Test Loss: 1.4754|lr = 0.00364\n",
      "Epoch:  631|steps:   60|Train Avg Loss: 0.0352 |Test Loss: 1.4127|lr = 0.00364\n",
      "Epoch:  632|steps:   30|Train Avg Loss: 0.0326 |Test Loss: 1.4714|lr = 0.00364\n",
      "Epoch:  632|steps:   60|Train Avg Loss: 0.0306 |Test Loss: 1.4664|lr = 0.00364\n",
      "Epoch:  633|steps:   30|Train Avg Loss: 0.0336 |Test Loss: 1.3761|lr = 0.00364\n",
      "Epoch:  633|steps:   60|Train Avg Loss: 0.0332 |Test Loss: 1.4878|lr = 0.00364\n",
      "Epoch:  634|steps:   30|Train Avg Loss: 0.0355 |Test Loss: 1.4946|lr = 0.00364\n",
      "Epoch:  634|steps:   60|Train Avg Loss: 0.0476 |Test Loss: 1.4342|lr = 0.00364\n",
      "Epoch:  635|steps:   30|Train Avg Loss: 0.0463 |Test Loss: 1.5114|lr = 0.00364\n",
      "Epoch:  635|steps:   60|Train Avg Loss: 0.0473 |Test Loss: 1.4350|lr = 0.00364\n",
      "Epoch:  636|steps:   30|Train Avg Loss: 0.0480 |Test Loss: 1.4576|lr = 0.00364\n",
      "Epoch:  636|steps:   60|Train Avg Loss: 0.0571 |Test Loss: 1.5337|lr = 0.00364\n",
      "Epoch:  637|steps:   30|Train Avg Loss: 0.0902 |Test Loss: 1.4784|lr = 0.00364\n",
      "Epoch:  637|steps:   60|Train Avg Loss: 0.0845 |Test Loss: 1.4607|lr = 0.00364\n",
      "Epoch:  638|steps:   30|Train Avg Loss: 0.0872 |Test Loss: 1.5805|lr = 0.00364\n",
      "Epoch:  638|steps:   60|Train Avg Loss: 0.0917 |Test Loss: 1.5209|lr = 0.00364\n",
      "Epoch:  639|steps:   30|Train Avg Loss: 0.0884 |Test Loss: 1.4979|lr = 0.00364\n",
      "Epoch:  639|steps:   60|Train Avg Loss: 0.0657 |Test Loss: 1.5242|lr = 0.00364\n",
      "Epoch:  640|steps:   30|Train Avg Loss: 0.0647 |Test Loss: 1.4390|lr = 0.00357\n",
      "Epoch:  640|steps:   60|Train Avg Loss: 0.0575 |Test Loss: 1.5311|lr = 0.00357\n",
      "Epoch:  641|steps:   30|Train Avg Loss: 0.0459 |Test Loss: 1.5582|lr = 0.00357\n",
      "Epoch:  641|steps:   60|Train Avg Loss: 0.0471 |Test Loss: 1.5349|lr = 0.00357\n",
      "Epoch:  642|steps:   30|Train Avg Loss: 0.0467 |Test Loss: 1.6039|lr = 0.00357\n",
      "Epoch:  642|steps:   60|Train Avg Loss: 0.0525 |Test Loss: 1.5681|lr = 0.00357\n",
      "Epoch:  643|steps:   30|Train Avg Loss: 0.0408 |Test Loss: 1.5474|lr = 0.00357\n",
      "Epoch:  643|steps:   60|Train Avg Loss: 0.0473 |Test Loss: 1.4850|lr = 0.00357\n",
      "Epoch:  644|steps:   30|Train Avg Loss: 0.0489 |Test Loss: 1.4289|lr = 0.00357\n",
      "Epoch:  644|steps:   60|Train Avg Loss: 0.0418 |Test Loss: 1.4711|lr = 0.00357\n",
      "Epoch:  645|steps:   30|Train Avg Loss: 0.0481 |Test Loss: 1.4999|lr = 0.00357\n",
      "Epoch:  645|steps:   60|Train Avg Loss: 0.0409 |Test Loss: 1.4508|lr = 0.00357\n",
      "Epoch:  646|steps:   30|Train Avg Loss: 0.0456 |Test Loss: 1.4518|lr = 0.00357\n",
      "Epoch:  646|steps:   60|Train Avg Loss: 0.0610 |Test Loss: 1.4515|lr = 0.00357\n",
      "Epoch:  647|steps:   30|Train Avg Loss: 0.0361 |Test Loss: 1.4693|lr = 0.00357\n",
      "Epoch:  647|steps:   60|Train Avg Loss: 0.0540 |Test Loss: 1.4026|lr = 0.00357\n",
      "Epoch:  648|steps:   30|Train Avg Loss: 0.0415 |Test Loss: 1.4121|lr = 0.00357\n",
      "Epoch:  648|steps:   60|Train Avg Loss: 0.0452 |Test Loss: 1.3926|lr = 0.00357\n",
      "Epoch:  649|steps:   30|Train Avg Loss: 0.0350 |Test Loss: 1.4907|lr = 0.00357\n",
      "Epoch:  649|steps:   60|Train Avg Loss: 0.0407 |Test Loss: 1.4194|lr = 0.00357\n",
      "Epoch:  650|steps:   30|Train Avg Loss: 0.0363 |Test Loss: 1.4313|lr = 0.00357\n",
      "Epoch:  650|steps:   60|Train Avg Loss: 0.0464 |Test Loss: 1.4452|lr = 0.00357\n",
      "Epoch:  651|steps:   30|Train Avg Loss: 0.0373 |Test Loss: 1.4181|lr = 0.00350\n",
      "Epoch:  651|steps:   60|Train Avg Loss: 0.0305 |Test Loss: 1.4435|lr = 0.00350\n",
      "Epoch:  652|steps:   30|Train Avg Loss: 0.0333 |Test Loss: 1.3589|lr = 0.00350\n",
      "Epoch:  652|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 1.3885|lr = 0.00350\n",
      "Epoch:  653|steps:   30|Train Avg Loss: 0.0389 |Test Loss: 1.4119|lr = 0.00350\n",
      "Epoch:  653|steps:   60|Train Avg Loss: 0.0340 |Test Loss: 1.4155|lr = 0.00350\n",
      "Epoch:  654|steps:   30|Train Avg Loss: 0.0285 |Test Loss: 1.4086|lr = 0.00350\n",
      "Epoch:  654|steps:   60|Train Avg Loss: 0.0344 |Test Loss: 1.3924|lr = 0.00350\n",
      "Epoch:  655|steps:   30|Train Avg Loss: 0.0261 |Test Loss: 1.3639|lr = 0.00350\n",
      "Epoch:  655|steps:   60|Train Avg Loss: 0.0345 |Test Loss: 1.4557|lr = 0.00350\n",
      "Epoch:  656|steps:   30|Train Avg Loss: 0.0315 |Test Loss: 1.4380|lr = 0.00350\n",
      "Epoch:  656|steps:   60|Train Avg Loss: 0.0541 |Test Loss: 1.5048|lr = 0.00350\n",
      "Epoch:  657|steps:   30|Train Avg Loss: 0.0450 |Test Loss: 1.3850|lr = 0.00350\n",
      "Epoch:  657|steps:   60|Train Avg Loss: 0.0396 |Test Loss: 1.4291|lr = 0.00350\n",
      "Epoch:  658|steps:   30|Train Avg Loss: 0.0387 |Test Loss: 1.3723|lr = 0.00350\n",
      "Epoch:  658|steps:   60|Train Avg Loss: 0.0437 |Test Loss: 1.3902|lr = 0.00350\n",
      "Epoch:  659|steps:   30|Train Avg Loss: 0.0525 |Test Loss: 1.4159|lr = 0.00350\n",
      "Epoch:  659|steps:   60|Train Avg Loss: 0.0928 |Test Loss: 1.4974|lr = 0.00350\n",
      "Epoch:  660|steps:   30|Train Avg Loss: 0.0993 |Test Loss: 1.4875|lr = 0.00350\n",
      "Epoch:  660|steps:   60|Train Avg Loss: 0.0835 |Test Loss: 1.4215|lr = 0.00350\n",
      "Epoch:  661|steps:   30|Train Avg Loss: 0.0750 |Test Loss: 1.5289|lr = 0.00350\n",
      "Epoch:  661|steps:   60|Train Avg Loss: 0.0744 |Test Loss: 1.4836|lr = 0.00350\n",
      "Epoch:  662|steps:   30|Train Avg Loss: 0.0483 |Test Loss: 1.4266|lr = 0.00343\n",
      "Epoch:  662|steps:   60|Train Avg Loss: 0.0444 |Test Loss: 1.5198|lr = 0.00343\n",
      "Epoch:  663|steps:   30|Train Avg Loss: 0.0333 |Test Loss: 1.4284|lr = 0.00343\n",
      "Epoch:  663|steps:   60|Train Avg Loss: 0.0515 |Test Loss: 1.4954|lr = 0.00343\n",
      "Epoch:  664|steps:   30|Train Avg Loss: 0.0514 |Test Loss: 1.4662|lr = 0.00343\n",
      "Epoch:  664|steps:   60|Train Avg Loss: 0.0436 |Test Loss: 1.4969|lr = 0.00343\n",
      "Epoch:  665|steps:   30|Train Avg Loss: 0.0587 |Test Loss: 1.4800|lr = 0.00343\n",
      "Epoch:  665|steps:   60|Train Avg Loss: 0.0442 |Test Loss: 1.4413|lr = 0.00343\n",
      "Epoch:  666|steps:   30|Train Avg Loss: 0.0403 |Test Loss: 1.4936|lr = 0.00343\n",
      "Epoch:  666|steps:   60|Train Avg Loss: 0.0371 |Test Loss: 1.4796|lr = 0.00343\n",
      "Epoch:  667|steps:   30|Train Avg Loss: 0.0284 |Test Loss: 1.4276|lr = 0.00343\n",
      "Epoch:  667|steps:   60|Train Avg Loss: 0.0315 |Test Loss: 1.3891|lr = 0.00343\n",
      "Epoch:  668|steps:   30|Train Avg Loss: 0.0311 |Test Loss: 1.4512|lr = 0.00343\n",
      "Epoch:  668|steps:   60|Train Avg Loss: 0.0310 |Test Loss: 1.4203|lr = 0.00343\n",
      "Epoch:  669|steps:   30|Train Avg Loss: 0.0278 |Test Loss: 1.3686|lr = 0.00343\n",
      "Epoch:  669|steps:   60|Train Avg Loss: 0.0336 |Test Loss: 1.3420|lr = 0.00343\n",
      "Epoch:  670|steps:   30|Train Avg Loss: 0.0257 |Test Loss: 1.4176|lr = 0.00343\n",
      "Epoch:  670|steps:   60|Train Avg Loss: 0.0265 |Test Loss: 1.3594|lr = 0.00343\n",
      "Epoch:  671|steps:   30|Train Avg Loss: 0.0312 |Test Loss: 1.4442|lr = 0.00343\n",
      "Epoch:  671|steps:   60|Train Avg Loss: 0.0381 |Test Loss: 1.3833|lr = 0.00343\n",
      "Epoch:  672|steps:   30|Train Avg Loss: 0.0355 |Test Loss: 1.4771|lr = 0.00343\n",
      "Epoch:  672|steps:   60|Train Avg Loss: 0.0415 |Test Loss: 1.4150|lr = 0.00343\n",
      "Epoch:  673|steps:   30|Train Avg Loss: 0.0289 |Test Loss: 1.4669|lr = 0.00336\n",
      "Epoch:  673|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 1.4278|lr = 0.00336\n",
      "Epoch:  674|steps:   30|Train Avg Loss: 0.0300 |Test Loss: 1.4275|lr = 0.00336\n",
      "Epoch:  674|steps:   60|Train Avg Loss: 0.0319 |Test Loss: 1.4077|lr = 0.00336\n",
      "Epoch:  675|steps:   30|Train Avg Loss: 0.0346 |Test Loss: 1.2616|lr = 0.00336\n",
      "Epoch:  675|steps:   60|Train Avg Loss: 0.0315 |Test Loss: 1.4798|lr = 0.00336\n",
      "Epoch:  676|steps:   30|Train Avg Loss: 0.0317 |Test Loss: 1.4365|lr = 0.00336\n",
      "Epoch:  676|steps:   60|Train Avg Loss: 0.0386 |Test Loss: 1.4694|lr = 0.00336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  677|steps:   30|Train Avg Loss: 0.0340 |Test Loss: 1.5021|lr = 0.00336\n",
      "Epoch:  677|steps:   60|Train Avg Loss: 0.0374 |Test Loss: 1.4093|lr = 0.00336\n",
      "Epoch:  678|steps:   30|Train Avg Loss: 0.0379 |Test Loss: 1.4007|lr = 0.00336\n",
      "Epoch:  678|steps:   60|Train Avg Loss: 0.0292 |Test Loss: 1.4369|lr = 0.00336\n",
      "Epoch:  679|steps:   30|Train Avg Loss: 0.0338 |Test Loss: 1.5040|lr = 0.00336\n",
      "Epoch:  679|steps:   60|Train Avg Loss: 0.0446 |Test Loss: 1.4905|lr = 0.00336\n",
      "Epoch:  680|steps:   30|Train Avg Loss: 0.0652 |Test Loss: 1.4551|lr = 0.00336\n",
      "Epoch:  680|steps:   60|Train Avg Loss: 0.0810 |Test Loss: 1.5445|lr = 0.00336\n",
      "Epoch:  681|steps:   30|Train Avg Loss: 0.0677 |Test Loss: 1.4263|lr = 0.00336\n",
      "Epoch:  681|steps:   60|Train Avg Loss: 0.0697 |Test Loss: 1.3959|lr = 0.00336\n",
      "Epoch:  682|steps:   30|Train Avg Loss: 0.0648 |Test Loss: 1.4878|lr = 0.00336\n",
      "Epoch:  682|steps:   60|Train Avg Loss: 0.0652 |Test Loss: 1.5660|lr = 0.00336\n",
      "Epoch:  683|steps:   30|Train Avg Loss: 0.0533 |Test Loss: 1.4889|lr = 0.00336\n",
      "Epoch:  683|steps:   60|Train Avg Loss: 0.0489 |Test Loss: 1.5608|lr = 0.00336\n",
      "Epoch:  684|steps:   30|Train Avg Loss: 0.0436 |Test Loss: 1.5342|lr = 0.00329\n",
      "Epoch:  684|steps:   60|Train Avg Loss: 0.0520 |Test Loss: 1.4929|lr = 0.00329\n",
      "Epoch:  685|steps:   30|Train Avg Loss: 0.0445 |Test Loss: 1.4574|lr = 0.00329\n",
      "Epoch:  685|steps:   60|Train Avg Loss: 0.0514 |Test Loss: 1.4902|lr = 0.00329\n",
      "Epoch:  686|steps:   30|Train Avg Loss: 0.0417 |Test Loss: 1.4334|lr = 0.00329\n",
      "Epoch:  686|steps:   60|Train Avg Loss: 0.0594 |Test Loss: 1.4563|lr = 0.00329\n",
      "Epoch:  687|steps:   30|Train Avg Loss: 0.0465 |Test Loss: 1.5744|lr = 0.00329\n",
      "Epoch:  687|steps:   60|Train Avg Loss: 0.0414 |Test Loss: 1.3852|lr = 0.00329\n",
      "Epoch:  688|steps:   30|Train Avg Loss: 0.0289 |Test Loss: 1.4401|lr = 0.00329\n",
      "Epoch:  688|steps:   60|Train Avg Loss: 0.0430 |Test Loss: 1.4526|lr = 0.00329\n",
      "Epoch:  689|steps:   30|Train Avg Loss: 0.0349 |Test Loss: 1.4216|lr = 0.00329\n",
      "Epoch:  689|steps:   60|Train Avg Loss: 0.0358 |Test Loss: 1.4488|lr = 0.00329\n",
      "Epoch:  690|steps:   30|Train Avg Loss: 0.0472 |Test Loss: 1.5110|lr = 0.00329\n",
      "Epoch:  690|steps:   60|Train Avg Loss: 0.0388 |Test Loss: 1.4411|lr = 0.00329\n",
      "Epoch:  691|steps:   30|Train Avg Loss: 0.0312 |Test Loss: 1.4886|lr = 0.00329\n",
      "Epoch:  691|steps:   60|Train Avg Loss: 0.0321 |Test Loss: 1.4910|lr = 0.00329\n",
      "Epoch:  692|steps:   30|Train Avg Loss: 0.0285 |Test Loss: 1.5145|lr = 0.00329\n",
      "Epoch:  692|steps:   60|Train Avg Loss: 0.0276 |Test Loss: 1.5174|lr = 0.00329\n",
      "Epoch:  693|steps:   30|Train Avg Loss: 0.0227 |Test Loss: 1.4542|lr = 0.00329\n",
      "Epoch:  693|steps:   60|Train Avg Loss: 0.0328 |Test Loss: 1.4410|lr = 0.00329\n",
      "Epoch:  694|steps:   30|Train Avg Loss: 0.0266 |Test Loss: 1.4493|lr = 0.00329\n",
      "Epoch:  694|steps:   60|Train Avg Loss: 0.0288 |Test Loss: 1.4727|lr = 0.00329\n",
      "Epoch:  695|steps:   30|Train Avg Loss: 0.0363 |Test Loss: 1.4788|lr = 0.00323\n",
      "Epoch:  695|steps:   60|Train Avg Loss: 0.0332 |Test Loss: 1.5759|lr = 0.00323\n",
      "Epoch:  696|steps:   30|Train Avg Loss: 0.0410 |Test Loss: 1.5467|lr = 0.00323\n",
      "Epoch:  696|steps:   60|Train Avg Loss: 0.0447 |Test Loss: 1.5214|lr = 0.00323\n",
      "Epoch:  697|steps:   30|Train Avg Loss: 0.0648 |Test Loss: 1.5297|lr = 0.00323\n",
      "Epoch:  697|steps:   60|Train Avg Loss: 0.0696 |Test Loss: 1.4299|lr = 0.00323\n",
      "Epoch:  698|steps:   30|Train Avg Loss: 0.0792 |Test Loss: 1.5075|lr = 0.00323\n",
      "Epoch:  698|steps:   60|Train Avg Loss: 0.0823 |Test Loss: 1.5187|lr = 0.00323\n",
      "Epoch:  699|steps:   30|Train Avg Loss: 0.0502 |Test Loss: 1.5351|lr = 0.00323\n",
      "Epoch:  699|steps:   60|Train Avg Loss: 0.0556 |Test Loss: 1.4880|lr = 0.00323\n",
      "Epoch:  700|steps:   30|Train Avg Loss: 0.0385 |Test Loss: 1.5435|lr = 0.00323\n",
      "Epoch:  700|steps:   60|Train Avg Loss: 0.0391 |Test Loss: 1.4386|lr = 0.00323\n",
      "Epoch:  701|steps:   30|Train Avg Loss: 0.0349 |Test Loss: 1.4734|lr = 0.00323\n",
      "Epoch:  701|steps:   60|Train Avg Loss: 0.0390 |Test Loss: 1.5073|lr = 0.00323\n",
      "Epoch:  702|steps:   30|Train Avg Loss: 0.0332 |Test Loss: 1.5382|lr = 0.00323\n",
      "Epoch:  702|steps:   60|Train Avg Loss: 0.0293 |Test Loss: 1.4793|lr = 0.00323\n",
      "Epoch:  703|steps:   30|Train Avg Loss: 0.0305 |Test Loss: 1.4918|lr = 0.00323\n",
      "Epoch:  703|steps:   60|Train Avg Loss: 0.0260 |Test Loss: 1.5410|lr = 0.00323\n",
      "Epoch:  704|steps:   30|Train Avg Loss: 0.0232 |Test Loss: 1.5310|lr = 0.00323\n",
      "Epoch:  704|steps:   60|Train Avg Loss: 0.0348 |Test Loss: 1.4855|lr = 0.00323\n",
      "Epoch:  705|steps:   30|Train Avg Loss: 0.0250 |Test Loss: 1.4878|lr = 0.00323\n",
      "Epoch:  705|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 1.4687|lr = 0.00323\n",
      "Epoch:  706|steps:   30|Train Avg Loss: 0.0320 |Test Loss: 1.4190|lr = 0.00316\n",
      "Epoch:  706|steps:   60|Train Avg Loss: 0.0331 |Test Loss: 1.5096|lr = 0.00316\n",
      "Epoch:  707|steps:   30|Train Avg Loss: 0.0198 |Test Loss: 1.4554|lr = 0.00316\n",
      "Epoch:  707|steps:   60|Train Avg Loss: 0.0353 |Test Loss: 1.5205|lr = 0.00316\n",
      "Epoch:  708|steps:   30|Train Avg Loss: 0.0309 |Test Loss: 1.5182|lr = 0.00316\n",
      "Epoch:  708|steps:   60|Train Avg Loss: 0.0394 |Test Loss: 1.4858|lr = 0.00316\n",
      "Epoch:  709|steps:   30|Train Avg Loss: 0.0416 |Test Loss: 1.5332|lr = 0.00316\n",
      "Epoch:  709|steps:   60|Train Avg Loss: 0.0500 |Test Loss: 1.4789|lr = 0.00316\n",
      "Epoch:  710|steps:   30|Train Avg Loss: 0.0482 |Test Loss: 1.5237|lr = 0.00316\n",
      "Epoch:  710|steps:   60|Train Avg Loss: 0.0476 |Test Loss: 1.4711|lr = 0.00316\n",
      "Epoch:  711|steps:   30|Train Avg Loss: 0.0439 |Test Loss: 1.3856|lr = 0.00316\n",
      "Epoch:  711|steps:   60|Train Avg Loss: 0.0424 |Test Loss: 1.4735|lr = 0.00316\n",
      "Epoch:  712|steps:   30|Train Avg Loss: 0.0308 |Test Loss: 1.5063|lr = 0.00316\n",
      "Epoch:  712|steps:   60|Train Avg Loss: 0.0335 |Test Loss: 1.4185|lr = 0.00316\n",
      "Epoch:  713|steps:   30|Train Avg Loss: 0.0323 |Test Loss: 1.4404|lr = 0.00316\n",
      "Epoch:  713|steps:   60|Train Avg Loss: 0.0296 |Test Loss: 1.4371|lr = 0.00316\n",
      "Epoch:  714|steps:   30|Train Avg Loss: 0.0321 |Test Loss: 1.5301|lr = 0.00316\n",
      "Epoch:  714|steps:   60|Train Avg Loss: 0.0502 |Test Loss: 1.5478|lr = 0.00316\n",
      "Epoch:  715|steps:   30|Train Avg Loss: 0.0500 |Test Loss: 1.5336|lr = 0.00316\n",
      "Epoch:  715|steps:   60|Train Avg Loss: 0.0486 |Test Loss: 1.5550|lr = 0.00316\n",
      "Epoch:  716|steps:   30|Train Avg Loss: 0.0472 |Test Loss: 1.5079|lr = 0.00316\n",
      "Epoch:  716|steps:   60|Train Avg Loss: 0.0617 |Test Loss: 1.5562|lr = 0.00316\n",
      "Epoch:  717|steps:   30|Train Avg Loss: 0.0624 |Test Loss: 1.5663|lr = 0.00310\n",
      "Epoch:  717|steps:   60|Train Avg Loss: 0.0706 |Test Loss: 1.5444|lr = 0.00310\n",
      "Epoch:  718|steps:   30|Train Avg Loss: 0.0870 |Test Loss: 1.4291|lr = 0.00310\n",
      "Epoch:  718|steps:   60|Train Avg Loss: 0.0688 |Test Loss: 1.4609|lr = 0.00310\n",
      "Epoch:  719|steps:   30|Train Avg Loss: 0.0622 |Test Loss: 1.5246|lr = 0.00310\n",
      "Epoch:  719|steps:   60|Train Avg Loss: 0.0639 |Test Loss: 1.5165|lr = 0.00310\n",
      "Epoch:  720|steps:   30|Train Avg Loss: 0.0575 |Test Loss: 1.4388|lr = 0.00310\n",
      "Epoch:  720|steps:   60|Train Avg Loss: 0.0508 |Test Loss: 1.4923|lr = 0.00310\n",
      "Epoch:  721|steps:   30|Train Avg Loss: 0.0604 |Test Loss: 1.5473|lr = 0.00310\n",
      "Epoch:  721|steps:   60|Train Avg Loss: 0.0487 |Test Loss: 1.4623|lr = 0.00310\n",
      "Epoch:  722|steps:   30|Train Avg Loss: 0.0385 |Test Loss: 1.5339|lr = 0.00310\n",
      "Epoch:  722|steps:   60|Train Avg Loss: 0.0554 |Test Loss: 1.3648|lr = 0.00310\n",
      "Epoch:  723|steps:   30|Train Avg Loss: 0.0489 |Test Loss: 1.4746|lr = 0.00310\n",
      "Epoch:  723|steps:   60|Train Avg Loss: 0.0399 |Test Loss: 1.5061|lr = 0.00310\n",
      "Epoch:  724|steps:   30|Train Avg Loss: 0.0250 |Test Loss: 1.4091|lr = 0.00310\n",
      "Epoch:  724|steps:   60|Train Avg Loss: 0.0383 |Test Loss: 1.4209|lr = 0.00310\n",
      "Epoch:  725|steps:   30|Train Avg Loss: 0.0364 |Test Loss: 1.4737|lr = 0.00310\n",
      "Epoch:  725|steps:   60|Train Avg Loss: 0.0247 |Test Loss: 1.4603|lr = 0.00310\n",
      "Epoch:  726|steps:   30|Train Avg Loss: 0.0394 |Test Loss: 1.4817|lr = 0.00310\n",
      "Epoch:  726|steps:   60|Train Avg Loss: 0.0321 |Test Loss: 1.4644|lr = 0.00310\n",
      "Epoch:  727|steps:   30|Train Avg Loss: 0.0264 |Test Loss: 1.5114|lr = 0.00310\n",
      "Epoch:  727|steps:   60|Train Avg Loss: 0.0367 |Test Loss: 1.4193|lr = 0.00310\n",
      "Epoch:  728|steps:   30|Train Avg Loss: 0.0241 |Test Loss: 1.4879|lr = 0.00304\n",
      "Epoch:  728|steps:   60|Train Avg Loss: 0.0338 |Test Loss: 1.5154|lr = 0.00304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  729|steps:   30|Train Avg Loss: 0.0208 |Test Loss: 1.4220|lr = 0.00304\n",
      "Epoch:  729|steps:   60|Train Avg Loss: 0.0286 |Test Loss: 1.5117|lr = 0.00304\n",
      "Epoch:  730|steps:   30|Train Avg Loss: 0.0254 |Test Loss: 1.4938|lr = 0.00304\n",
      "Epoch:  730|steps:   60|Train Avg Loss: 0.0267 |Test Loss: 1.4998|lr = 0.00304\n",
      "Epoch:  731|steps:   30|Train Avg Loss: 0.0271 |Test Loss: 1.5171|lr = 0.00304\n",
      "Epoch:  731|steps:   60|Train Avg Loss: 0.0228 |Test Loss: 1.5123|lr = 0.00304\n",
      "Epoch:  732|steps:   30|Train Avg Loss: 0.0329 |Test Loss: 1.4959|lr = 0.00304\n",
      "Epoch:  732|steps:   60|Train Avg Loss: 0.0419 |Test Loss: 1.5087|lr = 0.00304\n",
      "Epoch:  733|steps:   30|Train Avg Loss: 0.0288 |Test Loss: 1.5273|lr = 0.00304\n",
      "Epoch:  733|steps:   60|Train Avg Loss: 0.0546 |Test Loss: 1.4972|lr = 0.00304\n",
      "Epoch:  734|steps:   30|Train Avg Loss: 0.0346 |Test Loss: 1.5024|lr = 0.00304\n",
      "Epoch:  734|steps:   60|Train Avg Loss: 0.0479 |Test Loss: 1.4305|lr = 0.00304\n",
      "Epoch:  735|steps:   30|Train Avg Loss: 0.0263 |Test Loss: 1.4725|lr = 0.00304\n",
      "Epoch:  735|steps:   60|Train Avg Loss: 0.0431 |Test Loss: 1.4375|lr = 0.00304\n",
      "Epoch:  736|steps:   30|Train Avg Loss: 0.0319 |Test Loss: 1.4176|lr = 0.00304\n",
      "Epoch:  736|steps:   60|Train Avg Loss: 0.0331 |Test Loss: 1.4251|lr = 0.00304\n",
      "Epoch:  737|steps:   30|Train Avg Loss: 0.0344 |Test Loss: 1.4586|lr = 0.00304\n",
      "Epoch:  737|steps:   60|Train Avg Loss: 0.0358 |Test Loss: 1.4596|lr = 0.00304\n",
      "Epoch:  738|steps:   30|Train Avg Loss: 0.0366 |Test Loss: 1.4383|lr = 0.00304\n",
      "Epoch:  738|steps:   60|Train Avg Loss: 0.0419 |Test Loss: 1.4521|lr = 0.00304\n",
      "Epoch:  739|steps:   30|Train Avg Loss: 0.0430 |Test Loss: 1.3967|lr = 0.00298\n",
      "Epoch:  739|steps:   60|Train Avg Loss: 0.0378 |Test Loss: 1.4787|lr = 0.00298\n",
      "Epoch:  740|steps:   30|Train Avg Loss: 0.0342 |Test Loss: 1.4915|lr = 0.00298\n",
      "Epoch:  740|steps:   60|Train Avg Loss: 0.0323 |Test Loss: 1.5191|lr = 0.00298\n",
      "Epoch:  741|steps:   30|Train Avg Loss: 0.0382 |Test Loss: 1.5448|lr = 0.00298\n",
      "Epoch:  741|steps:   60|Train Avg Loss: 0.0509 |Test Loss: 1.5365|lr = 0.00298\n",
      "Epoch:  742|steps:   30|Train Avg Loss: 0.0342 |Test Loss: 1.6114|lr = 0.00298\n",
      "Epoch:  742|steps:   60|Train Avg Loss: 0.0464 |Test Loss: 1.6032|lr = 0.00298\n",
      "Epoch:  743|steps:   30|Train Avg Loss: 0.0520 |Test Loss: 1.5378|lr = 0.00298\n",
      "Epoch:  743|steps:   60|Train Avg Loss: 0.0675 |Test Loss: 1.5004|lr = 0.00298\n",
      "Epoch:  744|steps:   30|Train Avg Loss: 0.0613 |Test Loss: 1.4992|lr = 0.00298\n",
      "Epoch:  744|steps:   60|Train Avg Loss: 0.0590 |Test Loss: 1.5882|lr = 0.00298\n",
      "Epoch:  745|steps:   30|Train Avg Loss: 0.0720 |Test Loss: 1.5748|lr = 0.00298\n",
      "Epoch:  745|steps:   60|Train Avg Loss: 0.0595 |Test Loss: 1.4293|lr = 0.00298\n",
      "Epoch:  746|steps:   30|Train Avg Loss: 0.0488 |Test Loss: 1.5364|lr = 0.00298\n",
      "Epoch:  746|steps:   60|Train Avg Loss: 0.0464 |Test Loss: 1.4697|lr = 0.00298\n",
      "Epoch:  747|steps:   30|Train Avg Loss: 0.0321 |Test Loss: 1.4948|lr = 0.00298\n",
      "Epoch:  747|steps:   60|Train Avg Loss: 0.0300 |Test Loss: 1.5539|lr = 0.00298\n",
      "Epoch:  748|steps:   30|Train Avg Loss: 0.0337 |Test Loss: 1.4963|lr = 0.00298\n",
      "Epoch:  748|steps:   60|Train Avg Loss: 0.0340 |Test Loss: 1.5020|lr = 0.00298\n",
      "Epoch:  749|steps:   30|Train Avg Loss: 0.0319 |Test Loss: 1.4667|lr = 0.00298\n",
      "Epoch:  749|steps:   60|Train Avg Loss: 0.0300 |Test Loss: 1.5365|lr = 0.00298\n",
      "Epoch:  750|steps:   30|Train Avg Loss: 0.0329 |Test Loss: 1.5366|lr = 0.00292\n",
      "Epoch:  750|steps:   60|Train Avg Loss: 0.0362 |Test Loss: 1.5933|lr = 0.00292\n",
      "Epoch:  751|steps:   30|Train Avg Loss: 0.0302 |Test Loss: 1.5702|lr = 0.00292\n",
      "Epoch:  751|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 1.5163|lr = 0.00292\n",
      "Epoch:  752|steps:   30|Train Avg Loss: 0.0238 |Test Loss: 1.5198|lr = 0.00292\n",
      "Epoch:  752|steps:   60|Train Avg Loss: 0.0305 |Test Loss: 1.4882|lr = 0.00292\n",
      "Epoch:  753|steps:   30|Train Avg Loss: 0.0270 |Test Loss: 1.4520|lr = 0.00292\n",
      "Epoch:  753|steps:   60|Train Avg Loss: 0.0287 |Test Loss: 1.4652|lr = 0.00292\n",
      "Epoch:  754|steps:   30|Train Avg Loss: 0.0225 |Test Loss: 1.5573|lr = 0.00292\n",
      "Epoch:  754|steps:   60|Train Avg Loss: 0.0279 |Test Loss: 1.5034|lr = 0.00292\n",
      "Epoch:  755|steps:   30|Train Avg Loss: 0.0260 |Test Loss: 1.5312|lr = 0.00292\n",
      "Epoch:  755|steps:   60|Train Avg Loss: 0.0252 |Test Loss: 1.4991|lr = 0.00292\n",
      "Epoch:  756|steps:   30|Train Avg Loss: 0.0249 |Test Loss: 1.5018|lr = 0.00292\n",
      "Epoch:  756|steps:   60|Train Avg Loss: 0.0339 |Test Loss: 1.3831|lr = 0.00292\n",
      "Epoch:  757|steps:   30|Train Avg Loss: 0.0243 |Test Loss: 1.5143|lr = 0.00292\n",
      "Epoch:  757|steps:   60|Train Avg Loss: 0.0237 |Test Loss: 1.4549|lr = 0.00292\n",
      "Epoch:  758|steps:   30|Train Avg Loss: 0.0281 |Test Loss: 1.4472|lr = 0.00292\n",
      "Epoch:  758|steps:   60|Train Avg Loss: 0.0301 |Test Loss: 1.5371|lr = 0.00292\n",
      "Epoch:  759|steps:   30|Train Avg Loss: 0.0303 |Test Loss: 1.5678|lr = 0.00292\n",
      "Epoch:  759|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 1.5429|lr = 0.00292\n",
      "Epoch:  760|steps:   30|Train Avg Loss: 0.0253 |Test Loss: 1.5578|lr = 0.00292\n",
      "Epoch:  760|steps:   60|Train Avg Loss: 0.0299 |Test Loss: 1.4923|lr = 0.00292\n",
      "Epoch:  761|steps:   30|Train Avg Loss: 0.0394 |Test Loss: 1.4990|lr = 0.00292\n",
      "Epoch:  761|steps:   60|Train Avg Loss: 0.0453 |Test Loss: 1.4371|lr = 0.00292\n",
      "Epoch:  762|steps:   30|Train Avg Loss: 0.0752 |Test Loss: 1.4789|lr = 0.00292\n",
      "Epoch:  762|steps:   60|Train Avg Loss: 0.0995 |Test Loss: 1.5569|lr = 0.00292\n",
      "Epoch:  763|steps:   30|Train Avg Loss: 0.0758 |Test Loss: 1.4949|lr = 0.00292\n",
      "Epoch:  763|steps:   60|Train Avg Loss: 0.0569 |Test Loss: 1.4997|lr = 0.00292\n",
      "Epoch:  764|steps:   30|Train Avg Loss: 0.0503 |Test Loss: 1.4893|lr = 0.00292\n",
      "Epoch:  764|steps:   60|Train Avg Loss: 0.0501 |Test Loss: 1.4919|lr = 0.00292\n",
      "Epoch:  765|steps:   30|Train Avg Loss: 0.0347 |Test Loss: 1.4938|lr = 0.00292\n",
      "Epoch:  765|steps:   60|Train Avg Loss: 0.0314 |Test Loss: 1.5458|lr = 0.00292\n",
      "Epoch:  766|steps:   30|Train Avg Loss: 0.0327 |Test Loss: 1.5108|lr = 0.00286\n",
      "Epoch:  766|steps:   60|Train Avg Loss: 0.0306 |Test Loss: 1.5159|lr = 0.00286\n",
      "Epoch:  767|steps:   30|Train Avg Loss: 0.0323 |Test Loss: 1.4787|lr = 0.00286\n",
      "Epoch:  767|steps:   60|Train Avg Loss: 0.0330 |Test Loss: 1.4167|lr = 0.00286\n",
      "Epoch:  768|steps:   30|Train Avg Loss: 0.0298 |Test Loss: 1.4661|lr = 0.00286\n",
      "Epoch:  768|steps:   60|Train Avg Loss: 0.0300 |Test Loss: 1.4316|lr = 0.00286\n",
      "Epoch:  769|steps:   30|Train Avg Loss: 0.0319 |Test Loss: 1.4451|lr = 0.00286\n",
      "Epoch:  769|steps:   60|Train Avg Loss: 0.0309 |Test Loss: 1.5012|lr = 0.00286\n",
      "Epoch:  770|steps:   30|Train Avg Loss: 0.0267 |Test Loss: 1.5076|lr = 0.00286\n",
      "Epoch:  770|steps:   60|Train Avg Loss: 0.0308 |Test Loss: 1.5244|lr = 0.00286\n",
      "Epoch:  771|steps:   30|Train Avg Loss: 0.0293 |Test Loss: 1.5241|lr = 0.00286\n",
      "Epoch:  771|steps:   60|Train Avg Loss: 0.0295 |Test Loss: 1.5061|lr = 0.00286\n",
      "Epoch:  772|steps:   30|Train Avg Loss: 0.0330 |Test Loss: 1.4389|lr = 0.00286\n",
      "Epoch:  772|steps:   60|Train Avg Loss: 0.0271 |Test Loss: 1.4848|lr = 0.00286\n",
      "Epoch:  773|steps:   30|Train Avg Loss: 0.0318 |Test Loss: 1.4618|lr = 0.00286\n",
      "Epoch:  773|steps:   60|Train Avg Loss: 0.0311 |Test Loss: 1.5077|lr = 0.00286\n",
      "Epoch:  774|steps:   30|Train Avg Loss: 0.0231 |Test Loss: 1.5434|lr = 0.00286\n",
      "Epoch:  774|steps:   60|Train Avg Loss: 0.0290 |Test Loss: 1.4936|lr = 0.00286\n",
      "Epoch:  775|steps:   30|Train Avg Loss: 0.0250 |Test Loss: 1.4758|lr = 0.00286\n",
      "Epoch:  775|steps:   60|Train Avg Loss: 0.0352 |Test Loss: 1.4575|lr = 0.00286\n",
      "Epoch:  776|steps:   30|Train Avg Loss: 0.0454 |Test Loss: 1.5318|lr = 0.00286\n",
      "Epoch:  776|steps:   60|Train Avg Loss: 0.0345 |Test Loss: 1.4978|lr = 0.00286\n",
      "Epoch:  777|steps:   30|Train Avg Loss: 0.0369 |Test Loss: 1.4410|lr = 0.00280\n",
      "Epoch:  777|steps:   60|Train Avg Loss: 0.0370 |Test Loss: 1.4155|lr = 0.00280\n",
      "Epoch:  778|steps:   30|Train Avg Loss: 0.0325 |Test Loss: 1.5142|lr = 0.00280\n",
      "Epoch:  778|steps:   60|Train Avg Loss: 0.0308 |Test Loss: 1.4769|lr = 0.00280\n",
      "Epoch:  779|steps:   30|Train Avg Loss: 0.0267 |Test Loss: 1.4565|lr = 0.00280\n",
      "Epoch:  779|steps:   60|Train Avg Loss: 0.0342 |Test Loss: 1.4499|lr = 0.00280\n",
      "Epoch:  780|steps:   30|Train Avg Loss: 0.0347 |Test Loss: 1.4845|lr = 0.00280\n",
      "Epoch:  780|steps:   60|Train Avg Loss: 0.0379 |Test Loss: 1.4695|lr = 0.00280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  781|steps:   30|Train Avg Loss: 0.0348 |Test Loss: 1.4804|lr = 0.00280\n",
      "Epoch:  781|steps:   60|Train Avg Loss: 0.0427 |Test Loss: 1.5080|lr = 0.00280\n",
      "Epoch:  782|steps:   30|Train Avg Loss: 0.0403 |Test Loss: 1.4958|lr = 0.00280\n",
      "Epoch:  782|steps:   60|Train Avg Loss: 0.0514 |Test Loss: 1.4821|lr = 0.00280\n",
      "Epoch:  783|steps:   30|Train Avg Loss: 0.0489 |Test Loss: 1.5489|lr = 0.00280\n",
      "Epoch:  783|steps:   60|Train Avg Loss: 0.0398 |Test Loss: 1.5363|lr = 0.00280\n",
      "Epoch:  784|steps:   30|Train Avg Loss: 0.0321 |Test Loss: 1.5334|lr = 0.00280\n",
      "Epoch:  784|steps:   60|Train Avg Loss: 0.0354 |Test Loss: 1.5258|lr = 0.00280\n",
      "Epoch:  785|steps:   30|Train Avg Loss: 0.0441 |Test Loss: 1.5415|lr = 0.00280\n",
      "Epoch:  785|steps:   60|Train Avg Loss: 0.0326 |Test Loss: 1.5144|lr = 0.00280\n",
      "Epoch:  786|steps:   30|Train Avg Loss: 0.0421 |Test Loss: 1.4400|lr = 0.00280\n",
      "Epoch:  786|steps:   60|Train Avg Loss: 0.0442 |Test Loss: 1.4921|lr = 0.00280\n",
      "Epoch:  787|steps:   30|Train Avg Loss: 0.0458 |Test Loss: 1.5421|lr = 0.00280\n",
      "Epoch:  787|steps:   60|Train Avg Loss: 0.0750 |Test Loss: 1.3983|lr = 0.00280\n",
      "Epoch:  788|steps:   30|Train Avg Loss: 0.0475 |Test Loss: 1.4584|lr = 0.00280\n",
      "Epoch:  788|steps:   60|Train Avg Loss: 0.0731 |Test Loss: 1.4292|lr = 0.00280\n",
      "Epoch:  789|steps:   30|Train Avg Loss: 0.0627 |Test Loss: 1.4115|lr = 0.00280\n",
      "Epoch:  789|steps:   60|Train Avg Loss: 0.0525 |Test Loss: 1.5376|lr = 0.00280\n",
      "Epoch:  790|steps:   30|Train Avg Loss: 0.0736 |Test Loss: 1.4477|lr = 0.00280\n",
      "Epoch:  790|steps:   60|Train Avg Loss: 0.0626 |Test Loss: 1.5252|lr = 0.00280\n",
      "Epoch:  791|steps:   30|Train Avg Loss: 0.0480 |Test Loss: 1.5675|lr = 0.00280\n",
      "Epoch:  791|steps:   60|Train Avg Loss: 0.0469 |Test Loss: 1.4764|lr = 0.00280\n",
      "Epoch:  792|steps:   30|Train Avg Loss: 0.0427 |Test Loss: 1.4614|lr = 0.00280\n",
      "Epoch:  792|steps:   60|Train Avg Loss: 0.0414 |Test Loss: 1.4527|lr = 0.00280\n",
      "Epoch:  793|steps:   30|Train Avg Loss: 0.0334 |Test Loss: 1.4925|lr = 0.00280\n",
      "Epoch:  793|steps:   60|Train Avg Loss: 0.0354 |Test Loss: 1.4863|lr = 0.00280\n",
      "Epoch:  794|steps:   30|Train Avg Loss: 0.0241 |Test Loss: 1.4462|lr = 0.00280\n",
      "Epoch:  794|steps:   60|Train Avg Loss: 0.0231 |Test Loss: 1.4535|lr = 0.00280\n",
      "Epoch:  795|steps:   30|Train Avg Loss: 0.0199 |Test Loss: 1.5227|lr = 0.00280\n",
      "Epoch:  795|steps:   60|Train Avg Loss: 0.0236 |Test Loss: 1.4229|lr = 0.00280\n",
      "Epoch:  796|steps:   30|Train Avg Loss: 0.0244 |Test Loss: 1.5058|lr = 0.00274\n",
      "Epoch:  796|steps:   60|Train Avg Loss: 0.0252 |Test Loss: 1.4686|lr = 0.00274\n",
      "Epoch:  797|steps:   30|Train Avg Loss: 0.0232 |Test Loss: 1.4845|lr = 0.00274\n",
      "Epoch:  797|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 1.5117|lr = 0.00274\n",
      "Epoch:  798|steps:   30|Train Avg Loss: 0.0257 |Test Loss: 1.4624|lr = 0.00274\n",
      "Epoch:  798|steps:   60|Train Avg Loss: 0.0187 |Test Loss: 1.4342|lr = 0.00274\n",
      "Epoch:  799|steps:   30|Train Avg Loss: 0.0316 |Test Loss: 1.4266|lr = 0.00274\n",
      "Epoch:  799|steps:   60|Train Avg Loss: 0.0243 |Test Loss: 1.5142|lr = 0.00274\n",
      "Epoch:  800|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 1.5003|lr = 0.00274\n",
      "Epoch:  800|steps:   60|Train Avg Loss: 0.0260 |Test Loss: 1.5746|lr = 0.00274\n",
      "Epoch:  801|steps:   30|Train Avg Loss: 0.0242 |Test Loss: 1.4835|lr = 0.00274\n",
      "Epoch:  801|steps:   60|Train Avg Loss: 0.0291 |Test Loss: 1.5569|lr = 0.00274\n",
      "Epoch:  802|steps:   30|Train Avg Loss: 0.0264 |Test Loss: 1.5205|lr = 0.00274\n",
      "Epoch:  802|steps:   60|Train Avg Loss: 0.0213 |Test Loss: 1.4994|lr = 0.00274\n",
      "Epoch:  803|steps:   30|Train Avg Loss: 0.0225 |Test Loss: 1.5258|lr = 0.00274\n",
      "Epoch:  803|steps:   60|Train Avg Loss: 0.0224 |Test Loss: 1.4710|lr = 0.00274\n",
      "Epoch:  804|steps:   30|Train Avg Loss: 0.0233 |Test Loss: 1.4288|lr = 0.00274\n",
      "Epoch:  804|steps:   60|Train Avg Loss: 0.0248 |Test Loss: 1.4985|lr = 0.00274\n",
      "Epoch:  805|steps:   30|Train Avg Loss: 0.0271 |Test Loss: 1.4937|lr = 0.00274\n",
      "Epoch:  805|steps:   60|Train Avg Loss: 0.0252 |Test Loss: 1.4033|lr = 0.00274\n",
      "Epoch:  806|steps:   30|Train Avg Loss: 0.0264 |Test Loss: 1.5250|lr = 0.00274\n",
      "Epoch:  806|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 1.5903|lr = 0.00274\n",
      "Epoch:  807|steps:   30|Train Avg Loss: 0.0272 |Test Loss: 1.5173|lr = 0.00269\n",
      "Epoch:  807|steps:   60|Train Avg Loss: 0.0266 |Test Loss: 1.4974|lr = 0.00269\n",
      "Epoch:  808|steps:   30|Train Avg Loss: 0.0248 |Test Loss: 1.5077|lr = 0.00269\n",
      "Epoch:  808|steps:   60|Train Avg Loss: 0.0277 |Test Loss: 1.4694|lr = 0.00269\n",
      "Epoch:  809|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 1.4504|lr = 0.00269\n",
      "Epoch:  809|steps:   60|Train Avg Loss: 0.0282 |Test Loss: 1.4944|lr = 0.00269\n",
      "Epoch:  810|steps:   30|Train Avg Loss: 0.0232 |Test Loss: 1.5498|lr = 0.00269\n",
      "Epoch:  810|steps:   60|Train Avg Loss: 0.0272 |Test Loss: 1.4470|lr = 0.00269\n",
      "Epoch:  811|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.4389|lr = 0.00269\n",
      "Epoch:  811|steps:   60|Train Avg Loss: 0.0204 |Test Loss: 1.4715|lr = 0.00269\n",
      "Epoch:  812|steps:   30|Train Avg Loss: 0.0256 |Test Loss: 1.4748|lr = 0.00269\n",
      "Epoch:  812|steps:   60|Train Avg Loss: 0.0282 |Test Loss: 1.5324|lr = 0.00269\n",
      "Epoch:  813|steps:   30|Train Avg Loss: 0.0322 |Test Loss: 1.4642|lr = 0.00269\n",
      "Epoch:  813|steps:   60|Train Avg Loss: 0.0282 |Test Loss: 1.4937|lr = 0.00269\n",
      "Epoch:  814|steps:   30|Train Avg Loss: 0.0562 |Test Loss: 1.4452|lr = 0.00269\n",
      "Epoch:  814|steps:   60|Train Avg Loss: 0.0946 |Test Loss: 1.4711|lr = 0.00269\n",
      "Epoch:  815|steps:   30|Train Avg Loss: 0.1862 |Test Loss: 1.4345|lr = 0.00269\n",
      "Epoch:  815|steps:   60|Train Avg Loss: 0.1179 |Test Loss: 1.4991|lr = 0.00269\n",
      "Epoch:  816|steps:   30|Train Avg Loss: 0.0752 |Test Loss: 1.5300|lr = 0.00269\n",
      "Epoch:  816|steps:   60|Train Avg Loss: 0.0648 |Test Loss: 1.5621|lr = 0.00269\n",
      "Epoch:  817|steps:   30|Train Avg Loss: 0.0412 |Test Loss: 1.5545|lr = 0.00269\n",
      "Epoch:  817|steps:   60|Train Avg Loss: 0.0540 |Test Loss: 1.5730|lr = 0.00269\n",
      "Epoch:  818|steps:   30|Train Avg Loss: 0.0459 |Test Loss: 1.5556|lr = 0.00264\n",
      "Epoch:  818|steps:   60|Train Avg Loss: 0.0342 |Test Loss: 1.4557|lr = 0.00264\n",
      "Epoch:  819|steps:   30|Train Avg Loss: 0.0290 |Test Loss: 1.4966|lr = 0.00264\n",
      "Epoch:  819|steps:   60|Train Avg Loss: 0.0372 |Test Loss: 1.4542|lr = 0.00264\n",
      "Epoch:  820|steps:   30|Train Avg Loss: 0.0332 |Test Loss: 1.5036|lr = 0.00264\n",
      "Epoch:  820|steps:   60|Train Avg Loss: 0.0263 |Test Loss: 1.4849|lr = 0.00264\n",
      "Epoch:  821|steps:   30|Train Avg Loss: 0.0302 |Test Loss: 1.5116|lr = 0.00264\n",
      "Epoch:  821|steps:   60|Train Avg Loss: 0.0272 |Test Loss: 1.5043|lr = 0.00264\n",
      "Epoch:  822|steps:   30|Train Avg Loss: 0.0185 |Test Loss: 1.5273|lr = 0.00264\n",
      "Epoch:  822|steps:   60|Train Avg Loss: 0.0320 |Test Loss: 1.4730|lr = 0.00264\n",
      "Epoch:  823|steps:   30|Train Avg Loss: 0.0206 |Test Loss: 1.5821|lr = 0.00264\n",
      "Epoch:  823|steps:   60|Train Avg Loss: 0.0237 |Test Loss: 1.5620|lr = 0.00264\n",
      "Epoch:  824|steps:   30|Train Avg Loss: 0.0285 |Test Loss: 1.5533|lr = 0.00264\n",
      "Epoch:  824|steps:   60|Train Avg Loss: 0.0204 |Test Loss: 1.5039|lr = 0.00264\n",
      "Epoch:  825|steps:   30|Train Avg Loss: 0.0238 |Test Loss: 1.5266|lr = 0.00264\n",
      "Epoch:  825|steps:   60|Train Avg Loss: 0.0197 |Test Loss: 1.6055|lr = 0.00264\n",
      "Epoch:  826|steps:   30|Train Avg Loss: 0.0207 |Test Loss: 1.5305|lr = 0.00264\n",
      "Epoch:  826|steps:   60|Train Avg Loss: 0.0220 |Test Loss: 1.5566|lr = 0.00264\n",
      "Epoch:  827|steps:   30|Train Avg Loss: 0.0244 |Test Loss: 1.5283|lr = 0.00264\n",
      "Epoch:  827|steps:   60|Train Avg Loss: 0.0269 |Test Loss: 1.5335|lr = 0.00264\n",
      "Epoch:  828|steps:   30|Train Avg Loss: 0.0251 |Test Loss: 1.5090|lr = 0.00264\n",
      "Epoch:  828|steps:   60|Train Avg Loss: 0.0223 |Test Loss: 1.5708|lr = 0.00264\n",
      "Epoch:  829|steps:   30|Train Avg Loss: 0.0340 |Test Loss: 1.6867|lr = 0.00258\n",
      "Epoch:  829|steps:   60|Train Avg Loss: 0.0610 |Test Loss: 1.6327|lr = 0.00258\n",
      "Epoch:  830|steps:   30|Train Avg Loss: 0.0374 |Test Loss: 1.4514|lr = 0.00258\n",
      "Epoch:  830|steps:   60|Train Avg Loss: 0.0404 |Test Loss: 1.5356|lr = 0.00258\n",
      "Epoch:  831|steps:   30|Train Avg Loss: 0.0552 |Test Loss: 1.5061|lr = 0.00258\n",
      "Epoch:  831|steps:   60|Train Avg Loss: 0.0608 |Test Loss: 1.5153|lr = 0.00258\n",
      "Epoch:  832|steps:   30|Train Avg Loss: 0.0498 |Test Loss: 1.5040|lr = 0.00258\n",
      "Epoch:  832|steps:   60|Train Avg Loss: 0.0566 |Test Loss: 1.5025|lr = 0.00258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  833|steps:   30|Train Avg Loss: 0.0301 |Test Loss: 1.5688|lr = 0.00258\n",
      "Epoch:  833|steps:   60|Train Avg Loss: 0.0339 |Test Loss: 1.5035|lr = 0.00258\n",
      "Epoch:  834|steps:   30|Train Avg Loss: 0.0269 |Test Loss: 1.5154|lr = 0.00258\n",
      "Epoch:  834|steps:   60|Train Avg Loss: 0.0277 |Test Loss: 1.5224|lr = 0.00258\n",
      "Epoch:  835|steps:   30|Train Avg Loss: 0.0242 |Test Loss: 1.5579|lr = 0.00258\n",
      "Epoch:  835|steps:   60|Train Avg Loss: 0.0181 |Test Loss: 1.4793|lr = 0.00258\n",
      "Epoch:  836|steps:   30|Train Avg Loss: 0.0257 |Test Loss: 1.5101|lr = 0.00258\n",
      "Epoch:  836|steps:   60|Train Avg Loss: 0.0222 |Test Loss: 1.4030|lr = 0.00258\n",
      "Epoch:  837|steps:   30|Train Avg Loss: 0.0200 |Test Loss: 1.4819|lr = 0.00258\n",
      "Epoch:  837|steps:   60|Train Avg Loss: 0.0248 |Test Loss: 1.4594|lr = 0.00258\n",
      "Epoch:  838|steps:   30|Train Avg Loss: 0.0278 |Test Loss: 1.4989|lr = 0.00258\n",
      "Epoch:  838|steps:   60|Train Avg Loss: 0.0208 |Test Loss: 1.4420|lr = 0.00258\n",
      "Epoch:  839|steps:   30|Train Avg Loss: 0.0224 |Test Loss: 1.5036|lr = 0.00258\n",
      "Epoch:  839|steps:   60|Train Avg Loss: 0.0288 |Test Loss: 1.4523|lr = 0.00258\n",
      "Epoch:  840|steps:   30|Train Avg Loss: 0.0237 |Test Loss: 1.4626|lr = 0.00253\n",
      "Epoch:  840|steps:   60|Train Avg Loss: 0.0264 |Test Loss: 1.4778|lr = 0.00253\n",
      "Epoch:  841|steps:   30|Train Avg Loss: 0.0199 |Test Loss: 1.4945|lr = 0.00253\n",
      "Epoch:  841|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 1.4601|lr = 0.00253\n",
      "Epoch:  842|steps:   30|Train Avg Loss: 0.0220 |Test Loss: 1.4399|lr = 0.00253\n",
      "Epoch:  842|steps:   60|Train Avg Loss: 0.0203 |Test Loss: 1.4677|lr = 0.00253\n",
      "Epoch:  843|steps:   30|Train Avg Loss: 0.0237 |Test Loss: 1.4355|lr = 0.00253\n",
      "Epoch:  843|steps:   60|Train Avg Loss: 0.0215 |Test Loss: 1.4728|lr = 0.00253\n",
      "Epoch:  844|steps:   30|Train Avg Loss: 0.0177 |Test Loss: 1.4413|lr = 0.00253\n",
      "Epoch:  844|steps:   60|Train Avg Loss: 0.0210 |Test Loss: 1.4164|lr = 0.00253\n",
      "Epoch:  845|steps:   30|Train Avg Loss: 0.0261 |Test Loss: 1.4810|lr = 0.00253\n",
      "Epoch:  845|steps:   60|Train Avg Loss: 0.0199 |Test Loss: 1.5319|lr = 0.00253\n",
      "Epoch:  846|steps:   30|Train Avg Loss: 0.0219 |Test Loss: 1.4612|lr = 0.00253\n",
      "Epoch:  846|steps:   60|Train Avg Loss: 0.0198 |Test Loss: 1.4959|lr = 0.00253\n",
      "Epoch:  847|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.4664|lr = 0.00253\n",
      "Epoch:  847|steps:   60|Train Avg Loss: 0.0277 |Test Loss: 1.4700|lr = 0.00253\n",
      "Epoch:  848|steps:   30|Train Avg Loss: 0.0241 |Test Loss: 1.4912|lr = 0.00253\n",
      "Epoch:  848|steps:   60|Train Avg Loss: 0.0291 |Test Loss: 1.4971|lr = 0.00253\n",
      "Epoch:  849|steps:   30|Train Avg Loss: 0.0223 |Test Loss: 1.5197|lr = 0.00253\n",
      "Epoch:  849|steps:   60|Train Avg Loss: 0.0283 |Test Loss: 1.4957|lr = 0.00253\n",
      "Epoch:  850|steps:   30|Train Avg Loss: 0.0371 |Test Loss: 1.5298|lr = 0.00253\n",
      "Epoch:  850|steps:   60|Train Avg Loss: 0.0307 |Test Loss: 1.5139|lr = 0.00253\n",
      "Epoch:  851|steps:   30|Train Avg Loss: 0.0370 |Test Loss: 1.5347|lr = 0.00248\n",
      "Epoch:  851|steps:   60|Train Avg Loss: 0.0398 |Test Loss: 1.4701|lr = 0.00248\n",
      "Epoch:  852|steps:   30|Train Avg Loss: 0.0376 |Test Loss: 1.4494|lr = 0.00248\n",
      "Epoch:  852|steps:   60|Train Avg Loss: 0.0359 |Test Loss: 1.4814|lr = 0.00248\n",
      "Epoch:  853|steps:   30|Train Avg Loss: 0.0665 |Test Loss: 1.4870|lr = 0.00248\n",
      "Epoch:  853|steps:   60|Train Avg Loss: 0.0512 |Test Loss: 1.5390|lr = 0.00248\n",
      "Epoch:  854|steps:   30|Train Avg Loss: 0.0684 |Test Loss: 1.5480|lr = 0.00248\n",
      "Epoch:  854|steps:   60|Train Avg Loss: 0.0842 |Test Loss: 1.4132|lr = 0.00248\n",
      "Epoch:  855|steps:   30|Train Avg Loss: 0.0681 |Test Loss: 1.3929|lr = 0.00248\n",
      "Epoch:  855|steps:   60|Train Avg Loss: 0.0423 |Test Loss: 1.4599|lr = 0.00248\n",
      "Epoch:  856|steps:   30|Train Avg Loss: 0.0437 |Test Loss: 1.4689|lr = 0.00248\n",
      "Epoch:  856|steps:   60|Train Avg Loss: 0.0485 |Test Loss: 1.5094|lr = 0.00248\n",
      "Epoch:  857|steps:   30|Train Avg Loss: 0.0290 |Test Loss: 1.4498|lr = 0.00248\n",
      "Epoch:  857|steps:   60|Train Avg Loss: 0.0291 |Test Loss: 1.4935|lr = 0.00248\n",
      "Epoch:  858|steps:   30|Train Avg Loss: 0.0304 |Test Loss: 1.4857|lr = 0.00248\n",
      "Epoch:  858|steps:   60|Train Avg Loss: 0.0267 |Test Loss: 1.5126|lr = 0.00248\n",
      "Epoch:  859|steps:   30|Train Avg Loss: 0.0276 |Test Loss: 1.4799|lr = 0.00248\n",
      "Epoch:  859|steps:   60|Train Avg Loss: 0.0200 |Test Loss: 1.4794|lr = 0.00248\n",
      "Epoch:  860|steps:   30|Train Avg Loss: 0.0306 |Test Loss: 1.5931|lr = 0.00248\n",
      "Epoch:  860|steps:   60|Train Avg Loss: 0.0280 |Test Loss: 1.4762|lr = 0.00248\n",
      "Epoch:  861|steps:   30|Train Avg Loss: 0.0288 |Test Loss: 1.5251|lr = 0.00248\n",
      "Epoch:  861|steps:   60|Train Avg Loss: 0.0262 |Test Loss: 1.4188|lr = 0.00248\n",
      "Epoch:  862|steps:   30|Train Avg Loss: 0.0273 |Test Loss: 1.4446|lr = 0.00248\n",
      "Epoch:  862|steps:   60|Train Avg Loss: 0.0296 |Test Loss: 1.4912|lr = 0.00248\n",
      "Epoch:  863|steps:   30|Train Avg Loss: 0.0229 |Test Loss: 1.4992|lr = 0.00248\n",
      "Epoch:  863|steps:   60|Train Avg Loss: 0.0240 |Test Loss: 1.4396|lr = 0.00248\n",
      "Epoch:  864|steps:   30|Train Avg Loss: 0.0233 |Test Loss: 1.4763|lr = 0.00248\n",
      "Epoch:  864|steps:   60|Train Avg Loss: 0.0221 |Test Loss: 1.4670|lr = 0.00248\n",
      "Epoch:  865|steps:   30|Train Avg Loss: 0.0272 |Test Loss: 1.4534|lr = 0.00248\n",
      "Epoch:  865|steps:   60|Train Avg Loss: 0.0270 |Test Loss: 1.4977|lr = 0.00248\n",
      "Epoch:  866|steps:   30|Train Avg Loss: 0.0393 |Test Loss: 1.4638|lr = 0.00248\n",
      "Epoch:  866|steps:   60|Train Avg Loss: 0.0433 |Test Loss: 1.3314|lr = 0.00248\n",
      "Epoch:  867|steps:   30|Train Avg Loss: 0.0332 |Test Loss: 1.3909|lr = 0.00248\n",
      "Epoch:  867|steps:   60|Train Avg Loss: 0.0280 |Test Loss: 1.5100|lr = 0.00248\n",
      "Epoch:  868|steps:   30|Train Avg Loss: 0.0390 |Test Loss: 1.5326|lr = 0.00248\n",
      "Epoch:  868|steps:   60|Train Avg Loss: 0.0351 |Test Loss: 1.5315|lr = 0.00248\n",
      "Epoch:  869|steps:   30|Train Avg Loss: 0.0337 |Test Loss: 1.4692|lr = 0.00248\n",
      "Epoch:  869|steps:   60|Train Avg Loss: 0.0354 |Test Loss: 1.4418|lr = 0.00248\n",
      "Epoch:  870|steps:   30|Train Avg Loss: 0.0348 |Test Loss: 1.4718|lr = 0.00248\n",
      "Epoch:  870|steps:   60|Train Avg Loss: 0.0410 |Test Loss: 1.4713|lr = 0.00248\n",
      "Epoch:  871|steps:   30|Train Avg Loss: 0.0527 |Test Loss: 1.4606|lr = 0.00248\n",
      "Epoch:  871|steps:   60|Train Avg Loss: 0.0530 |Test Loss: 1.4828|lr = 0.00248\n",
      "Epoch:  872|steps:   30|Train Avg Loss: 0.0373 |Test Loss: 1.5267|lr = 0.00243\n",
      "Epoch:  872|steps:   60|Train Avg Loss: 0.0233 |Test Loss: 1.5195|lr = 0.00243\n",
      "Epoch:  873|steps:   30|Train Avg Loss: 0.0288 |Test Loss: 1.4895|lr = 0.00243\n",
      "Epoch:  873|steps:   60|Train Avg Loss: 0.0249 |Test Loss: 1.4568|lr = 0.00243\n",
      "Epoch:  874|steps:   30|Train Avg Loss: 0.0225 |Test Loss: 1.4787|lr = 0.00243\n",
      "Epoch:  874|steps:   60|Train Avg Loss: 0.0311 |Test Loss: 1.5041|lr = 0.00243\n",
      "Epoch:  875|steps:   30|Train Avg Loss: 0.0222 |Test Loss: 1.4234|lr = 0.00243\n",
      "Epoch:  875|steps:   60|Train Avg Loss: 0.0261 |Test Loss: 1.4118|lr = 0.00243\n",
      "Epoch:  876|steps:   30|Train Avg Loss: 0.0294 |Test Loss: 1.4630|lr = 0.00243\n",
      "Epoch:  876|steps:   60|Train Avg Loss: 0.0196 |Test Loss: 1.4563|lr = 0.00243\n",
      "Epoch:  877|steps:   30|Train Avg Loss: 0.0168 |Test Loss: 1.5014|lr = 0.00243\n",
      "Epoch:  877|steps:   60|Train Avg Loss: 0.0269 |Test Loss: 1.4751|lr = 0.00243\n",
      "Epoch:  878|steps:   30|Train Avg Loss: 0.0252 |Test Loss: 1.4691|lr = 0.00243\n",
      "Epoch:  878|steps:   60|Train Avg Loss: 0.0187 |Test Loss: 1.4570|lr = 0.00243\n",
      "Epoch:  879|steps:   30|Train Avg Loss: 0.0206 |Test Loss: 1.4547|lr = 0.00243\n",
      "Epoch:  879|steps:   60|Train Avg Loss: 0.0182 |Test Loss: 1.4782|lr = 0.00243\n",
      "Epoch:  880|steps:   30|Train Avg Loss: 0.0269 |Test Loss: 1.4629|lr = 0.00243\n",
      "Epoch:  880|steps:   60|Train Avg Loss: 0.0237 |Test Loss: 1.5268|lr = 0.00243\n",
      "Epoch:  881|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 1.5466|lr = 0.00243\n",
      "Epoch:  881|steps:   60|Train Avg Loss: 0.0271 |Test Loss: 1.4638|lr = 0.00243\n",
      "Epoch:  882|steps:   30|Train Avg Loss: 0.0284 |Test Loss: 1.4800|lr = 0.00243\n",
      "Epoch:  882|steps:   60|Train Avg Loss: 0.0246 |Test Loss: 1.4532|lr = 0.00243\n",
      "Epoch:  883|steps:   30|Train Avg Loss: 0.0230 |Test Loss: 1.4582|lr = 0.00238\n",
      "Epoch:  883|steps:   60|Train Avg Loss: 0.0331 |Test Loss: 1.4607|lr = 0.00238\n",
      "Epoch:  884|steps:   30|Train Avg Loss: 0.0270 |Test Loss: 1.5270|lr = 0.00238\n",
      "Epoch:  884|steps:   60|Train Avg Loss: 0.0300 |Test Loss: 1.5490|lr = 0.00238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  885|steps:   30|Train Avg Loss: 0.0296 |Test Loss: 1.5364|lr = 0.00238\n",
      "Epoch:  885|steps:   60|Train Avg Loss: 0.0371 |Test Loss: 1.4982|lr = 0.00238\n",
      "Epoch:  886|steps:   30|Train Avg Loss: 0.0434 |Test Loss: 1.5188|lr = 0.00238\n",
      "Epoch:  886|steps:   60|Train Avg Loss: 0.0397 |Test Loss: 1.5016|lr = 0.00238\n",
      "Epoch:  887|steps:   30|Train Avg Loss: 0.0421 |Test Loss: 1.5457|lr = 0.00238\n",
      "Epoch:  887|steps:   60|Train Avg Loss: 0.0385 |Test Loss: 1.5050|lr = 0.00238\n",
      "Epoch:  888|steps:   30|Train Avg Loss: 0.0342 |Test Loss: 1.4682|lr = 0.00238\n",
      "Epoch:  888|steps:   60|Train Avg Loss: 0.0264 |Test Loss: 1.4874|lr = 0.00238\n",
      "Epoch:  889|steps:   30|Train Avg Loss: 0.0374 |Test Loss: 1.4919|lr = 0.00238\n",
      "Epoch:  889|steps:   60|Train Avg Loss: 0.0411 |Test Loss: 1.4985|lr = 0.00238\n",
      "Epoch:  890|steps:   30|Train Avg Loss: 0.0309 |Test Loss: 1.4841|lr = 0.00238\n",
      "Epoch:  890|steps:   60|Train Avg Loss: 0.0304 |Test Loss: 1.5396|lr = 0.00238\n",
      "Epoch:  891|steps:   30|Train Avg Loss: 0.0312 |Test Loss: 1.5733|lr = 0.00238\n",
      "Epoch:  891|steps:   60|Train Avg Loss: 0.0353 |Test Loss: 1.5265|lr = 0.00238\n",
      "Epoch:  892|steps:   30|Train Avg Loss: 0.0466 |Test Loss: 1.4866|lr = 0.00238\n",
      "Epoch:  892|steps:   60|Train Avg Loss: 0.0431 |Test Loss: 1.4398|lr = 0.00238\n",
      "Epoch:  893|steps:   30|Train Avg Loss: 0.0401 |Test Loss: 1.4580|lr = 0.00238\n",
      "Epoch:  893|steps:   60|Train Avg Loss: 0.0402 |Test Loss: 1.4224|lr = 0.00238\n",
      "Epoch:  894|steps:   30|Train Avg Loss: 0.0263 |Test Loss: 1.4544|lr = 0.00233\n",
      "Epoch:  894|steps:   60|Train Avg Loss: 0.0315 |Test Loss: 1.3823|lr = 0.00233\n",
      "Epoch:  895|steps:   30|Train Avg Loss: 0.0271 |Test Loss: 1.4361|lr = 0.00233\n",
      "Epoch:  895|steps:   60|Train Avg Loss: 0.0342 |Test Loss: 1.5016|lr = 0.00233\n",
      "Epoch:  896|steps:   30|Train Avg Loss: 0.0233 |Test Loss: 1.4187|lr = 0.00233\n",
      "Epoch:  896|steps:   60|Train Avg Loss: 0.0269 |Test Loss: 1.4894|lr = 0.00233\n",
      "Epoch:  897|steps:   30|Train Avg Loss: 0.0259 |Test Loss: 1.4030|lr = 0.00233\n",
      "Epoch:  897|steps:   60|Train Avg Loss: 0.0190 |Test Loss: 1.3962|lr = 0.00233\n",
      "Epoch:  898|steps:   30|Train Avg Loss: 0.0247 |Test Loss: 1.4626|lr = 0.00233\n",
      "Epoch:  898|steps:   60|Train Avg Loss: 0.0187 |Test Loss: 1.4323|lr = 0.00233\n",
      "Epoch:  899|steps:   30|Train Avg Loss: 0.0208 |Test Loss: 1.4456|lr = 0.00233\n",
      "Epoch:  899|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 1.4574|lr = 0.00233\n",
      "Epoch:  900|steps:   30|Train Avg Loss: 0.0253 |Test Loss: 1.4468|lr = 0.00233\n",
      "Epoch:  900|steps:   60|Train Avg Loss: 0.0213 |Test Loss: 1.4194|lr = 0.00233\n",
      "Epoch:  901|steps:   30|Train Avg Loss: 0.0227 |Test Loss: 1.4424|lr = 0.00233\n",
      "Epoch:  901|steps:   60|Train Avg Loss: 0.0187 |Test Loss: 1.4441|lr = 0.00233\n",
      "Epoch:  902|steps:   30|Train Avg Loss: 0.0211 |Test Loss: 1.4532|lr = 0.00233\n",
      "Epoch:  902|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 1.4343|lr = 0.00233\n",
      "Epoch:  903|steps:   30|Train Avg Loss: 0.0219 |Test Loss: 1.4560|lr = 0.00233\n",
      "Epoch:  903|steps:   60|Train Avg Loss: 0.0237 |Test Loss: 1.4428|lr = 0.00233\n",
      "Epoch:  904|steps:   30|Train Avg Loss: 0.0188 |Test Loss: 1.4502|lr = 0.00233\n",
      "Epoch:  904|steps:   60|Train Avg Loss: 0.0234 |Test Loss: 1.4349|lr = 0.00233\n",
      "Epoch:  905|steps:   30|Train Avg Loss: 0.0253 |Test Loss: 1.5124|lr = 0.00229\n",
      "Epoch:  905|steps:   60|Train Avg Loss: 0.0324 |Test Loss: 1.4362|lr = 0.00229\n",
      "Epoch:  906|steps:   30|Train Avg Loss: 0.0304 |Test Loss: 1.4764|lr = 0.00229\n",
      "Epoch:  906|steps:   60|Train Avg Loss: 0.0268 |Test Loss: 1.6277|lr = 0.00229\n",
      "Epoch:  907|steps:   30|Train Avg Loss: 0.0292 |Test Loss: 1.5977|lr = 0.00229\n",
      "Epoch:  907|steps:   60|Train Avg Loss: 0.0456 |Test Loss: 1.4845|lr = 0.00229\n",
      "Epoch:  908|steps:   30|Train Avg Loss: 0.0434 |Test Loss: 1.5187|lr = 0.00229\n",
      "Epoch:  908|steps:   60|Train Avg Loss: 0.0451 |Test Loss: 1.4862|lr = 0.00229\n",
      "Epoch:  909|steps:   30|Train Avg Loss: 0.0415 |Test Loss: 1.4863|lr = 0.00229\n",
      "Epoch:  909|steps:   60|Train Avg Loss: 0.0436 |Test Loss: 1.4326|lr = 0.00229\n",
      "Epoch:  910|steps:   30|Train Avg Loss: 0.0377 |Test Loss: 1.4856|lr = 0.00229\n",
      "Epoch:  910|steps:   60|Train Avg Loss: 0.0425 |Test Loss: 1.4870|lr = 0.00229\n",
      "Epoch:  911|steps:   30|Train Avg Loss: 0.0517 |Test Loss: 1.4352|lr = 0.00229\n",
      "Epoch:  911|steps:   60|Train Avg Loss: 0.0416 |Test Loss: 1.4419|lr = 0.00229\n",
      "Epoch:  912|steps:   30|Train Avg Loss: 0.0506 |Test Loss: 1.4724|lr = 0.00229\n",
      "Epoch:  912|steps:   60|Train Avg Loss: 0.0441 |Test Loss: 1.5231|lr = 0.00229\n",
      "Epoch:  913|steps:   30|Train Avg Loss: 0.0356 |Test Loss: 1.5211|lr = 0.00229\n",
      "Epoch:  913|steps:   60|Train Avg Loss: 0.0310 |Test Loss: 1.5750|lr = 0.00229\n",
      "Epoch:  914|steps:   30|Train Avg Loss: 0.0224 |Test Loss: 1.5186|lr = 0.00229\n",
      "Epoch:  914|steps:   60|Train Avg Loss: 0.0268 |Test Loss: 1.4709|lr = 0.00229\n",
      "Epoch:  915|steps:   30|Train Avg Loss: 0.0300 |Test Loss: 1.4552|lr = 0.00229\n",
      "Epoch:  915|steps:   60|Train Avg Loss: 0.0273 |Test Loss: 1.4386|lr = 0.00229\n",
      "Epoch:  916|steps:   30|Train Avg Loss: 0.0254 |Test Loss: 1.5090|lr = 0.00224\n",
      "Epoch:  916|steps:   60|Train Avg Loss: 0.0206 |Test Loss: 1.4576|lr = 0.00224\n",
      "Epoch:  917|steps:   30|Train Avg Loss: 0.0237 |Test Loss: 1.4715|lr = 0.00224\n",
      "Epoch:  917|steps:   60|Train Avg Loss: 0.0253 |Test Loss: 1.5083|lr = 0.00224\n",
      "Epoch:  918|steps:   30|Train Avg Loss: 0.0188 |Test Loss: 1.3651|lr = 0.00224\n",
      "Epoch:  918|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 1.4075|lr = 0.00224\n",
      "Epoch:  919|steps:   30|Train Avg Loss: 0.0246 |Test Loss: 1.5429|lr = 0.00224\n",
      "Epoch:  919|steps:   60|Train Avg Loss: 0.0234 |Test Loss: 1.4663|lr = 0.00224\n",
      "Epoch:  920|steps:   30|Train Avg Loss: 0.0219 |Test Loss: 1.3992|lr = 0.00224\n",
      "Epoch:  920|steps:   60|Train Avg Loss: 0.0164 |Test Loss: 1.4214|lr = 0.00224\n",
      "Epoch:  921|steps:   30|Train Avg Loss: 0.0209 |Test Loss: 1.5163|lr = 0.00224\n",
      "Epoch:  921|steps:   60|Train Avg Loss: 0.0228 |Test Loss: 1.4755|lr = 0.00224\n",
      "Epoch:  922|steps:   30|Train Avg Loss: 0.0205 |Test Loss: 1.4746|lr = 0.00224\n",
      "Epoch:  922|steps:   60|Train Avg Loss: 0.0191 |Test Loss: 1.4812|lr = 0.00224\n",
      "Epoch:  923|steps:   30|Train Avg Loss: 0.0197 |Test Loss: 1.4097|lr = 0.00224\n",
      "Epoch:  923|steps:   60|Train Avg Loss: 0.0217 |Test Loss: 1.4414|lr = 0.00224\n",
      "Epoch:  924|steps:   30|Train Avg Loss: 0.0211 |Test Loss: 1.4081|lr = 0.00224\n",
      "Epoch:  924|steps:   60|Train Avg Loss: 0.0236 |Test Loss: 1.4342|lr = 0.00224\n",
      "Epoch:  925|steps:   30|Train Avg Loss: 0.0228 |Test Loss: 1.4412|lr = 0.00224\n",
      "Epoch:  925|steps:   60|Train Avg Loss: 0.0228 |Test Loss: 1.4464|lr = 0.00224\n",
      "Epoch:  926|steps:   30|Train Avg Loss: 0.0222 |Test Loss: 1.5330|lr = 0.00224\n",
      "Epoch:  926|steps:   60|Train Avg Loss: 0.0185 |Test Loss: 1.5279|lr = 0.00224\n",
      "Epoch:  927|steps:   30|Train Avg Loss: 0.0286 |Test Loss: 1.4461|lr = 0.00220\n",
      "Epoch:  927|steps:   60|Train Avg Loss: 0.0260 |Test Loss: 1.4057|lr = 0.00220\n",
      "Epoch:  928|steps:   30|Train Avg Loss: 0.0257 |Test Loss: 1.4421|lr = 0.00220\n",
      "Epoch:  928|steps:   60|Train Avg Loss: 0.0201 |Test Loss: 1.4354|lr = 0.00220\n",
      "Epoch:  929|steps:   30|Train Avg Loss: 0.0263 |Test Loss: 1.4579|lr = 0.00220\n",
      "Epoch:  929|steps:   60|Train Avg Loss: 0.0308 |Test Loss: 1.5598|lr = 0.00220\n",
      "Epoch:  930|steps:   30|Train Avg Loss: 0.0239 |Test Loss: 1.4757|lr = 0.00220\n",
      "Epoch:  930|steps:   60|Train Avg Loss: 0.0258 |Test Loss: 1.5279|lr = 0.00220\n",
      "Epoch:  931|steps:   30|Train Avg Loss: 0.0251 |Test Loss: 1.5383|lr = 0.00220\n",
      "Epoch:  931|steps:   60|Train Avg Loss: 0.0231 |Test Loss: 1.5195|lr = 0.00220\n",
      "Epoch:  932|steps:   30|Train Avg Loss: 0.0250 |Test Loss: 1.5017|lr = 0.00220\n",
      "Epoch:  932|steps:   60|Train Avg Loss: 0.0212 |Test Loss: 1.4646|lr = 0.00220\n",
      "Epoch:  933|steps:   30|Train Avg Loss: 0.0231 |Test Loss: 1.5074|lr = 0.00220\n",
      "Epoch:  933|steps:   60|Train Avg Loss: 0.0219 |Test Loss: 1.4661|lr = 0.00220\n",
      "Epoch:  934|steps:   30|Train Avg Loss: 0.0221 |Test Loss: 1.5200|lr = 0.00220\n",
      "Epoch:  934|steps:   60|Train Avg Loss: 0.0221 |Test Loss: 1.4920|lr = 0.00220\n",
      "Epoch:  935|steps:   30|Train Avg Loss: 0.0324 |Test Loss: 1.5194|lr = 0.00220\n",
      "Epoch:  935|steps:   60|Train Avg Loss: 0.0248 |Test Loss: 1.5572|lr = 0.00220\n",
      "Epoch:  936|steps:   30|Train Avg Loss: 0.0335 |Test Loss: 1.5632|lr = 0.00220\n",
      "Epoch:  936|steps:   60|Train Avg Loss: 0.0583 |Test Loss: 1.5380|lr = 0.00220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  937|steps:   30|Train Avg Loss: 0.0904 |Test Loss: 1.4711|lr = 0.00220\n",
      "Epoch:  937|steps:   60|Train Avg Loss: 0.0580 |Test Loss: 1.5579|lr = 0.00220\n",
      "Epoch:  938|steps:   30|Train Avg Loss: 0.0505 |Test Loss: 1.4561|lr = 0.00215\n",
      "Epoch:  938|steps:   60|Train Avg Loss: 0.0686 |Test Loss: 1.4240|lr = 0.00215\n",
      "Epoch:  939|steps:   30|Train Avg Loss: 0.0347 |Test Loss: 1.4521|lr = 0.00215\n",
      "Epoch:  939|steps:   60|Train Avg Loss: 0.0427 |Test Loss: 1.4883|lr = 0.00215\n",
      "Epoch:  940|steps:   30|Train Avg Loss: 0.0362 |Test Loss: 1.5317|lr = 0.00215\n",
      "Epoch:  940|steps:   60|Train Avg Loss: 0.0311 |Test Loss: 1.4876|lr = 0.00215\n",
      "Epoch:  941|steps:   30|Train Avg Loss: 0.0349 |Test Loss: 1.4848|lr = 0.00215\n",
      "Epoch:  941|steps:   60|Train Avg Loss: 0.0255 |Test Loss: 1.4733|lr = 0.00215\n",
      "Epoch:  942|steps:   30|Train Avg Loss: 0.0219 |Test Loss: 1.4852|lr = 0.00215\n",
      "Epoch:  942|steps:   60|Train Avg Loss: 0.0255 |Test Loss: 1.5381|lr = 0.00215\n",
      "Epoch:  943|steps:   30|Train Avg Loss: 0.0226 |Test Loss: 1.4819|lr = 0.00215\n",
      "Epoch:  943|steps:   60|Train Avg Loss: 0.0276 |Test Loss: 1.5077|lr = 0.00215\n",
      "Epoch:  944|steps:   30|Train Avg Loss: 0.0226 |Test Loss: 1.4698|lr = 0.00215\n",
      "Epoch:  944|steps:   60|Train Avg Loss: 0.0238 |Test Loss: 1.4509|lr = 0.00215\n",
      "Epoch:  945|steps:   30|Train Avg Loss: 0.0247 |Test Loss: 1.4173|lr = 0.00215\n",
      "Epoch:  945|steps:   60|Train Avg Loss: 0.0227 |Test Loss: 1.5017|lr = 0.00215\n",
      "Epoch:  946|steps:   30|Train Avg Loss: 0.0199 |Test Loss: 1.5154|lr = 0.00215\n",
      "Epoch:  946|steps:   60|Train Avg Loss: 0.0297 |Test Loss: 1.5159|lr = 0.00215\n",
      "Epoch:  947|steps:   30|Train Avg Loss: 0.0154 |Test Loss: 1.5096|lr = 0.00215\n",
      "Epoch:  947|steps:   60|Train Avg Loss: 0.0240 |Test Loss: 1.4655|lr = 0.00215\n",
      "Epoch:  948|steps:   30|Train Avg Loss: 0.0169 |Test Loss: 1.5479|lr = 0.00215\n",
      "Epoch:  948|steps:   60|Train Avg Loss: 0.0238 |Test Loss: 1.4930|lr = 0.00215\n",
      "Epoch:  949|steps:   30|Train Avg Loss: 0.0222 |Test Loss: 1.5164|lr = 0.00211\n",
      "Epoch:  949|steps:   60|Train Avg Loss: 0.0235 |Test Loss: 1.4331|lr = 0.00211\n",
      "Epoch:  950|steps:   30|Train Avg Loss: 0.0217 |Test Loss: 1.4527|lr = 0.00211\n",
      "Epoch:  950|steps:   60|Train Avg Loss: 0.0201 |Test Loss: 1.4601|lr = 0.00211\n",
      "Epoch:  951|steps:   30|Train Avg Loss: 0.0227 |Test Loss: 1.4419|lr = 0.00211\n",
      "Epoch:  951|steps:   60|Train Avg Loss: 0.0190 |Test Loss: 1.4582|lr = 0.00211\n",
      "Epoch:  952|steps:   30|Train Avg Loss: 0.0181 |Test Loss: 1.4220|lr = 0.00211\n",
      "Epoch:  952|steps:   60|Train Avg Loss: 0.0257 |Test Loss: 1.4543|lr = 0.00211\n",
      "Epoch:  953|steps:   30|Train Avg Loss: 0.0205 |Test Loss: 1.4940|lr = 0.00211\n",
      "Epoch:  953|steps:   60|Train Avg Loss: 0.0218 |Test Loss: 1.4377|lr = 0.00211\n",
      "Epoch:  954|steps:   30|Train Avg Loss: 0.0223 |Test Loss: 1.5162|lr = 0.00211\n",
      "Epoch:  954|steps:   60|Train Avg Loss: 0.0199 |Test Loss: 1.4835|lr = 0.00211\n",
      "Epoch:  955|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.5407|lr = 0.00211\n",
      "Epoch:  955|steps:   60|Train Avg Loss: 0.0196 |Test Loss: 1.4414|lr = 0.00211\n",
      "Epoch:  956|steps:   30|Train Avg Loss: 0.0233 |Test Loss: 1.4516|lr = 0.00211\n",
      "Epoch:  956|steps:   60|Train Avg Loss: 0.0248 |Test Loss: 1.4674|lr = 0.00211\n",
      "Epoch:  957|steps:   30|Train Avg Loss: 0.0258 |Test Loss: 1.4594|lr = 0.00211\n",
      "Epoch:  957|steps:   60|Train Avg Loss: 0.0294 |Test Loss: 1.4644|lr = 0.00211\n",
      "Epoch:  958|steps:   30|Train Avg Loss: 0.0313 |Test Loss: 1.5312|lr = 0.00211\n",
      "Epoch:  958|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 1.5779|lr = 0.00211\n",
      "Epoch:  959|steps:   30|Train Avg Loss: 0.0323 |Test Loss: 1.5356|lr = 0.00211\n",
      "Epoch:  959|steps:   60|Train Avg Loss: 0.0350 |Test Loss: 1.5434|lr = 0.00211\n",
      "Epoch:  960|steps:   30|Train Avg Loss: 0.0258 |Test Loss: 1.4424|lr = 0.00211\n",
      "Epoch:  960|steps:   60|Train Avg Loss: 0.0255 |Test Loss: 1.5576|lr = 0.00211\n",
      "Epoch:  961|steps:   30|Train Avg Loss: 0.0336 |Test Loss: 1.4883|lr = 0.00211\n",
      "Epoch:  961|steps:   60|Train Avg Loss: 0.0266 |Test Loss: 1.4998|lr = 0.00211\n",
      "Epoch:  962|steps:   30|Train Avg Loss: 0.0174 |Test Loss: 1.5139|lr = 0.00211\n",
      "Epoch:  962|steps:   60|Train Avg Loss: 0.0290 |Test Loss: 1.5212|lr = 0.00211\n",
      "Epoch:  963|steps:   30|Train Avg Loss: 0.0219 |Test Loss: 1.4628|lr = 0.00211\n",
      "Epoch:  963|steps:   60|Train Avg Loss: 0.0265 |Test Loss: 1.4627|lr = 0.00211\n",
      "Epoch:  964|steps:   30|Train Avg Loss: 0.0280 |Test Loss: 1.5251|lr = 0.00211\n",
      "Epoch:  964|steps:   60|Train Avg Loss: 0.0439 |Test Loss: 1.4581|lr = 0.00211\n",
      "Epoch:  965|steps:   30|Train Avg Loss: 0.0267 |Test Loss: 1.4848|lr = 0.00211\n",
      "Epoch:  965|steps:   60|Train Avg Loss: 0.0274 |Test Loss: 1.4660|lr = 0.00211\n",
      "Epoch:  966|steps:   30|Train Avg Loss: 0.0235 |Test Loss: 1.5327|lr = 0.00211\n",
      "Epoch:  966|steps:   60|Train Avg Loss: 0.0209 |Test Loss: 1.4880|lr = 0.00211\n",
      "Epoch:  967|steps:   30|Train Avg Loss: 0.0277 |Test Loss: 1.5116|lr = 0.00211\n",
      "Epoch:  967|steps:   60|Train Avg Loss: 0.0231 |Test Loss: 1.5095|lr = 0.00211\n",
      "Epoch:  968|steps:   30|Train Avg Loss: 0.0297 |Test Loss: 1.5194|lr = 0.00211\n",
      "Epoch:  968|steps:   60|Train Avg Loss: 0.0266 |Test Loss: 1.5736|lr = 0.00211\n",
      "Epoch:  969|steps:   30|Train Avg Loss: 0.0281 |Test Loss: 1.4871|lr = 0.00211\n",
      "Epoch:  969|steps:   60|Train Avg Loss: 0.0256 |Test Loss: 1.5029|lr = 0.00211\n",
      "Epoch:  970|steps:   30|Train Avg Loss: 0.0328 |Test Loss: 1.5967|lr = 0.00207\n",
      "Epoch:  970|steps:   60|Train Avg Loss: 0.0881 |Test Loss: 1.6526|lr = 0.00207\n",
      "Epoch:  971|steps:   30|Train Avg Loss: 0.0605 |Test Loss: 1.5356|lr = 0.00207\n",
      "Epoch:  971|steps:   60|Train Avg Loss: 0.0780 |Test Loss: 1.5088|lr = 0.00207\n",
      "Epoch:  972|steps:   30|Train Avg Loss: 0.0344 |Test Loss: 1.6404|lr = 0.00207\n",
      "Epoch:  972|steps:   60|Train Avg Loss: 0.0346 |Test Loss: 1.6080|lr = 0.00207\n",
      "Epoch:  973|steps:   30|Train Avg Loss: 0.0382 |Test Loss: 1.5916|lr = 0.00207\n",
      "Epoch:  973|steps:   60|Train Avg Loss: 0.0524 |Test Loss: 1.6617|lr = 0.00207\n",
      "Epoch:  974|steps:   30|Train Avg Loss: 0.0324 |Test Loss: 1.6065|lr = 0.00207\n",
      "Epoch:  974|steps:   60|Train Avg Loss: 0.0278 |Test Loss: 1.5842|lr = 0.00207\n",
      "Epoch:  975|steps:   30|Train Avg Loss: 0.0196 |Test Loss: 1.5565|lr = 0.00207\n",
      "Epoch:  975|steps:   60|Train Avg Loss: 0.0241 |Test Loss: 1.5162|lr = 0.00207\n",
      "Epoch:  976|steps:   30|Train Avg Loss: 0.0231 |Test Loss: 1.5529|lr = 0.00207\n",
      "Epoch:  976|steps:   60|Train Avg Loss: 0.0220 |Test Loss: 1.5253|lr = 0.00207\n",
      "Epoch:  977|steps:   30|Train Avg Loss: 0.0155 |Test Loss: 1.5323|lr = 0.00207\n",
      "Epoch:  977|steps:   60|Train Avg Loss: 0.0251 |Test Loss: 1.5163|lr = 0.00207\n",
      "Epoch:  978|steps:   30|Train Avg Loss: 0.0274 |Test Loss: 1.5188|lr = 0.00207\n",
      "Epoch:  978|steps:   60|Train Avg Loss: 0.0207 |Test Loss: 1.5346|lr = 0.00207\n",
      "Epoch:  979|steps:   30|Train Avg Loss: 0.0214 |Test Loss: 1.5345|lr = 0.00207\n",
      "Epoch:  979|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.4542|lr = 0.00207\n",
      "Epoch:  980|steps:   30|Train Avg Loss: 0.0179 |Test Loss: 1.4459|lr = 0.00207\n",
      "Epoch:  980|steps:   60|Train Avg Loss: 0.0225 |Test Loss: 1.4301|lr = 0.00207\n",
      "Epoch:  981|steps:   30|Train Avg Loss: 0.0155 |Test Loss: 1.4657|lr = 0.00203\n",
      "Epoch:  981|steps:   60|Train Avg Loss: 0.0185 |Test Loss: 1.4951|lr = 0.00203\n",
      "Epoch:  982|steps:   30|Train Avg Loss: 0.0171 |Test Loss: 1.5225|lr = 0.00203\n",
      "Epoch:  982|steps:   60|Train Avg Loss: 0.0207 |Test Loss: 1.4944|lr = 0.00203\n",
      "Epoch:  983|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.5199|lr = 0.00203\n",
      "Epoch:  983|steps:   60|Train Avg Loss: 0.0215 |Test Loss: 1.5210|lr = 0.00203\n",
      "Epoch:  984|steps:   30|Train Avg Loss: 0.0244 |Test Loss: 1.4957|lr = 0.00203\n",
      "Epoch:  984|steps:   60|Train Avg Loss: 0.0189 |Test Loss: 1.5373|lr = 0.00203\n",
      "Epoch:  985|steps:   30|Train Avg Loss: 0.0243 |Test Loss: 1.5105|lr = 0.00203\n",
      "Epoch:  985|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.4863|lr = 0.00203\n",
      "Epoch:  986|steps:   30|Train Avg Loss: 0.0190 |Test Loss: 1.5417|lr = 0.00203\n",
      "Epoch:  986|steps:   60|Train Avg Loss: 0.0213 |Test Loss: 1.4905|lr = 0.00203\n",
      "Epoch:  987|steps:   30|Train Avg Loss: 0.0239 |Test Loss: 1.5115|lr = 0.00203\n",
      "Epoch:  987|steps:   60|Train Avg Loss: 0.0215 |Test Loss: 1.4667|lr = 0.00203\n",
      "Epoch:  988|steps:   30|Train Avg Loss: 0.0191 |Test Loss: 1.5284|lr = 0.00203\n",
      "Epoch:  988|steps:   60|Train Avg Loss: 0.0259 |Test Loss: 1.5364|lr = 0.00203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  989|steps:   30|Train Avg Loss: 0.0292 |Test Loss: 1.4718|lr = 0.00203\n",
      "Epoch:  989|steps:   60|Train Avg Loss: 0.0208 |Test Loss: 1.5662|lr = 0.00203\n",
      "Epoch:  990|steps:   30|Train Avg Loss: 0.0170 |Test Loss: 1.5179|lr = 0.00203\n",
      "Epoch:  990|steps:   60|Train Avg Loss: 0.0258 |Test Loss: 1.4777|lr = 0.00203\n",
      "Epoch:  991|steps:   30|Train Avg Loss: 0.0270 |Test Loss: 1.4603|lr = 0.00203\n",
      "Epoch:  991|steps:   60|Train Avg Loss: 0.0176 |Test Loss: 1.5239|lr = 0.00203\n",
      "Epoch:  992|steps:   30|Train Avg Loss: 0.0221 |Test Loss: 1.4667|lr = 0.00199\n",
      "Epoch:  992|steps:   60|Train Avg Loss: 0.0206 |Test Loss: 1.4675|lr = 0.00199\n",
      "Epoch:  993|steps:   30|Train Avg Loss: 0.0191 |Test Loss: 1.4528|lr = 0.00199\n",
      "Epoch:  993|steps:   60|Train Avg Loss: 0.0233 |Test Loss: 1.4155|lr = 0.00199\n",
      "Epoch:  994|steps:   30|Train Avg Loss: 0.0170 |Test Loss: 1.4886|lr = 0.00199\n",
      "Epoch:  994|steps:   60|Train Avg Loss: 0.0184 |Test Loss: 1.4850|lr = 0.00199\n",
      "Epoch:  995|steps:   30|Train Avg Loss: 0.0208 |Test Loss: 1.4976|lr = 0.00199\n",
      "Epoch:  995|steps:   60|Train Avg Loss: 0.0191 |Test Loss: 1.5072|lr = 0.00199\n",
      "Epoch:  996|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.5045|lr = 0.00199\n",
      "Epoch:  996|steps:   60|Train Avg Loss: 0.0375 |Test Loss: 1.5439|lr = 0.00199\n",
      "Epoch:  997|steps:   30|Train Avg Loss: 0.0303 |Test Loss: 1.5005|lr = 0.00199\n",
      "Epoch:  997|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 1.5034|lr = 0.00199\n",
      "Epoch:  998|steps:   30|Train Avg Loss: 0.0210 |Test Loss: 1.4859|lr = 0.00199\n",
      "Epoch:  998|steps:   60|Train Avg Loss: 0.0263 |Test Loss: 1.5138|lr = 0.00199\n",
      "Epoch:  999|steps:   30|Train Avg Loss: 0.0222 |Test Loss: 1.4883|lr = 0.00199\n",
      "Epoch:  999|steps:   60|Train Avg Loss: 0.0209 |Test Loss: 1.4753|lr = 0.00199\n",
      "Epoch: 1000|steps:   30|Train Avg Loss: 0.0242 |Test Loss: 1.4325|lr = 0.00199\n",
      "Epoch: 1000|steps:   60|Train Avg Loss: 0.0215 |Test Loss: 1.4613|lr = 0.00199\n",
      "Epoch: 1001|steps:   30|Train Avg Loss: 0.0234 |Test Loss: 1.4845|lr = 0.00199\n",
      "Epoch: 1001|steps:   60|Train Avg Loss: 0.0458 |Test Loss: 1.5661|lr = 0.00199\n",
      "Epoch: 1002|steps:   30|Train Avg Loss: 0.0466 |Test Loss: 1.4569|lr = 0.00199\n",
      "Epoch: 1002|steps:   60|Train Avg Loss: 0.0829 |Test Loss: 1.5159|lr = 0.00199\n",
      "Epoch: 1003|steps:   30|Train Avg Loss: 0.0674 |Test Loss: 1.5712|lr = 0.00195\n",
      "Epoch: 1003|steps:   60|Train Avg Loss: 0.0880 |Test Loss: 1.4314|lr = 0.00195\n",
      "Epoch: 1004|steps:   30|Train Avg Loss: 0.0516 |Test Loss: 1.4604|lr = 0.00195\n",
      "Epoch: 1004|steps:   60|Train Avg Loss: 0.0327 |Test Loss: 1.4801|lr = 0.00195\n",
      "Epoch: 1005|steps:   30|Train Avg Loss: 0.0289 |Test Loss: 1.4829|lr = 0.00195\n",
      "Epoch: 1005|steps:   60|Train Avg Loss: 0.0351 |Test Loss: 1.4246|lr = 0.00195\n",
      "Epoch: 1006|steps:   30|Train Avg Loss: 0.0203 |Test Loss: 1.4866|lr = 0.00195\n",
      "Epoch: 1006|steps:   60|Train Avg Loss: 0.0207 |Test Loss: 1.5117|lr = 0.00195\n",
      "Epoch: 1007|steps:   30|Train Avg Loss: 0.0193 |Test Loss: 1.4762|lr = 0.00195\n",
      "Epoch: 1007|steps:   60|Train Avg Loss: 0.0164 |Test Loss: 1.4867|lr = 0.00195\n",
      "Epoch: 1008|steps:   30|Train Avg Loss: 0.0180 |Test Loss: 1.5095|lr = 0.00195\n",
      "Epoch: 1008|steps:   60|Train Avg Loss: 0.0167 |Test Loss: 1.4750|lr = 0.00195\n",
      "Epoch: 1009|steps:   30|Train Avg Loss: 0.0183 |Test Loss: 1.4909|lr = 0.00195\n",
      "Epoch: 1009|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.4519|lr = 0.00195\n",
      "Epoch: 1010|steps:   30|Train Avg Loss: 0.0187 |Test Loss: 1.4379|lr = 0.00195\n",
      "Epoch: 1010|steps:   60|Train Avg Loss: 0.0179 |Test Loss: 1.4807|lr = 0.00195\n",
      "Epoch: 1011|steps:   30|Train Avg Loss: 0.0164 |Test Loss: 1.4239|lr = 0.00195\n",
      "Epoch: 1011|steps:   60|Train Avg Loss: 0.0195 |Test Loss: 1.5089|lr = 0.00195\n",
      "Epoch: 1012|steps:   30|Train Avg Loss: 0.0185 |Test Loss: 1.4370|lr = 0.00195\n",
      "Epoch: 1012|steps:   60|Train Avg Loss: 0.0182 |Test Loss: 1.4170|lr = 0.00195\n",
      "Epoch: 1013|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.4709|lr = 0.00195\n",
      "Epoch: 1013|steps:   60|Train Avg Loss: 0.0155 |Test Loss: 1.4849|lr = 0.00195\n",
      "Epoch: 1014|steps:   30|Train Avg Loss: 0.0165 |Test Loss: 1.4670|lr = 0.00191\n",
      "Epoch: 1014|steps:   60|Train Avg Loss: 0.0204 |Test Loss: 1.4963|lr = 0.00191\n",
      "Epoch: 1015|steps:   30|Train Avg Loss: 0.0160 |Test Loss: 1.4400|lr = 0.00191\n",
      "Epoch: 1015|steps:   60|Train Avg Loss: 0.0210 |Test Loss: 1.4707|lr = 0.00191\n",
      "Epoch: 1016|steps:   30|Train Avg Loss: 0.0144 |Test Loss: 1.4861|lr = 0.00191\n",
      "Epoch: 1016|steps:   60|Train Avg Loss: 0.0185 |Test Loss: 1.4800|lr = 0.00191\n",
      "Epoch: 1017|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.4838|lr = 0.00191\n",
      "Epoch: 1017|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.5166|lr = 0.00191\n",
      "Epoch: 1018|steps:   30|Train Avg Loss: 0.0211 |Test Loss: 1.4684|lr = 0.00191\n",
      "Epoch: 1018|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.3804|lr = 0.00191\n",
      "Epoch: 1019|steps:   30|Train Avg Loss: 0.0206 |Test Loss: 1.5000|lr = 0.00191\n",
      "Epoch: 1019|steps:   60|Train Avg Loss: 0.0206 |Test Loss: 1.5004|lr = 0.00191\n",
      "Epoch: 1020|steps:   30|Train Avg Loss: 0.0190 |Test Loss: 1.5090|lr = 0.00191\n",
      "Epoch: 1020|steps:   60|Train Avg Loss: 0.0215 |Test Loss: 1.5324|lr = 0.00191\n",
      "Epoch: 1021|steps:   30|Train Avg Loss: 0.0162 |Test Loss: 1.4754|lr = 0.00191\n",
      "Epoch: 1021|steps:   60|Train Avg Loss: 0.0256 |Test Loss: 1.4703|lr = 0.00191\n",
      "Epoch: 1022|steps:   30|Train Avg Loss: 0.0243 |Test Loss: 1.5343|lr = 0.00191\n",
      "Epoch: 1022|steps:   60|Train Avg Loss: 0.0210 |Test Loss: 1.4675|lr = 0.00191\n",
      "Epoch: 1023|steps:   30|Train Avg Loss: 0.0221 |Test Loss: 1.4850|lr = 0.00191\n",
      "Epoch: 1023|steps:   60|Train Avg Loss: 0.0195 |Test Loss: 1.4091|lr = 0.00191\n",
      "Epoch: 1024|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 1.4813|lr = 0.00191\n",
      "Epoch: 1024|steps:   60|Train Avg Loss: 0.0246 |Test Loss: 1.4960|lr = 0.00191\n",
      "Epoch: 1025|steps:   30|Train Avg Loss: 0.0220 |Test Loss: 1.5177|lr = 0.00187\n",
      "Epoch: 1025|steps:   60|Train Avg Loss: 0.0203 |Test Loss: 1.4709|lr = 0.00187\n",
      "Epoch: 1026|steps:   30|Train Avg Loss: 0.0209 |Test Loss: 1.4903|lr = 0.00187\n",
      "Epoch: 1026|steps:   60|Train Avg Loss: 0.0196 |Test Loss: 1.4839|lr = 0.00187\n",
      "Epoch: 1027|steps:   30|Train Avg Loss: 0.0231 |Test Loss: 1.5687|lr = 0.00187\n",
      "Epoch: 1027|steps:   60|Train Avg Loss: 0.0232 |Test Loss: 1.5060|lr = 0.00187\n",
      "Epoch: 1028|steps:   30|Train Avg Loss: 0.0194 |Test Loss: 1.5275|lr = 0.00187\n",
      "Epoch: 1028|steps:   60|Train Avg Loss: 0.0203 |Test Loss: 1.4805|lr = 0.00187\n",
      "Epoch: 1029|steps:   30|Train Avg Loss: 0.0206 |Test Loss: 1.4648|lr = 0.00187\n",
      "Epoch: 1029|steps:   60|Train Avg Loss: 0.0213 |Test Loss: 1.4503|lr = 0.00187\n",
      "Epoch: 1030|steps:   30|Train Avg Loss: 0.0211 |Test Loss: 1.4194|lr = 0.00187\n",
      "Epoch: 1030|steps:   60|Train Avg Loss: 0.0275 |Test Loss: 1.5054|lr = 0.00187\n",
      "Epoch: 1031|steps:   30|Train Avg Loss: 0.0246 |Test Loss: 1.4543|lr = 0.00187\n",
      "Epoch: 1031|steps:   60|Train Avg Loss: 0.0207 |Test Loss: 1.4682|lr = 0.00187\n",
      "Epoch: 1032|steps:   30|Train Avg Loss: 0.0305 |Test Loss: 1.5551|lr = 0.00187\n",
      "Epoch: 1032|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 1.4681|lr = 0.00187\n",
      "Epoch: 1033|steps:   30|Train Avg Loss: 0.0382 |Test Loss: 1.4769|lr = 0.00187\n",
      "Epoch: 1033|steps:   60|Train Avg Loss: 0.0267 |Test Loss: 1.5128|lr = 0.00187\n",
      "Epoch: 1034|steps:   30|Train Avg Loss: 0.0415 |Test Loss: 1.4969|lr = 0.00187\n",
      "Epoch: 1034|steps:   60|Train Avg Loss: 0.0374 |Test Loss: 1.5536|lr = 0.00187\n",
      "Epoch: 1035|steps:   30|Train Avg Loss: 0.0319 |Test Loss: 1.5273|lr = 0.00187\n",
      "Epoch: 1035|steps:   60|Train Avg Loss: 0.0636 |Test Loss: 1.5258|lr = 0.00187\n",
      "Epoch: 1036|steps:   30|Train Avg Loss: 0.0464 |Test Loss: 1.4660|lr = 0.00183\n",
      "Epoch: 1036|steps:   60|Train Avg Loss: 0.0584 |Test Loss: 1.5370|lr = 0.00183\n",
      "Epoch: 1037|steps:   30|Train Avg Loss: 0.0378 |Test Loss: 1.4435|lr = 0.00183\n",
      "Epoch: 1037|steps:   60|Train Avg Loss: 0.0272 |Test Loss: 1.4246|lr = 0.00183\n",
      "Epoch: 1038|steps:   30|Train Avg Loss: 0.0216 |Test Loss: 1.4154|lr = 0.00183\n",
      "Epoch: 1038|steps:   60|Train Avg Loss: 0.0187 |Test Loss: 1.4570|lr = 0.00183\n",
      "Epoch: 1039|steps:   30|Train Avg Loss: 0.0204 |Test Loss: 1.5208|lr = 0.00183\n",
      "Epoch: 1039|steps:   60|Train Avg Loss: 0.0225 |Test Loss: 1.4802|lr = 0.00183\n",
      "Epoch: 1040|steps:   30|Train Avg Loss: 0.0177 |Test Loss: 1.5007|lr = 0.00183\n",
      "Epoch: 1040|steps:   60|Train Avg Loss: 0.0208 |Test Loss: 1.4644|lr = 0.00183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1041|steps:   30|Train Avg Loss: 0.0216 |Test Loss: 1.4745|lr = 0.00183\n",
      "Epoch: 1041|steps:   60|Train Avg Loss: 0.0194 |Test Loss: 1.5304|lr = 0.00183\n",
      "Epoch: 1042|steps:   30|Train Avg Loss: 0.0419 |Test Loss: 1.5755|lr = 0.00183\n",
      "Epoch: 1042|steps:   60|Train Avg Loss: 0.0279 |Test Loss: 1.5654|lr = 0.00183\n",
      "Epoch: 1043|steps:   30|Train Avg Loss: 0.0378 |Test Loss: 1.4609|lr = 0.00183\n",
      "Epoch: 1043|steps:   60|Train Avg Loss: 0.0293 |Test Loss: 1.5428|lr = 0.00183\n",
      "Epoch: 1044|steps:   30|Train Avg Loss: 0.0190 |Test Loss: 1.4616|lr = 0.00183\n",
      "Epoch: 1044|steps:   60|Train Avg Loss: 0.0198 |Test Loss: 1.4800|lr = 0.00183\n",
      "Epoch: 1045|steps:   30|Train Avg Loss: 0.0223 |Test Loss: 1.4659|lr = 0.00183\n",
      "Epoch: 1045|steps:   60|Train Avg Loss: 0.0180 |Test Loss: 1.4842|lr = 0.00183\n",
      "Epoch: 1046|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.4905|lr = 0.00183\n",
      "Epoch: 1046|steps:   60|Train Avg Loss: 0.0202 |Test Loss: 1.5048|lr = 0.00183\n",
      "Epoch: 1047|steps:   30|Train Avg Loss: 0.0203 |Test Loss: 1.5297|lr = 0.00180\n",
      "Epoch: 1047|steps:   60|Train Avg Loss: 0.0222 |Test Loss: 1.4566|lr = 0.00180\n",
      "Epoch: 1048|steps:   30|Train Avg Loss: 0.0252 |Test Loss: 1.4748|lr = 0.00180\n",
      "Epoch: 1048|steps:   60|Train Avg Loss: 0.0180 |Test Loss: 1.5567|lr = 0.00180\n",
      "Epoch: 1049|steps:   30|Train Avg Loss: 0.0154 |Test Loss: 1.4926|lr = 0.00180\n",
      "Epoch: 1049|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.4858|lr = 0.00180\n",
      "Epoch: 1050|steps:   30|Train Avg Loss: 0.0170 |Test Loss: 1.5152|lr = 0.00180\n",
      "Epoch: 1050|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.4458|lr = 0.00180\n",
      "Epoch: 1051|steps:   30|Train Avg Loss: 0.0161 |Test Loss: 1.5118|lr = 0.00180\n",
      "Epoch: 1051|steps:   60|Train Avg Loss: 0.0194 |Test Loss: 1.4411|lr = 0.00180\n",
      "Epoch: 1052|steps:   30|Train Avg Loss: 0.0206 |Test Loss: 1.4819|lr = 0.00180\n",
      "Epoch: 1052|steps:   60|Train Avg Loss: 0.0152 |Test Loss: 1.4719|lr = 0.00180\n",
      "Epoch: 1053|steps:   30|Train Avg Loss: 0.0200 |Test Loss: 1.5194|lr = 0.00180\n",
      "Epoch: 1053|steps:   60|Train Avg Loss: 0.0212 |Test Loss: 1.4570|lr = 0.00180\n",
      "Epoch: 1054|steps:   30|Train Avg Loss: 0.0187 |Test Loss: 1.4526|lr = 0.00180\n",
      "Epoch: 1054|steps:   60|Train Avg Loss: 0.0224 |Test Loss: 1.5224|lr = 0.00180\n",
      "Epoch: 1055|steps:   30|Train Avg Loss: 0.0218 |Test Loss: 1.5557|lr = 0.00180\n",
      "Epoch: 1055|steps:   60|Train Avg Loss: 0.0186 |Test Loss: 1.5166|lr = 0.00180\n",
      "Epoch: 1056|steps:   30|Train Avg Loss: 0.0194 |Test Loss: 1.5536|lr = 0.00180\n",
      "Epoch: 1056|steps:   60|Train Avg Loss: 0.0205 |Test Loss: 1.5117|lr = 0.00180\n",
      "Epoch: 1057|steps:   30|Train Avg Loss: 0.0212 |Test Loss: 1.4543|lr = 0.00180\n",
      "Epoch: 1057|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.4813|lr = 0.00180\n",
      "Epoch: 1058|steps:   30|Train Avg Loss: 0.0161 |Test Loss: 1.4774|lr = 0.00176\n",
      "Epoch: 1058|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.5133|lr = 0.00176\n",
      "Epoch: 1059|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.5180|lr = 0.00176\n",
      "Epoch: 1059|steps:   60|Train Avg Loss: 0.0193 |Test Loss: 1.3948|lr = 0.00176\n",
      "Epoch: 1060|steps:   30|Train Avg Loss: 0.0167 |Test Loss: 1.4729|lr = 0.00176\n",
      "Epoch: 1060|steps:   60|Train Avg Loss: 0.0219 |Test Loss: 1.4494|lr = 0.00176\n",
      "Epoch: 1061|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.4985|lr = 0.00176\n",
      "Epoch: 1061|steps:   60|Train Avg Loss: 0.0229 |Test Loss: 1.5326|lr = 0.00176\n",
      "Epoch: 1062|steps:   30|Train Avg Loss: 0.0252 |Test Loss: 1.4790|lr = 0.00176\n",
      "Epoch: 1062|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 1.4318|lr = 0.00176\n",
      "Epoch: 1063|steps:   30|Train Avg Loss: 0.0300 |Test Loss: 1.5104|lr = 0.00176\n",
      "Epoch: 1063|steps:   60|Train Avg Loss: 0.0260 |Test Loss: 1.5845|lr = 0.00176\n",
      "Epoch: 1064|steps:   30|Train Avg Loss: 0.0244 |Test Loss: 1.4971|lr = 0.00176\n",
      "Epoch: 1064|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 1.5098|lr = 0.00176\n",
      "Epoch: 1065|steps:   30|Train Avg Loss: 0.0177 |Test Loss: 1.4224|lr = 0.00176\n",
      "Epoch: 1065|steps:   60|Train Avg Loss: 0.0257 |Test Loss: 1.5368|lr = 0.00176\n",
      "Epoch: 1066|steps:   30|Train Avg Loss: 0.0236 |Test Loss: 1.5017|lr = 0.00176\n",
      "Epoch: 1066|steps:   60|Train Avg Loss: 0.0198 |Test Loss: 1.5290|lr = 0.00176\n",
      "Epoch: 1067|steps:   30|Train Avg Loss: 0.0283 |Test Loss: 1.5907|lr = 0.00176\n",
      "Epoch: 1067|steps:   60|Train Avg Loss: 0.0217 |Test Loss: 1.5439|lr = 0.00176\n",
      "Epoch: 1068|steps:   30|Train Avg Loss: 0.0204 |Test Loss: 1.5036|lr = 0.00176\n",
      "Epoch: 1068|steps:   60|Train Avg Loss: 0.0194 |Test Loss: 1.5691|lr = 0.00176\n",
      "Epoch: 1069|steps:   30|Train Avg Loss: 0.0183 |Test Loss: 1.5215|lr = 0.00172\n",
      "Epoch: 1069|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.5484|lr = 0.00172\n",
      "Epoch: 1070|steps:   30|Train Avg Loss: 0.0173 |Test Loss: 1.5768|lr = 0.00172\n",
      "Epoch: 1070|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.5656|lr = 0.00172\n",
      "Epoch: 1071|steps:   30|Train Avg Loss: 0.0227 |Test Loss: 1.5374|lr = 0.00172\n",
      "Epoch: 1071|steps:   60|Train Avg Loss: 0.0202 |Test Loss: 1.5618|lr = 0.00172\n",
      "Epoch: 1072|steps:   30|Train Avg Loss: 0.0221 |Test Loss: 1.6004|lr = 0.00172\n",
      "Epoch: 1072|steps:   60|Train Avg Loss: 0.0232 |Test Loss: 1.5285|lr = 0.00172\n",
      "Epoch: 1073|steps:   30|Train Avg Loss: 0.0221 |Test Loss: 1.5465|lr = 0.00172\n",
      "Epoch: 1073|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 1.5135|lr = 0.00172\n",
      "Epoch: 1074|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.4742|lr = 0.00172\n",
      "Epoch: 1074|steps:   60|Train Avg Loss: 0.0226 |Test Loss: 1.5698|lr = 0.00172\n",
      "Epoch: 1075|steps:   30|Train Avg Loss: 0.0251 |Test Loss: 1.5935|lr = 0.00172\n",
      "Epoch: 1075|steps:   60|Train Avg Loss: 0.0229 |Test Loss: 1.5215|lr = 0.00172\n",
      "Epoch: 1076|steps:   30|Train Avg Loss: 0.0253 |Test Loss: 1.5587|lr = 0.00172\n",
      "Epoch: 1076|steps:   60|Train Avg Loss: 0.0391 |Test Loss: 1.5781|lr = 0.00172\n",
      "Epoch: 1077|steps:   30|Train Avg Loss: 0.0448 |Test Loss: 1.5769|lr = 0.00172\n",
      "Epoch: 1077|steps:   60|Train Avg Loss: 0.0437 |Test Loss: 1.5204|lr = 0.00172\n",
      "Epoch: 1078|steps:   30|Train Avg Loss: 0.0394 |Test Loss: 1.5572|lr = 0.00172\n",
      "Epoch: 1078|steps:   60|Train Avg Loss: 0.0760 |Test Loss: 1.4972|lr = 0.00172\n",
      "Epoch: 1079|steps:   30|Train Avg Loss: 0.0503 |Test Loss: 1.5448|lr = 0.00172\n",
      "Epoch: 1079|steps:   60|Train Avg Loss: 0.0467 |Test Loss: 1.5007|lr = 0.00172\n",
      "Epoch: 1080|steps:   30|Train Avg Loss: 0.0337 |Test Loss: 1.5103|lr = 0.00169\n",
      "Epoch: 1080|steps:   60|Train Avg Loss: 0.0309 |Test Loss: 1.4972|lr = 0.00169\n",
      "Epoch: 1081|steps:   30|Train Avg Loss: 0.0311 |Test Loss: 1.4911|lr = 0.00169\n",
      "Epoch: 1081|steps:   60|Train Avg Loss: 0.0272 |Test Loss: 1.5057|lr = 0.00169\n",
      "Epoch: 1082|steps:   30|Train Avg Loss: 0.0173 |Test Loss: 1.5149|lr = 0.00169\n",
      "Epoch: 1082|steps:   60|Train Avg Loss: 0.0257 |Test Loss: 1.5022|lr = 0.00169\n",
      "Epoch: 1083|steps:   30|Train Avg Loss: 0.0220 |Test Loss: 1.4563|lr = 0.00169\n",
      "Epoch: 1083|steps:   60|Train Avg Loss: 0.0183 |Test Loss: 1.4769|lr = 0.00169\n",
      "Epoch: 1084|steps:   30|Train Avg Loss: 0.0144 |Test Loss: 1.5268|lr = 0.00169\n",
      "Epoch: 1084|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.4810|lr = 0.00169\n",
      "Epoch: 1085|steps:   30|Train Avg Loss: 0.0190 |Test Loss: 1.4716|lr = 0.00169\n",
      "Epoch: 1085|steps:   60|Train Avg Loss: 0.0191 |Test Loss: 1.4760|lr = 0.00169\n",
      "Epoch: 1086|steps:   30|Train Avg Loss: 0.0201 |Test Loss: 1.4152|lr = 0.00169\n",
      "Epoch: 1086|steps:   60|Train Avg Loss: 0.0151 |Test Loss: 1.4938|lr = 0.00169\n",
      "Epoch: 1087|steps:   30|Train Avg Loss: 0.0205 |Test Loss: 1.4772|lr = 0.00169\n",
      "Epoch: 1087|steps:   60|Train Avg Loss: 0.0173 |Test Loss: 1.5077|lr = 0.00169\n",
      "Epoch: 1088|steps:   30|Train Avg Loss: 0.0194 |Test Loss: 1.4474|lr = 0.00169\n",
      "Epoch: 1088|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.4982|lr = 0.00169\n",
      "Epoch: 1089|steps:   30|Train Avg Loss: 0.0174 |Test Loss: 1.4388|lr = 0.00169\n",
      "Epoch: 1089|steps:   60|Train Avg Loss: 0.0150 |Test Loss: 1.5110|lr = 0.00169\n",
      "Epoch: 1090|steps:   30|Train Avg Loss: 0.0164 |Test Loss: 1.4705|lr = 0.00169\n",
      "Epoch: 1090|steps:   60|Train Avg Loss: 0.0136 |Test Loss: 1.4503|lr = 0.00169\n",
      "Epoch: 1091|steps:   30|Train Avg Loss: 0.0174 |Test Loss: 1.4567|lr = 0.00166\n",
      "Epoch: 1091|steps:   60|Train Avg Loss: 0.0193 |Test Loss: 1.4952|lr = 0.00166\n",
      "Epoch: 1092|steps:   30|Train Avg Loss: 0.0146 |Test Loss: 1.5560|lr = 0.00166\n",
      "Epoch: 1092|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.4917|lr = 0.00166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1093|steps:   30|Train Avg Loss: 0.0178 |Test Loss: 1.5121|lr = 0.00166\n",
      "Epoch: 1093|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.4486|lr = 0.00166\n",
      "Epoch: 1094|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.5446|lr = 0.00166\n",
      "Epoch: 1094|steps:   60|Train Avg Loss: 0.0221 |Test Loss: 1.5071|lr = 0.00166\n",
      "Epoch: 1095|steps:   30|Train Avg Loss: 0.0197 |Test Loss: 1.4934|lr = 0.00166\n",
      "Epoch: 1095|steps:   60|Train Avg Loss: 0.0173 |Test Loss: 1.4778|lr = 0.00166\n",
      "Epoch: 1096|steps:   30|Train Avg Loss: 0.0131 |Test Loss: 1.4675|lr = 0.00166\n",
      "Epoch: 1096|steps:   60|Train Avg Loss: 0.0155 |Test Loss: 1.4790|lr = 0.00166\n",
      "Epoch: 1097|steps:   30|Train Avg Loss: 0.0143 |Test Loss: 1.4671|lr = 0.00166\n",
      "Epoch: 1097|steps:   60|Train Avg Loss: 0.0160 |Test Loss: 1.5125|lr = 0.00166\n",
      "Epoch: 1098|steps:   30|Train Avg Loss: 0.0240 |Test Loss: 1.4483|lr = 0.00166\n",
      "Epoch: 1098|steps:   60|Train Avg Loss: 0.0199 |Test Loss: 1.4878|lr = 0.00166\n",
      "Epoch: 1099|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.4965|lr = 0.00166\n",
      "Epoch: 1099|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.4460|lr = 0.00166\n",
      "Epoch: 1100|steps:   30|Train Avg Loss: 0.0196 |Test Loss: 1.4765|lr = 0.00166\n",
      "Epoch: 1100|steps:   60|Train Avg Loss: 0.0187 |Test Loss: 1.4799|lr = 0.00166\n",
      "Epoch: 1101|steps:   30|Train Avg Loss: 0.0168 |Test Loss: 1.4814|lr = 0.00166\n",
      "Epoch: 1101|steps:   60|Train Avg Loss: 0.0185 |Test Loss: 1.4038|lr = 0.00166\n",
      "Epoch: 1102|steps:   30|Train Avg Loss: 0.0241 |Test Loss: 1.5195|lr = 0.00162\n",
      "Epoch: 1102|steps:   60|Train Avg Loss: 0.0180 |Test Loss: 1.5184|lr = 0.00162\n",
      "Epoch: 1103|steps:   30|Train Avg Loss: 0.0201 |Test Loss: 1.4589|lr = 0.00162\n",
      "Epoch: 1103|steps:   60|Train Avg Loss: 0.0172 |Test Loss: 1.5303|lr = 0.00162\n",
      "Epoch: 1104|steps:   30|Train Avg Loss: 0.0210 |Test Loss: 1.4593|lr = 0.00162\n",
      "Epoch: 1104|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.4967|lr = 0.00162\n",
      "Epoch: 1105|steps:   30|Train Avg Loss: 0.0271 |Test Loss: 1.5287|lr = 0.00162\n",
      "Epoch: 1105|steps:   60|Train Avg Loss: 0.0173 |Test Loss: 1.5652|lr = 0.00162\n",
      "Epoch: 1106|steps:   30|Train Avg Loss: 0.0201 |Test Loss: 1.5262|lr = 0.00162\n",
      "Epoch: 1106|steps:   60|Train Avg Loss: 0.0306 |Test Loss: 1.5264|lr = 0.00162\n",
      "Epoch: 1107|steps:   30|Train Avg Loss: 0.0323 |Test Loss: 1.4996|lr = 0.00162\n",
      "Epoch: 1107|steps:   60|Train Avg Loss: 0.0283 |Test Loss: 1.5609|lr = 0.00162\n",
      "Epoch: 1108|steps:   30|Train Avg Loss: 0.0209 |Test Loss: 1.5539|lr = 0.00162\n",
      "Epoch: 1108|steps:   60|Train Avg Loss: 0.0225 |Test Loss: 1.5595|lr = 0.00162\n",
      "Epoch: 1109|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5496|lr = 0.00162\n",
      "Epoch: 1109|steps:   60|Train Avg Loss: 0.0199 |Test Loss: 1.5910|lr = 0.00162\n",
      "Epoch: 1110|steps:   30|Train Avg Loss: 0.0200 |Test Loss: 1.5407|lr = 0.00162\n",
      "Epoch: 1110|steps:   60|Train Avg Loss: 0.0253 |Test Loss: 1.5001|lr = 0.00162\n",
      "Epoch: 1111|steps:   30|Train Avg Loss: 0.0248 |Test Loss: 1.5610|lr = 0.00162\n",
      "Epoch: 1111|steps:   60|Train Avg Loss: 0.0267 |Test Loss: 1.5390|lr = 0.00162\n",
      "Epoch: 1112|steps:   30|Train Avg Loss: 0.0240 |Test Loss: 1.5522|lr = 0.00162\n",
      "Epoch: 1112|steps:   60|Train Avg Loss: 0.0225 |Test Loss: 1.5311|lr = 0.00162\n",
      "Epoch: 1113|steps:   30|Train Avg Loss: 0.0412 |Test Loss: 1.6008|lr = 0.00162\n",
      "Epoch: 1113|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 1.5424|lr = 0.00162\n",
      "Epoch: 1114|steps:   30|Train Avg Loss: 0.0362 |Test Loss: 1.4899|lr = 0.00162\n",
      "Epoch: 1114|steps:   60|Train Avg Loss: 0.0603 |Test Loss: 1.6055|lr = 0.00162\n",
      "Epoch: 1115|steps:   30|Train Avg Loss: 0.0597 |Test Loss: 1.6471|lr = 0.00162\n",
      "Epoch: 1115|steps:   60|Train Avg Loss: 0.0542 |Test Loss: 1.5030|lr = 0.00162\n",
      "Epoch: 1116|steps:   30|Train Avg Loss: 0.0372 |Test Loss: 1.5614|lr = 0.00162\n",
      "Epoch: 1116|steps:   60|Train Avg Loss: 0.0307 |Test Loss: 1.5574|lr = 0.00162\n",
      "Epoch: 1117|steps:   30|Train Avg Loss: 0.0205 |Test Loss: 1.5352|lr = 0.00162\n",
      "Epoch: 1117|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 1.6178|lr = 0.00162\n",
      "Epoch: 1118|steps:   30|Train Avg Loss: 0.0232 |Test Loss: 1.5559|lr = 0.00162\n",
      "Epoch: 1118|steps:   60|Train Avg Loss: 0.0192 |Test Loss: 1.5371|lr = 0.00162\n",
      "Epoch: 1119|steps:   30|Train Avg Loss: 0.0278 |Test Loss: 1.5677|lr = 0.00162\n",
      "Epoch: 1119|steps:   60|Train Avg Loss: 0.0264 |Test Loss: 1.4967|lr = 0.00162\n",
      "Epoch: 1120|steps:   30|Train Avg Loss: 0.0281 |Test Loss: 1.5510|lr = 0.00162\n",
      "Epoch: 1120|steps:   60|Train Avg Loss: 0.0189 |Test Loss: 1.5796|lr = 0.00162\n",
      "Epoch: 1121|steps:   30|Train Avg Loss: 0.0208 |Test Loss: 1.5236|lr = 0.00162\n",
      "Epoch: 1121|steps:   60|Train Avg Loss: 0.0230 |Test Loss: 1.5595|lr = 0.00162\n",
      "Epoch: 1122|steps:   30|Train Avg Loss: 0.0212 |Test Loss: 1.4969|lr = 0.00159\n",
      "Epoch: 1122|steps:   60|Train Avg Loss: 0.0216 |Test Loss: 1.5090|lr = 0.00159\n",
      "Epoch: 1123|steps:   30|Train Avg Loss: 0.0139 |Test Loss: 1.5160|lr = 0.00159\n",
      "Epoch: 1123|steps:   60|Train Avg Loss: 0.0132 |Test Loss: 1.5080|lr = 0.00159\n",
      "Epoch: 1124|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.5262|lr = 0.00159\n",
      "Epoch: 1124|steps:   60|Train Avg Loss: 0.0187 |Test Loss: 1.5599|lr = 0.00159\n",
      "Epoch: 1125|steps:   30|Train Avg Loss: 0.0162 |Test Loss: 1.5742|lr = 0.00159\n",
      "Epoch: 1125|steps:   60|Train Avg Loss: 0.0154 |Test Loss: 1.5525|lr = 0.00159\n",
      "Epoch: 1126|steps:   30|Train Avg Loss: 0.0170 |Test Loss: 1.5079|lr = 0.00159\n",
      "Epoch: 1126|steps:   60|Train Avg Loss: 0.0174 |Test Loss: 1.4641|lr = 0.00159\n",
      "Epoch: 1127|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 1.5218|lr = 0.00159\n",
      "Epoch: 1127|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.5152|lr = 0.00159\n",
      "Epoch: 1128|steps:   30|Train Avg Loss: 0.0167 |Test Loss: 1.5671|lr = 0.00159\n",
      "Epoch: 1128|steps:   60|Train Avg Loss: 0.0167 |Test Loss: 1.5326|lr = 0.00159\n",
      "Epoch: 1129|steps:   30|Train Avg Loss: 0.0147 |Test Loss: 1.5692|lr = 0.00159\n",
      "Epoch: 1129|steps:   60|Train Avg Loss: 0.0120 |Test Loss: 1.4926|lr = 0.00159\n",
      "Epoch: 1130|steps:   30|Train Avg Loss: 0.0207 |Test Loss: 1.5001|lr = 0.00159\n",
      "Epoch: 1130|steps:   60|Train Avg Loss: 0.0148 |Test Loss: 1.5420|lr = 0.00159\n",
      "Epoch: 1131|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.5359|lr = 0.00159\n",
      "Epoch: 1131|steps:   60|Train Avg Loss: 0.0130 |Test Loss: 1.5650|lr = 0.00159\n",
      "Epoch: 1132|steps:   30|Train Avg Loss: 0.0174 |Test Loss: 1.5315|lr = 0.00159\n",
      "Epoch: 1132|steps:   60|Train Avg Loss: 0.0130 |Test Loss: 1.5398|lr = 0.00159\n",
      "Epoch: 1133|steps:   30|Train Avg Loss: 0.0187 |Test Loss: 1.5590|lr = 0.00156\n",
      "Epoch: 1133|steps:   60|Train Avg Loss: 0.0157 |Test Loss: 1.5715|lr = 0.00156\n",
      "Epoch: 1134|steps:   30|Train Avg Loss: 0.0177 |Test Loss: 1.5674|lr = 0.00156\n",
      "Epoch: 1134|steps:   60|Train Avg Loss: 0.0201 |Test Loss: 1.5399|lr = 0.00156\n",
      "Epoch: 1135|steps:   30|Train Avg Loss: 0.0130 |Test Loss: 1.4967|lr = 0.00156\n",
      "Epoch: 1135|steps:   60|Train Avg Loss: 0.0170 |Test Loss: 1.5602|lr = 0.00156\n",
      "Epoch: 1136|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.5238|lr = 0.00156\n",
      "Epoch: 1136|steps:   60|Train Avg Loss: 0.0190 |Test Loss: 1.5688|lr = 0.00156\n",
      "Epoch: 1137|steps:   30|Train Avg Loss: 0.0157 |Test Loss: 1.5624|lr = 0.00156\n",
      "Epoch: 1137|steps:   60|Train Avg Loss: 0.0204 |Test Loss: 1.5418|lr = 0.00156\n",
      "Epoch: 1138|steps:   30|Train Avg Loss: 0.0169 |Test Loss: 1.4985|lr = 0.00156\n",
      "Epoch: 1138|steps:   60|Train Avg Loss: 0.0152 |Test Loss: 1.5659|lr = 0.00156\n",
      "Epoch: 1139|steps:   30|Train Avg Loss: 0.0168 |Test Loss: 1.5193|lr = 0.00156\n",
      "Epoch: 1139|steps:   60|Train Avg Loss: 0.0169 |Test Loss: 1.5198|lr = 0.00156\n",
      "Epoch: 1140|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.5262|lr = 0.00156\n",
      "Epoch: 1140|steps:   60|Train Avg Loss: 0.0173 |Test Loss: 1.5218|lr = 0.00156\n",
      "Epoch: 1141|steps:   30|Train Avg Loss: 0.0210 |Test Loss: 1.5281|lr = 0.00156\n",
      "Epoch: 1141|steps:   60|Train Avg Loss: 0.0150 |Test Loss: 1.5014|lr = 0.00156\n",
      "Epoch: 1142|steps:   30|Train Avg Loss: 0.0172 |Test Loss: 1.5705|lr = 0.00156\n",
      "Epoch: 1142|steps:   60|Train Avg Loss: 0.0199 |Test Loss: 1.5336|lr = 0.00156\n",
      "Epoch: 1143|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.5797|lr = 0.00156\n",
      "Epoch: 1143|steps:   60|Train Avg Loss: 0.0153 |Test Loss: 1.5598|lr = 0.00156\n",
      "Epoch: 1144|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.5302|lr = 0.00153\n",
      "Epoch: 1144|steps:   60|Train Avg Loss: 0.0193 |Test Loss: 1.5252|lr = 0.00153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1145|steps:   30|Train Avg Loss: 0.0209 |Test Loss: 1.4947|lr = 0.00153\n",
      "Epoch: 1145|steps:   60|Train Avg Loss: 0.0244 |Test Loss: 1.5451|lr = 0.00153\n",
      "Epoch: 1146|steps:   30|Train Avg Loss: 0.0233 |Test Loss: 1.5064|lr = 0.00153\n",
      "Epoch: 1146|steps:   60|Train Avg Loss: 0.0239 |Test Loss: 1.5498|lr = 0.00153\n",
      "Epoch: 1147|steps:   30|Train Avg Loss: 0.0287 |Test Loss: 1.5353|lr = 0.00153\n",
      "Epoch: 1147|steps:   60|Train Avg Loss: 0.0297 |Test Loss: 1.4735|lr = 0.00153\n",
      "Epoch: 1148|steps:   30|Train Avg Loss: 0.0214 |Test Loss: 1.5302|lr = 0.00153\n",
      "Epoch: 1148|steps:   60|Train Avg Loss: 0.0299 |Test Loss: 1.5059|lr = 0.00153\n",
      "Epoch: 1149|steps:   30|Train Avg Loss: 0.0230 |Test Loss: 1.4850|lr = 0.00153\n",
      "Epoch: 1149|steps:   60|Train Avg Loss: 0.0173 |Test Loss: 1.5231|lr = 0.00153\n",
      "Epoch: 1150|steps:   30|Train Avg Loss: 0.0207 |Test Loss: 1.5449|lr = 0.00153\n",
      "Epoch: 1150|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.5353|lr = 0.00153\n",
      "Epoch: 1151|steps:   30|Train Avg Loss: 0.0175 |Test Loss: 1.5546|lr = 0.00153\n",
      "Epoch: 1151|steps:   60|Train Avg Loss: 0.0196 |Test Loss: 1.5422|lr = 0.00153\n",
      "Epoch: 1152|steps:   30|Train Avg Loss: 0.0237 |Test Loss: 1.5339|lr = 0.00153\n",
      "Epoch: 1152|steps:   60|Train Avg Loss: 0.0182 |Test Loss: 1.4753|lr = 0.00153\n",
      "Epoch: 1153|steps:   30|Train Avg Loss: 0.0223 |Test Loss: 1.5304|lr = 0.00153\n",
      "Epoch: 1153|steps:   60|Train Avg Loss: 0.0184 |Test Loss: 1.5118|lr = 0.00153\n",
      "Epoch: 1154|steps:   30|Train Avg Loss: 0.0247 |Test Loss: 1.5888|lr = 0.00153\n",
      "Epoch: 1154|steps:   60|Train Avg Loss: 0.0219 |Test Loss: 1.5154|lr = 0.00153\n",
      "Epoch: 1155|steps:   30|Train Avg Loss: 0.0206 |Test Loss: 1.5433|lr = 0.00150\n",
      "Epoch: 1155|steps:   60|Train Avg Loss: 0.0241 |Test Loss: 1.5237|lr = 0.00150\n",
      "Epoch: 1156|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.5287|lr = 0.00150\n",
      "Epoch: 1156|steps:   60|Train Avg Loss: 0.0337 |Test Loss: 1.5512|lr = 0.00150\n",
      "Epoch: 1157|steps:   30|Train Avg Loss: 0.0239 |Test Loss: 1.6420|lr = 0.00150\n",
      "Epoch: 1157|steps:   60|Train Avg Loss: 0.0224 |Test Loss: 1.5650|lr = 0.00150\n",
      "Epoch: 1158|steps:   30|Train Avg Loss: 0.0245 |Test Loss: 1.5172|lr = 0.00150\n",
      "Epoch: 1158|steps:   60|Train Avg Loss: 0.0367 |Test Loss: 1.5646|lr = 0.00150\n",
      "Epoch: 1159|steps:   30|Train Avg Loss: 0.0365 |Test Loss: 1.7079|lr = 0.00150\n",
      "Epoch: 1159|steps:   60|Train Avg Loss: 0.0371 |Test Loss: 1.5296|lr = 0.00150\n",
      "Epoch: 1160|steps:   30|Train Avg Loss: 0.0262 |Test Loss: 1.5306|lr = 0.00150\n",
      "Epoch: 1160|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 1.5080|lr = 0.00150\n",
      "Epoch: 1161|steps:   30|Train Avg Loss: 0.0192 |Test Loss: 1.5435|lr = 0.00150\n",
      "Epoch: 1161|steps:   60|Train Avg Loss: 0.0203 |Test Loss: 1.5116|lr = 0.00150\n",
      "Epoch: 1162|steps:   30|Train Avg Loss: 0.0287 |Test Loss: 1.5594|lr = 0.00150\n",
      "Epoch: 1162|steps:   60|Train Avg Loss: 0.0336 |Test Loss: 1.5023|lr = 0.00150\n",
      "Epoch: 1163|steps:   30|Train Avg Loss: 0.0277 |Test Loss: 1.5641|lr = 0.00150\n",
      "Epoch: 1163|steps:   60|Train Avg Loss: 0.0303 |Test Loss: 1.5440|lr = 0.00150\n",
      "Epoch: 1164|steps:   30|Train Avg Loss: 0.0231 |Test Loss: 1.5288|lr = 0.00150\n",
      "Epoch: 1164|steps:   60|Train Avg Loss: 0.0195 |Test Loss: 1.5335|lr = 0.00150\n",
      "Epoch: 1165|steps:   30|Train Avg Loss: 0.0178 |Test Loss: 1.5336|lr = 0.00150\n",
      "Epoch: 1165|steps:   60|Train Avg Loss: 0.0199 |Test Loss: 1.5081|lr = 0.00150\n",
      "Epoch: 1166|steps:   30|Train Avg Loss: 0.0167 |Test Loss: 1.5164|lr = 0.00147\n",
      "Epoch: 1166|steps:   60|Train Avg Loss: 0.0185 |Test Loss: 1.4980|lr = 0.00147\n",
      "Epoch: 1167|steps:   30|Train Avg Loss: 0.0116 |Test Loss: 1.5352|lr = 0.00147\n",
      "Epoch: 1167|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.5697|lr = 0.00147\n",
      "Epoch: 1168|steps:   30|Train Avg Loss: 0.0154 |Test Loss: 1.5701|lr = 0.00147\n",
      "Epoch: 1168|steps:   60|Train Avg Loss: 0.0126 |Test Loss: 1.5138|lr = 0.00147\n",
      "Epoch: 1169|steps:   30|Train Avg Loss: 0.0143 |Test Loss: 1.5680|lr = 0.00147\n",
      "Epoch: 1169|steps:   60|Train Avg Loss: 0.0159 |Test Loss: 1.5454|lr = 0.00147\n",
      "Epoch: 1170|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.5517|lr = 0.00147\n",
      "Epoch: 1170|steps:   60|Train Avg Loss: 0.0152 |Test Loss: 1.5400|lr = 0.00147\n",
      "Epoch: 1171|steps:   30|Train Avg Loss: 0.0160 |Test Loss: 1.5395|lr = 0.00147\n",
      "Epoch: 1171|steps:   60|Train Avg Loss: 0.0146 |Test Loss: 1.5201|lr = 0.00147\n",
      "Epoch: 1172|steps:   30|Train Avg Loss: 0.0176 |Test Loss: 1.5139|lr = 0.00147\n",
      "Epoch: 1172|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.5296|lr = 0.00147\n",
      "Epoch: 1173|steps:   30|Train Avg Loss: 0.0154 |Test Loss: 1.5056|lr = 0.00147\n",
      "Epoch: 1173|steps:   60|Train Avg Loss: 0.0160 |Test Loss: 1.5005|lr = 0.00147\n",
      "Epoch: 1174|steps:   30|Train Avg Loss: 0.0156 |Test Loss: 1.5158|lr = 0.00147\n",
      "Epoch: 1174|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.4759|lr = 0.00147\n",
      "Epoch: 1175|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.5088|lr = 0.00147\n",
      "Epoch: 1175|steps:   60|Train Avg Loss: 0.0209 |Test Loss: 1.5289|lr = 0.00147\n",
      "Epoch: 1176|steps:   30|Train Avg Loss: 0.0205 |Test Loss: 1.5316|lr = 0.00147\n",
      "Epoch: 1176|steps:   60|Train Avg Loss: 0.0195 |Test Loss: 1.5277|lr = 0.00147\n",
      "Epoch: 1177|steps:   30|Train Avg Loss: 0.0148 |Test Loss: 1.5184|lr = 0.00144\n",
      "Epoch: 1177|steps:   60|Train Avg Loss: 0.0200 |Test Loss: 1.5092|lr = 0.00144\n",
      "Epoch: 1178|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.4996|lr = 0.00144\n",
      "Epoch: 1178|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.5194|lr = 0.00144\n",
      "Epoch: 1179|steps:   30|Train Avg Loss: 0.0134 |Test Loss: 1.5297|lr = 0.00144\n",
      "Epoch: 1179|steps:   60|Train Avg Loss: 0.0177 |Test Loss: 1.4898|lr = 0.00144\n",
      "Epoch: 1180|steps:   30|Train Avg Loss: 0.0125 |Test Loss: 1.5969|lr = 0.00144\n",
      "Epoch: 1180|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.5214|lr = 0.00144\n",
      "Epoch: 1181|steps:   30|Train Avg Loss: 0.0185 |Test Loss: 1.5543|lr = 0.00144\n",
      "Epoch: 1181|steps:   60|Train Avg Loss: 0.0155 |Test Loss: 1.5331|lr = 0.00144\n",
      "Epoch: 1182|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.5499|lr = 0.00144\n",
      "Epoch: 1182|steps:   60|Train Avg Loss: 0.0167 |Test Loss: 1.5350|lr = 0.00144\n",
      "Epoch: 1183|steps:   30|Train Avg Loss: 0.0164 |Test Loss: 1.5636|lr = 0.00144\n",
      "Epoch: 1183|steps:   60|Train Avg Loss: 0.0148 |Test Loss: 1.5433|lr = 0.00144\n",
      "Epoch: 1184|steps:   30|Train Avg Loss: 0.0164 |Test Loss: 1.5549|lr = 0.00144\n",
      "Epoch: 1184|steps:   60|Train Avg Loss: 0.0184 |Test Loss: 1.5427|lr = 0.00144\n",
      "Epoch: 1185|steps:   30|Train Avg Loss: 0.0131 |Test Loss: 1.5526|lr = 0.00144\n",
      "Epoch: 1185|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.5616|lr = 0.00144\n",
      "Epoch: 1186|steps:   30|Train Avg Loss: 0.0200 |Test Loss: 1.5690|lr = 0.00144\n",
      "Epoch: 1186|steps:   60|Train Avg Loss: 0.0202 |Test Loss: 1.5862|lr = 0.00144\n",
      "Epoch: 1187|steps:   30|Train Avg Loss: 0.0185 |Test Loss: 1.5672|lr = 0.00144\n",
      "Epoch: 1187|steps:   60|Train Avg Loss: 0.0185 |Test Loss: 1.5394|lr = 0.00144\n",
      "Epoch: 1188|steps:   30|Train Avg Loss: 0.0167 |Test Loss: 1.5275|lr = 0.00141\n",
      "Epoch: 1188|steps:   60|Train Avg Loss: 0.0181 |Test Loss: 1.5116|lr = 0.00141\n",
      "Epoch: 1189|steps:   30|Train Avg Loss: 0.0171 |Test Loss: 1.5467|lr = 0.00141\n",
      "Epoch: 1189|steps:   60|Train Avg Loss: 0.0147 |Test Loss: 1.5669|lr = 0.00141\n",
      "Epoch: 1190|steps:   30|Train Avg Loss: 0.0161 |Test Loss: 1.5178|lr = 0.00141\n",
      "Epoch: 1190|steps:   60|Train Avg Loss: 0.0140 |Test Loss: 1.5977|lr = 0.00141\n",
      "Epoch: 1191|steps:   30|Train Avg Loss: 0.0199 |Test Loss: 1.5623|lr = 0.00141\n",
      "Epoch: 1191|steps:   60|Train Avg Loss: 0.0146 |Test Loss: 1.5293|lr = 0.00141\n",
      "Epoch: 1192|steps:   30|Train Avg Loss: 0.0160 |Test Loss: 1.5710|lr = 0.00141\n",
      "Epoch: 1192|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.5763|lr = 0.00141\n",
      "Epoch: 1193|steps:   30|Train Avg Loss: 0.0174 |Test Loss: 1.5200|lr = 0.00141\n",
      "Epoch: 1193|steps:   60|Train Avg Loss: 0.0188 |Test Loss: 1.5386|lr = 0.00141\n",
      "Epoch: 1194|steps:   30|Train Avg Loss: 0.0165 |Test Loss: 1.5560|lr = 0.00141\n",
      "Epoch: 1194|steps:   60|Train Avg Loss: 0.0176 |Test Loss: 1.6268|lr = 0.00141\n",
      "Epoch: 1195|steps:   30|Train Avg Loss: 0.0324 |Test Loss: 1.5309|lr = 0.00141\n",
      "Epoch: 1195|steps:   60|Train Avg Loss: 0.0263 |Test Loss: 1.5644|lr = 0.00141\n",
      "Epoch: 1196|steps:   30|Train Avg Loss: 0.0286 |Test Loss: 1.5229|lr = 0.00141\n",
      "Epoch: 1196|steps:   60|Train Avg Loss: 0.0300 |Test Loss: 1.6739|lr = 0.00141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1197|steps:   30|Train Avg Loss: 0.0212 |Test Loss: 1.5697|lr = 0.00141\n",
      "Epoch: 1197|steps:   60|Train Avg Loss: 0.0309 |Test Loss: 1.5518|lr = 0.00141\n",
      "Epoch: 1198|steps:   30|Train Avg Loss: 0.0207 |Test Loss: 1.6117|lr = 0.00141\n",
      "Epoch: 1198|steps:   60|Train Avg Loss: 0.0218 |Test Loss: 1.5578|lr = 0.00141\n",
      "Epoch: 1199|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.5324|lr = 0.00138\n",
      "Epoch: 1199|steps:   60|Train Avg Loss: 0.0199 |Test Loss: 1.5747|lr = 0.00138\n",
      "Epoch: 1200|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.5540|lr = 0.00138\n",
      "Epoch: 1200|steps:   60|Train Avg Loss: 0.0135 |Test Loss: 1.5931|lr = 0.00138\n",
      "Epoch: 1201|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5780|lr = 0.00138\n",
      "Epoch: 1201|steps:   60|Train Avg Loss: 0.0174 |Test Loss: 1.5799|lr = 0.00138\n",
      "Epoch: 1202|steps:   30|Train Avg Loss: 0.0214 |Test Loss: 1.5295|lr = 0.00138\n",
      "Epoch: 1202|steps:   60|Train Avg Loss: 0.0179 |Test Loss: 1.4872|lr = 0.00138\n",
      "Epoch: 1203|steps:   30|Train Avg Loss: 0.0190 |Test Loss: 1.5299|lr = 0.00138\n",
      "Epoch: 1203|steps:   60|Train Avg Loss: 0.0191 |Test Loss: 1.5773|lr = 0.00138\n",
      "Epoch: 1204|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.5574|lr = 0.00138\n",
      "Epoch: 1204|steps:   60|Train Avg Loss: 0.0155 |Test Loss: 1.5737|lr = 0.00138\n",
      "Epoch: 1205|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.5864|lr = 0.00138\n",
      "Epoch: 1205|steps:   60|Train Avg Loss: 0.0180 |Test Loss: 1.5416|lr = 0.00138\n",
      "Epoch: 1206|steps:   30|Train Avg Loss: 0.0155 |Test Loss: 1.5305|lr = 0.00138\n",
      "Epoch: 1206|steps:   60|Train Avg Loss: 0.0131 |Test Loss: 1.5765|lr = 0.00138\n",
      "Epoch: 1207|steps:   30|Train Avg Loss: 0.0185 |Test Loss: 1.5589|lr = 0.00138\n",
      "Epoch: 1207|steps:   60|Train Avg Loss: 0.0206 |Test Loss: 1.5255|lr = 0.00138\n",
      "Epoch: 1208|steps:   30|Train Avg Loss: 0.0179 |Test Loss: 1.5791|lr = 0.00138\n",
      "Epoch: 1208|steps:   60|Train Avg Loss: 0.0178 |Test Loss: 1.5981|lr = 0.00138\n",
      "Epoch: 1209|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.5560|lr = 0.00138\n",
      "Epoch: 1209|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.6011|lr = 0.00138\n",
      "Epoch: 1210|steps:   30|Train Avg Loss: 0.0169 |Test Loss: 1.5968|lr = 0.00135\n",
      "Epoch: 1210|steps:   60|Train Avg Loss: 0.0166 |Test Loss: 1.6606|lr = 0.00135\n",
      "Epoch: 1211|steps:   30|Train Avg Loss: 0.0190 |Test Loss: 1.6093|lr = 0.00135\n",
      "Epoch: 1211|steps:   60|Train Avg Loss: 0.0205 |Test Loss: 1.5772|lr = 0.00135\n",
      "Epoch: 1212|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.6079|lr = 0.00135\n",
      "Epoch: 1212|steps:   60|Train Avg Loss: 0.0192 |Test Loss: 1.6874|lr = 0.00135\n",
      "Epoch: 1213|steps:   30|Train Avg Loss: 0.0192 |Test Loss: 1.6072|lr = 0.00135\n",
      "Epoch: 1213|steps:   60|Train Avg Loss: 0.0190 |Test Loss: 1.5905|lr = 0.00135\n",
      "Epoch: 1214|steps:   30|Train Avg Loss: 0.0178 |Test Loss: 1.5648|lr = 0.00135\n",
      "Epoch: 1214|steps:   60|Train Avg Loss: 0.0197 |Test Loss: 1.5895|lr = 0.00135\n",
      "Epoch: 1215|steps:   30|Train Avg Loss: 0.0190 |Test Loss: 1.6051|lr = 0.00135\n",
      "Epoch: 1215|steps:   60|Train Avg Loss: 0.0164 |Test Loss: 1.5114|lr = 0.00135\n",
      "Epoch: 1216|steps:   30|Train Avg Loss: 0.0201 |Test Loss: 1.5752|lr = 0.00135\n",
      "Epoch: 1216|steps:   60|Train Avg Loss: 0.0158 |Test Loss: 1.5895|lr = 0.00135\n",
      "Epoch: 1217|steps:   30|Train Avg Loss: 0.0189 |Test Loss: 1.5834|lr = 0.00135\n",
      "Epoch: 1217|steps:   60|Train Avg Loss: 0.0134 |Test Loss: 1.6117|lr = 0.00135\n",
      "Epoch: 1218|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.5166|lr = 0.00135\n",
      "Epoch: 1218|steps:   60|Train Avg Loss: 0.0135 |Test Loss: 1.6008|lr = 0.00135\n",
      "Epoch: 1219|steps:   30|Train Avg Loss: 0.0165 |Test Loss: 1.5164|lr = 0.00135\n",
      "Epoch: 1219|steps:   60|Train Avg Loss: 0.0207 |Test Loss: 1.5807|lr = 0.00135\n",
      "Epoch: 1220|steps:   30|Train Avg Loss: 0.0152 |Test Loss: 1.5546|lr = 0.00135\n",
      "Epoch: 1220|steps:   60|Train Avg Loss: 0.0181 |Test Loss: 1.5815|lr = 0.00135\n",
      "Epoch: 1221|steps:   30|Train Avg Loss: 0.0167 |Test Loss: 1.5747|lr = 0.00133\n",
      "Epoch: 1221|steps:   60|Train Avg Loss: 0.0159 |Test Loss: 1.5481|lr = 0.00133\n",
      "Epoch: 1222|steps:   30|Train Avg Loss: 0.0116 |Test Loss: 1.5034|lr = 0.00133\n",
      "Epoch: 1222|steps:   60|Train Avg Loss: 0.0152 |Test Loss: 1.5231|lr = 0.00133\n",
      "Epoch: 1223|steps:   30|Train Avg Loss: 0.0139 |Test Loss: 1.5973|lr = 0.00133\n",
      "Epoch: 1223|steps:   60|Train Avg Loss: 0.0158 |Test Loss: 1.5321|lr = 0.00133\n",
      "Epoch: 1224|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.4748|lr = 0.00133\n",
      "Epoch: 1224|steps:   60|Train Avg Loss: 0.0178 |Test Loss: 1.5322|lr = 0.00133\n",
      "Epoch: 1225|steps:   30|Train Avg Loss: 0.0181 |Test Loss: 1.5944|lr = 0.00133\n",
      "Epoch: 1225|steps:   60|Train Avg Loss: 0.0206 |Test Loss: 1.5530|lr = 0.00133\n",
      "Epoch: 1226|steps:   30|Train Avg Loss: 0.0168 |Test Loss: 1.5140|lr = 0.00133\n",
      "Epoch: 1226|steps:   60|Train Avg Loss: 0.0147 |Test Loss: 1.5756|lr = 0.00133\n",
      "Epoch: 1227|steps:   30|Train Avg Loss: 0.0293 |Test Loss: 1.6864|lr = 0.00133\n",
      "Epoch: 1227|steps:   60|Train Avg Loss: 0.0379 |Test Loss: 1.4635|lr = 0.00133\n",
      "Epoch: 1228|steps:   30|Train Avg Loss: 0.0216 |Test Loss: 1.4572|lr = 0.00133\n",
      "Epoch: 1228|steps:   60|Train Avg Loss: 0.0302 |Test Loss: 1.6007|lr = 0.00133\n",
      "Epoch: 1229|steps:   30|Train Avg Loss: 0.0174 |Test Loss: 1.5253|lr = 0.00133\n",
      "Epoch: 1229|steps:   60|Train Avg Loss: 0.0196 |Test Loss: 1.5238|lr = 0.00133\n",
      "Epoch: 1230|steps:   30|Train Avg Loss: 0.0160 |Test Loss: 1.4786|lr = 0.00133\n",
      "Epoch: 1230|steps:   60|Train Avg Loss: 0.0189 |Test Loss: 1.5452|lr = 0.00133\n",
      "Epoch: 1231|steps:   30|Train Avg Loss: 0.0221 |Test Loss: 1.5581|lr = 0.00133\n",
      "Epoch: 1231|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.5470|lr = 0.00133\n",
      "Epoch: 1232|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5812|lr = 0.00130\n",
      "Epoch: 1232|steps:   60|Train Avg Loss: 0.0146 |Test Loss: 1.4558|lr = 0.00130\n",
      "Epoch: 1233|steps:   30|Train Avg Loss: 0.0181 |Test Loss: 1.4451|lr = 0.00130\n",
      "Epoch: 1233|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.5700|lr = 0.00130\n",
      "Epoch: 1234|steps:   30|Train Avg Loss: 0.0248 |Test Loss: 1.5432|lr = 0.00130\n",
      "Epoch: 1234|steps:   60|Train Avg Loss: 0.0173 |Test Loss: 1.6131|lr = 0.00130\n",
      "Epoch: 1235|steps:   30|Train Avg Loss: 0.0130 |Test Loss: 1.5199|lr = 0.00130\n",
      "Epoch: 1235|steps:   60|Train Avg Loss: 0.0152 |Test Loss: 1.5872|lr = 0.00130\n",
      "Epoch: 1236|steps:   30|Train Avg Loss: 0.0188 |Test Loss: 1.5198|lr = 0.00130\n",
      "Epoch: 1236|steps:   60|Train Avg Loss: 0.0162 |Test Loss: 1.5517|lr = 0.00130\n",
      "Epoch: 1237|steps:   30|Train Avg Loss: 0.0224 |Test Loss: 1.5394|lr = 0.00130\n",
      "Epoch: 1237|steps:   60|Train Avg Loss: 0.0195 |Test Loss: 1.6087|lr = 0.00130\n",
      "Epoch: 1238|steps:   30|Train Avg Loss: 0.0217 |Test Loss: 1.5757|lr = 0.00130\n",
      "Epoch: 1238|steps:   60|Train Avg Loss: 0.0261 |Test Loss: 1.6003|lr = 0.00130\n",
      "Epoch: 1239|steps:   30|Train Avg Loss: 0.0232 |Test Loss: 1.5572|lr = 0.00130\n",
      "Epoch: 1239|steps:   60|Train Avg Loss: 0.0204 |Test Loss: 1.5799|lr = 0.00130\n",
      "Epoch: 1240|steps:   30|Train Avg Loss: 0.0176 |Test Loss: 1.5604|lr = 0.00130\n",
      "Epoch: 1240|steps:   60|Train Avg Loss: 0.0132 |Test Loss: 1.5690|lr = 0.00130\n",
      "Epoch: 1241|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.5606|lr = 0.00130\n",
      "Epoch: 1241|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.5822|lr = 0.00130\n",
      "Epoch: 1242|steps:   30|Train Avg Loss: 0.0154 |Test Loss: 1.5323|lr = 0.00130\n",
      "Epoch: 1242|steps:   60|Train Avg Loss: 0.0156 |Test Loss: 1.5447|lr = 0.00130\n",
      "Epoch: 1243|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.5820|lr = 0.00127\n",
      "Epoch: 1243|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.5297|lr = 0.00127\n",
      "Epoch: 1244|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.5719|lr = 0.00127\n",
      "Epoch: 1244|steps:   60|Train Avg Loss: 0.0158 |Test Loss: 1.6223|lr = 0.00127\n",
      "Epoch: 1245|steps:   30|Train Avg Loss: 0.0117 |Test Loss: 1.5861|lr = 0.00127\n",
      "Epoch: 1245|steps:   60|Train Avg Loss: 0.0184 |Test Loss: 1.5891|lr = 0.00127\n",
      "Epoch: 1246|steps:   30|Train Avg Loss: 0.0116 |Test Loss: 1.5103|lr = 0.00127\n",
      "Epoch: 1246|steps:   60|Train Avg Loss: 0.0147 |Test Loss: 1.5553|lr = 0.00127\n",
      "Epoch: 1247|steps:   30|Train Avg Loss: 0.0134 |Test Loss: 1.6118|lr = 0.00127\n",
      "Epoch: 1247|steps:   60|Train Avg Loss: 0.0151 |Test Loss: 1.5460|lr = 0.00127\n",
      "Epoch: 1248|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.5483|lr = 0.00127\n",
      "Epoch: 1248|steps:   60|Train Avg Loss: 0.0134 |Test Loss: 1.5387|lr = 0.00127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1249|steps:   30|Train Avg Loss: 0.0177 |Test Loss: 1.5839|lr = 0.00127\n",
      "Epoch: 1249|steps:   60|Train Avg Loss: 0.0125 |Test Loss: 1.5126|lr = 0.00127\n",
      "Epoch: 1250|steps:   30|Train Avg Loss: 0.0148 |Test Loss: 1.5586|lr = 0.00127\n",
      "Epoch: 1250|steps:   60|Train Avg Loss: 0.0146 |Test Loss: 1.5629|lr = 0.00127\n",
      "Epoch: 1251|steps:   30|Train Avg Loss: 0.0172 |Test Loss: 1.5579|lr = 0.00127\n",
      "Epoch: 1251|steps:   60|Train Avg Loss: 0.0153 |Test Loss: 1.5532|lr = 0.00127\n",
      "Epoch: 1252|steps:   30|Train Avg Loss: 0.0175 |Test Loss: 1.5617|lr = 0.00127\n",
      "Epoch: 1252|steps:   60|Train Avg Loss: 0.0169 |Test Loss: 1.5544|lr = 0.00127\n",
      "Epoch: 1253|steps:   30|Train Avg Loss: 0.0170 |Test Loss: 1.5813|lr = 0.00127\n",
      "Epoch: 1253|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.5983|lr = 0.00127\n",
      "Epoch: 1254|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.6263|lr = 0.00125\n",
      "Epoch: 1254|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.5735|lr = 0.00125\n",
      "Epoch: 1255|steps:   30|Train Avg Loss: 0.0138 |Test Loss: 1.5465|lr = 0.00125\n",
      "Epoch: 1255|steps:   60|Train Avg Loss: 0.0144 |Test Loss: 1.5444|lr = 0.00125\n",
      "Epoch: 1256|steps:   30|Train Avg Loss: 0.0138 |Test Loss: 1.5568|lr = 0.00125\n",
      "Epoch: 1256|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.5843|lr = 0.00125\n",
      "Epoch: 1257|steps:   30|Train Avg Loss: 0.0126 |Test Loss: 1.5492|lr = 0.00125\n",
      "Epoch: 1257|steps:   60|Train Avg Loss: 0.0136 |Test Loss: 1.5210|lr = 0.00125\n",
      "Epoch: 1258|steps:   30|Train Avg Loss: 0.0151 |Test Loss: 1.6135|lr = 0.00125\n",
      "Epoch: 1258|steps:   60|Train Avg Loss: 0.0166 |Test Loss: 1.5562|lr = 0.00125\n",
      "Epoch: 1259|steps:   30|Train Avg Loss: 0.0173 |Test Loss: 1.6252|lr = 0.00125\n",
      "Epoch: 1259|steps:   60|Train Avg Loss: 0.0197 |Test Loss: 1.6075|lr = 0.00125\n",
      "Epoch: 1260|steps:   30|Train Avg Loss: 0.0144 |Test Loss: 1.6015|lr = 0.00125\n",
      "Epoch: 1260|steps:   60|Train Avg Loss: 0.0169 |Test Loss: 1.5816|lr = 0.00125\n",
      "Epoch: 1261|steps:   30|Train Avg Loss: 0.0223 |Test Loss: 1.5626|lr = 0.00125\n",
      "Epoch: 1261|steps:   60|Train Avg Loss: 0.0157 |Test Loss: 1.6004|lr = 0.00125\n",
      "Epoch: 1262|steps:   30|Train Avg Loss: 0.0136 |Test Loss: 1.6057|lr = 0.00125\n",
      "Epoch: 1262|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.5869|lr = 0.00125\n",
      "Epoch: 1263|steps:   30|Train Avg Loss: 0.0176 |Test Loss: 1.5805|lr = 0.00125\n",
      "Epoch: 1263|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.6156|lr = 0.00125\n",
      "Epoch: 1264|steps:   30|Train Avg Loss: 0.0163 |Test Loss: 1.6057|lr = 0.00125\n",
      "Epoch: 1264|steps:   60|Train Avg Loss: 0.0158 |Test Loss: 1.5773|lr = 0.00125\n",
      "Epoch: 1265|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.5697|lr = 0.00122\n",
      "Epoch: 1265|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.5820|lr = 0.00122\n",
      "Epoch: 1266|steps:   30|Train Avg Loss: 0.0138 |Test Loss: 1.5693|lr = 0.00122\n",
      "Epoch: 1266|steps:   60|Train Avg Loss: 0.0143 |Test Loss: 1.5581|lr = 0.00122\n",
      "Epoch: 1267|steps:   30|Train Avg Loss: 0.0162 |Test Loss: 1.5736|lr = 0.00122\n",
      "Epoch: 1267|steps:   60|Train Avg Loss: 0.0151 |Test Loss: 1.5440|lr = 0.00122\n",
      "Epoch: 1268|steps:   30|Train Avg Loss: 0.0158 |Test Loss: 1.5933|lr = 0.00122\n",
      "Epoch: 1268|steps:   60|Train Avg Loss: 0.0148 |Test Loss: 1.5985|lr = 0.00122\n",
      "Epoch: 1269|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.6281|lr = 0.00122\n",
      "Epoch: 1269|steps:   60|Train Avg Loss: 0.0177 |Test Loss: 1.6081|lr = 0.00122\n",
      "Epoch: 1270|steps:   30|Train Avg Loss: 0.0189 |Test Loss: 1.5469|lr = 0.00122\n",
      "Epoch: 1270|steps:   60|Train Avg Loss: 0.0143 |Test Loss: 1.5809|lr = 0.00122\n",
      "Epoch: 1271|steps:   30|Train Avg Loss: 0.0196 |Test Loss: 1.6033|lr = 0.00122\n",
      "Epoch: 1271|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.6285|lr = 0.00122\n",
      "Epoch: 1272|steps:   30|Train Avg Loss: 0.0187 |Test Loss: 1.5841|lr = 0.00122\n",
      "Epoch: 1272|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 1.4880|lr = 0.00122\n",
      "Epoch: 1273|steps:   30|Train Avg Loss: 0.0192 |Test Loss: 1.5670|lr = 0.00122\n",
      "Epoch: 1273|steps:   60|Train Avg Loss: 0.0149 |Test Loss: 1.6123|lr = 0.00122\n",
      "Epoch: 1274|steps:   30|Train Avg Loss: 0.0151 |Test Loss: 1.5603|lr = 0.00122\n",
      "Epoch: 1274|steps:   60|Train Avg Loss: 0.0206 |Test Loss: 1.5966|lr = 0.00122\n",
      "Epoch: 1275|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.5083|lr = 0.00122\n",
      "Epoch: 1275|steps:   60|Train Avg Loss: 0.0243 |Test Loss: 1.5800|lr = 0.00122\n",
      "Epoch: 1276|steps:   30|Train Avg Loss: 0.0161 |Test Loss: 1.6257|lr = 0.00120\n",
      "Epoch: 1276|steps:   60|Train Avg Loss: 0.0166 |Test Loss: 1.6368|lr = 0.00120\n",
      "Epoch: 1277|steps:   30|Train Avg Loss: 0.0160 |Test Loss: 1.6164|lr = 0.00120\n",
      "Epoch: 1277|steps:   60|Train Avg Loss: 0.0188 |Test Loss: 1.5634|lr = 0.00120\n",
      "Epoch: 1278|steps:   30|Train Avg Loss: 0.0239 |Test Loss: 1.5945|lr = 0.00120\n",
      "Epoch: 1278|steps:   60|Train Avg Loss: 0.0304 |Test Loss: 1.6241|lr = 0.00120\n",
      "Epoch: 1279|steps:   30|Train Avg Loss: 0.0243 |Test Loss: 1.6415|lr = 0.00120\n",
      "Epoch: 1279|steps:   60|Train Avg Loss: 0.0252 |Test Loss: 1.6025|lr = 0.00120\n",
      "Epoch: 1280|steps:   30|Train Avg Loss: 0.0135 |Test Loss: 1.5404|lr = 0.00120\n",
      "Epoch: 1280|steps:   60|Train Avg Loss: 0.0178 |Test Loss: 1.6027|lr = 0.00120\n",
      "Epoch: 1281|steps:   30|Train Avg Loss: 0.0153 |Test Loss: 1.6458|lr = 0.00120\n",
      "Epoch: 1281|steps:   60|Train Avg Loss: 0.0194 |Test Loss: 1.5627|lr = 0.00120\n",
      "Epoch: 1282|steps:   30|Train Avg Loss: 0.0180 |Test Loss: 1.5582|lr = 0.00120\n",
      "Epoch: 1282|steps:   60|Train Avg Loss: 0.0168 |Test Loss: 1.6114|lr = 0.00120\n",
      "Epoch: 1283|steps:   30|Train Avg Loss: 0.0119 |Test Loss: 1.5908|lr = 0.00120\n",
      "Epoch: 1283|steps:   60|Train Avg Loss: 0.0175 |Test Loss: 1.6076|lr = 0.00120\n",
      "Epoch: 1284|steps:   30|Train Avg Loss: 0.0135 |Test Loss: 1.5833|lr = 0.00120\n",
      "Epoch: 1284|steps:   60|Train Avg Loss: 0.0154 |Test Loss: 1.5592|lr = 0.00120\n",
      "Epoch: 1285|steps:   30|Train Avg Loss: 0.0148 |Test Loss: 1.5513|lr = 0.00120\n",
      "Epoch: 1285|steps:   60|Train Avg Loss: 0.0156 |Test Loss: 1.6860|lr = 0.00120\n",
      "Epoch: 1286|steps:   30|Train Avg Loss: 0.0182 |Test Loss: 1.5953|lr = 0.00120\n",
      "Epoch: 1286|steps:   60|Train Avg Loss: 0.0149 |Test Loss: 1.5651|lr = 0.00120\n",
      "Epoch: 1287|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.6190|lr = 0.00117\n",
      "Epoch: 1287|steps:   60|Train Avg Loss: 0.0173 |Test Loss: 1.6033|lr = 0.00117\n",
      "Epoch: 1288|steps:   30|Train Avg Loss: 0.0168 |Test Loss: 1.5837|lr = 0.00117\n",
      "Epoch: 1288|steps:   60|Train Avg Loss: 0.0150 |Test Loss: 1.5375|lr = 0.00117\n",
      "Epoch: 1289|steps:   30|Train Avg Loss: 0.0140 |Test Loss: 1.5601|lr = 0.00117\n",
      "Epoch: 1289|steps:   60|Train Avg Loss: 0.0154 |Test Loss: 1.5779|lr = 0.00117\n",
      "Epoch: 1290|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5783|lr = 0.00117\n",
      "Epoch: 1290|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.5595|lr = 0.00117\n",
      "Epoch: 1291|steps:   30|Train Avg Loss: 0.0130 |Test Loss: 1.6100|lr = 0.00117\n",
      "Epoch: 1291|steps:   60|Train Avg Loss: 0.0139 |Test Loss: 1.6487|lr = 0.00117\n",
      "Epoch: 1292|steps:   30|Train Avg Loss: 0.0102 |Test Loss: 1.5696|lr = 0.00117\n",
      "Epoch: 1292|steps:   60|Train Avg Loss: 0.0133 |Test Loss: 1.6168|lr = 0.00117\n",
      "Epoch: 1293|steps:   30|Train Avg Loss: 0.0143 |Test Loss: 1.5704|lr = 0.00117\n",
      "Epoch: 1293|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6196|lr = 0.00117\n",
      "Epoch: 1294|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.6169|lr = 0.00117\n",
      "Epoch: 1294|steps:   60|Train Avg Loss: 0.0139 |Test Loss: 1.5724|lr = 0.00117\n",
      "Epoch: 1295|steps:   30|Train Avg Loss: 0.0197 |Test Loss: 1.5761|lr = 0.00117\n",
      "Epoch: 1295|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.5876|lr = 0.00117\n",
      "Epoch: 1296|steps:   30|Train Avg Loss: 0.0144 |Test Loss: 1.6105|lr = 0.00117\n",
      "Epoch: 1296|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.5561|lr = 0.00117\n",
      "Epoch: 1297|steps:   30|Train Avg Loss: 0.0268 |Test Loss: 1.5671|lr = 0.00117\n",
      "Epoch: 1297|steps:   60|Train Avg Loss: 0.0191 |Test Loss: 1.4983|lr = 0.00117\n",
      "Epoch: 1298|steps:   30|Train Avg Loss: 0.0201 |Test Loss: 1.6041|lr = 0.00115\n",
      "Epoch: 1298|steps:   60|Train Avg Loss: 0.0150 |Test Loss: 1.5184|lr = 0.00115\n",
      "Epoch: 1299|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 1.5400|lr = 0.00115\n",
      "Epoch: 1299|steps:   60|Train Avg Loss: 0.0233 |Test Loss: 1.6323|lr = 0.00115\n",
      "Epoch: 1300|steps:   30|Train Avg Loss: 0.0239 |Test Loss: 1.6036|lr = 0.00115\n",
      "Epoch: 1300|steps:   60|Train Avg Loss: 0.0349 |Test Loss: 1.6309|lr = 0.00115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1301|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 1.5847|lr = 0.00115\n",
      "Epoch: 1301|steps:   60|Train Avg Loss: 0.0166 |Test Loss: 1.5977|lr = 0.00115\n",
      "Epoch: 1302|steps:   30|Train Avg Loss: 0.0131 |Test Loss: 1.5872|lr = 0.00115\n",
      "Epoch: 1302|steps:   60|Train Avg Loss: 0.0162 |Test Loss: 1.5637|lr = 0.00115\n",
      "Epoch: 1303|steps:   30|Train Avg Loss: 0.0129 |Test Loss: 1.6298|lr = 0.00115\n",
      "Epoch: 1303|steps:   60|Train Avg Loss: 0.0136 |Test Loss: 1.6123|lr = 0.00115\n",
      "Epoch: 1304|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.6108|lr = 0.00115\n",
      "Epoch: 1304|steps:   60|Train Avg Loss: 0.0134 |Test Loss: 1.6403|lr = 0.00115\n",
      "Epoch: 1305|steps:   30|Train Avg Loss: 0.0129 |Test Loss: 1.6433|lr = 0.00115\n",
      "Epoch: 1305|steps:   60|Train Avg Loss: 0.0129 |Test Loss: 1.6011|lr = 0.00115\n",
      "Epoch: 1306|steps:   30|Train Avg Loss: 0.0156 |Test Loss: 1.6065|lr = 0.00115\n",
      "Epoch: 1306|steps:   60|Train Avg Loss: 0.0137 |Test Loss: 1.5649|lr = 0.00115\n",
      "Epoch: 1307|steps:   30|Train Avg Loss: 0.0125 |Test Loss: 1.5500|lr = 0.00115\n",
      "Epoch: 1307|steps:   60|Train Avg Loss: 0.0138 |Test Loss: 1.5613|lr = 0.00115\n",
      "Epoch: 1308|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.5646|lr = 0.00115\n",
      "Epoch: 1308|steps:   60|Train Avg Loss: 0.0146 |Test Loss: 1.5712|lr = 0.00115\n",
      "Epoch: 1309|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.5888|lr = 0.00113\n",
      "Epoch: 1309|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.5499|lr = 0.00113\n",
      "Epoch: 1310|steps:   30|Train Avg Loss: 0.0135 |Test Loss: 1.6002|lr = 0.00113\n",
      "Epoch: 1310|steps:   60|Train Avg Loss: 0.0191 |Test Loss: 1.6034|lr = 0.00113\n",
      "Epoch: 1311|steps:   30|Train Avg Loss: 0.0116 |Test Loss: 1.6050|lr = 0.00113\n",
      "Epoch: 1311|steps:   60|Train Avg Loss: 0.0159 |Test Loss: 1.5575|lr = 0.00113\n",
      "Epoch: 1312|steps:   30|Train Avg Loss: 0.0153 |Test Loss: 1.5749|lr = 0.00113\n",
      "Epoch: 1312|steps:   60|Train Avg Loss: 0.0175 |Test Loss: 1.5867|lr = 0.00113\n",
      "Epoch: 1313|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6246|lr = 0.00113\n",
      "Epoch: 1313|steps:   60|Train Avg Loss: 0.0158 |Test Loss: 1.6423|lr = 0.00113\n",
      "Epoch: 1314|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.5665|lr = 0.00113\n",
      "Epoch: 1314|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.5899|lr = 0.00113\n",
      "Epoch: 1315|steps:   30|Train Avg Loss: 0.0114 |Test Loss: 1.5896|lr = 0.00113\n",
      "Epoch: 1315|steps:   60|Train Avg Loss: 0.0155 |Test Loss: 1.5960|lr = 0.00113\n",
      "Epoch: 1316|steps:   30|Train Avg Loss: 0.0129 |Test Loss: 1.5760|lr = 0.00113\n",
      "Epoch: 1316|steps:   60|Train Avg Loss: 0.0138 |Test Loss: 1.5757|lr = 0.00113\n",
      "Epoch: 1317|steps:   30|Train Avg Loss: 0.0126 |Test Loss: 1.5739|lr = 0.00113\n",
      "Epoch: 1317|steps:   60|Train Avg Loss: 0.0128 |Test Loss: 1.5526|lr = 0.00113\n",
      "Epoch: 1318|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.5440|lr = 0.00113\n",
      "Epoch: 1318|steps:   60|Train Avg Loss: 0.0149 |Test Loss: 1.5119|lr = 0.00113\n",
      "Epoch: 1319|steps:   30|Train Avg Loss: 0.0181 |Test Loss: 1.5695|lr = 0.00113\n",
      "Epoch: 1319|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.6038|lr = 0.00113\n",
      "Epoch: 1320|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.5664|lr = 0.00111\n",
      "Epoch: 1320|steps:   60|Train Avg Loss: 0.0182 |Test Loss: 1.5984|lr = 0.00111\n",
      "Epoch: 1321|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.6228|lr = 0.00111\n",
      "Epoch: 1321|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.6078|lr = 0.00111\n",
      "Epoch: 1322|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.5826|lr = 0.00111\n",
      "Epoch: 1322|steps:   60|Train Avg Loss: 0.0169 |Test Loss: 1.6034|lr = 0.00111\n",
      "Epoch: 1323|steps:   30|Train Avg Loss: 0.0132 |Test Loss: 1.6005|lr = 0.00111\n",
      "Epoch: 1323|steps:   60|Train Avg Loss: 0.0126 |Test Loss: 1.5870|lr = 0.00111\n",
      "Epoch: 1324|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5880|lr = 0.00111\n",
      "Epoch: 1324|steps:   60|Train Avg Loss: 0.0135 |Test Loss: 1.5968|lr = 0.00111\n",
      "Epoch: 1325|steps:   30|Train Avg Loss: 0.0167 |Test Loss: 1.5539|lr = 0.00111\n",
      "Epoch: 1325|steps:   60|Train Avg Loss: 0.0153 |Test Loss: 1.5685|lr = 0.00111\n",
      "Epoch: 1326|steps:   30|Train Avg Loss: 0.0153 |Test Loss: 1.6177|lr = 0.00111\n",
      "Epoch: 1326|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.5718|lr = 0.00111\n",
      "Epoch: 1327|steps:   30|Train Avg Loss: 0.0181 |Test Loss: 1.5945|lr = 0.00111\n",
      "Epoch: 1327|steps:   60|Train Avg Loss: 0.0227 |Test Loss: 1.5627|lr = 0.00111\n",
      "Epoch: 1328|steps:   30|Train Avg Loss: 0.0120 |Test Loss: 1.5669|lr = 0.00111\n",
      "Epoch: 1328|steps:   60|Train Avg Loss: 0.0171 |Test Loss: 1.5733|lr = 0.00111\n",
      "Epoch: 1329|steps:   30|Train Avg Loss: 0.0123 |Test Loss: 1.5860|lr = 0.00111\n",
      "Epoch: 1329|steps:   60|Train Avg Loss: 0.0125 |Test Loss: 1.6411|lr = 0.00111\n",
      "Epoch: 1330|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.6012|lr = 0.00111\n",
      "Epoch: 1330|steps:   60|Train Avg Loss: 0.0131 |Test Loss: 1.5808|lr = 0.00111\n",
      "Epoch: 1331|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6500|lr = 0.00111\n",
      "Epoch: 1331|steps:   60|Train Avg Loss: 0.0110 |Test Loss: 1.6550|lr = 0.00111\n",
      "Epoch: 1332|steps:   30|Train Avg Loss: 0.0135 |Test Loss: 1.6068|lr = 0.00111\n",
      "Epoch: 1332|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.5961|lr = 0.00111\n",
      "Epoch: 1333|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.5989|lr = 0.00111\n",
      "Epoch: 1333|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6063|lr = 0.00111\n",
      "Epoch: 1334|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.6489|lr = 0.00111\n",
      "Epoch: 1334|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.6205|lr = 0.00111\n",
      "Epoch: 1335|steps:   30|Train Avg Loss: 0.0144 |Test Loss: 1.5981|lr = 0.00111\n",
      "Epoch: 1335|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.5976|lr = 0.00111\n",
      "Epoch: 1336|steps:   30|Train Avg Loss: 0.0163 |Test Loss: 1.5810|lr = 0.00111\n",
      "Epoch: 1336|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.5843|lr = 0.00111\n",
      "Epoch: 1337|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.6112|lr = 0.00111\n",
      "Epoch: 1337|steps:   60|Train Avg Loss: 0.0135 |Test Loss: 1.5670|lr = 0.00111\n",
      "Epoch: 1338|steps:   30|Train Avg Loss: 0.0152 |Test Loss: 1.5830|lr = 0.00111\n",
      "Epoch: 1338|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.5649|lr = 0.00111\n",
      "Epoch: 1339|steps:   30|Train Avg Loss: 0.0211 |Test Loss: 1.6251|lr = 0.00111\n",
      "Epoch: 1339|steps:   60|Train Avg Loss: 0.0143 |Test Loss: 1.6065|lr = 0.00111\n",
      "Epoch: 1340|steps:   30|Train Avg Loss: 0.0161 |Test Loss: 1.6251|lr = 0.00111\n",
      "Epoch: 1340|steps:   60|Train Avg Loss: 0.0160 |Test Loss: 1.6511|lr = 0.00111\n",
      "Epoch: 1341|steps:   30|Train Avg Loss: 0.0138 |Test Loss: 1.6228|lr = 0.00111\n",
      "Epoch: 1341|steps:   60|Train Avg Loss: 0.0156 |Test Loss: 1.6494|lr = 0.00111\n",
      "Epoch: 1342|steps:   30|Train Avg Loss: 0.0242 |Test Loss: 1.6812|lr = 0.00108\n",
      "Epoch: 1342|steps:   60|Train Avg Loss: 0.0158 |Test Loss: 1.6082|lr = 0.00108\n",
      "Epoch: 1343|steps:   30|Train Avg Loss: 0.0165 |Test Loss: 1.6553|lr = 0.00108\n",
      "Epoch: 1343|steps:   60|Train Avg Loss: 0.0162 |Test Loss: 1.6306|lr = 0.00108\n",
      "Epoch: 1344|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.5902|lr = 0.00108\n",
      "Epoch: 1344|steps:   60|Train Avg Loss: 0.0149 |Test Loss: 1.6269|lr = 0.00108\n",
      "Epoch: 1345|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.6142|lr = 0.00108\n",
      "Epoch: 1345|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.6225|lr = 0.00108\n",
      "Epoch: 1346|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.6184|lr = 0.00108\n",
      "Epoch: 1346|steps:   60|Train Avg Loss: 0.0162 |Test Loss: 1.6528|lr = 0.00108\n",
      "Epoch: 1347|steps:   30|Train Avg Loss: 0.0298 |Test Loss: 1.5756|lr = 0.00108\n",
      "Epoch: 1347|steps:   60|Train Avg Loss: 0.0200 |Test Loss: 1.5492|lr = 0.00108\n",
      "Epoch: 1348|steps:   30|Train Avg Loss: 0.0251 |Test Loss: 1.6014|lr = 0.00108\n",
      "Epoch: 1348|steps:   60|Train Avg Loss: 0.0124 |Test Loss: 1.5495|lr = 0.00108\n",
      "Epoch: 1349|steps:   30|Train Avg Loss: 0.0173 |Test Loss: 1.6353|lr = 0.00108\n",
      "Epoch: 1349|steps:   60|Train Avg Loss: 0.0161 |Test Loss: 1.6669|lr = 0.00108\n",
      "Epoch: 1350|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.6200|lr = 0.00108\n",
      "Epoch: 1350|steps:   60|Train Avg Loss: 0.0123 |Test Loss: 1.5707|lr = 0.00108\n",
      "Epoch: 1351|steps:   30|Train Avg Loss: 0.0113 |Test Loss: 1.5843|lr = 0.00108\n",
      "Epoch: 1351|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.6294|lr = 0.00108\n",
      "Epoch: 1352|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.6352|lr = 0.00108\n",
      "Epoch: 1352|steps:   60|Train Avg Loss: 0.0124 |Test Loss: 1.6095|lr = 0.00108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1353|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.5843|lr = 0.00106\n",
      "Epoch: 1353|steps:   60|Train Avg Loss: 0.0102 |Test Loss: 1.5788|lr = 0.00106\n",
      "Epoch: 1354|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5698|lr = 0.00106\n",
      "Epoch: 1354|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.5606|lr = 0.00106\n",
      "Epoch: 1355|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5963|lr = 0.00106\n",
      "Epoch: 1355|steps:   60|Train Avg Loss: 0.0112 |Test Loss: 1.6437|lr = 0.00106\n",
      "Epoch: 1356|steps:   30|Train Avg Loss: 0.0131 |Test Loss: 1.6069|lr = 0.00106\n",
      "Epoch: 1356|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.5333|lr = 0.00106\n",
      "Epoch: 1357|steps:   30|Train Avg Loss: 0.0140 |Test Loss: 1.5754|lr = 0.00106\n",
      "Epoch: 1357|steps:   60|Train Avg Loss: 0.0131 |Test Loss: 1.6192|lr = 0.00106\n",
      "Epoch: 1358|steps:   30|Train Avg Loss: 0.0179 |Test Loss: 1.5824|lr = 0.00106\n",
      "Epoch: 1358|steps:   60|Train Avg Loss: 0.0118 |Test Loss: 1.5794|lr = 0.00106\n",
      "Epoch: 1359|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.5182|lr = 0.00106\n",
      "Epoch: 1359|steps:   60|Train Avg Loss: 0.0246 |Test Loss: 1.6075|lr = 0.00106\n",
      "Epoch: 1360|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6110|lr = 0.00106\n",
      "Epoch: 1360|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.5951|lr = 0.00106\n",
      "Epoch: 1361|steps:   30|Train Avg Loss: 0.0139 |Test Loss: 1.5668|lr = 0.00106\n",
      "Epoch: 1361|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6123|lr = 0.00106\n",
      "Epoch: 1362|steps:   30|Train Avg Loss: 0.0154 |Test Loss: 1.6163|lr = 0.00106\n",
      "Epoch: 1362|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.6315|lr = 0.00106\n",
      "Epoch: 1363|steps:   30|Train Avg Loss: 0.0163 |Test Loss: 1.6042|lr = 0.00106\n",
      "Epoch: 1363|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.5995|lr = 0.00106\n",
      "Epoch: 1364|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.5844|lr = 0.00104\n",
      "Epoch: 1364|steps:   60|Train Avg Loss: 0.0131 |Test Loss: 1.5631|lr = 0.00104\n",
      "Epoch: 1365|steps:   30|Train Avg Loss: 0.0144 |Test Loss: 1.6133|lr = 0.00104\n",
      "Epoch: 1365|steps:   60|Train Avg Loss: 0.0151 |Test Loss: 1.5653|lr = 0.00104\n",
      "Epoch: 1366|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.5836|lr = 0.00104\n",
      "Epoch: 1366|steps:   60|Train Avg Loss: 0.0123 |Test Loss: 1.5949|lr = 0.00104\n",
      "Epoch: 1367|steps:   30|Train Avg Loss: 0.0118 |Test Loss: 1.5664|lr = 0.00104\n",
      "Epoch: 1367|steps:   60|Train Avg Loss: 0.0160 |Test Loss: 1.5913|lr = 0.00104\n",
      "Epoch: 1368|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.6210|lr = 0.00104\n",
      "Epoch: 1368|steps:   60|Train Avg Loss: 0.0165 |Test Loss: 1.6492|lr = 0.00104\n",
      "Epoch: 1369|steps:   30|Train Avg Loss: 0.0148 |Test Loss: 1.6233|lr = 0.00104\n",
      "Epoch: 1369|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6052|lr = 0.00104\n",
      "Epoch: 1370|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.6675|lr = 0.00104\n",
      "Epoch: 1370|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6352|lr = 0.00104\n",
      "Epoch: 1371|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.6255|lr = 0.00104\n",
      "Epoch: 1371|steps:   60|Train Avg Loss: 0.0112 |Test Loss: 1.6327|lr = 0.00104\n",
      "Epoch: 1372|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.5816|lr = 0.00104\n",
      "Epoch: 1372|steps:   60|Train Avg Loss: 0.0128 |Test Loss: 1.5215|lr = 0.00104\n",
      "Epoch: 1373|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.6204|lr = 0.00104\n",
      "Epoch: 1373|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.5547|lr = 0.00104\n",
      "Epoch: 1374|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6369|lr = 0.00104\n",
      "Epoch: 1374|steps:   60|Train Avg Loss: 0.0130 |Test Loss: 1.5983|lr = 0.00104\n",
      "Epoch: 1375|steps:   30|Train Avg Loss: 0.0131 |Test Loss: 1.5762|lr = 0.00102\n",
      "Epoch: 1375|steps:   60|Train Avg Loss: 0.0125 |Test Loss: 1.6078|lr = 0.00102\n",
      "Epoch: 1376|steps:   30|Train Avg Loss: 0.0139 |Test Loss: 1.5972|lr = 0.00102\n",
      "Epoch: 1376|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6092|lr = 0.00102\n",
      "Epoch: 1377|steps:   30|Train Avg Loss: 0.0122 |Test Loss: 1.6112|lr = 0.00102\n",
      "Epoch: 1377|steps:   60|Train Avg Loss: 0.0168 |Test Loss: 1.6107|lr = 0.00102\n",
      "Epoch: 1378|steps:   30|Train Avg Loss: 0.0123 |Test Loss: 1.6229|lr = 0.00102\n",
      "Epoch: 1378|steps:   60|Train Avg Loss: 0.0133 |Test Loss: 1.5657|lr = 0.00102\n",
      "Epoch: 1379|steps:   30|Train Avg Loss: 0.0123 |Test Loss: 1.5875|lr = 0.00102\n",
      "Epoch: 1379|steps:   60|Train Avg Loss: 0.0131 |Test Loss: 1.6037|lr = 0.00102\n",
      "Epoch: 1380|steps:   30|Train Avg Loss: 0.0129 |Test Loss: 1.5944|lr = 0.00102\n",
      "Epoch: 1380|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6104|lr = 0.00102\n",
      "Epoch: 1381|steps:   30|Train Avg Loss: 0.0152 |Test Loss: 1.5884|lr = 0.00102\n",
      "Epoch: 1381|steps:   60|Train Avg Loss: 0.0159 |Test Loss: 1.5647|lr = 0.00102\n",
      "Epoch: 1382|steps:   30|Train Avg Loss: 0.0151 |Test Loss: 1.5704|lr = 0.00102\n",
      "Epoch: 1382|steps:   60|Train Avg Loss: 0.0153 |Test Loss: 1.5694|lr = 0.00102\n",
      "Epoch: 1383|steps:   30|Train Avg Loss: 0.0172 |Test Loss: 1.5841|lr = 0.00102\n",
      "Epoch: 1383|steps:   60|Train Avg Loss: 0.0182 |Test Loss: 1.6015|lr = 0.00102\n",
      "Epoch: 1384|steps:   30|Train Avg Loss: 0.0154 |Test Loss: 1.6203|lr = 0.00102\n",
      "Epoch: 1384|steps:   60|Train Avg Loss: 0.0164 |Test Loss: 1.5687|lr = 0.00102\n",
      "Epoch: 1385|steps:   30|Train Avg Loss: 0.0206 |Test Loss: 1.6238|lr = 0.00102\n",
      "Epoch: 1385|steps:   60|Train Avg Loss: 0.0141 |Test Loss: 1.6270|lr = 0.00102\n",
      "Epoch: 1386|steps:   30|Train Avg Loss: 0.0130 |Test Loss: 1.6070|lr = 0.00100\n",
      "Epoch: 1386|steps:   60|Train Avg Loss: 0.0163 |Test Loss: 1.6159|lr = 0.00100\n",
      "Epoch: 1387|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.5747|lr = 0.00100\n",
      "Epoch: 1387|steps:   60|Train Avg Loss: 0.0133 |Test Loss: 1.6024|lr = 0.00100\n",
      "Epoch: 1388|steps:   30|Train Avg Loss: 0.0169 |Test Loss: 1.6016|lr = 0.00100\n",
      "Epoch: 1388|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6152|lr = 0.00100\n",
      "Epoch: 1389|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.6367|lr = 0.00100\n",
      "Epoch: 1389|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6271|lr = 0.00100\n",
      "Epoch: 1390|steps:   30|Train Avg Loss: 0.0125 |Test Loss: 1.6715|lr = 0.00100\n",
      "Epoch: 1390|steps:   60|Train Avg Loss: 0.0128 |Test Loss: 1.6386|lr = 0.00100\n",
      "Epoch: 1391|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.5889|lr = 0.00100\n",
      "Epoch: 1391|steps:   60|Train Avg Loss: 0.0134 |Test Loss: 1.6444|lr = 0.00100\n",
      "Epoch: 1392|steps:   30|Train Avg Loss: 0.0152 |Test Loss: 1.6268|lr = 0.00100\n",
      "Epoch: 1392|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.6456|lr = 0.00100\n",
      "Epoch: 1393|steps:   30|Train Avg Loss: 0.0158 |Test Loss: 1.5984|lr = 0.00100\n",
      "Epoch: 1393|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.5809|lr = 0.00100\n",
      "Epoch: 1394|steps:   30|Train Avg Loss: 0.0119 |Test Loss: 1.5745|lr = 0.00100\n",
      "Epoch: 1394|steps:   60|Train Avg Loss: 0.0160 |Test Loss: 1.5837|lr = 0.00100\n",
      "Epoch: 1395|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6017|lr = 0.00100\n",
      "Epoch: 1395|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.6684|lr = 0.00100\n",
      "Epoch: 1396|steps:   30|Train Avg Loss: 0.0110 |Test Loss: 1.6444|lr = 0.00100\n",
      "Epoch: 1396|steps:   60|Train Avg Loss: 0.0105 |Test Loss: 1.6793|lr = 0.00100\n",
      "Epoch: 1397|steps:   30|Train Avg Loss: 0.0139 |Test Loss: 1.5292|lr = 0.00098\n",
      "Epoch: 1397|steps:   60|Train Avg Loss: 0.0119 |Test Loss: 1.5830|lr = 0.00098\n",
      "Epoch: 1398|steps:   30|Train Avg Loss: 0.0106 |Test Loss: 1.6538|lr = 0.00098\n",
      "Epoch: 1398|steps:   60|Train Avg Loss: 0.0260 |Test Loss: 1.5581|lr = 0.00098\n",
      "Epoch: 1399|steps:   30|Train Avg Loss: 0.0262 |Test Loss: 1.5696|lr = 0.00098\n",
      "Epoch: 1399|steps:   60|Train Avg Loss: 0.0358 |Test Loss: 1.5563|lr = 0.00098\n",
      "Epoch: 1400|steps:   30|Train Avg Loss: 0.0192 |Test Loss: 1.6523|lr = 0.00098\n",
      "Epoch: 1400|steps:   60|Train Avg Loss: 0.0204 |Test Loss: 1.6143|lr = 0.00098\n",
      "Epoch: 1401|steps:   30|Train Avg Loss: 0.0250 |Test Loss: 1.6130|lr = 0.00098\n",
      "Epoch: 1401|steps:   60|Train Avg Loss: 0.0153 |Test Loss: 1.6272|lr = 0.00098\n",
      "Epoch: 1402|steps:   30|Train Avg Loss: 0.0118 |Test Loss: 1.6512|lr = 0.00098\n",
      "Epoch: 1402|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6238|lr = 0.00098\n",
      "Epoch: 1403|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6072|lr = 0.00098\n",
      "Epoch: 1403|steps:   60|Train Avg Loss: 0.0120 |Test Loss: 1.6057|lr = 0.00098\n",
      "Epoch: 1404|steps:   30|Train Avg Loss: 0.0146 |Test Loss: 1.6560|lr = 0.00098\n",
      "Epoch: 1404|steps:   60|Train Avg Loss: 0.0105 |Test Loss: 1.6119|lr = 0.00098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1405|steps:   30|Train Avg Loss: 0.0139 |Test Loss: 1.6310|lr = 0.00098\n",
      "Epoch: 1405|steps:   60|Train Avg Loss: 0.0123 |Test Loss: 1.6107|lr = 0.00098\n",
      "Epoch: 1406|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.5713|lr = 0.00098\n",
      "Epoch: 1406|steps:   60|Train Avg Loss: 0.0103 |Test Loss: 1.6074|lr = 0.00098\n",
      "Epoch: 1407|steps:   30|Train Avg Loss: 0.0120 |Test Loss: 1.5407|lr = 0.00098\n",
      "Epoch: 1407|steps:   60|Train Avg Loss: 0.0126 |Test Loss: 1.5788|lr = 0.00098\n",
      "Epoch: 1408|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.5994|lr = 0.00096\n",
      "Epoch: 1408|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.5911|lr = 0.00096\n",
      "Epoch: 1409|steps:   30|Train Avg Loss: 0.0133 |Test Loss: 1.5975|lr = 0.00096\n",
      "Epoch: 1409|steps:   60|Train Avg Loss: 0.0132 |Test Loss: 1.5887|lr = 0.00096\n",
      "Epoch: 1410|steps:   30|Train Avg Loss: 0.0116 |Test Loss: 1.6665|lr = 0.00096\n",
      "Epoch: 1410|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.5938|lr = 0.00096\n",
      "Epoch: 1411|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.5918|lr = 0.00096\n",
      "Epoch: 1411|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.5895|lr = 0.00096\n",
      "Epoch: 1412|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6171|lr = 0.00096\n",
      "Epoch: 1412|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.6193|lr = 0.00096\n",
      "Epoch: 1413|steps:   30|Train Avg Loss: 0.0106 |Test Loss: 1.5467|lr = 0.00096\n",
      "Epoch: 1413|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.5673|lr = 0.00096\n",
      "Epoch: 1414|steps:   30|Train Avg Loss: 0.0126 |Test Loss: 1.5849|lr = 0.00096\n",
      "Epoch: 1414|steps:   60|Train Avg Loss: 0.0124 |Test Loss: 1.5846|lr = 0.00096\n",
      "Epoch: 1415|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.6626|lr = 0.00096\n",
      "Epoch: 1415|steps:   60|Train Avg Loss: 0.0133 |Test Loss: 1.6493|lr = 0.00096\n",
      "Epoch: 1416|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.5722|lr = 0.00096\n",
      "Epoch: 1416|steps:   60|Train Avg Loss: 0.0111 |Test Loss: 1.6356|lr = 0.00096\n",
      "Epoch: 1417|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.5970|lr = 0.00096\n",
      "Epoch: 1417|steps:   60|Train Avg Loss: 0.0139 |Test Loss: 1.5260|lr = 0.00096\n",
      "Epoch: 1418|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.5584|lr = 0.00096\n",
      "Epoch: 1418|steps:   60|Train Avg Loss: 0.0130 |Test Loss: 1.6226|lr = 0.00096\n",
      "Epoch: 1419|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.6372|lr = 0.00094\n",
      "Epoch: 1419|steps:   60|Train Avg Loss: 0.0119 |Test Loss: 1.5835|lr = 0.00094\n",
      "Epoch: 1420|steps:   30|Train Avg Loss: 0.0130 |Test Loss: 1.6225|lr = 0.00094\n",
      "Epoch: 1420|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.5647|lr = 0.00094\n",
      "Epoch: 1421|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.6523|lr = 0.00094\n",
      "Epoch: 1421|steps:   60|Train Avg Loss: 0.0113 |Test Loss: 1.6592|lr = 0.00094\n",
      "Epoch: 1422|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.6914|lr = 0.00094\n",
      "Epoch: 1422|steps:   60|Train Avg Loss: 0.0107 |Test Loss: 1.6148|lr = 0.00094\n",
      "Epoch: 1423|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.6147|lr = 0.00094\n",
      "Epoch: 1423|steps:   60|Train Avg Loss: 0.0124 |Test Loss: 1.6144|lr = 0.00094\n",
      "Epoch: 1424|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6158|lr = 0.00094\n",
      "Epoch: 1424|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.6438|lr = 0.00094\n",
      "Epoch: 1425|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6291|lr = 0.00094\n",
      "Epoch: 1425|steps:   60|Train Avg Loss: 0.0143 |Test Loss: 1.5698|lr = 0.00094\n",
      "Epoch: 1426|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.5734|lr = 0.00094\n",
      "Epoch: 1426|steps:   60|Train Avg Loss: 0.0108 |Test Loss: 1.6050|lr = 0.00094\n",
      "Epoch: 1427|steps:   30|Train Avg Loss: 0.0145 |Test Loss: 1.5706|lr = 0.00094\n",
      "Epoch: 1427|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.5927|lr = 0.00094\n",
      "Epoch: 1428|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.6158|lr = 0.00094\n",
      "Epoch: 1428|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6247|lr = 0.00094\n",
      "Epoch: 1429|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.6026|lr = 0.00094\n",
      "Epoch: 1429|steps:   60|Train Avg Loss: 0.0090 |Test Loss: 1.6408|lr = 0.00094\n",
      "Epoch: 1430|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.5840|lr = 0.00092\n",
      "Epoch: 1430|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6089|lr = 0.00092\n",
      "Epoch: 1431|steps:   30|Train Avg Loss: 0.0176 |Test Loss: 1.6184|lr = 0.00092\n",
      "Epoch: 1431|steps:   60|Train Avg Loss: 0.0132 |Test Loss: 1.6156|lr = 0.00092\n",
      "Epoch: 1432|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6066|lr = 0.00092\n",
      "Epoch: 1432|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6177|lr = 0.00092\n",
      "Epoch: 1433|steps:   30|Train Avg Loss: 0.0119 |Test Loss: 1.5464|lr = 0.00092\n",
      "Epoch: 1433|steps:   60|Train Avg Loss: 0.0132 |Test Loss: 1.5858|lr = 0.00092\n",
      "Epoch: 1434|steps:   30|Train Avg Loss: 0.0119 |Test Loss: 1.6416|lr = 0.00092\n",
      "Epoch: 1434|steps:   60|Train Avg Loss: 0.0124 |Test Loss: 1.6473|lr = 0.00092\n",
      "Epoch: 1435|steps:   30|Train Avg Loss: 0.0108 |Test Loss: 1.6017|lr = 0.00092\n",
      "Epoch: 1435|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.6181|lr = 0.00092\n",
      "Epoch: 1436|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6689|lr = 0.00092\n",
      "Epoch: 1436|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.6230|lr = 0.00092\n",
      "Epoch: 1437|steps:   30|Train Avg Loss: 0.0142 |Test Loss: 1.5966|lr = 0.00092\n",
      "Epoch: 1437|steps:   60|Train Avg Loss: 0.0128 |Test Loss: 1.6034|lr = 0.00092\n",
      "Epoch: 1438|steps:   30|Train Avg Loss: 0.0114 |Test Loss: 1.5913|lr = 0.00092\n",
      "Epoch: 1438|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6474|lr = 0.00092\n",
      "Epoch: 1439|steps:   30|Train Avg Loss: 0.0125 |Test Loss: 1.6200|lr = 0.00092\n",
      "Epoch: 1439|steps:   60|Train Avg Loss: 0.0150 |Test Loss: 1.6430|lr = 0.00092\n",
      "Epoch: 1440|steps:   30|Train Avg Loss: 0.0147 |Test Loss: 1.6405|lr = 0.00092\n",
      "Epoch: 1440|steps:   60|Train Avg Loss: 0.0117 |Test Loss: 1.5881|lr = 0.00092\n",
      "Epoch: 1441|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.6242|lr = 0.00090\n",
      "Epoch: 1441|steps:   60|Train Avg Loss: 0.0125 |Test Loss: 1.6434|lr = 0.00090\n",
      "Epoch: 1442|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6415|lr = 0.00090\n",
      "Epoch: 1442|steps:   60|Train Avg Loss: 0.0110 |Test Loss: 1.6173|lr = 0.00090\n",
      "Epoch: 1443|steps:   30|Train Avg Loss: 0.0080 |Test Loss: 1.6266|lr = 0.00090\n",
      "Epoch: 1443|steps:   60|Train Avg Loss: 0.0154 |Test Loss: 1.5532|lr = 0.00090\n",
      "Epoch: 1444|steps:   30|Train Avg Loss: 0.0120 |Test Loss: 1.6016|lr = 0.00090\n",
      "Epoch: 1444|steps:   60|Train Avg Loss: 0.0130 |Test Loss: 1.6117|lr = 0.00090\n",
      "Epoch: 1445|steps:   30|Train Avg Loss: 0.0132 |Test Loss: 1.6708|lr = 0.00090\n",
      "Epoch: 1445|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6058|lr = 0.00090\n",
      "Epoch: 1446|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.6390|lr = 0.00090\n",
      "Epoch: 1446|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6662|lr = 0.00090\n",
      "Epoch: 1447|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.6232|lr = 0.00090\n",
      "Epoch: 1447|steps:   60|Train Avg Loss: 0.0157 |Test Loss: 1.6109|lr = 0.00090\n",
      "Epoch: 1448|steps:   30|Train Avg Loss: 0.0162 |Test Loss: 1.6202|lr = 0.00090\n",
      "Epoch: 1448|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6247|lr = 0.00090\n",
      "Epoch: 1449|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.6240|lr = 0.00090\n",
      "Epoch: 1449|steps:   60|Train Avg Loss: 0.0108 |Test Loss: 1.5929|lr = 0.00090\n",
      "Epoch: 1450|steps:   30|Train Avg Loss: 0.0132 |Test Loss: 1.6426|lr = 0.00090\n",
      "Epoch: 1450|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6156|lr = 0.00090\n",
      "Epoch: 1451|steps:   30|Train Avg Loss: 0.0158 |Test Loss: 1.6145|lr = 0.00090\n",
      "Epoch: 1451|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6166|lr = 0.00090\n",
      "Epoch: 1452|steps:   30|Train Avg Loss: 0.0102 |Test Loss: 1.6231|lr = 0.00089\n",
      "Epoch: 1452|steps:   60|Train Avg Loss: 0.0128 |Test Loss: 1.6044|lr = 0.00089\n",
      "Epoch: 1453|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.5819|lr = 0.00089\n",
      "Epoch: 1453|steps:   60|Train Avg Loss: 0.0096 |Test Loss: 1.6535|lr = 0.00089\n",
      "Epoch: 1454|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.6427|lr = 0.00089\n",
      "Epoch: 1454|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.5941|lr = 0.00089\n",
      "Epoch: 1455|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6229|lr = 0.00089\n",
      "Epoch: 1455|steps:   60|Train Avg Loss: 0.0111 |Test Loss: 1.6072|lr = 0.00089\n",
      "Epoch: 1456|steps:   30|Train Avg Loss: 0.0109 |Test Loss: 1.6197|lr = 0.00089\n",
      "Epoch: 1456|steps:   60|Train Avg Loss: 0.0144 |Test Loss: 1.6277|lr = 0.00089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1457|steps:   30|Train Avg Loss: 0.0112 |Test Loss: 1.6339|lr = 0.00089\n",
      "Epoch: 1457|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.6174|lr = 0.00089\n",
      "Epoch: 1458|steps:   30|Train Avg Loss: 0.0133 |Test Loss: 1.6233|lr = 0.00089\n",
      "Epoch: 1458|steps:   60|Train Avg Loss: 0.0136 |Test Loss: 1.6387|lr = 0.00089\n",
      "Epoch: 1459|steps:   30|Train Avg Loss: 0.0242 |Test Loss: 1.5953|lr = 0.00089\n",
      "Epoch: 1459|steps:   60|Train Avg Loss: 0.0366 |Test Loss: 1.5823|lr = 0.00089\n",
      "Epoch: 1460|steps:   30|Train Avg Loss: 0.0244 |Test Loss: 1.6670|lr = 0.00089\n",
      "Epoch: 1460|steps:   60|Train Avg Loss: 0.0186 |Test Loss: 1.6803|lr = 0.00089\n",
      "Epoch: 1461|steps:   30|Train Avg Loss: 0.0159 |Test Loss: 1.6212|lr = 0.00089\n",
      "Epoch: 1461|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.6285|lr = 0.00089\n",
      "Epoch: 1462|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6403|lr = 0.00089\n",
      "Epoch: 1462|steps:   60|Train Avg Loss: 0.0120 |Test Loss: 1.6050|lr = 0.00089\n",
      "Epoch: 1463|steps:   30|Train Avg Loss: 0.0123 |Test Loss: 1.6111|lr = 0.00087\n",
      "Epoch: 1463|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.6358|lr = 0.00087\n",
      "Epoch: 1464|steps:   30|Train Avg Loss: 0.0120 |Test Loss: 1.6744|lr = 0.00087\n",
      "Epoch: 1464|steps:   60|Train Avg Loss: 0.0110 |Test Loss: 1.6507|lr = 0.00087\n",
      "Epoch: 1465|steps:   30|Train Avg Loss: 0.0120 |Test Loss: 1.6282|lr = 0.00087\n",
      "Epoch: 1465|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.6503|lr = 0.00087\n",
      "Epoch: 1466|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.6191|lr = 0.00087\n",
      "Epoch: 1466|steps:   60|Train Avg Loss: 0.0101 |Test Loss: 1.6295|lr = 0.00087\n",
      "Epoch: 1467|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6148|lr = 0.00087\n",
      "Epoch: 1467|steps:   60|Train Avg Loss: 0.0125 |Test Loss: 1.6523|lr = 0.00087\n",
      "Epoch: 1468|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6393|lr = 0.00087\n",
      "Epoch: 1468|steps:   60|Train Avg Loss: 0.0111 |Test Loss: 1.6237|lr = 0.00087\n",
      "Epoch: 1469|steps:   30|Train Avg Loss: 0.0106 |Test Loss: 1.6408|lr = 0.00087\n",
      "Epoch: 1469|steps:   60|Train Avg Loss: 0.0136 |Test Loss: 1.5885|lr = 0.00087\n",
      "Epoch: 1470|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6221|lr = 0.00087\n",
      "Epoch: 1470|steps:   60|Train Avg Loss: 0.0086 |Test Loss: 1.6230|lr = 0.00087\n",
      "Epoch: 1471|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.6656|lr = 0.00087\n",
      "Epoch: 1471|steps:   60|Train Avg Loss: 0.0126 |Test Loss: 1.6143|lr = 0.00087\n",
      "Epoch: 1472|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.5999|lr = 0.00087\n",
      "Epoch: 1472|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.5796|lr = 0.00087\n",
      "Epoch: 1473|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.6138|lr = 0.00087\n",
      "Epoch: 1473|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.5999|lr = 0.00087\n",
      "Epoch: 1474|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6120|lr = 0.00085\n",
      "Epoch: 1474|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6457|lr = 0.00085\n",
      "Epoch: 1475|steps:   30|Train Avg Loss: 0.0127 |Test Loss: 1.6225|lr = 0.00085\n",
      "Epoch: 1475|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6166|lr = 0.00085\n",
      "Epoch: 1476|steps:   30|Train Avg Loss: 0.0130 |Test Loss: 1.6464|lr = 0.00085\n",
      "Epoch: 1476|steps:   60|Train Avg Loss: 0.0103 |Test Loss: 1.6469|lr = 0.00085\n",
      "Epoch: 1477|steps:   30|Train Avg Loss: 0.0098 |Test Loss: 1.6365|lr = 0.00085\n",
      "Epoch: 1477|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6300|lr = 0.00085\n",
      "Epoch: 1478|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.6202|lr = 0.00085\n",
      "Epoch: 1478|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.5768|lr = 0.00085\n",
      "Epoch: 1479|steps:   30|Train Avg Loss: 0.0146 |Test Loss: 1.6193|lr = 0.00085\n",
      "Epoch: 1479|steps:   60|Train Avg Loss: 0.0113 |Test Loss: 1.6516|lr = 0.00085\n",
      "Epoch: 1480|steps:   30|Train Avg Loss: 0.0122 |Test Loss: 1.6239|lr = 0.00085\n",
      "Epoch: 1480|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.6178|lr = 0.00085\n",
      "Epoch: 1481|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6630|lr = 0.00085\n",
      "Epoch: 1481|steps:   60|Train Avg Loss: 0.0119 |Test Loss: 1.6619|lr = 0.00085\n",
      "Epoch: 1482|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.6572|lr = 0.00085\n",
      "Epoch: 1482|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.6993|lr = 0.00085\n",
      "Epoch: 1483|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6534|lr = 0.00085\n",
      "Epoch: 1483|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.6406|lr = 0.00085\n",
      "Epoch: 1484|steps:   30|Train Avg Loss: 0.0109 |Test Loss: 1.5930|lr = 0.00085\n",
      "Epoch: 1484|steps:   60|Train Avg Loss: 0.0120 |Test Loss: 1.6258|lr = 0.00085\n",
      "Epoch: 1485|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.6629|lr = 0.00083\n",
      "Epoch: 1485|steps:   60|Train Avg Loss: 0.0137 |Test Loss: 1.5973|lr = 0.00083\n",
      "Epoch: 1486|steps:   30|Train Avg Loss: 0.0165 |Test Loss: 1.6281|lr = 0.00083\n",
      "Epoch: 1486|steps:   60|Train Avg Loss: 0.0181 |Test Loss: 1.6365|lr = 0.00083\n",
      "Epoch: 1487|steps:   30|Train Avg Loss: 0.0116 |Test Loss: 1.5942|lr = 0.00083\n",
      "Epoch: 1487|steps:   60|Train Avg Loss: 0.0138 |Test Loss: 1.6207|lr = 0.00083\n",
      "Epoch: 1488|steps:   30|Train Avg Loss: 0.0119 |Test Loss: 1.6023|lr = 0.00083\n",
      "Epoch: 1488|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.6325|lr = 0.00083\n",
      "Epoch: 1489|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6308|lr = 0.00083\n",
      "Epoch: 1489|steps:   60|Train Avg Loss: 0.0111 |Test Loss: 1.6005|lr = 0.00083\n",
      "Epoch: 1490|steps:   30|Train Avg Loss: 0.0108 |Test Loss: 1.6537|lr = 0.00083\n",
      "Epoch: 1490|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6788|lr = 0.00083\n",
      "Epoch: 1491|steps:   30|Train Avg Loss: 0.0125 |Test Loss: 1.6361|lr = 0.00083\n",
      "Epoch: 1491|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.6267|lr = 0.00083\n",
      "Epoch: 1492|steps:   30|Train Avg Loss: 0.0124 |Test Loss: 1.5913|lr = 0.00083\n",
      "Epoch: 1492|steps:   60|Train Avg Loss: 0.0132 |Test Loss: 1.6380|lr = 0.00083\n",
      "Epoch: 1493|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6394|lr = 0.00083\n",
      "Epoch: 1493|steps:   60|Train Avg Loss: 0.0118 |Test Loss: 1.6301|lr = 0.00083\n",
      "Epoch: 1494|steps:   30|Train Avg Loss: 0.0138 |Test Loss: 1.6448|lr = 0.00083\n",
      "Epoch: 1494|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6390|lr = 0.00083\n",
      "Epoch: 1495|steps:   30|Train Avg Loss: 0.0153 |Test Loss: 1.6185|lr = 0.00083\n",
      "Epoch: 1495|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.6164|lr = 0.00083\n",
      "Epoch: 1496|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6290|lr = 0.00083\n",
      "Epoch: 1496|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6415|lr = 0.00083\n",
      "Epoch: 1497|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6478|lr = 0.00083\n",
      "Epoch: 1497|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6389|lr = 0.00083\n",
      "Epoch: 1498|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6279|lr = 0.00083\n",
      "Epoch: 1498|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6644|lr = 0.00083\n",
      "Epoch: 1499|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6130|lr = 0.00083\n",
      "Epoch: 1499|steps:   60|Train Avg Loss: 0.0111 |Test Loss: 1.6491|lr = 0.00083\n",
      "Epoch: 1500|steps:   30|Train Avg Loss: 0.0200 |Test Loss: 1.6513|lr = 0.00083\n",
      "Epoch: 1500|steps:   60|Train Avg Loss: 0.0135 |Test Loss: 1.6218|lr = 0.00083\n",
      "Epoch: 1501|steps:   30|Train Avg Loss: 0.0120 |Test Loss: 1.6459|lr = 0.00083\n",
      "Epoch: 1501|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.5882|lr = 0.00083\n",
      "Epoch: 1502|steps:   30|Train Avg Loss: 0.0166 |Test Loss: 1.6371|lr = 0.00083\n",
      "Epoch: 1502|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6499|lr = 0.00083\n",
      "Epoch: 1503|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.6626|lr = 0.00083\n",
      "Epoch: 1503|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.6750|lr = 0.00083\n",
      "Epoch: 1504|steps:   30|Train Avg Loss: 0.0146 |Test Loss: 1.6871|lr = 0.00083\n",
      "Epoch: 1504|steps:   60|Train Avg Loss: 0.0124 |Test Loss: 1.6223|lr = 0.00083\n",
      "Epoch: 1505|steps:   30|Train Avg Loss: 0.0109 |Test Loss: 1.6449|lr = 0.00083\n",
      "Epoch: 1505|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.6915|lr = 0.00083\n",
      "Epoch: 1506|steps:   30|Train Avg Loss: 0.0126 |Test Loss: 1.6371|lr = 0.00083\n",
      "Epoch: 1506|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.5969|lr = 0.00083\n",
      "Epoch: 1507|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6235|lr = 0.00083\n",
      "Epoch: 1507|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6169|lr = 0.00083\n",
      "Epoch: 1508|steps:   30|Train Avg Loss: 0.0118 |Test Loss: 1.5897|lr = 0.00083\n",
      "Epoch: 1508|steps:   60|Train Avg Loss: 0.0102 |Test Loss: 1.6168|lr = 0.00083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1509|steps:   30|Train Avg Loss: 0.0113 |Test Loss: 1.6095|lr = 0.00083\n",
      "Epoch: 1509|steps:   60|Train Avg Loss: 0.0127 |Test Loss: 1.6433|lr = 0.00083\n",
      "Epoch: 1510|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6327|lr = 0.00083\n",
      "Epoch: 1510|steps:   60|Train Avg Loss: 0.0101 |Test Loss: 1.6041|lr = 0.00083\n",
      "Epoch: 1511|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.5847|lr = 0.00083\n",
      "Epoch: 1511|steps:   60|Train Avg Loss: 0.0105 |Test Loss: 1.6183|lr = 0.00083\n",
      "Epoch: 1512|steps:   30|Train Avg Loss: 0.0133 |Test Loss: 1.6848|lr = 0.00083\n",
      "Epoch: 1512|steps:   60|Train Avg Loss: 0.0131 |Test Loss: 1.6026|lr = 0.00083\n",
      "Epoch: 1513|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6127|lr = 0.00083\n",
      "Epoch: 1513|steps:   60|Train Avg Loss: 0.0131 |Test Loss: 1.6443|lr = 0.00083\n",
      "Epoch: 1514|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.6357|lr = 0.00083\n",
      "Epoch: 1514|steps:   60|Train Avg Loss: 0.0124 |Test Loss: 1.6661|lr = 0.00083\n",
      "Epoch: 1515|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6223|lr = 0.00083\n",
      "Epoch: 1515|steps:   60|Train Avg Loss: 0.0107 |Test Loss: 1.6373|lr = 0.00083\n",
      "Epoch: 1516|steps:   30|Train Avg Loss: 0.0123 |Test Loss: 1.6567|lr = 0.00083\n",
      "Epoch: 1516|steps:   60|Train Avg Loss: 0.0138 |Test Loss: 1.6583|lr = 0.00083\n",
      "Epoch: 1517|steps:   30|Train Avg Loss: 0.0106 |Test Loss: 1.6389|lr = 0.00083\n",
      "Epoch: 1517|steps:   60|Train Avg Loss: 0.0113 |Test Loss: 1.6548|lr = 0.00083\n",
      "Epoch: 1518|steps:   30|Train Avg Loss: 0.0150 |Test Loss: 1.6651|lr = 0.00082\n",
      "Epoch: 1518|steps:   60|Train Avg Loss: 0.0198 |Test Loss: 1.6264|lr = 0.00082\n",
      "Epoch: 1519|steps:   30|Train Avg Loss: 0.0133 |Test Loss: 1.6347|lr = 0.00082\n",
      "Epoch: 1519|steps:   60|Train Avg Loss: 0.0105 |Test Loss: 1.6703|lr = 0.00082\n",
      "Epoch: 1520|steps:   30|Train Avg Loss: 0.0141 |Test Loss: 1.6534|lr = 0.00082\n",
      "Epoch: 1520|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.6520|lr = 0.00082\n",
      "Epoch: 1521|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.6797|lr = 0.00082\n",
      "Epoch: 1521|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.6061|lr = 0.00082\n",
      "Epoch: 1522|steps:   30|Train Avg Loss: 0.0113 |Test Loss: 1.6689|lr = 0.00082\n",
      "Epoch: 1522|steps:   60|Train Avg Loss: 0.0123 |Test Loss: 1.6917|lr = 0.00082\n",
      "Epoch: 1523|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.6616|lr = 0.00082\n",
      "Epoch: 1523|steps:   60|Train Avg Loss: 0.0142 |Test Loss: 1.6640|lr = 0.00082\n",
      "Epoch: 1524|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.6069|lr = 0.00082\n",
      "Epoch: 1524|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6703|lr = 0.00082\n",
      "Epoch: 1525|steps:   30|Train Avg Loss: 0.0108 |Test Loss: 1.6416|lr = 0.00082\n",
      "Epoch: 1525|steps:   60|Train Avg Loss: 0.0134 |Test Loss: 1.6166|lr = 0.00082\n",
      "Epoch: 1526|steps:   30|Train Avg Loss: 0.0135 |Test Loss: 1.6527|lr = 0.00082\n",
      "Epoch: 1526|steps:   60|Train Avg Loss: 0.0118 |Test Loss: 1.6212|lr = 0.00082\n",
      "Epoch: 1527|steps:   30|Train Avg Loss: 0.0140 |Test Loss: 1.5890|lr = 0.00082\n",
      "Epoch: 1527|steps:   60|Train Avg Loss: 0.0113 |Test Loss: 1.6040|lr = 0.00082\n",
      "Epoch: 1528|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.6639|lr = 0.00082\n",
      "Epoch: 1528|steps:   60|Train Avg Loss: 0.0183 |Test Loss: 1.6016|lr = 0.00082\n",
      "Epoch: 1529|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.5938|lr = 0.00080\n",
      "Epoch: 1529|steps:   60|Train Avg Loss: 0.0144 |Test Loss: 1.6496|lr = 0.00080\n",
      "Epoch: 1530|steps:   30|Train Avg Loss: 0.0200 |Test Loss: 1.6772|lr = 0.00080\n",
      "Epoch: 1530|steps:   60|Train Avg Loss: 0.0350 |Test Loss: 1.6782|lr = 0.00080\n",
      "Epoch: 1531|steps:   30|Train Avg Loss: 0.0234 |Test Loss: 1.6557|lr = 0.00080\n",
      "Epoch: 1531|steps:   60|Train Avg Loss: 0.0153 |Test Loss: 1.6617|lr = 0.00080\n",
      "Epoch: 1532|steps:   30|Train Avg Loss: 0.0176 |Test Loss: 1.6647|lr = 0.00080\n",
      "Epoch: 1532|steps:   60|Train Avg Loss: 0.0158 |Test Loss: 1.6798|lr = 0.00080\n",
      "Epoch: 1533|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6389|lr = 0.00080\n",
      "Epoch: 1533|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6337|lr = 0.00080\n",
      "Epoch: 1534|steps:   30|Train Avg Loss: 0.0070 |Test Loss: 1.6719|lr = 0.00080\n",
      "Epoch: 1534|steps:   60|Train Avg Loss: 0.0119 |Test Loss: 1.6387|lr = 0.00080\n",
      "Epoch: 1535|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.6153|lr = 0.00080\n",
      "Epoch: 1535|steps:   60|Train Avg Loss: 0.0096 |Test Loss: 1.6083|lr = 0.00080\n",
      "Epoch: 1536|steps:   30|Train Avg Loss: 0.0082 |Test Loss: 1.6389|lr = 0.00080\n",
      "Epoch: 1536|steps:   60|Train Avg Loss: 0.0107 |Test Loss: 1.6582|lr = 0.00080\n",
      "Epoch: 1537|steps:   30|Train Avg Loss: 0.0092 |Test Loss: 1.6361|lr = 0.00080\n",
      "Epoch: 1537|steps:   60|Train Avg Loss: 0.0096 |Test Loss: 1.6484|lr = 0.00080\n",
      "Epoch: 1538|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6383|lr = 0.00080\n",
      "Epoch: 1538|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6696|lr = 0.00080\n",
      "Epoch: 1539|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.6187|lr = 0.00080\n",
      "Epoch: 1539|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6261|lr = 0.00080\n",
      "Epoch: 1540|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.5888|lr = 0.00078\n",
      "Epoch: 1540|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6313|lr = 0.00078\n",
      "Epoch: 1541|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.6079|lr = 0.00078\n",
      "Epoch: 1541|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.6186|lr = 0.00078\n",
      "Epoch: 1542|steps:   30|Train Avg Loss: 0.0082 |Test Loss: 1.5525|lr = 0.00078\n",
      "Epoch: 1542|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6090|lr = 0.00078\n",
      "Epoch: 1543|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6407|lr = 0.00078\n",
      "Epoch: 1543|steps:   60|Train Avg Loss: 0.0117 |Test Loss: 1.6421|lr = 0.00078\n",
      "Epoch: 1544|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6400|lr = 0.00078\n",
      "Epoch: 1544|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.6323|lr = 0.00078\n",
      "Epoch: 1545|steps:   30|Train Avg Loss: 0.0114 |Test Loss: 1.6589|lr = 0.00078\n",
      "Epoch: 1545|steps:   60|Train Avg Loss: 0.0116 |Test Loss: 1.6722|lr = 0.00078\n",
      "Epoch: 1546|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6020|lr = 0.00078\n",
      "Epoch: 1546|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6512|lr = 0.00078\n",
      "Epoch: 1547|steps:   30|Train Avg Loss: 0.0093 |Test Loss: 1.6215|lr = 0.00078\n",
      "Epoch: 1547|steps:   60|Train Avg Loss: 0.0097 |Test Loss: 1.5900|lr = 0.00078\n",
      "Epoch: 1548|steps:   30|Train Avg Loss: 0.0134 |Test Loss: 1.6474|lr = 0.00078\n",
      "Epoch: 1548|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.6303|lr = 0.00078\n",
      "Epoch: 1549|steps:   30|Train Avg Loss: 0.0093 |Test Loss: 1.6168|lr = 0.00078\n",
      "Epoch: 1549|steps:   60|Train Avg Loss: 0.0119 |Test Loss: 1.6445|lr = 0.00078\n",
      "Epoch: 1550|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6316|lr = 0.00078\n",
      "Epoch: 1550|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6582|lr = 0.00078\n",
      "Epoch: 1551|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6058|lr = 0.00077\n",
      "Epoch: 1551|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6150|lr = 0.00077\n",
      "Epoch: 1552|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6479|lr = 0.00077\n",
      "Epoch: 1552|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6266|lr = 0.00077\n",
      "Epoch: 1553|steps:   30|Train Avg Loss: 0.0104 |Test Loss: 1.6299|lr = 0.00077\n",
      "Epoch: 1553|steps:   60|Train Avg Loss: 0.0132 |Test Loss: 1.6215|lr = 0.00077\n",
      "Epoch: 1554|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.5975|lr = 0.00077\n",
      "Epoch: 1554|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6563|lr = 0.00077\n",
      "Epoch: 1555|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6279|lr = 0.00077\n",
      "Epoch: 1555|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6320|lr = 0.00077\n",
      "Epoch: 1556|steps:   30|Train Avg Loss: 0.0132 |Test Loss: 1.6377|lr = 0.00077\n",
      "Epoch: 1556|steps:   60|Train Avg Loss: 0.0086 |Test Loss: 1.6923|lr = 0.00077\n",
      "Epoch: 1557|steps:   30|Train Avg Loss: 0.0137 |Test Loss: 1.6092|lr = 0.00077\n",
      "Epoch: 1557|steps:   60|Train Avg Loss: 0.0135 |Test Loss: 1.6650|lr = 0.00077\n",
      "Epoch: 1558|steps:   30|Train Avg Loss: 0.0187 |Test Loss: 1.6359|lr = 0.00077\n",
      "Epoch: 1558|steps:   60|Train Avg Loss: 0.0227 |Test Loss: 1.6717|lr = 0.00077\n",
      "Epoch: 1559|steps:   30|Train Avg Loss: 0.0149 |Test Loss: 1.6440|lr = 0.00077\n",
      "Epoch: 1559|steps:   60|Train Avg Loss: 0.0108 |Test Loss: 1.6938|lr = 0.00077\n",
      "Epoch: 1560|steps:   30|Train Avg Loss: 0.0102 |Test Loss: 1.6577|lr = 0.00077\n",
      "Epoch: 1560|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.6264|lr = 0.00077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1561|steps:   30|Train Avg Loss: 0.0110 |Test Loss: 1.6268|lr = 0.00077\n",
      "Epoch: 1561|steps:   60|Train Avg Loss: 0.0099 |Test Loss: 1.6462|lr = 0.00077\n",
      "Epoch: 1562|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.6747|lr = 0.00075\n",
      "Epoch: 1562|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6960|lr = 0.00075\n",
      "Epoch: 1563|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.6149|lr = 0.00075\n",
      "Epoch: 1563|steps:   60|Train Avg Loss: 0.0086 |Test Loss: 1.6119|lr = 0.00075\n",
      "Epoch: 1564|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.6551|lr = 0.00075\n",
      "Epoch: 1564|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.5950|lr = 0.00075\n",
      "Epoch: 1565|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.6152|lr = 0.00075\n",
      "Epoch: 1565|steps:   60|Train Avg Loss: 0.0090 |Test Loss: 1.6349|lr = 0.00075\n",
      "Epoch: 1566|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6565|lr = 0.00075\n",
      "Epoch: 1566|steps:   60|Train Avg Loss: 0.0123 |Test Loss: 1.6465|lr = 0.00075\n",
      "Epoch: 1567|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6435|lr = 0.00075\n",
      "Epoch: 1567|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.6331|lr = 0.00075\n",
      "Epoch: 1568|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6397|lr = 0.00075\n",
      "Epoch: 1568|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6224|lr = 0.00075\n",
      "Epoch: 1569|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6300|lr = 0.00075\n",
      "Epoch: 1569|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.6343|lr = 0.00075\n",
      "Epoch: 1570|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6281|lr = 0.00075\n",
      "Epoch: 1570|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.6392|lr = 0.00075\n",
      "Epoch: 1571|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6557|lr = 0.00075\n",
      "Epoch: 1571|steps:   60|Train Avg Loss: 0.0108 |Test Loss: 1.6570|lr = 0.00075\n",
      "Epoch: 1572|steps:   30|Train Avg Loss: 0.0132 |Test Loss: 1.6595|lr = 0.00075\n",
      "Epoch: 1572|steps:   60|Train Avg Loss: 0.0130 |Test Loss: 1.6598|lr = 0.00075\n",
      "Epoch: 1573|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6250|lr = 0.00074\n",
      "Epoch: 1573|steps:   60|Train Avg Loss: 0.0110 |Test Loss: 1.6531|lr = 0.00074\n",
      "Epoch: 1574|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6750|lr = 0.00074\n",
      "Epoch: 1574|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6439|lr = 0.00074\n",
      "Epoch: 1575|steps:   30|Train Avg Loss: 0.0098 |Test Loss: 1.6407|lr = 0.00074\n",
      "Epoch: 1575|steps:   60|Train Avg Loss: 0.0123 |Test Loss: 1.6260|lr = 0.00074\n",
      "Epoch: 1576|steps:   30|Train Avg Loss: 0.0147 |Test Loss: 1.6059|lr = 0.00074\n",
      "Epoch: 1576|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6534|lr = 0.00074\n",
      "Epoch: 1577|steps:   30|Train Avg Loss: 0.0126 |Test Loss: 1.6463|lr = 0.00074\n",
      "Epoch: 1577|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.6263|lr = 0.00074\n",
      "Epoch: 1578|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.6390|lr = 0.00074\n",
      "Epoch: 1578|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6654|lr = 0.00074\n",
      "Epoch: 1579|steps:   30|Train Avg Loss: 0.0108 |Test Loss: 1.6267|lr = 0.00074\n",
      "Epoch: 1579|steps:   60|Train Avg Loss: 0.0119 |Test Loss: 1.6197|lr = 0.00074\n",
      "Epoch: 1580|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.6263|lr = 0.00074\n",
      "Epoch: 1580|steps:   60|Train Avg Loss: 0.0103 |Test Loss: 1.5982|lr = 0.00074\n",
      "Epoch: 1581|steps:   30|Train Avg Loss: 0.0120 |Test Loss: 1.6570|lr = 0.00074\n",
      "Epoch: 1581|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.5867|lr = 0.00074\n",
      "Epoch: 1582|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.6123|lr = 0.00074\n",
      "Epoch: 1582|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.6611|lr = 0.00074\n",
      "Epoch: 1583|steps:   30|Train Avg Loss: 0.0080 |Test Loss: 1.6467|lr = 0.00074\n",
      "Epoch: 1583|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6360|lr = 0.00074\n",
      "Epoch: 1584|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6816|lr = 0.00072\n",
      "Epoch: 1584|steps:   60|Train Avg Loss: 0.0113 |Test Loss: 1.5967|lr = 0.00072\n",
      "Epoch: 1585|steps:   30|Train Avg Loss: 0.0081 |Test Loss: 1.6574|lr = 0.00072\n",
      "Epoch: 1585|steps:   60|Train Avg Loss: 0.0167 |Test Loss: 1.6554|lr = 0.00072\n",
      "Epoch: 1586|steps:   30|Train Avg Loss: 0.0123 |Test Loss: 1.6323|lr = 0.00072\n",
      "Epoch: 1586|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6543|lr = 0.00072\n",
      "Epoch: 1587|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6510|lr = 0.00072\n",
      "Epoch: 1587|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.6618|lr = 0.00072\n",
      "Epoch: 1588|steps:   30|Train Avg Loss: 0.0143 |Test Loss: 1.6399|lr = 0.00072\n",
      "Epoch: 1588|steps:   60|Train Avg Loss: 0.0096 |Test Loss: 1.6362|lr = 0.00072\n",
      "Epoch: 1589|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6335|lr = 0.00072\n",
      "Epoch: 1589|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6806|lr = 0.00072\n",
      "Epoch: 1590|steps:   30|Train Avg Loss: 0.0135 |Test Loss: 1.6744|lr = 0.00072\n",
      "Epoch: 1590|steps:   60|Train Avg Loss: 0.0098 |Test Loss: 1.6482|lr = 0.00072\n",
      "Epoch: 1591|steps:   30|Train Avg Loss: 0.0121 |Test Loss: 1.6074|lr = 0.00072\n",
      "Epoch: 1591|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6391|lr = 0.00072\n",
      "Epoch: 1592|steps:   30|Train Avg Loss: 0.0108 |Test Loss: 1.6265|lr = 0.00072\n",
      "Epoch: 1592|steps:   60|Train Avg Loss: 0.0098 |Test Loss: 1.6315|lr = 0.00072\n",
      "Epoch: 1593|steps:   30|Train Avg Loss: 0.0118 |Test Loss: 1.6597|lr = 0.00072\n",
      "Epoch: 1593|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6712|lr = 0.00072\n",
      "Epoch: 1594|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6639|lr = 0.00072\n",
      "Epoch: 1594|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6637|lr = 0.00072\n",
      "Epoch: 1595|steps:   30|Train Avg Loss: 0.0093 |Test Loss: 1.6757|lr = 0.00071\n",
      "Epoch: 1595|steps:   60|Train Avg Loss: 0.0101 |Test Loss: 1.6636|lr = 0.00071\n",
      "Epoch: 1596|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.6490|lr = 0.00071\n",
      "Epoch: 1596|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6312|lr = 0.00071\n",
      "Epoch: 1597|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6456|lr = 0.00071\n",
      "Epoch: 1597|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6049|lr = 0.00071\n",
      "Epoch: 1598|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6529|lr = 0.00071\n",
      "Epoch: 1598|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6679|lr = 0.00071\n",
      "Epoch: 1599|steps:   30|Train Avg Loss: 0.0113 |Test Loss: 1.6702|lr = 0.00071\n",
      "Epoch: 1599|steps:   60|Train Avg Loss: 0.0101 |Test Loss: 1.6250|lr = 0.00071\n",
      "Epoch: 1600|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6260|lr = 0.00071\n",
      "Epoch: 1600|steps:   60|Train Avg Loss: 0.0143 |Test Loss: 1.7087|lr = 0.00071\n",
      "Epoch: 1601|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.6642|lr = 0.00071\n",
      "Epoch: 1601|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.6783|lr = 0.00071\n",
      "Epoch: 1602|steps:   30|Train Avg Loss: 0.0087 |Test Loss: 1.6298|lr = 0.00071\n",
      "Epoch: 1602|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.6536|lr = 0.00071\n",
      "Epoch: 1603|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6684|lr = 0.00071\n",
      "Epoch: 1603|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.6712|lr = 0.00071\n",
      "Epoch: 1604|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6907|lr = 0.00071\n",
      "Epoch: 1604|steps:   60|Train Avg Loss: 0.0078 |Test Loss: 1.6596|lr = 0.00071\n",
      "Epoch: 1605|steps:   30|Train Avg Loss: 0.0128 |Test Loss: 1.6379|lr = 0.00071\n",
      "Epoch: 1605|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.6460|lr = 0.00071\n",
      "Epoch: 1606|steps:   30|Train Avg Loss: 0.0104 |Test Loss: 1.6148|lr = 0.00069\n",
      "Epoch: 1606|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.6472|lr = 0.00069\n",
      "Epoch: 1607|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.6746|lr = 0.00069\n",
      "Epoch: 1607|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6547|lr = 0.00069\n",
      "Epoch: 1608|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.6464|lr = 0.00069\n",
      "Epoch: 1608|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6317|lr = 0.00069\n",
      "Epoch: 1609|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6524|lr = 0.00069\n",
      "Epoch: 1609|steps:   60|Train Avg Loss: 0.0111 |Test Loss: 1.6419|lr = 0.00069\n",
      "Epoch: 1610|steps:   30|Train Avg Loss: 0.0114 |Test Loss: 1.6672|lr = 0.00069\n",
      "Epoch: 1610|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.6253|lr = 0.00069\n",
      "Epoch: 1611|steps:   30|Train Avg Loss: 0.0093 |Test Loss: 1.6238|lr = 0.00069\n",
      "Epoch: 1611|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6939|lr = 0.00069\n",
      "Epoch: 1612|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6064|lr = 0.00069\n",
      "Epoch: 1612|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6160|lr = 0.00069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1613|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.6245|lr = 0.00069\n",
      "Epoch: 1613|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6152|lr = 0.00069\n",
      "Epoch: 1614|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.6282|lr = 0.00069\n",
      "Epoch: 1614|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6703|lr = 0.00069\n",
      "Epoch: 1615|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6609|lr = 0.00069\n",
      "Epoch: 1615|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.6723|lr = 0.00069\n",
      "Epoch: 1616|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6450|lr = 0.00069\n",
      "Epoch: 1616|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6433|lr = 0.00069\n",
      "Epoch: 1617|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.6384|lr = 0.00068\n",
      "Epoch: 1617|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6571|lr = 0.00068\n",
      "Epoch: 1618|steps:   30|Train Avg Loss: 0.0140 |Test Loss: 1.6447|lr = 0.00068\n",
      "Epoch: 1618|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6508|lr = 0.00068\n",
      "Epoch: 1619|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6648|lr = 0.00068\n",
      "Epoch: 1619|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6551|lr = 0.00068\n",
      "Epoch: 1620|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6905|lr = 0.00068\n",
      "Epoch: 1620|steps:   60|Train Avg Loss: 0.0102 |Test Loss: 1.6431|lr = 0.00068\n",
      "Epoch: 1621|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6405|lr = 0.00068\n",
      "Epoch: 1621|steps:   60|Train Avg Loss: 0.0108 |Test Loss: 1.6394|lr = 0.00068\n",
      "Epoch: 1622|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.6604|lr = 0.00068\n",
      "Epoch: 1622|steps:   60|Train Avg Loss: 0.0118 |Test Loss: 1.6573|lr = 0.00068\n",
      "Epoch: 1623|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.6664|lr = 0.00068\n",
      "Epoch: 1623|steps:   60|Train Avg Loss: 0.0098 |Test Loss: 1.6916|lr = 0.00068\n",
      "Epoch: 1624|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6049|lr = 0.00068\n",
      "Epoch: 1624|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.6471|lr = 0.00068\n",
      "Epoch: 1625|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.6633|lr = 0.00068\n",
      "Epoch: 1625|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.6719|lr = 0.00068\n",
      "Epoch: 1626|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.6361|lr = 0.00068\n",
      "Epoch: 1626|steps:   60|Train Avg Loss: 0.0145 |Test Loss: 1.6807|lr = 0.00068\n",
      "Epoch: 1627|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.6922|lr = 0.00068\n",
      "Epoch: 1627|steps:   60|Train Avg Loss: 0.0108 |Test Loss: 1.6451|lr = 0.00068\n",
      "Epoch: 1628|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6866|lr = 0.00067\n",
      "Epoch: 1628|steps:   60|Train Avg Loss: 0.0102 |Test Loss: 1.6831|lr = 0.00067\n",
      "Epoch: 1629|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6309|lr = 0.00067\n",
      "Epoch: 1629|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6725|lr = 0.00067\n",
      "Epoch: 1630|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.6280|lr = 0.00067\n",
      "Epoch: 1630|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6515|lr = 0.00067\n",
      "Epoch: 1631|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6680|lr = 0.00067\n",
      "Epoch: 1631|steps:   60|Train Avg Loss: 0.0086 |Test Loss: 1.6445|lr = 0.00067\n",
      "Epoch: 1632|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6626|lr = 0.00067\n",
      "Epoch: 1632|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6915|lr = 0.00067\n",
      "Epoch: 1633|steps:   30|Train Avg Loss: 0.0106 |Test Loss: 1.6658|lr = 0.00067\n",
      "Epoch: 1633|steps:   60|Train Avg Loss: 0.0090 |Test Loss: 1.6345|lr = 0.00067\n",
      "Epoch: 1634|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.6360|lr = 0.00067\n",
      "Epoch: 1634|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6631|lr = 0.00067\n",
      "Epoch: 1635|steps:   30|Train Avg Loss: 0.0116 |Test Loss: 1.6623|lr = 0.00067\n",
      "Epoch: 1635|steps:   60|Train Avg Loss: 0.0118 |Test Loss: 1.6396|lr = 0.00067\n",
      "Epoch: 1636|steps:   30|Train Avg Loss: 0.0083 |Test Loss: 1.6647|lr = 0.00067\n",
      "Epoch: 1636|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.6512|lr = 0.00067\n",
      "Epoch: 1637|steps:   30|Train Avg Loss: 0.0093 |Test Loss: 1.6507|lr = 0.00067\n",
      "Epoch: 1637|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6688|lr = 0.00067\n",
      "Epoch: 1638|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6650|lr = 0.00067\n",
      "Epoch: 1638|steps:   60|Train Avg Loss: 0.0086 |Test Loss: 1.6550|lr = 0.00067\n",
      "Epoch: 1639|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6767|lr = 0.00065\n",
      "Epoch: 1639|steps:   60|Train Avg Loss: 0.0090 |Test Loss: 1.6510|lr = 0.00065\n",
      "Epoch: 1640|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6441|lr = 0.00065\n",
      "Epoch: 1640|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6513|lr = 0.00065\n",
      "Epoch: 1641|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6643|lr = 0.00065\n",
      "Epoch: 1641|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6552|lr = 0.00065\n",
      "Epoch: 1642|steps:   30|Train Avg Loss: 0.0083 |Test Loss: 1.6664|lr = 0.00065\n",
      "Epoch: 1642|steps:   60|Train Avg Loss: 0.0136 |Test Loss: 1.6250|lr = 0.00065\n",
      "Epoch: 1643|steps:   30|Train Avg Loss: 0.0115 |Test Loss: 1.6763|lr = 0.00065\n",
      "Epoch: 1643|steps:   60|Train Avg Loss: 0.0119 |Test Loss: 1.6677|lr = 0.00065\n",
      "Epoch: 1644|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6514|lr = 0.00065\n",
      "Epoch: 1644|steps:   60|Train Avg Loss: 0.0098 |Test Loss: 1.6369|lr = 0.00065\n",
      "Epoch: 1645|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.6451|lr = 0.00065\n",
      "Epoch: 1645|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6530|lr = 0.00065\n",
      "Epoch: 1646|steps:   30|Train Avg Loss: 0.0113 |Test Loss: 1.6413|lr = 0.00065\n",
      "Epoch: 1646|steps:   60|Train Avg Loss: 0.0108 |Test Loss: 1.6229|lr = 0.00065\n",
      "Epoch: 1647|steps:   30|Train Avg Loss: 0.0144 |Test Loss: 1.6867|lr = 0.00065\n",
      "Epoch: 1647|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6544|lr = 0.00065\n",
      "Epoch: 1648|steps:   30|Train Avg Loss: 0.0110 |Test Loss: 1.6705|lr = 0.00065\n",
      "Epoch: 1648|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.6644|lr = 0.00065\n",
      "Epoch: 1649|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6337|lr = 0.00065\n",
      "Epoch: 1649|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.6163|lr = 0.00065\n",
      "Epoch: 1650|steps:   30|Train Avg Loss: 0.0125 |Test Loss: 1.6576|lr = 0.00064\n",
      "Epoch: 1650|steps:   60|Train Avg Loss: 0.0096 |Test Loss: 1.6844|lr = 0.00064\n",
      "Epoch: 1651|steps:   30|Train Avg Loss: 0.0110 |Test Loss: 1.6475|lr = 0.00064\n",
      "Epoch: 1651|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.6153|lr = 0.00064\n",
      "Epoch: 1652|steps:   30|Train Avg Loss: 0.0119 |Test Loss: 1.6470|lr = 0.00064\n",
      "Epoch: 1652|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6121|lr = 0.00064\n",
      "Epoch: 1653|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6457|lr = 0.00064\n",
      "Epoch: 1653|steps:   60|Train Avg Loss: 0.0105 |Test Loss: 1.6373|lr = 0.00064\n",
      "Epoch: 1654|steps:   30|Train Avg Loss: 0.0081 |Test Loss: 1.6372|lr = 0.00064\n",
      "Epoch: 1654|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6344|lr = 0.00064\n",
      "Epoch: 1655|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6543|lr = 0.00064\n",
      "Epoch: 1655|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6612|lr = 0.00064\n",
      "Epoch: 1656|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.6372|lr = 0.00064\n",
      "Epoch: 1656|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6218|lr = 0.00064\n",
      "Epoch: 1657|steps:   30|Train Avg Loss: 0.0083 |Test Loss: 1.6467|lr = 0.00064\n",
      "Epoch: 1657|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6741|lr = 0.00064\n",
      "Epoch: 1658|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.6412|lr = 0.00064\n",
      "Epoch: 1658|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6266|lr = 0.00064\n",
      "Epoch: 1659|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6489|lr = 0.00064\n",
      "Epoch: 1659|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6508|lr = 0.00064\n",
      "Epoch: 1660|steps:   30|Train Avg Loss: 0.0109 |Test Loss: 1.6522|lr = 0.00064\n",
      "Epoch: 1660|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.6138|lr = 0.00064\n",
      "Epoch: 1661|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6558|lr = 0.00063\n",
      "Epoch: 1661|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.6554|lr = 0.00063\n",
      "Epoch: 1662|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6296|lr = 0.00063\n",
      "Epoch: 1662|steps:   60|Train Avg Loss: 0.0110 |Test Loss: 1.6585|lr = 0.00063\n",
      "Epoch: 1663|steps:   30|Train Avg Loss: 0.0087 |Test Loss: 1.6440|lr = 0.00063\n",
      "Epoch: 1663|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6822|lr = 0.00063\n",
      "Epoch: 1664|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.6496|lr = 0.00063\n",
      "Epoch: 1664|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.6511|lr = 0.00063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1665|steps:   30|Train Avg Loss: 0.0106 |Test Loss: 1.6046|lr = 0.00063\n",
      "Epoch: 1665|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6625|lr = 0.00063\n",
      "Epoch: 1666|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.5924|lr = 0.00063\n",
      "Epoch: 1666|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.6556|lr = 0.00063\n",
      "Epoch: 1667|steps:   30|Train Avg Loss: 0.0102 |Test Loss: 1.6471|lr = 0.00063\n",
      "Epoch: 1667|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.6587|lr = 0.00063\n",
      "Epoch: 1668|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.6840|lr = 0.00063\n",
      "Epoch: 1668|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.6082|lr = 0.00063\n",
      "Epoch: 1669|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.6744|lr = 0.00063\n",
      "Epoch: 1669|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.5968|lr = 0.00063\n",
      "Epoch: 1670|steps:   30|Train Avg Loss: 0.0104 |Test Loss: 1.6483|lr = 0.00063\n",
      "Epoch: 1670|steps:   60|Train Avg Loss: 0.0102 |Test Loss: 1.6905|lr = 0.00063\n",
      "Epoch: 1671|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.6212|lr = 0.00063\n",
      "Epoch: 1671|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.6774|lr = 0.00063\n",
      "Epoch: 1672|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.6516|lr = 0.00062\n",
      "Epoch: 1672|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6623|lr = 0.00062\n",
      "Epoch: 1673|steps:   30|Train Avg Loss: 0.0081 |Test Loss: 1.7022|lr = 0.00062\n",
      "Epoch: 1673|steps:   60|Train Avg Loss: 0.0137 |Test Loss: 1.6644|lr = 0.00062\n",
      "Epoch: 1674|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.6492|lr = 0.00062\n",
      "Epoch: 1674|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6589|lr = 0.00062\n",
      "Epoch: 1675|steps:   30|Train Avg Loss: 0.0108 |Test Loss: 1.6600|lr = 0.00062\n",
      "Epoch: 1675|steps:   60|Train Avg Loss: 0.0121 |Test Loss: 1.6662|lr = 0.00062\n",
      "Epoch: 1676|steps:   30|Train Avg Loss: 0.0122 |Test Loss: 1.6449|lr = 0.00062\n",
      "Epoch: 1676|steps:   60|Train Avg Loss: 0.0097 |Test Loss: 1.6826|lr = 0.00062\n",
      "Epoch: 1677|steps:   30|Train Avg Loss: 0.0114 |Test Loss: 1.6382|lr = 0.00062\n",
      "Epoch: 1677|steps:   60|Train Avg Loss: 0.0090 |Test Loss: 1.6612|lr = 0.00062\n",
      "Epoch: 1678|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6646|lr = 0.00062\n",
      "Epoch: 1678|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.6506|lr = 0.00062\n",
      "Epoch: 1679|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6634|lr = 0.00062\n",
      "Epoch: 1679|steps:   60|Train Avg Loss: 0.0098 |Test Loss: 1.6178|lr = 0.00062\n",
      "Epoch: 1680|steps:   30|Train Avg Loss: 0.0112 |Test Loss: 1.6082|lr = 0.00062\n",
      "Epoch: 1680|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6935|lr = 0.00062\n",
      "Epoch: 1681|steps:   30|Train Avg Loss: 0.0092 |Test Loss: 1.6109|lr = 0.00062\n",
      "Epoch: 1681|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.6883|lr = 0.00062\n",
      "Epoch: 1682|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.6320|lr = 0.00062\n",
      "Epoch: 1682|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6809|lr = 0.00062\n",
      "Epoch: 1683|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6571|lr = 0.00060\n",
      "Epoch: 1683|steps:   60|Train Avg Loss: 0.0090 |Test Loss: 1.6915|lr = 0.00060\n",
      "Epoch: 1684|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.6615|lr = 0.00060\n",
      "Epoch: 1684|steps:   60|Train Avg Loss: 0.0123 |Test Loss: 1.6641|lr = 0.00060\n",
      "Epoch: 1685|steps:   30|Train Avg Loss: 0.0102 |Test Loss: 1.6738|lr = 0.00060\n",
      "Epoch: 1685|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6271|lr = 0.00060\n",
      "Epoch: 1686|steps:   30|Train Avg Loss: 0.0109 |Test Loss: 1.6802|lr = 0.00060\n",
      "Epoch: 1686|steps:   60|Train Avg Loss: 0.0107 |Test Loss: 1.6394|lr = 0.00060\n",
      "Epoch: 1687|steps:   30|Train Avg Loss: 0.0086 |Test Loss: 1.6566|lr = 0.00060\n",
      "Epoch: 1687|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6833|lr = 0.00060\n",
      "Epoch: 1688|steps:   30|Train Avg Loss: 0.0086 |Test Loss: 1.5970|lr = 0.00060\n",
      "Epoch: 1688|steps:   60|Train Avg Loss: 0.0122 |Test Loss: 1.6768|lr = 0.00060\n",
      "Epoch: 1689|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6574|lr = 0.00060\n",
      "Epoch: 1689|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.6530|lr = 0.00060\n",
      "Epoch: 1690|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6541|lr = 0.00060\n",
      "Epoch: 1690|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.7186|lr = 0.00060\n",
      "Epoch: 1691|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.7041|lr = 0.00060\n",
      "Epoch: 1691|steps:   60|Train Avg Loss: 0.0096 |Test Loss: 1.6835|lr = 0.00060\n",
      "Epoch: 1692|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.7052|lr = 0.00060\n",
      "Epoch: 1692|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.7043|lr = 0.00060\n",
      "Epoch: 1693|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6968|lr = 0.00060\n",
      "Epoch: 1693|steps:   60|Train Avg Loss: 0.0102 |Test Loss: 1.6930|lr = 0.00060\n",
      "Epoch: 1694|steps:   30|Train Avg Loss: 0.0104 |Test Loss: 1.6664|lr = 0.00060\n",
      "Epoch: 1694|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.7202|lr = 0.00060\n",
      "Epoch: 1695|steps:   30|Train Avg Loss: 0.0134 |Test Loss: 1.6877|lr = 0.00060\n",
      "Epoch: 1695|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6335|lr = 0.00060\n",
      "Epoch: 1696|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6636|lr = 0.00060\n",
      "Epoch: 1696|steps:   60|Train Avg Loss: 0.0114 |Test Loss: 1.6631|lr = 0.00060\n",
      "Epoch: 1697|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6592|lr = 0.00060\n",
      "Epoch: 1697|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6659|lr = 0.00060\n",
      "Epoch: 1698|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6593|lr = 0.00060\n",
      "Epoch: 1698|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6935|lr = 0.00060\n",
      "Epoch: 1699|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.6572|lr = 0.00060\n",
      "Epoch: 1699|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6795|lr = 0.00060\n",
      "Epoch: 1700|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.6255|lr = 0.00060\n",
      "Epoch: 1700|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.6520|lr = 0.00060\n",
      "Epoch: 1701|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.6702|lr = 0.00060\n",
      "Epoch: 1701|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6845|lr = 0.00060\n",
      "Epoch: 1702|steps:   30|Train Avg Loss: 0.0093 |Test Loss: 1.6717|lr = 0.00060\n",
      "Epoch: 1702|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6534|lr = 0.00060\n",
      "Epoch: 1703|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6728|lr = 0.00060\n",
      "Epoch: 1703|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.7009|lr = 0.00060\n",
      "Epoch: 1704|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.6514|lr = 0.00060\n",
      "Epoch: 1704|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.6573|lr = 0.00060\n",
      "Epoch: 1705|steps:   30|Train Avg Loss: 0.0103 |Test Loss: 1.6816|lr = 0.00059\n",
      "Epoch: 1705|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6862|lr = 0.00059\n",
      "Epoch: 1706|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.7540|lr = 0.00059\n",
      "Epoch: 1706|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.6652|lr = 0.00059\n",
      "Epoch: 1707|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.6960|lr = 0.00059\n",
      "Epoch: 1707|steps:   60|Train Avg Loss: 0.0069 |Test Loss: 1.6784|lr = 0.00059\n",
      "Epoch: 1708|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6689|lr = 0.00059\n",
      "Epoch: 1708|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.6820|lr = 0.00059\n",
      "Epoch: 1709|steps:   30|Train Avg Loss: 0.0087 |Test Loss: 1.6782|lr = 0.00059\n",
      "Epoch: 1709|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.7146|lr = 0.00059\n",
      "Epoch: 1710|steps:   30|Train Avg Loss: 0.0108 |Test Loss: 1.6620|lr = 0.00059\n",
      "Epoch: 1710|steps:   60|Train Avg Loss: 0.0115 |Test Loss: 1.6479|lr = 0.00059\n",
      "Epoch: 1711|steps:   30|Train Avg Loss: 0.0117 |Test Loss: 1.6637|lr = 0.00059\n",
      "Epoch: 1711|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.7294|lr = 0.00059\n",
      "Epoch: 1712|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.7166|lr = 0.00059\n",
      "Epoch: 1712|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.6612|lr = 0.00059\n",
      "Epoch: 1713|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.6829|lr = 0.00059\n",
      "Epoch: 1713|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.6786|lr = 0.00059\n",
      "Epoch: 1714|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6696|lr = 0.00059\n",
      "Epoch: 1714|steps:   60|Train Avg Loss: 0.0100 |Test Loss: 1.6806|lr = 0.00059\n",
      "Epoch: 1715|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.7107|lr = 0.00059\n",
      "Epoch: 1715|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.7131|lr = 0.00059\n",
      "Epoch: 1716|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.6915|lr = 0.00058\n",
      "Epoch: 1716|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6341|lr = 0.00058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1717|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6618|lr = 0.00058\n",
      "Epoch: 1717|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.6397|lr = 0.00058\n",
      "Epoch: 1718|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6533|lr = 0.00058\n",
      "Epoch: 1718|steps:   60|Train Avg Loss: 0.0098 |Test Loss: 1.6691|lr = 0.00058\n",
      "Epoch: 1719|steps:   30|Train Avg Loss: 0.0095 |Test Loss: 1.6698|lr = 0.00058\n",
      "Epoch: 1719|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.6505|lr = 0.00058\n",
      "Epoch: 1720|steps:   30|Train Avg Loss: 0.0109 |Test Loss: 1.6429|lr = 0.00058\n",
      "Epoch: 1720|steps:   60|Train Avg Loss: 0.0099 |Test Loss: 1.6744|lr = 0.00058\n",
      "Epoch: 1721|steps:   30|Train Avg Loss: 0.0075 |Test Loss: 1.6786|lr = 0.00058\n",
      "Epoch: 1721|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6415|lr = 0.00058\n",
      "Epoch: 1722|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.6701|lr = 0.00058\n",
      "Epoch: 1722|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.6836|lr = 0.00058\n",
      "Epoch: 1723|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.7082|lr = 0.00058\n",
      "Epoch: 1723|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.6969|lr = 0.00058\n",
      "Epoch: 1724|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.6982|lr = 0.00058\n",
      "Epoch: 1724|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.6916|lr = 0.00058\n",
      "Epoch: 1725|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.6776|lr = 0.00058\n",
      "Epoch: 1725|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.6924|lr = 0.00058\n",
      "Epoch: 1726|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6525|lr = 0.00058\n",
      "Epoch: 1726|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6839|lr = 0.00058\n",
      "Epoch: 1727|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6600|lr = 0.00057\n",
      "Epoch: 1727|steps:   60|Train Avg Loss: 0.0103 |Test Loss: 1.7058|lr = 0.00057\n",
      "Epoch: 1728|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6804|lr = 0.00057\n",
      "Epoch: 1728|steps:   60|Train Avg Loss: 0.0101 |Test Loss: 1.6905|lr = 0.00057\n",
      "Epoch: 1729|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6947|lr = 0.00057\n",
      "Epoch: 1729|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.7326|lr = 0.00057\n",
      "Epoch: 1730|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.6883|lr = 0.00057\n",
      "Epoch: 1730|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.6993|lr = 0.00057\n",
      "Epoch: 1731|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.6951|lr = 0.00057\n",
      "Epoch: 1731|steps:   60|Train Avg Loss: 0.0113 |Test Loss: 1.7205|lr = 0.00057\n",
      "Epoch: 1732|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6530|lr = 0.00057\n",
      "Epoch: 1732|steps:   60|Train Avg Loss: 0.0078 |Test Loss: 1.6837|lr = 0.00057\n",
      "Epoch: 1733|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7011|lr = 0.00057\n",
      "Epoch: 1733|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6899|lr = 0.00057\n",
      "Epoch: 1734|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7073|lr = 0.00057\n",
      "Epoch: 1734|steps:   60|Train Avg Loss: 0.0096 |Test Loss: 1.6321|lr = 0.00057\n",
      "Epoch: 1735|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.6826|lr = 0.00057\n",
      "Epoch: 1735|steps:   60|Train Avg Loss: 0.0104 |Test Loss: 1.6460|lr = 0.00057\n",
      "Epoch: 1736|steps:   30|Train Avg Loss: 0.0080 |Test Loss: 1.6922|lr = 0.00057\n",
      "Epoch: 1736|steps:   60|Train Avg Loss: 0.0101 |Test Loss: 1.7314|lr = 0.00057\n",
      "Epoch: 1737|steps:   30|Train Avg Loss: 0.0104 |Test Loss: 1.6495|lr = 0.00057\n",
      "Epoch: 1737|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6835|lr = 0.00057\n",
      "Epoch: 1738|steps:   30|Train Avg Loss: 0.0075 |Test Loss: 1.6817|lr = 0.00056\n",
      "Epoch: 1738|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.6451|lr = 0.00056\n",
      "Epoch: 1739|steps:   30|Train Avg Loss: 0.0086 |Test Loss: 1.6623|lr = 0.00056\n",
      "Epoch: 1739|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.7101|lr = 0.00056\n",
      "Epoch: 1740|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.6948|lr = 0.00056\n",
      "Epoch: 1740|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.7043|lr = 0.00056\n",
      "Epoch: 1741|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6515|lr = 0.00056\n",
      "Epoch: 1741|steps:   60|Train Avg Loss: 0.0101 |Test Loss: 1.6918|lr = 0.00056\n",
      "Epoch: 1742|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6305|lr = 0.00056\n",
      "Epoch: 1742|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6602|lr = 0.00056\n",
      "Epoch: 1743|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.6625|lr = 0.00056\n",
      "Epoch: 1743|steps:   60|Train Avg Loss: 0.0097 |Test Loss: 1.6648|lr = 0.00056\n",
      "Epoch: 1744|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.6900|lr = 0.00056\n",
      "Epoch: 1744|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6958|lr = 0.00056\n",
      "Epoch: 1745|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.6356|lr = 0.00056\n",
      "Epoch: 1745|steps:   60|Train Avg Loss: 0.0109 |Test Loss: 1.6812|lr = 0.00056\n",
      "Epoch: 1746|steps:   30|Train Avg Loss: 0.0087 |Test Loss: 1.6993|lr = 0.00056\n",
      "Epoch: 1746|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6809|lr = 0.00056\n",
      "Epoch: 1747|steps:   30|Train Avg Loss: 0.0097 |Test Loss: 1.7047|lr = 0.00056\n",
      "Epoch: 1747|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.6792|lr = 0.00056\n",
      "Epoch: 1748|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6694|lr = 0.00056\n",
      "Epoch: 1748|steps:   60|Train Avg Loss: 0.0095 |Test Loss: 1.6789|lr = 0.00056\n",
      "Epoch: 1749|steps:   30|Train Avg Loss: 0.0081 |Test Loss: 1.6718|lr = 0.00055\n",
      "Epoch: 1749|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6896|lr = 0.00055\n",
      "Epoch: 1750|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6636|lr = 0.00055\n",
      "Epoch: 1750|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.6810|lr = 0.00055\n",
      "Epoch: 1751|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6771|lr = 0.00055\n",
      "Epoch: 1751|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.7012|lr = 0.00055\n",
      "Epoch: 1752|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.6953|lr = 0.00055\n",
      "Epoch: 1752|steps:   60|Train Avg Loss: 0.0069 |Test Loss: 1.7034|lr = 0.00055\n",
      "Epoch: 1753|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.6893|lr = 0.00055\n",
      "Epoch: 1753|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6747|lr = 0.00055\n",
      "Epoch: 1754|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6769|lr = 0.00055\n",
      "Epoch: 1754|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.6837|lr = 0.00055\n",
      "Epoch: 1755|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.6859|lr = 0.00055\n",
      "Epoch: 1755|steps:   60|Train Avg Loss: 0.0105 |Test Loss: 1.6752|lr = 0.00055\n",
      "Epoch: 1756|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.6920|lr = 0.00055\n",
      "Epoch: 1756|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.7199|lr = 0.00055\n",
      "Epoch: 1757|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.7075|lr = 0.00055\n",
      "Epoch: 1757|steps:   60|Train Avg Loss: 0.0098 |Test Loss: 1.6887|lr = 0.00055\n",
      "Epoch: 1758|steps:   30|Train Avg Loss: 0.0257 |Test Loss: 1.6335|lr = 0.00055\n",
      "Epoch: 1758|steps:   60|Train Avg Loss: 0.0350 |Test Loss: 1.6778|lr = 0.00055\n",
      "Epoch: 1759|steps:   30|Train Avg Loss: 0.0245 |Test Loss: 1.6240|lr = 0.00055\n",
      "Epoch: 1759|steps:   60|Train Avg Loss: 0.0120 |Test Loss: 1.6313|lr = 0.00055\n",
      "Epoch: 1760|steps:   30|Train Avg Loss: 0.0102 |Test Loss: 1.6716|lr = 0.00053\n",
      "Epoch: 1760|steps:   60|Train Avg Loss: 0.0093 |Test Loss: 1.6749|lr = 0.00053\n",
      "Epoch: 1761|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.7101|lr = 0.00053\n",
      "Epoch: 1761|steps:   60|Train Avg Loss: 0.0086 |Test Loss: 1.6914|lr = 0.00053\n",
      "Epoch: 1762|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.6668|lr = 0.00053\n",
      "Epoch: 1762|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6844|lr = 0.00053\n",
      "Epoch: 1763|steps:   30|Train Avg Loss: 0.0114 |Test Loss: 1.6542|lr = 0.00053\n",
      "Epoch: 1763|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.6561|lr = 0.00053\n",
      "Epoch: 1764|steps:   30|Train Avg Loss: 0.0092 |Test Loss: 1.6991|lr = 0.00053\n",
      "Epoch: 1764|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.6749|lr = 0.00053\n",
      "Epoch: 1765|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6763|lr = 0.00053\n",
      "Epoch: 1765|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.6867|lr = 0.00053\n",
      "Epoch: 1766|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.7390|lr = 0.00053\n",
      "Epoch: 1766|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.6964|lr = 0.00053\n",
      "Epoch: 1767|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.6966|lr = 0.00053\n",
      "Epoch: 1767|steps:   60|Train Avg Loss: 0.0078 |Test Loss: 1.6477|lr = 0.00053\n",
      "Epoch: 1768|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.6780|lr = 0.00053\n",
      "Epoch: 1768|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.6563|lr = 0.00053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1769|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6794|lr = 0.00053\n",
      "Epoch: 1769|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7033|lr = 0.00053\n",
      "Epoch: 1770|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7041|lr = 0.00053\n",
      "Epoch: 1770|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.6810|lr = 0.00053\n",
      "Epoch: 1771|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7011|lr = 0.00052\n",
      "Epoch: 1771|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6439|lr = 0.00052\n",
      "Epoch: 1772|steps:   30|Train Avg Loss: 0.0093 |Test Loss: 1.6669|lr = 0.00052\n",
      "Epoch: 1772|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6647|lr = 0.00052\n",
      "Epoch: 1773|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.6603|lr = 0.00052\n",
      "Epoch: 1773|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.6763|lr = 0.00052\n",
      "Epoch: 1774|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.6933|lr = 0.00052\n",
      "Epoch: 1774|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.6776|lr = 0.00052\n",
      "Epoch: 1775|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7002|lr = 0.00052\n",
      "Epoch: 1775|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.6600|lr = 0.00052\n",
      "Epoch: 1776|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.7241|lr = 0.00052\n",
      "Epoch: 1776|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.7090|lr = 0.00052\n",
      "Epoch: 1777|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.7272|lr = 0.00052\n",
      "Epoch: 1777|steps:   60|Train Avg Loss: 0.0078 |Test Loss: 1.7163|lr = 0.00052\n",
      "Epoch: 1778|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.6881|lr = 0.00052\n",
      "Epoch: 1778|steps:   60|Train Avg Loss: 0.0106 |Test Loss: 1.6354|lr = 0.00052\n",
      "Epoch: 1779|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.6382|lr = 0.00052\n",
      "Epoch: 1779|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.6675|lr = 0.00052\n",
      "Epoch: 1780|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.6971|lr = 0.00052\n",
      "Epoch: 1780|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.6928|lr = 0.00052\n",
      "Epoch: 1781|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.6719|lr = 0.00052\n",
      "Epoch: 1781|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7051|lr = 0.00052\n",
      "Epoch: 1782|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.6597|lr = 0.00051\n",
      "Epoch: 1782|steps:   60|Train Avg Loss: 0.0086 |Test Loss: 1.6829|lr = 0.00051\n",
      "Epoch: 1783|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.6534|lr = 0.00051\n",
      "Epoch: 1783|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.6587|lr = 0.00051\n",
      "Epoch: 1784|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6817|lr = 0.00051\n",
      "Epoch: 1784|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.6891|lr = 0.00051\n",
      "Epoch: 1785|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.7005|lr = 0.00051\n",
      "Epoch: 1785|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6868|lr = 0.00051\n",
      "Epoch: 1786|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6814|lr = 0.00051\n",
      "Epoch: 1786|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.7137|lr = 0.00051\n",
      "Epoch: 1787|steps:   30|Train Avg Loss: 0.0080 |Test Loss: 1.7127|lr = 0.00051\n",
      "Epoch: 1787|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.6272|lr = 0.00051\n",
      "Epoch: 1788|steps:   30|Train Avg Loss: 0.0082 |Test Loss: 1.6815|lr = 0.00051\n",
      "Epoch: 1788|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.6867|lr = 0.00051\n",
      "Epoch: 1789|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6831|lr = 0.00051\n",
      "Epoch: 1789|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.6774|lr = 0.00051\n",
      "Epoch: 1790|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.6737|lr = 0.00051\n",
      "Epoch: 1790|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.6915|lr = 0.00051\n",
      "Epoch: 1791|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.7176|lr = 0.00051\n",
      "Epoch: 1791|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7065|lr = 0.00051\n",
      "Epoch: 1792|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6989|lr = 0.00051\n",
      "Epoch: 1792|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7422|lr = 0.00051\n",
      "Epoch: 1793|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.7450|lr = 0.00050\n",
      "Epoch: 1793|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.7172|lr = 0.00050\n",
      "Epoch: 1794|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.6991|lr = 0.00050\n",
      "Epoch: 1794|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6838|lr = 0.00050\n",
      "Epoch: 1795|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7023|lr = 0.00050\n",
      "Epoch: 1795|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.7021|lr = 0.00050\n",
      "Epoch: 1796|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.6806|lr = 0.00050\n",
      "Epoch: 1796|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7164|lr = 0.00050\n",
      "Epoch: 1797|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7009|lr = 0.00050\n",
      "Epoch: 1797|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.7451|lr = 0.00050\n",
      "Epoch: 1798|steps:   30|Train Avg Loss: 0.0114 |Test Loss: 1.6936|lr = 0.00050\n",
      "Epoch: 1798|steps:   60|Train Avg Loss: 0.0078 |Test Loss: 1.7087|lr = 0.00050\n",
      "Epoch: 1799|steps:   30|Train Avg Loss: 0.0070 |Test Loss: 1.6819|lr = 0.00050\n",
      "Epoch: 1799|steps:   60|Train Avg Loss: 0.0094 |Test Loss: 1.6740|lr = 0.00050\n",
      "Epoch: 1800|steps:   30|Train Avg Loss: 0.0111 |Test Loss: 1.6827|lr = 0.00050\n",
      "Epoch: 1800|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.6483|lr = 0.00050\n",
      "Epoch: 1801|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.7157|lr = 0.00050\n",
      "Epoch: 1801|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.7018|lr = 0.00050\n",
      "Epoch: 1802|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6937|lr = 0.00050\n",
      "Epoch: 1802|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.6965|lr = 0.00050\n",
      "Epoch: 1803|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.6997|lr = 0.00050\n",
      "Epoch: 1803|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.6847|lr = 0.00050\n",
      "Epoch: 1804|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.6782|lr = 0.00049\n",
      "Epoch: 1804|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.7090|lr = 0.00049\n",
      "Epoch: 1805|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.7039|lr = 0.00049\n",
      "Epoch: 1805|steps:   60|Train Avg Loss: 0.0097 |Test Loss: 1.7165|lr = 0.00049\n",
      "Epoch: 1806|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.7538|lr = 0.00049\n",
      "Epoch: 1806|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.7154|lr = 0.00049\n",
      "Epoch: 1807|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7222|lr = 0.00049\n",
      "Epoch: 1807|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6679|lr = 0.00049\n",
      "Epoch: 1808|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7378|lr = 0.00049\n",
      "Epoch: 1808|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.7182|lr = 0.00049\n",
      "Epoch: 1809|steps:   30|Train Avg Loss: 0.0070 |Test Loss: 1.7336|lr = 0.00049\n",
      "Epoch: 1809|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.7422|lr = 0.00049\n",
      "Epoch: 1810|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.7378|lr = 0.00049\n",
      "Epoch: 1810|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.6847|lr = 0.00049\n",
      "Epoch: 1811|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6663|lr = 0.00049\n",
      "Epoch: 1811|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.6741|lr = 0.00049\n",
      "Epoch: 1812|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.6748|lr = 0.00049\n",
      "Epoch: 1812|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6746|lr = 0.00049\n",
      "Epoch: 1813|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.7210|lr = 0.00049\n",
      "Epoch: 1813|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.7259|lr = 0.00049\n",
      "Epoch: 1814|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.6802|lr = 0.00049\n",
      "Epoch: 1814|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.7071|lr = 0.00049\n",
      "Epoch: 1815|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6989|lr = 0.00048\n",
      "Epoch: 1815|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.6871|lr = 0.00048\n",
      "Epoch: 1816|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.6942|lr = 0.00048\n",
      "Epoch: 1816|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.6674|lr = 0.00048\n",
      "Epoch: 1817|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.6977|lr = 0.00048\n",
      "Epoch: 1817|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.7169|lr = 0.00048\n",
      "Epoch: 1818|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.6870|lr = 0.00048\n",
      "Epoch: 1818|steps:   60|Train Avg Loss: 0.0088 |Test Loss: 1.6854|lr = 0.00048\n",
      "Epoch: 1819|steps:   30|Train Avg Loss: 0.0105 |Test Loss: 1.6892|lr = 0.00048\n",
      "Epoch: 1819|steps:   60|Train Avg Loss: 0.0126 |Test Loss: 1.6901|lr = 0.00048\n",
      "Epoch: 1820|steps:   30|Train Avg Loss: 0.0096 |Test Loss: 1.6672|lr = 0.00048\n",
      "Epoch: 1820|steps:   60|Train Avg Loss: 0.0090 |Test Loss: 1.6643|lr = 0.00048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1821|steps:   30|Train Avg Loss: 0.0087 |Test Loss: 1.6545|lr = 0.00048\n",
      "Epoch: 1821|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.6834|lr = 0.00048\n",
      "Epoch: 1822|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.6923|lr = 0.00048\n",
      "Epoch: 1822|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.6809|lr = 0.00048\n",
      "Epoch: 1823|steps:   30|Train Avg Loss: 0.0070 |Test Loss: 1.6837|lr = 0.00048\n",
      "Epoch: 1823|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.7108|lr = 0.00048\n",
      "Epoch: 1824|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7106|lr = 0.00048\n",
      "Epoch: 1824|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.6887|lr = 0.00048\n",
      "Epoch: 1825|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.7110|lr = 0.00048\n",
      "Epoch: 1825|steps:   60|Train Avg Loss: 0.0069 |Test Loss: 1.7123|lr = 0.00048\n",
      "Epoch: 1826|steps:   30|Train Avg Loss: 0.0094 |Test Loss: 1.6873|lr = 0.00047\n",
      "Epoch: 1826|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.6686|lr = 0.00047\n",
      "Epoch: 1827|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.6784|lr = 0.00047\n",
      "Epoch: 1827|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.6951|lr = 0.00047\n",
      "Epoch: 1828|steps:   30|Train Avg Loss: 0.0081 |Test Loss: 1.7307|lr = 0.00047\n",
      "Epoch: 1828|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7311|lr = 0.00047\n",
      "Epoch: 1829|steps:   30|Train Avg Loss: 0.0066 |Test Loss: 1.7018|lr = 0.00047\n",
      "Epoch: 1829|steps:   60|Train Avg Loss: 0.0069 |Test Loss: 1.6767|lr = 0.00047\n",
      "Epoch: 1830|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.6852|lr = 0.00047\n",
      "Epoch: 1830|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.6903|lr = 0.00047\n",
      "Epoch: 1831|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.6917|lr = 0.00047\n",
      "Epoch: 1831|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7068|lr = 0.00047\n",
      "Epoch: 1832|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7142|lr = 0.00047\n",
      "Epoch: 1832|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.7630|lr = 0.00047\n",
      "Epoch: 1833|steps:   30|Train Avg Loss: 0.0099 |Test Loss: 1.6896|lr = 0.00047\n",
      "Epoch: 1833|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.6962|lr = 0.00047\n",
      "Epoch: 1834|steps:   30|Train Avg Loss: 0.0080 |Test Loss: 1.6790|lr = 0.00047\n",
      "Epoch: 1834|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.6701|lr = 0.00047\n",
      "Epoch: 1835|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.7086|lr = 0.00047\n",
      "Epoch: 1835|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.6899|lr = 0.00047\n",
      "Epoch: 1836|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.6930|lr = 0.00047\n",
      "Epoch: 1836|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.7144|lr = 0.00047\n",
      "Epoch: 1837|steps:   30|Train Avg Loss: 0.0066 |Test Loss: 1.7262|lr = 0.00046\n",
      "Epoch: 1837|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.7264|lr = 0.00046\n",
      "Epoch: 1838|steps:   30|Train Avg Loss: 0.0082 |Test Loss: 1.7212|lr = 0.00046\n",
      "Epoch: 1838|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7189|lr = 0.00046\n",
      "Epoch: 1839|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.7014|lr = 0.00046\n",
      "Epoch: 1839|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.7127|lr = 0.00046\n",
      "Epoch: 1840|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.6778|lr = 0.00046\n",
      "Epoch: 1840|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.7214|lr = 0.00046\n",
      "Epoch: 1841|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.7224|lr = 0.00046\n",
      "Epoch: 1841|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.7088|lr = 0.00046\n",
      "Epoch: 1842|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7042|lr = 0.00046\n",
      "Epoch: 1842|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.6572|lr = 0.00046\n",
      "Epoch: 1843|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.7238|lr = 0.00046\n",
      "Epoch: 1843|steps:   60|Train Avg Loss: 0.0087 |Test Loss: 1.6864|lr = 0.00046\n",
      "Epoch: 1844|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.7026|lr = 0.00046\n",
      "Epoch: 1844|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.6797|lr = 0.00046\n",
      "Epoch: 1845|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7164|lr = 0.00046\n",
      "Epoch: 1845|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7031|lr = 0.00046\n",
      "Epoch: 1846|steps:   30|Train Avg Loss: 0.0080 |Test Loss: 1.6889|lr = 0.00046\n",
      "Epoch: 1846|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.6875|lr = 0.00046\n",
      "Epoch: 1847|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.6793|lr = 0.00046\n",
      "Epoch: 1847|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.7082|lr = 0.00046\n",
      "Epoch: 1848|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7044|lr = 0.00045\n",
      "Epoch: 1848|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.6784|lr = 0.00045\n",
      "Epoch: 1849|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.7187|lr = 0.00045\n",
      "Epoch: 1849|steps:   60|Train Avg Loss: 0.0085 |Test Loss: 1.7245|lr = 0.00045\n",
      "Epoch: 1850|steps:   30|Train Avg Loss: 0.0107 |Test Loss: 1.7148|lr = 0.00045\n",
      "Epoch: 1850|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7229|lr = 0.00045\n",
      "Epoch: 1851|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7063|lr = 0.00045\n",
      "Epoch: 1851|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7098|lr = 0.00045\n",
      "Epoch: 1852|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.6889|lr = 0.00045\n",
      "Epoch: 1852|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7166|lr = 0.00045\n",
      "Epoch: 1853|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.7184|lr = 0.00045\n",
      "Epoch: 1853|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.7137|lr = 0.00045\n",
      "Epoch: 1854|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.6906|lr = 0.00045\n",
      "Epoch: 1854|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7042|lr = 0.00045\n",
      "Epoch: 1855|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.6873|lr = 0.00045\n",
      "Epoch: 1855|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.6932|lr = 0.00045\n",
      "Epoch: 1856|steps:   30|Train Avg Loss: 0.0083 |Test Loss: 1.6746|lr = 0.00045\n",
      "Epoch: 1856|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.6979|lr = 0.00045\n",
      "Epoch: 1857|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.6920|lr = 0.00045\n",
      "Epoch: 1857|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.7394|lr = 0.00045\n",
      "Epoch: 1858|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7106|lr = 0.00045\n",
      "Epoch: 1858|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.7153|lr = 0.00045\n",
      "Epoch: 1859|steps:   30|Train Avg Loss: 0.0087 |Test Loss: 1.7162|lr = 0.00045\n",
      "Epoch: 1859|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.7204|lr = 0.00045\n",
      "Epoch: 1860|steps:   30|Train Avg Loss: 0.0090 |Test Loss: 1.7148|lr = 0.00045\n",
      "Epoch: 1860|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.6979|lr = 0.00045\n",
      "Epoch: 1861|steps:   30|Train Avg Loss: 0.0066 |Test Loss: 1.6960|lr = 0.00045\n",
      "Epoch: 1861|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.7115|lr = 0.00045\n",
      "Epoch: 1862|steps:   30|Train Avg Loss: 0.0100 |Test Loss: 1.7264|lr = 0.00045\n",
      "Epoch: 1862|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6920|lr = 0.00045\n",
      "Epoch: 1863|steps:   30|Train Avg Loss: 0.0075 |Test Loss: 1.6745|lr = 0.00045\n",
      "Epoch: 1863|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6792|lr = 0.00045\n",
      "Epoch: 1864|steps:   30|Train Avg Loss: 0.0066 |Test Loss: 1.7253|lr = 0.00045\n",
      "Epoch: 1864|steps:   60|Train Avg Loss: 0.0082 |Test Loss: 1.6933|lr = 0.00045\n",
      "Epoch: 1865|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.7155|lr = 0.00045\n",
      "Epoch: 1865|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7018|lr = 0.00045\n",
      "Epoch: 1866|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.7068|lr = 0.00045\n",
      "Epoch: 1866|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.6742|lr = 0.00045\n",
      "Epoch: 1867|steps:   30|Train Avg Loss: 0.0084 |Test Loss: 1.7049|lr = 0.00045\n",
      "Epoch: 1867|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.6759|lr = 0.00045\n",
      "Epoch: 1868|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.6983|lr = 0.00045\n",
      "Epoch: 1868|steps:   60|Train Avg Loss: 0.0092 |Test Loss: 1.7309|lr = 0.00045\n",
      "Epoch: 1869|steps:   30|Train Avg Loss: 0.0086 |Test Loss: 1.6926|lr = 0.00045\n",
      "Epoch: 1869|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.6809|lr = 0.00045\n",
      "Epoch: 1870|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.6918|lr = 0.00044\n",
      "Epoch: 1870|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.6786|lr = 0.00044\n",
      "Epoch: 1871|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.6962|lr = 0.00044\n",
      "Epoch: 1871|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.6759|lr = 0.00044\n",
      "Epoch: 1872|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.6773|lr = 0.00044\n",
      "Epoch: 1872|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.6864|lr = 0.00044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1873|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7100|lr = 0.00044\n",
      "Epoch: 1873|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.6878|lr = 0.00044\n",
      "Epoch: 1874|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7151|lr = 0.00044\n",
      "Epoch: 1874|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7026|lr = 0.00044\n",
      "Epoch: 1875|steps:   30|Train Avg Loss: 0.0083 |Test Loss: 1.6897|lr = 0.00044\n",
      "Epoch: 1875|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.6884|lr = 0.00044\n",
      "Epoch: 1876|steps:   30|Train Avg Loss: 0.0089 |Test Loss: 1.6846|lr = 0.00044\n",
      "Epoch: 1876|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.6629|lr = 0.00044\n",
      "Epoch: 1877|steps:   30|Train Avg Loss: 0.0077 |Test Loss: 1.6314|lr = 0.00044\n",
      "Epoch: 1877|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.6785|lr = 0.00044\n",
      "Epoch: 1878|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7175|lr = 0.00044\n",
      "Epoch: 1878|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7104|lr = 0.00044\n",
      "Epoch: 1879|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.7222|lr = 0.00044\n",
      "Epoch: 1879|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7227|lr = 0.00044\n",
      "Epoch: 1880|steps:   30|Train Avg Loss: 0.0112 |Test Loss: 1.7495|lr = 0.00044\n",
      "Epoch: 1880|steps:   60|Train Avg Loss: 0.0129 |Test Loss: 1.6964|lr = 0.00044\n",
      "Epoch: 1881|steps:   30|Train Avg Loss: 0.0102 |Test Loss: 1.6453|lr = 0.00043\n",
      "Epoch: 1881|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.7066|lr = 0.00043\n",
      "Epoch: 1882|steps:   30|Train Avg Loss: 0.0091 |Test Loss: 1.7001|lr = 0.00043\n",
      "Epoch: 1882|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7257|lr = 0.00043\n",
      "Epoch: 1883|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7454|lr = 0.00043\n",
      "Epoch: 1883|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7343|lr = 0.00043\n",
      "Epoch: 1884|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7270|lr = 0.00043\n",
      "Epoch: 1884|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.7642|lr = 0.00043\n",
      "Epoch: 1885|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7341|lr = 0.00043\n",
      "Epoch: 1885|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.7260|lr = 0.00043\n",
      "Epoch: 1886|steps:   30|Train Avg Loss: 0.0066 |Test Loss: 1.6947|lr = 0.00043\n",
      "Epoch: 1886|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.6972|lr = 0.00043\n",
      "Epoch: 1887|steps:   30|Train Avg Loss: 0.0086 |Test Loss: 1.6946|lr = 0.00043\n",
      "Epoch: 1887|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7216|lr = 0.00043\n",
      "Epoch: 1888|steps:   30|Train Avg Loss: 0.0082 |Test Loss: 1.7000|lr = 0.00043\n",
      "Epoch: 1888|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7159|lr = 0.00043\n",
      "Epoch: 1889|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.7012|lr = 0.00043\n",
      "Epoch: 1889|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7431|lr = 0.00043\n",
      "Epoch: 1890|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.6614|lr = 0.00043\n",
      "Epoch: 1890|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7139|lr = 0.00043\n",
      "Epoch: 1891|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.6660|lr = 0.00043\n",
      "Epoch: 1891|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.6954|lr = 0.00043\n",
      "Epoch: 1892|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.7641|lr = 0.00042\n",
      "Epoch: 1892|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.6999|lr = 0.00042\n",
      "Epoch: 1893|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.6817|lr = 0.00042\n",
      "Epoch: 1893|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7046|lr = 0.00042\n",
      "Epoch: 1894|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.7130|lr = 0.00042\n",
      "Epoch: 1894|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.7418|lr = 0.00042\n",
      "Epoch: 1895|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.7443|lr = 0.00042\n",
      "Epoch: 1895|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.7609|lr = 0.00042\n",
      "Epoch: 1896|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7433|lr = 0.00042\n",
      "Epoch: 1896|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.7440|lr = 0.00042\n",
      "Epoch: 1897|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.7156|lr = 0.00042\n",
      "Epoch: 1897|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.7011|lr = 0.00042\n",
      "Epoch: 1898|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7218|lr = 0.00042\n",
      "Epoch: 1898|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.7194|lr = 0.00042\n",
      "Epoch: 1899|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.7009|lr = 0.00042\n",
      "Epoch: 1899|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7049|lr = 0.00042\n",
      "Epoch: 1900|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.6878|lr = 0.00042\n",
      "Epoch: 1900|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7064|lr = 0.00042\n",
      "Epoch: 1901|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.7159|lr = 0.00042\n",
      "Epoch: 1901|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.7193|lr = 0.00042\n",
      "Epoch: 1902|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7116|lr = 0.00042\n",
      "Epoch: 1902|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7342|lr = 0.00042\n",
      "Epoch: 1903|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.7326|lr = 0.00041\n",
      "Epoch: 1903|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.7295|lr = 0.00041\n",
      "Epoch: 1904|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.6895|lr = 0.00041\n",
      "Epoch: 1904|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7113|lr = 0.00041\n",
      "Epoch: 1905|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.6978|lr = 0.00041\n",
      "Epoch: 1905|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7188|lr = 0.00041\n",
      "Epoch: 1906|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7240|lr = 0.00041\n",
      "Epoch: 1906|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.7313|lr = 0.00041\n",
      "Epoch: 1907|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.7158|lr = 0.00041\n",
      "Epoch: 1907|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.7091|lr = 0.00041\n",
      "Epoch: 1908|steps:   30|Train Avg Loss: 0.0070 |Test Loss: 1.7078|lr = 0.00041\n",
      "Epoch: 1908|steps:   60|Train Avg Loss: 0.0076 |Test Loss: 1.6515|lr = 0.00041\n",
      "Epoch: 1909|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.7179|lr = 0.00041\n",
      "Epoch: 1909|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.6944|lr = 0.00041\n",
      "Epoch: 1910|steps:   30|Train Avg Loss: 0.0081 |Test Loss: 1.7382|lr = 0.00041\n",
      "Epoch: 1910|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7293|lr = 0.00041\n",
      "Epoch: 1911|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.7361|lr = 0.00041\n",
      "Epoch: 1911|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.7480|lr = 0.00041\n",
      "Epoch: 1912|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7052|lr = 0.00041\n",
      "Epoch: 1912|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7003|lr = 0.00041\n",
      "Epoch: 1913|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.6910|lr = 0.00041\n",
      "Epoch: 1913|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.7714|lr = 0.00041\n",
      "Epoch: 1914|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7164|lr = 0.00040\n",
      "Epoch: 1914|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.7152|lr = 0.00040\n",
      "Epoch: 1915|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.7527|lr = 0.00040\n",
      "Epoch: 1915|steps:   60|Train Avg Loss: 0.0099 |Test Loss: 1.7116|lr = 0.00040\n",
      "Epoch: 1916|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.6952|lr = 0.00040\n",
      "Epoch: 1916|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.7231|lr = 0.00040\n",
      "Epoch: 1917|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7319|lr = 0.00040\n",
      "Epoch: 1917|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.7165|lr = 0.00040\n",
      "Epoch: 1918|steps:   30|Train Avg Loss: 0.0070 |Test Loss: 1.6913|lr = 0.00040\n",
      "Epoch: 1918|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7412|lr = 0.00040\n",
      "Epoch: 1919|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.7392|lr = 0.00040\n",
      "Epoch: 1919|steps:   60|Train Avg Loss: 0.0079 |Test Loss: 1.7253|lr = 0.00040\n",
      "Epoch: 1920|steps:   30|Train Avg Loss: 0.0087 |Test Loss: 1.7109|lr = 0.00040\n",
      "Epoch: 1920|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7051|lr = 0.00040\n",
      "Epoch: 1921|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7033|lr = 0.00040\n",
      "Epoch: 1921|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.7039|lr = 0.00040\n",
      "Epoch: 1922|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.6863|lr = 0.00040\n",
      "Epoch: 1922|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7214|lr = 0.00040\n",
      "Epoch: 1923|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7730|lr = 0.00040\n",
      "Epoch: 1923|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7391|lr = 0.00040\n",
      "Epoch: 1924|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.7168|lr = 0.00040\n",
      "Epoch: 1924|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7291|lr = 0.00040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1925|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.7242|lr = 0.00039\n",
      "Epoch: 1925|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.7161|lr = 0.00039\n",
      "Epoch: 1926|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.7348|lr = 0.00039\n",
      "Epoch: 1926|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7564|lr = 0.00039\n",
      "Epoch: 1927|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7574|lr = 0.00039\n",
      "Epoch: 1927|steps:   60|Train Avg Loss: 0.0078 |Test Loss: 1.7433|lr = 0.00039\n",
      "Epoch: 1928|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7360|lr = 0.00039\n",
      "Epoch: 1928|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7431|lr = 0.00039\n",
      "Epoch: 1929|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.7300|lr = 0.00039\n",
      "Epoch: 1929|steps:   60|Train Avg Loss: 0.0069 |Test Loss: 1.7033|lr = 0.00039\n",
      "Epoch: 1930|steps:   30|Train Avg Loss: 0.0070 |Test Loss: 1.7077|lr = 0.00039\n",
      "Epoch: 1930|steps:   60|Train Avg Loss: 0.0089 |Test Loss: 1.6980|lr = 0.00039\n",
      "Epoch: 1931|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.7302|lr = 0.00039\n",
      "Epoch: 1931|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7440|lr = 0.00039\n",
      "Epoch: 1932|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.7483|lr = 0.00039\n",
      "Epoch: 1932|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7287|lr = 0.00039\n",
      "Epoch: 1933|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7597|lr = 0.00039\n",
      "Epoch: 1933|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.7139|lr = 0.00039\n",
      "Epoch: 1934|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7426|lr = 0.00039\n",
      "Epoch: 1934|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7263|lr = 0.00039\n",
      "Epoch: 1935|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.6824|lr = 0.00039\n",
      "Epoch: 1935|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7364|lr = 0.00039\n",
      "Epoch: 1936|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7437|lr = 0.00039\n",
      "Epoch: 1936|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.7508|lr = 0.00039\n",
      "Epoch: 1937|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7003|lr = 0.00039\n",
      "Epoch: 1937|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.7288|lr = 0.00039\n",
      "Epoch: 1938|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.6974|lr = 0.00039\n",
      "Epoch: 1938|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7302|lr = 0.00039\n",
      "Epoch: 1939|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.6997|lr = 0.00039\n",
      "Epoch: 1939|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.6998|lr = 0.00039\n",
      "Epoch: 1940|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.6761|lr = 0.00039\n",
      "Epoch: 1940|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7675|lr = 0.00039\n",
      "Epoch: 1941|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7144|lr = 0.00039\n",
      "Epoch: 1941|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.6951|lr = 0.00039\n",
      "Epoch: 1942|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7195|lr = 0.00039\n",
      "Epoch: 1942|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7283|lr = 0.00039\n",
      "Epoch: 1943|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.7458|lr = 0.00039\n",
      "Epoch: 1943|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7454|lr = 0.00039\n",
      "Epoch: 1944|steps:   30|Train Avg Loss: 0.0081 |Test Loss: 1.7842|lr = 0.00039\n",
      "Epoch: 1944|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.7505|lr = 0.00039\n",
      "Epoch: 1945|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7081|lr = 0.00039\n",
      "Epoch: 1945|steps:   60|Train Avg Loss: 0.0069 |Test Loss: 1.7027|lr = 0.00039\n",
      "Epoch: 1946|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.7061|lr = 0.00039\n",
      "Epoch: 1946|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7278|lr = 0.00039\n",
      "Epoch: 1947|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.7101|lr = 0.00038\n",
      "Epoch: 1947|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7124|lr = 0.00038\n",
      "Epoch: 1948|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7202|lr = 0.00038\n",
      "Epoch: 1948|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7409|lr = 0.00038\n",
      "Epoch: 1949|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.7167|lr = 0.00038\n",
      "Epoch: 1949|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.7003|lr = 0.00038\n",
      "Epoch: 1950|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7305|lr = 0.00038\n",
      "Epoch: 1950|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7378|lr = 0.00038\n",
      "Epoch: 1951|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.6933|lr = 0.00038\n",
      "Epoch: 1951|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7258|lr = 0.00038\n",
      "Epoch: 1952|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7429|lr = 0.00038\n",
      "Epoch: 1952|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7218|lr = 0.00038\n",
      "Epoch: 1953|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7267|lr = 0.00038\n",
      "Epoch: 1953|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.6996|lr = 0.00038\n",
      "Epoch: 1954|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7287|lr = 0.00038\n",
      "Epoch: 1954|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.7531|lr = 0.00038\n",
      "Epoch: 1955|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7202|lr = 0.00038\n",
      "Epoch: 1955|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7302|lr = 0.00038\n",
      "Epoch: 1956|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.7134|lr = 0.00038\n",
      "Epoch: 1956|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.7412|lr = 0.00038\n",
      "Epoch: 1957|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7242|lr = 0.00038\n",
      "Epoch: 1957|steps:   60|Train Avg Loss: 0.0084 |Test Loss: 1.7182|lr = 0.00038\n",
      "Epoch: 1958|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7165|lr = 0.00037\n",
      "Epoch: 1958|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7461|lr = 0.00037\n",
      "Epoch: 1959|steps:   30|Train Avg Loss: 0.0083 |Test Loss: 1.7568|lr = 0.00037\n",
      "Epoch: 1959|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7360|lr = 0.00037\n",
      "Epoch: 1960|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7348|lr = 0.00037\n",
      "Epoch: 1960|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7197|lr = 0.00037\n",
      "Epoch: 1961|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.6872|lr = 0.00037\n",
      "Epoch: 1961|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.6928|lr = 0.00037\n",
      "Epoch: 1962|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7168|lr = 0.00037\n",
      "Epoch: 1962|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7353|lr = 0.00037\n",
      "Epoch: 1963|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.6864|lr = 0.00037\n",
      "Epoch: 1963|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7151|lr = 0.00037\n",
      "Epoch: 1964|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.6938|lr = 0.00037\n",
      "Epoch: 1964|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.6854|lr = 0.00037\n",
      "Epoch: 1965|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.6772|lr = 0.00037\n",
      "Epoch: 1965|steps:   60|Train Avg Loss: 0.0075 |Test Loss: 1.7019|lr = 0.00037\n",
      "Epoch: 1966|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.6739|lr = 0.00037\n",
      "Epoch: 1966|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.7008|lr = 0.00037\n",
      "Epoch: 1967|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7015|lr = 0.00037\n",
      "Epoch: 1967|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.6998|lr = 0.00037\n",
      "Epoch: 1968|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.7166|lr = 0.00037\n",
      "Epoch: 1968|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7260|lr = 0.00037\n",
      "Epoch: 1969|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.6792|lr = 0.00036\n",
      "Epoch: 1969|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7157|lr = 0.00036\n",
      "Epoch: 1970|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7018|lr = 0.00036\n",
      "Epoch: 1970|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.6970|lr = 0.00036\n",
      "Epoch: 1971|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.7193|lr = 0.00036\n",
      "Epoch: 1971|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7047|lr = 0.00036\n",
      "Epoch: 1972|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.6881|lr = 0.00036\n",
      "Epoch: 1972|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7038|lr = 0.00036\n",
      "Epoch: 1973|steps:   30|Train Avg Loss: 0.0088 |Test Loss: 1.7216|lr = 0.00036\n",
      "Epoch: 1973|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.6881|lr = 0.00036\n",
      "Epoch: 1974|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.6830|lr = 0.00036\n",
      "Epoch: 1974|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.6639|lr = 0.00036\n",
      "Epoch: 1975|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7017|lr = 0.00036\n",
      "Epoch: 1975|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.6940|lr = 0.00036\n",
      "Epoch: 1976|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.6856|lr = 0.00036\n",
      "Epoch: 1976|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.6871|lr = 0.00036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1977|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.6748|lr = 0.00036\n",
      "Epoch: 1977|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.7402|lr = 0.00036\n",
      "Epoch: 1978|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7182|lr = 0.00036\n",
      "Epoch: 1978|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7448|lr = 0.00036\n",
      "Epoch: 1979|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7354|lr = 0.00036\n",
      "Epoch: 1979|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.7204|lr = 0.00036\n",
      "Epoch: 1980|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.6710|lr = 0.00036\n",
      "Epoch: 1980|steps:   60|Train Avg Loss: 0.0083 |Test Loss: 1.7480|lr = 0.00036\n",
      "Epoch: 1981|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.7334|lr = 0.00036\n",
      "Epoch: 1981|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.7282|lr = 0.00036\n",
      "Epoch: 1982|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7280|lr = 0.00036\n",
      "Epoch: 1982|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7314|lr = 0.00036\n",
      "Epoch: 1983|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7005|lr = 0.00036\n",
      "Epoch: 1983|steps:   60|Train Avg Loss: 0.0091 |Test Loss: 1.7063|lr = 0.00036\n",
      "Epoch: 1984|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7374|lr = 0.00036\n",
      "Epoch: 1984|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7118|lr = 0.00036\n",
      "Epoch: 1985|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.6875|lr = 0.00036\n",
      "Epoch: 1985|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7227|lr = 0.00036\n",
      "Epoch: 1986|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7622|lr = 0.00036\n",
      "Epoch: 1986|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7172|lr = 0.00036\n",
      "Epoch: 1987|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.7159|lr = 0.00036\n",
      "Epoch: 1987|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.6998|lr = 0.00036\n",
      "Epoch: 1988|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.6916|lr = 0.00036\n",
      "Epoch: 1988|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7126|lr = 0.00036\n",
      "Epoch: 1989|steps:   30|Train Avg Loss: 0.0080 |Test Loss: 1.6806|lr = 0.00036\n",
      "Epoch: 1989|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7110|lr = 0.00036\n",
      "Epoch: 1990|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.6871|lr = 0.00036\n",
      "Epoch: 1990|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7133|lr = 0.00036\n",
      "Epoch: 1991|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7145|lr = 0.00035\n",
      "Epoch: 1991|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7097|lr = 0.00035\n",
      "Epoch: 1992|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7354|lr = 0.00035\n",
      "Epoch: 1992|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7297|lr = 0.00035\n",
      "Epoch: 1993|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7194|lr = 0.00035\n",
      "Epoch: 1993|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.6877|lr = 0.00035\n",
      "Epoch: 1994|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7099|lr = 0.00035\n",
      "Epoch: 1994|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7230|lr = 0.00035\n",
      "Epoch: 1995|steps:   30|Train Avg Loss: 0.0068 |Test Loss: 1.7161|lr = 0.00035\n",
      "Epoch: 1995|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7035|lr = 0.00035\n",
      "Epoch: 1996|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7245|lr = 0.00035\n",
      "Epoch: 1996|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.6988|lr = 0.00035\n",
      "Epoch: 1997|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7416|lr = 0.00035\n",
      "Epoch: 1997|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.7591|lr = 0.00035\n",
      "Epoch: 1998|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.7113|lr = 0.00035\n",
      "Epoch: 1998|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7289|lr = 0.00035\n",
      "Epoch: 1999|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7306|lr = 0.00035\n",
      "Epoch: 1999|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.7185|lr = 0.00035\n",
      "Epoch: 2000|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.7164|lr = 0.00035\n",
      "Epoch: 2000|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7368|lr = 0.00035\n",
      "Epoch: 2001|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7189|lr = 0.00035\n",
      "Epoch: 2001|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.6942|lr = 0.00035\n",
      "Epoch: 2002|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7387|lr = 0.00034\n",
      "Epoch: 2002|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7135|lr = 0.00034\n",
      "Epoch: 2003|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7458|lr = 0.00034\n",
      "Epoch: 2003|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7639|lr = 0.00034\n",
      "Epoch: 2004|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.7224|lr = 0.00034\n",
      "Epoch: 2004|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.6945|lr = 0.00034\n",
      "Epoch: 2005|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7234|lr = 0.00034\n",
      "Epoch: 2005|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.6938|lr = 0.00034\n",
      "Epoch: 2006|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7014|lr = 0.00034\n",
      "Epoch: 2006|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7246|lr = 0.00034\n",
      "Epoch: 2007|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.6946|lr = 0.00034\n",
      "Epoch: 2007|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7248|lr = 0.00034\n",
      "Epoch: 2008|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.6761|lr = 0.00034\n",
      "Epoch: 2008|steps:   60|Train Avg Loss: 0.0080 |Test Loss: 1.7305|lr = 0.00034\n",
      "Epoch: 2009|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.6960|lr = 0.00034\n",
      "Epoch: 2009|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.7054|lr = 0.00034\n",
      "Epoch: 2010|steps:   30|Train Avg Loss: 0.0085 |Test Loss: 1.7050|lr = 0.00034\n",
      "Epoch: 2010|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.6612|lr = 0.00034\n",
      "Epoch: 2011|steps:   30|Train Avg Loss: 0.0078 |Test Loss: 1.7174|lr = 0.00034\n",
      "Epoch: 2011|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7159|lr = 0.00034\n",
      "Epoch: 2012|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7322|lr = 0.00034\n",
      "Epoch: 2012|steps:   60|Train Avg Loss: 0.0069 |Test Loss: 1.6860|lr = 0.00034\n",
      "Epoch: 2013|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7157|lr = 0.00034\n",
      "Epoch: 2013|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.6934|lr = 0.00034\n",
      "Epoch: 2014|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.6971|lr = 0.00034\n",
      "Epoch: 2014|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.6714|lr = 0.00034\n",
      "Epoch: 2015|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.6805|lr = 0.00034\n",
      "Epoch: 2015|steps:   60|Train Avg Loss: 0.0074 |Test Loss: 1.6672|lr = 0.00034\n",
      "Epoch: 2016|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.6891|lr = 0.00034\n",
      "Epoch: 2016|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.6905|lr = 0.00034\n",
      "Epoch: 2017|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.6978|lr = 0.00034\n",
      "Epoch: 2017|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.6917|lr = 0.00034\n",
      "Epoch: 2018|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.6898|lr = 0.00034\n",
      "Epoch: 2018|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.6966|lr = 0.00034\n",
      "Epoch: 2019|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7211|lr = 0.00034\n",
      "Epoch: 2019|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7356|lr = 0.00034\n",
      "Epoch: 2020|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.6836|lr = 0.00034\n",
      "Epoch: 2020|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.6807|lr = 0.00034\n",
      "Epoch: 2021|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7121|lr = 0.00034\n",
      "Epoch: 2021|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7106|lr = 0.00034\n",
      "Epoch: 2022|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.6916|lr = 0.00034\n",
      "Epoch: 2022|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.6780|lr = 0.00034\n",
      "Epoch: 2023|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.6894|lr = 0.00034\n",
      "Epoch: 2023|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7090|lr = 0.00034\n",
      "Epoch: 2024|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7130|lr = 0.00033\n",
      "Epoch: 2024|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.7062|lr = 0.00033\n",
      "Epoch: 2025|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.6916|lr = 0.00033\n",
      "Epoch: 2025|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7174|lr = 0.00033\n",
      "Epoch: 2026|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.7145|lr = 0.00033\n",
      "Epoch: 2026|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7163|lr = 0.00033\n",
      "Epoch: 2027|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.7473|lr = 0.00033\n",
      "Epoch: 2027|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7091|lr = 0.00033\n",
      "Epoch: 2028|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7224|lr = 0.00033\n",
      "Epoch: 2028|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7053|lr = 0.00033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2029|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7062|lr = 0.00033\n",
      "Epoch: 2029|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7561|lr = 0.00033\n",
      "Epoch: 2030|steps:   30|Train Avg Loss: 0.0079 |Test Loss: 1.7476|lr = 0.00033\n",
      "Epoch: 2030|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.6698|lr = 0.00033\n",
      "Epoch: 2031|steps:   30|Train Avg Loss: 0.0071 |Test Loss: 1.7018|lr = 0.00033\n",
      "Epoch: 2031|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7108|lr = 0.00033\n",
      "Epoch: 2032|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7367|lr = 0.00033\n",
      "Epoch: 2032|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7454|lr = 0.00033\n",
      "Epoch: 2033|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7341|lr = 0.00033\n",
      "Epoch: 2033|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7884|lr = 0.00033\n",
      "Epoch: 2034|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7395|lr = 0.00033\n",
      "Epoch: 2034|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7189|lr = 0.00033\n",
      "Epoch: 2035|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7388|lr = 0.00032\n",
      "Epoch: 2035|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7204|lr = 0.00032\n",
      "Epoch: 2036|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7513|lr = 0.00032\n",
      "Epoch: 2036|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7124|lr = 0.00032\n",
      "Epoch: 2037|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7373|lr = 0.00032\n",
      "Epoch: 2037|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7127|lr = 0.00032\n",
      "Epoch: 2038|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7165|lr = 0.00032\n",
      "Epoch: 2038|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.6932|lr = 0.00032\n",
      "Epoch: 2039|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7074|lr = 0.00032\n",
      "Epoch: 2039|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7321|lr = 0.00032\n",
      "Epoch: 2040|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7345|lr = 0.00032\n",
      "Epoch: 2040|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7392|lr = 0.00032\n",
      "Epoch: 2041|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7134|lr = 0.00032\n",
      "Epoch: 2041|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7636|lr = 0.00032\n",
      "Epoch: 2042|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7571|lr = 0.00032\n",
      "Epoch: 2042|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7275|lr = 0.00032\n",
      "Epoch: 2043|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7070|lr = 0.00032\n",
      "Epoch: 2043|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7240|lr = 0.00032\n",
      "Epoch: 2044|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7328|lr = 0.00032\n",
      "Epoch: 2044|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.6991|lr = 0.00032\n",
      "Epoch: 2045|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7459|lr = 0.00032\n",
      "Epoch: 2045|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7192|lr = 0.00032\n",
      "Epoch: 2046|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7320|lr = 0.00032\n",
      "Epoch: 2046|steps:   60|Train Avg Loss: 0.0070 |Test Loss: 1.7391|lr = 0.00032\n",
      "Epoch: 2047|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7304|lr = 0.00032\n",
      "Epoch: 2047|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7395|lr = 0.00032\n",
      "Epoch: 2048|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7359|lr = 0.00032\n",
      "Epoch: 2048|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.6874|lr = 0.00032\n",
      "Epoch: 2049|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.6845|lr = 0.00032\n",
      "Epoch: 2049|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7169|lr = 0.00032\n",
      "Epoch: 2050|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.6983|lr = 0.00032\n",
      "Epoch: 2050|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7311|lr = 0.00032\n",
      "Epoch: 2051|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7201|lr = 0.00032\n",
      "Epoch: 2051|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7386|lr = 0.00032\n",
      "Epoch: 2052|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7212|lr = 0.00032\n",
      "Epoch: 2052|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7146|lr = 0.00032\n",
      "Epoch: 2053|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7160|lr = 0.00032\n",
      "Epoch: 2053|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7051|lr = 0.00032\n",
      "Epoch: 2054|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7256|lr = 0.00032\n",
      "Epoch: 2054|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7538|lr = 0.00032\n",
      "Epoch: 2055|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7622|lr = 0.00032\n",
      "Epoch: 2055|steps:   60|Train Avg Loss: 0.0068 |Test Loss: 1.7250|lr = 0.00032\n",
      "Epoch: 2056|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7161|lr = 0.00032\n",
      "Epoch: 2056|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7315|lr = 0.00032\n",
      "Epoch: 2057|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7278|lr = 0.00031\n",
      "Epoch: 2057|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7467|lr = 0.00031\n",
      "Epoch: 2058|steps:   30|Train Avg Loss: 0.0074 |Test Loss: 1.7602|lr = 0.00031\n",
      "Epoch: 2058|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7543|lr = 0.00031\n",
      "Epoch: 2059|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.7049|lr = 0.00031\n",
      "Epoch: 2059|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7595|lr = 0.00031\n",
      "Epoch: 2060|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7124|lr = 0.00031\n",
      "Epoch: 2060|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7243|lr = 0.00031\n",
      "Epoch: 2061|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7271|lr = 0.00031\n",
      "Epoch: 2061|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7571|lr = 0.00031\n",
      "Epoch: 2062|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7059|lr = 0.00031\n",
      "Epoch: 2062|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7041|lr = 0.00031\n",
      "Epoch: 2063|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7617|lr = 0.00031\n",
      "Epoch: 2063|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7287|lr = 0.00031\n",
      "Epoch: 2064|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7269|lr = 0.00031\n",
      "Epoch: 2064|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7224|lr = 0.00031\n",
      "Epoch: 2065|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.6958|lr = 0.00031\n",
      "Epoch: 2065|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7264|lr = 0.00031\n",
      "Epoch: 2066|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7292|lr = 0.00031\n",
      "Epoch: 2066|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7347|lr = 0.00031\n",
      "Epoch: 2067|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7156|lr = 0.00031\n",
      "Epoch: 2067|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7179|lr = 0.00031\n",
      "Epoch: 2068|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.7306|lr = 0.00031\n",
      "Epoch: 2068|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7271|lr = 0.00031\n",
      "Epoch: 2069|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7249|lr = 0.00031\n",
      "Epoch: 2069|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7343|lr = 0.00031\n",
      "Epoch: 2070|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7103|lr = 0.00031\n",
      "Epoch: 2070|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7296|lr = 0.00031\n",
      "Epoch: 2071|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.6736|lr = 0.00031\n",
      "Epoch: 2071|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.6983|lr = 0.00031\n",
      "Epoch: 2072|steps:   30|Train Avg Loss: 0.0082 |Test Loss: 1.7162|lr = 0.00031\n",
      "Epoch: 2072|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7269|lr = 0.00031\n",
      "Epoch: 2073|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7371|lr = 0.00031\n",
      "Epoch: 2073|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7467|lr = 0.00031\n",
      "Epoch: 2074|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7556|lr = 0.00030\n",
      "Epoch: 2074|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7270|lr = 0.00030\n",
      "Epoch: 2075|steps:   30|Train Avg Loss: 0.0072 |Test Loss: 1.7509|lr = 0.00030\n",
      "Epoch: 2075|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7236|lr = 0.00030\n",
      "Epoch: 2076|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7019|lr = 0.00030\n",
      "Epoch: 2076|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7247|lr = 0.00030\n",
      "Epoch: 2077|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.7254|lr = 0.00030\n",
      "Epoch: 2077|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7130|lr = 0.00030\n",
      "Epoch: 2078|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7239|lr = 0.00030\n",
      "Epoch: 2078|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7119|lr = 0.00030\n",
      "Epoch: 2079|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7433|lr = 0.00030\n",
      "Epoch: 2079|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7353|lr = 0.00030\n",
      "Epoch: 2080|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7349|lr = 0.00030\n",
      "Epoch: 2080|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7325|lr = 0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2081|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7481|lr = 0.00030\n",
      "Epoch: 2081|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.6975|lr = 0.00030\n",
      "Epoch: 2082|steps:   30|Train Avg Loss: 0.0086 |Test Loss: 1.6945|lr = 0.00030\n",
      "Epoch: 2082|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.6797|lr = 0.00030\n",
      "Epoch: 2083|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7283|lr = 0.00030\n",
      "Epoch: 2083|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7148|lr = 0.00030\n",
      "Epoch: 2084|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7167|lr = 0.00030\n",
      "Epoch: 2084|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7274|lr = 0.00030\n",
      "Epoch: 2085|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7013|lr = 0.00030\n",
      "Epoch: 2085|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.7228|lr = 0.00030\n",
      "Epoch: 2086|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.7203|lr = 0.00030\n",
      "Epoch: 2086|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7162|lr = 0.00030\n",
      "Epoch: 2087|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7421|lr = 0.00030\n",
      "Epoch: 2087|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7418|lr = 0.00030\n",
      "Epoch: 2088|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7028|lr = 0.00030\n",
      "Epoch: 2088|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7231|lr = 0.00030\n",
      "Epoch: 2089|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7455|lr = 0.00030\n",
      "Epoch: 2089|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7289|lr = 0.00030\n",
      "Epoch: 2090|steps:   30|Train Avg Loss: 0.0076 |Test Loss: 1.7377|lr = 0.00030\n",
      "Epoch: 2090|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7025|lr = 0.00030\n",
      "Epoch: 2091|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7006|lr = 0.00030\n",
      "Epoch: 2091|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.6795|lr = 0.00030\n",
      "Epoch: 2092|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.7659|lr = 0.00030\n",
      "Epoch: 2092|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7540|lr = 0.00030\n",
      "Epoch: 2093|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7098|lr = 0.00030\n",
      "Epoch: 2093|steps:   60|Train Avg Loss: 0.0072 |Test Loss: 1.7039|lr = 0.00030\n",
      "Epoch: 2094|steps:   30|Train Avg Loss: 0.0073 |Test Loss: 1.7499|lr = 0.00030\n",
      "Epoch: 2094|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7572|lr = 0.00030\n",
      "Epoch: 2095|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7246|lr = 0.00030\n",
      "Epoch: 2095|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7466|lr = 0.00030\n",
      "Epoch: 2096|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7338|lr = 0.00029\n",
      "Epoch: 2096|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7157|lr = 0.00029\n",
      "Epoch: 2097|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7360|lr = 0.00029\n",
      "Epoch: 2097|steps:   60|Train Avg Loss: 0.0077 |Test Loss: 1.6973|lr = 0.00029\n",
      "Epoch: 2098|steps:   30|Train Avg Loss: 0.0101 |Test Loss: 1.7337|lr = 0.00029\n",
      "Epoch: 2098|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7274|lr = 0.00029\n",
      "Epoch: 2099|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7372|lr = 0.00029\n",
      "Epoch: 2099|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7490|lr = 0.00029\n",
      "Epoch: 2100|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7307|lr = 0.00029\n",
      "Epoch: 2100|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7213|lr = 0.00029\n",
      "Epoch: 2101|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7304|lr = 0.00029\n",
      "Epoch: 2101|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7147|lr = 0.00029\n",
      "Epoch: 2102|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7508|lr = 0.00029\n",
      "Epoch: 2102|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7247|lr = 0.00029\n",
      "Epoch: 2103|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7320|lr = 0.00029\n",
      "Epoch: 2103|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7353|lr = 0.00029\n",
      "Epoch: 2104|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.6985|lr = 0.00029\n",
      "Epoch: 2104|steps:   60|Train Avg Loss: 0.0073 |Test Loss: 1.7107|lr = 0.00029\n",
      "Epoch: 2105|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7120|lr = 0.00029\n",
      "Epoch: 2105|steps:   60|Train Avg Loss: 0.0062 |Test Loss: 1.7668|lr = 0.00029\n",
      "Epoch: 2106|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7103|lr = 0.00029\n",
      "Epoch: 2106|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.6957|lr = 0.00029\n",
      "Epoch: 2107|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.6827|lr = 0.00029\n",
      "Epoch: 2107|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7026|lr = 0.00029\n",
      "Epoch: 2108|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7217|lr = 0.00029\n",
      "Epoch: 2108|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7093|lr = 0.00029\n",
      "Epoch: 2109|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7121|lr = 0.00029\n",
      "Epoch: 2109|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7079|lr = 0.00029\n",
      "Epoch: 2110|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7170|lr = 0.00029\n",
      "Epoch: 2110|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7123|lr = 0.00029\n",
      "Epoch: 2111|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7671|lr = 0.00029\n",
      "Epoch: 2111|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7349|lr = 0.00029\n",
      "Epoch: 2112|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7297|lr = 0.00029\n",
      "Epoch: 2112|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7280|lr = 0.00029\n",
      "Epoch: 2113|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7030|lr = 0.00029\n",
      "Epoch: 2113|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7248|lr = 0.00029\n",
      "Epoch: 2114|steps:   30|Train Avg Loss: 0.0060 |Test Loss: 1.7373|lr = 0.00029\n",
      "Epoch: 2114|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7611|lr = 0.00029\n",
      "Epoch: 2115|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7247|lr = 0.00029\n",
      "Epoch: 2115|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7305|lr = 0.00029\n",
      "Epoch: 2116|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7591|lr = 0.00029\n",
      "Epoch: 2116|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7233|lr = 0.00029\n",
      "Epoch: 2117|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7209|lr = 0.00029\n",
      "Epoch: 2117|steps:   60|Train Avg Loss: 0.0067 |Test Loss: 1.7017|lr = 0.00029\n",
      "Epoch: 2118|steps:   30|Train Avg Loss: 0.0067 |Test Loss: 1.7065|lr = 0.00028\n",
      "Epoch: 2118|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.6985|lr = 0.00028\n",
      "Epoch: 2119|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7298|lr = 0.00028\n",
      "Epoch: 2119|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7189|lr = 0.00028\n",
      "Epoch: 2120|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7405|lr = 0.00028\n",
      "Epoch: 2120|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7550|lr = 0.00028\n",
      "Epoch: 2121|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7042|lr = 0.00028\n",
      "Epoch: 2121|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.6787|lr = 0.00028\n",
      "Epoch: 2122|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7106|lr = 0.00028\n",
      "Epoch: 2122|steps:   60|Train Avg Loss: 0.0066 |Test Loss: 1.7191|lr = 0.00028\n",
      "Epoch: 2123|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7308|lr = 0.00028\n",
      "Epoch: 2123|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7007|lr = 0.00028\n",
      "Epoch: 2124|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7102|lr = 0.00028\n",
      "Epoch: 2124|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7205|lr = 0.00028\n",
      "Epoch: 2125|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7039|lr = 0.00028\n",
      "Epoch: 2125|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7320|lr = 0.00028\n",
      "Epoch: 2126|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7127|lr = 0.00028\n",
      "Epoch: 2126|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7281|lr = 0.00028\n",
      "Epoch: 2127|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7270|lr = 0.00028\n",
      "Epoch: 2127|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7370|lr = 0.00028\n",
      "Epoch: 2128|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7509|lr = 0.00028\n",
      "Epoch: 2128|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7269|lr = 0.00028\n",
      "Epoch: 2129|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7414|lr = 0.00027\n",
      "Epoch: 2129|steps:   60|Train Avg Loss: 0.0056 |Test Loss: 1.7071|lr = 0.00027\n",
      "Epoch: 2130|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7154|lr = 0.00027\n",
      "Epoch: 2130|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7342|lr = 0.00027\n",
      "Epoch: 2131|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7190|lr = 0.00027\n",
      "Epoch: 2131|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7359|lr = 0.00027\n",
      "Epoch: 2132|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7586|lr = 0.00027\n",
      "Epoch: 2132|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7363|lr = 0.00027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2133|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7512|lr = 0.00027\n",
      "Epoch: 2133|steps:   60|Train Avg Loss: 0.0081 |Test Loss: 1.7238|lr = 0.00027\n",
      "Epoch: 2134|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7391|lr = 0.00027\n",
      "Epoch: 2134|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7523|lr = 0.00027\n",
      "Epoch: 2135|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.7173|lr = 0.00027\n",
      "Epoch: 2135|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7528|lr = 0.00027\n",
      "Epoch: 2136|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7069|lr = 0.00027\n",
      "Epoch: 2136|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7240|lr = 0.00027\n",
      "Epoch: 2137|steps:   30|Train Avg Loss: 0.0065 |Test Loss: 1.7718|lr = 0.00027\n",
      "Epoch: 2137|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7597|lr = 0.00027\n",
      "Epoch: 2138|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7482|lr = 0.00027\n",
      "Epoch: 2138|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7290|lr = 0.00027\n",
      "Epoch: 2139|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7215|lr = 0.00027\n",
      "Epoch: 2139|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7361|lr = 0.00027\n",
      "Epoch: 2140|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.6985|lr = 0.00027\n",
      "Epoch: 2140|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7761|lr = 0.00027\n",
      "Epoch: 2141|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7388|lr = 0.00027\n",
      "Epoch: 2141|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7553|lr = 0.00027\n",
      "Epoch: 2142|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7279|lr = 0.00027\n",
      "Epoch: 2142|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7381|lr = 0.00027\n",
      "Epoch: 2143|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7477|lr = 0.00027\n",
      "Epoch: 2143|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7599|lr = 0.00027\n",
      "Epoch: 2144|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7247|lr = 0.00027\n",
      "Epoch: 2144|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7544|lr = 0.00027\n",
      "Epoch: 2145|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7204|lr = 0.00027\n",
      "Epoch: 2145|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7347|lr = 0.00027\n",
      "Epoch: 2146|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7355|lr = 0.00027\n",
      "Epoch: 2146|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7125|lr = 0.00027\n",
      "Epoch: 2147|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7199|lr = 0.00027\n",
      "Epoch: 2147|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.6961|lr = 0.00027\n",
      "Epoch: 2148|steps:   30|Train Avg Loss: 0.0066 |Test Loss: 1.7198|lr = 0.00027\n",
      "Epoch: 2148|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.6995|lr = 0.00027\n",
      "Epoch: 2149|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.6998|lr = 0.00027\n",
      "Epoch: 2149|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.6904|lr = 0.00027\n",
      "Epoch: 2150|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7365|lr = 0.00027\n",
      "Epoch: 2150|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7004|lr = 0.00027\n",
      "Epoch: 2151|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.6835|lr = 0.00026\n",
      "Epoch: 2151|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7413|lr = 0.00026\n",
      "Epoch: 2152|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7601|lr = 0.00026\n",
      "Epoch: 2152|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7009|lr = 0.00026\n",
      "Epoch: 2153|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7215|lr = 0.00026\n",
      "Epoch: 2153|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7324|lr = 0.00026\n",
      "Epoch: 2154|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7081|lr = 0.00026\n",
      "Epoch: 2154|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7516|lr = 0.00026\n",
      "Epoch: 2155|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7506|lr = 0.00026\n",
      "Epoch: 2155|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7187|lr = 0.00026\n",
      "Epoch: 2156|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7179|lr = 0.00026\n",
      "Epoch: 2156|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.6949|lr = 0.00026\n",
      "Epoch: 2157|steps:   30|Train Avg Loss: 0.0069 |Test Loss: 1.6724|lr = 0.00026\n",
      "Epoch: 2157|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7011|lr = 0.00026\n",
      "Epoch: 2158|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7420|lr = 0.00026\n",
      "Epoch: 2158|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7107|lr = 0.00026\n",
      "Epoch: 2159|steps:   30|Train Avg Loss: 0.0062 |Test Loss: 1.6971|lr = 0.00026\n",
      "Epoch: 2159|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7379|lr = 0.00026\n",
      "Epoch: 2160|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7271|lr = 0.00026\n",
      "Epoch: 2160|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7323|lr = 0.00026\n",
      "Epoch: 2161|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.6915|lr = 0.00026\n",
      "Epoch: 2161|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7004|lr = 0.00026\n",
      "Epoch: 2162|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7124|lr = 0.00026\n",
      "Epoch: 2162|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7164|lr = 0.00026\n",
      "Epoch: 2163|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7033|lr = 0.00026\n",
      "Epoch: 2163|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7154|lr = 0.00026\n",
      "Epoch: 2164|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.6930|lr = 0.00026\n",
      "Epoch: 2164|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7224|lr = 0.00026\n",
      "Epoch: 2165|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7286|lr = 0.00026\n",
      "Epoch: 2165|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7174|lr = 0.00026\n",
      "Epoch: 2166|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7419|lr = 0.00026\n",
      "Epoch: 2166|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7235|lr = 0.00026\n",
      "Epoch: 2167|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7303|lr = 0.00026\n",
      "Epoch: 2167|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7207|lr = 0.00026\n",
      "Epoch: 2168|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7332|lr = 0.00026\n",
      "Epoch: 2168|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7064|lr = 0.00026\n",
      "Epoch: 2169|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7280|lr = 0.00026\n",
      "Epoch: 2169|steps:   60|Train Avg Loss: 0.0061 |Test Loss: 1.7126|lr = 0.00026\n",
      "Epoch: 2170|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.7099|lr = 0.00026\n",
      "Epoch: 2170|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7309|lr = 0.00026\n",
      "Epoch: 2171|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7129|lr = 0.00026\n",
      "Epoch: 2171|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7499|lr = 0.00026\n",
      "Epoch: 2172|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7199|lr = 0.00026\n",
      "Epoch: 2172|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7152|lr = 0.00026\n",
      "Epoch: 2173|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7477|lr = 0.00025\n",
      "Epoch: 2173|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7271|lr = 0.00025\n",
      "Epoch: 2174|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7427|lr = 0.00025\n",
      "Epoch: 2174|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7602|lr = 0.00025\n",
      "Epoch: 2175|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7190|lr = 0.00025\n",
      "Epoch: 2175|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7232|lr = 0.00025\n",
      "Epoch: 2176|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7137|lr = 0.00025\n",
      "Epoch: 2176|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7431|lr = 0.00025\n",
      "Epoch: 2177|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7277|lr = 0.00025\n",
      "Epoch: 2177|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7435|lr = 0.00025\n",
      "Epoch: 2178|steps:   30|Train Avg Loss: 0.0058 |Test Loss: 1.6991|lr = 0.00025\n",
      "Epoch: 2178|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7241|lr = 0.00025\n",
      "Epoch: 2179|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.6946|lr = 0.00025\n",
      "Epoch: 2179|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7349|lr = 0.00025\n",
      "Epoch: 2180|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.6995|lr = 0.00025\n",
      "Epoch: 2180|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7264|lr = 0.00025\n",
      "Epoch: 2181|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7334|lr = 0.00025\n",
      "Epoch: 2181|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7291|lr = 0.00025\n",
      "Epoch: 2182|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7029|lr = 0.00025\n",
      "Epoch: 2182|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7212|lr = 0.00025\n",
      "Epoch: 2183|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7436|lr = 0.00025\n",
      "Epoch: 2183|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7109|lr = 0.00025\n",
      "Epoch: 2184|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.6525|lr = 0.00025\n",
      "Epoch: 2184|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.6971|lr = 0.00025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2185|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.6944|lr = 0.00025\n",
      "Epoch: 2185|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7392|lr = 0.00025\n",
      "Epoch: 2186|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7266|lr = 0.00025\n",
      "Epoch: 2186|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7557|lr = 0.00025\n",
      "Epoch: 2187|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7067|lr = 0.00025\n",
      "Epoch: 2187|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7450|lr = 0.00025\n",
      "Epoch: 2188|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.6976|lr = 0.00025\n",
      "Epoch: 2188|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7371|lr = 0.00025\n",
      "Epoch: 2189|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7130|lr = 0.00025\n",
      "Epoch: 2189|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7307|lr = 0.00025\n",
      "Epoch: 2190|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7334|lr = 0.00025\n",
      "Epoch: 2190|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7370|lr = 0.00025\n",
      "Epoch: 2191|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7700|lr = 0.00025\n",
      "Epoch: 2191|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.6879|lr = 0.00025\n",
      "Epoch: 2192|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7513|lr = 0.00025\n",
      "Epoch: 2192|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7363|lr = 0.00025\n",
      "Epoch: 2193|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7161|lr = 0.00025\n",
      "Epoch: 2193|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7015|lr = 0.00025\n",
      "Epoch: 2194|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7353|lr = 0.00025\n",
      "Epoch: 2194|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7066|lr = 0.00025\n",
      "Epoch: 2195|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7124|lr = 0.00024\n",
      "Epoch: 2195|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7115|lr = 0.00024\n",
      "Epoch: 2196|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7165|lr = 0.00024\n",
      "Epoch: 2196|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7292|lr = 0.00024\n",
      "Epoch: 2197|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7592|lr = 0.00024\n",
      "Epoch: 2197|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7275|lr = 0.00024\n",
      "Epoch: 2198|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.6892|lr = 0.00024\n",
      "Epoch: 2198|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.6681|lr = 0.00024\n",
      "Epoch: 2199|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7281|lr = 0.00024\n",
      "Epoch: 2199|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7283|lr = 0.00024\n",
      "Epoch: 2200|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7325|lr = 0.00024\n",
      "Epoch: 2200|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7186|lr = 0.00024\n",
      "Epoch: 2201|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7398|lr = 0.00024\n",
      "Epoch: 2201|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7109|lr = 0.00024\n",
      "Epoch: 2202|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7343|lr = 0.00024\n",
      "Epoch: 2202|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7181|lr = 0.00024\n",
      "Epoch: 2203|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7223|lr = 0.00024\n",
      "Epoch: 2203|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7443|lr = 0.00024\n",
      "Epoch: 2204|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7114|lr = 0.00024\n",
      "Epoch: 2204|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7103|lr = 0.00024\n",
      "Epoch: 2205|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7398|lr = 0.00024\n",
      "Epoch: 2205|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7266|lr = 0.00024\n",
      "Epoch: 2206|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7431|lr = 0.00024\n",
      "Epoch: 2206|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7340|lr = 0.00024\n",
      "Epoch: 2207|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7253|lr = 0.00024\n",
      "Epoch: 2207|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7507|lr = 0.00024\n",
      "Epoch: 2208|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7225|lr = 0.00024\n",
      "Epoch: 2208|steps:   60|Train Avg Loss: 0.0071 |Test Loss: 1.7617|lr = 0.00024\n",
      "Epoch: 2209|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7156|lr = 0.00024\n",
      "Epoch: 2209|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7215|lr = 0.00024\n",
      "Epoch: 2210|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7139|lr = 0.00024\n",
      "Epoch: 2210|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7212|lr = 0.00024\n",
      "Epoch: 2211|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7230|lr = 0.00024\n",
      "Epoch: 2211|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7533|lr = 0.00024\n",
      "Epoch: 2212|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7173|lr = 0.00024\n",
      "Epoch: 2212|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7644|lr = 0.00024\n",
      "Epoch: 2213|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7186|lr = 0.00024\n",
      "Epoch: 2213|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7331|lr = 0.00024\n",
      "Epoch: 2214|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7231|lr = 0.00024\n",
      "Epoch: 2214|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7567|lr = 0.00024\n",
      "Epoch: 2215|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7342|lr = 0.00024\n",
      "Epoch: 2215|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7320|lr = 0.00024\n",
      "Epoch: 2216|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7017|lr = 0.00024\n",
      "Epoch: 2216|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7015|lr = 0.00024\n",
      "Epoch: 2217|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7284|lr = 0.00023\n",
      "Epoch: 2217|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7240|lr = 0.00023\n",
      "Epoch: 2218|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7227|lr = 0.00023\n",
      "Epoch: 2218|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.6909|lr = 0.00023\n",
      "Epoch: 2219|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7181|lr = 0.00023\n",
      "Epoch: 2219|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7577|lr = 0.00023\n",
      "Epoch: 2220|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7498|lr = 0.00023\n",
      "Epoch: 2220|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.6997|lr = 0.00023\n",
      "Epoch: 2221|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.6981|lr = 0.00023\n",
      "Epoch: 2221|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7496|lr = 0.00023\n",
      "Epoch: 2222|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7484|lr = 0.00023\n",
      "Epoch: 2222|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.6931|lr = 0.00023\n",
      "Epoch: 2223|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.6887|lr = 0.00023\n",
      "Epoch: 2223|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7088|lr = 0.00023\n",
      "Epoch: 2224|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.6889|lr = 0.00023\n",
      "Epoch: 2224|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7108|lr = 0.00023\n",
      "Epoch: 2225|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7404|lr = 0.00023\n",
      "Epoch: 2225|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7545|lr = 0.00023\n",
      "Epoch: 2226|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7549|lr = 0.00023\n",
      "Epoch: 2226|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7097|lr = 0.00023\n",
      "Epoch: 2227|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7427|lr = 0.00023\n",
      "Epoch: 2227|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7716|lr = 0.00023\n",
      "Epoch: 2228|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7497|lr = 0.00023\n",
      "Epoch: 2228|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7277|lr = 0.00023\n",
      "Epoch: 2229|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7343|lr = 0.00023\n",
      "Epoch: 2229|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7446|lr = 0.00023\n",
      "Epoch: 2230|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7305|lr = 0.00023\n",
      "Epoch: 2230|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7298|lr = 0.00023\n",
      "Epoch: 2231|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7331|lr = 0.00023\n",
      "Epoch: 2231|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7486|lr = 0.00023\n",
      "Epoch: 2232|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7276|lr = 0.00023\n",
      "Epoch: 2232|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7366|lr = 0.00023\n",
      "Epoch: 2233|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7446|lr = 0.00023\n",
      "Epoch: 2233|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7343|lr = 0.00023\n",
      "Epoch: 2234|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7176|lr = 0.00023\n",
      "Epoch: 2234|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7355|lr = 0.00023\n",
      "Epoch: 2235|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7306|lr = 0.00023\n",
      "Epoch: 2235|steps:   60|Train Avg Loss: 0.0059 |Test Loss: 1.7125|lr = 0.00023\n",
      "Epoch: 2236|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7149|lr = 0.00023\n",
      "Epoch: 2236|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7086|lr = 0.00023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2237|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.6927|lr = 0.00023\n",
      "Epoch: 2237|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7282|lr = 0.00023\n",
      "Epoch: 2238|steps:   30|Train Avg Loss: 0.0056 |Test Loss: 1.7294|lr = 0.00023\n",
      "Epoch: 2238|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7450|lr = 0.00023\n",
      "Epoch: 2239|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7119|lr = 0.00022\n",
      "Epoch: 2239|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7139|lr = 0.00022\n",
      "Epoch: 2240|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7198|lr = 0.00022\n",
      "Epoch: 2240|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7238|lr = 0.00022\n",
      "Epoch: 2241|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7683|lr = 0.00022\n",
      "Epoch: 2241|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7385|lr = 0.00022\n",
      "Epoch: 2242|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7269|lr = 0.00022\n",
      "Epoch: 2242|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7099|lr = 0.00022\n",
      "Epoch: 2243|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7374|lr = 0.00022\n",
      "Epoch: 2243|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7334|lr = 0.00022\n",
      "Epoch: 2244|steps:   30|Train Avg Loss: 0.0055 |Test Loss: 1.7169|lr = 0.00022\n",
      "Epoch: 2244|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7067|lr = 0.00022\n",
      "Epoch: 2245|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7577|lr = 0.00022\n",
      "Epoch: 2245|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7246|lr = 0.00022\n",
      "Epoch: 2246|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7428|lr = 0.00022\n",
      "Epoch: 2246|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7195|lr = 0.00022\n",
      "Epoch: 2247|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7487|lr = 0.00022\n",
      "Epoch: 2247|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7280|lr = 0.00022\n",
      "Epoch: 2248|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7306|lr = 0.00022\n",
      "Epoch: 2248|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7528|lr = 0.00022\n",
      "Epoch: 2249|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7349|lr = 0.00022\n",
      "Epoch: 2249|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7166|lr = 0.00022\n",
      "Epoch: 2250|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7437|lr = 0.00022\n",
      "Epoch: 2250|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7558|lr = 0.00022\n",
      "Epoch: 2251|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7229|lr = 0.00022\n",
      "Epoch: 2251|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7365|lr = 0.00022\n",
      "Epoch: 2252|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7423|lr = 0.00022\n",
      "Epoch: 2252|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7800|lr = 0.00022\n",
      "Epoch: 2253|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7366|lr = 0.00022\n",
      "Epoch: 2253|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7294|lr = 0.00022\n",
      "Epoch: 2254|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7452|lr = 0.00022\n",
      "Epoch: 2254|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7261|lr = 0.00022\n",
      "Epoch: 2255|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7521|lr = 0.00022\n",
      "Epoch: 2255|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7135|lr = 0.00022\n",
      "Epoch: 2256|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7250|lr = 0.00022\n",
      "Epoch: 2256|steps:   60|Train Avg Loss: 0.0064 |Test Loss: 1.7377|lr = 0.00022\n",
      "Epoch: 2257|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7134|lr = 0.00022\n",
      "Epoch: 2257|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.6949|lr = 0.00022\n",
      "Epoch: 2258|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.6976|lr = 0.00022\n",
      "Epoch: 2258|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7278|lr = 0.00022\n",
      "Epoch: 2259|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7643|lr = 0.00022\n",
      "Epoch: 2259|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7346|lr = 0.00022\n",
      "Epoch: 2260|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7421|lr = 0.00022\n",
      "Epoch: 2260|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7622|lr = 0.00022\n",
      "Epoch: 2261|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7607|lr = 0.00022\n",
      "Epoch: 2261|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7391|lr = 0.00022\n",
      "Epoch: 2262|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7384|lr = 0.00022\n",
      "Epoch: 2262|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7428|lr = 0.00022\n",
      "Epoch: 2263|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7254|lr = 0.00022\n",
      "Epoch: 2263|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7278|lr = 0.00022\n",
      "Epoch: 2264|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7338|lr = 0.00022\n",
      "Epoch: 2264|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7212|lr = 0.00022\n",
      "Epoch: 2265|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7515|lr = 0.00022\n",
      "Epoch: 2265|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7502|lr = 0.00022\n",
      "Epoch: 2266|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7371|lr = 0.00022\n",
      "Epoch: 2266|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7552|lr = 0.00022\n",
      "Epoch: 2267|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7528|lr = 0.00022\n",
      "Epoch: 2267|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7497|lr = 0.00022\n",
      "Epoch: 2268|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7066|lr = 0.00022\n",
      "Epoch: 2268|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7251|lr = 0.00022\n",
      "Epoch: 2269|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7715|lr = 0.00022\n",
      "Epoch: 2269|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7257|lr = 0.00022\n",
      "Epoch: 2270|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7192|lr = 0.00022\n",
      "Epoch: 2270|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7640|lr = 0.00022\n",
      "Epoch: 2271|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7517|lr = 0.00022\n",
      "Epoch: 2271|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7339|lr = 0.00022\n",
      "Epoch: 2272|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7312|lr = 0.00021\n",
      "Epoch: 2272|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7240|lr = 0.00021\n",
      "Epoch: 2273|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7042|lr = 0.00021\n",
      "Epoch: 2273|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7343|lr = 0.00021\n",
      "Epoch: 2274|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7389|lr = 0.00021\n",
      "Epoch: 2274|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7784|lr = 0.00021\n",
      "Epoch: 2275|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7598|lr = 0.00021\n",
      "Epoch: 2275|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7713|lr = 0.00021\n",
      "Epoch: 2276|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7320|lr = 0.00021\n",
      "Epoch: 2276|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7295|lr = 0.00021\n",
      "Epoch: 2277|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7503|lr = 0.00021\n",
      "Epoch: 2277|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7315|lr = 0.00021\n",
      "Epoch: 2278|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7463|lr = 0.00021\n",
      "Epoch: 2278|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7392|lr = 0.00021\n",
      "Epoch: 2279|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7175|lr = 0.00021\n",
      "Epoch: 2279|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7150|lr = 0.00021\n",
      "Epoch: 2280|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7482|lr = 0.00021\n",
      "Epoch: 2280|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7241|lr = 0.00021\n",
      "Epoch: 2281|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7291|lr = 0.00021\n",
      "Epoch: 2281|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7577|lr = 0.00021\n",
      "Epoch: 2282|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7233|lr = 0.00021\n",
      "Epoch: 2282|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7180|lr = 0.00021\n",
      "Epoch: 2283|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7330|lr = 0.00021\n",
      "Epoch: 2283|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7383|lr = 0.00021\n",
      "Epoch: 2284|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7159|lr = 0.00021\n",
      "Epoch: 2284|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7332|lr = 0.00021\n",
      "Epoch: 2285|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7374|lr = 0.00021\n",
      "Epoch: 2285|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7279|lr = 0.00021\n",
      "Epoch: 2286|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7248|lr = 0.00021\n",
      "Epoch: 2286|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7669|lr = 0.00021\n",
      "Epoch: 2287|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7297|lr = 0.00021\n",
      "Epoch: 2287|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7202|lr = 0.00021\n",
      "Epoch: 2288|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7480|lr = 0.00021\n",
      "Epoch: 2288|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7574|lr = 0.00021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2289|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7543|lr = 0.00021\n",
      "Epoch: 2289|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7559|lr = 0.00021\n",
      "Epoch: 2290|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7496|lr = 0.00021\n",
      "Epoch: 2290|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7817|lr = 0.00021\n",
      "Epoch: 2291|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7173|lr = 0.00021\n",
      "Epoch: 2291|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7274|lr = 0.00021\n",
      "Epoch: 2292|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7647|lr = 0.00021\n",
      "Epoch: 2292|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7427|lr = 0.00021\n",
      "Epoch: 2293|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7704|lr = 0.00021\n",
      "Epoch: 2293|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7173|lr = 0.00021\n",
      "Epoch: 2294|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7765|lr = 0.00020\n",
      "Epoch: 2294|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7507|lr = 0.00020\n",
      "Epoch: 2295|steps:   30|Train Avg Loss: 0.0057 |Test Loss: 1.7656|lr = 0.00020\n",
      "Epoch: 2295|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7577|lr = 0.00020\n",
      "Epoch: 2296|steps:   30|Train Avg Loss: 0.0064 |Test Loss: 1.7092|lr = 0.00020\n",
      "Epoch: 2296|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7353|lr = 0.00020\n",
      "Epoch: 2297|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7133|lr = 0.00020\n",
      "Epoch: 2297|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7279|lr = 0.00020\n",
      "Epoch: 2298|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7605|lr = 0.00020\n",
      "Epoch: 2298|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7370|lr = 0.00020\n",
      "Epoch: 2299|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7493|lr = 0.00020\n",
      "Epoch: 2299|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7353|lr = 0.00020\n",
      "Epoch: 2300|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7683|lr = 0.00020\n",
      "Epoch: 2300|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7358|lr = 0.00020\n",
      "Epoch: 2301|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7348|lr = 0.00020\n",
      "Epoch: 2301|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7947|lr = 0.00020\n",
      "Epoch: 2302|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7587|lr = 0.00020\n",
      "Epoch: 2302|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7573|lr = 0.00020\n",
      "Epoch: 2303|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7785|lr = 0.00020\n",
      "Epoch: 2303|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7484|lr = 0.00020\n",
      "Epoch: 2304|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7971|lr = 0.00020\n",
      "Epoch: 2304|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7749|lr = 0.00020\n",
      "Epoch: 2305|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7455|lr = 0.00020\n",
      "Epoch: 2305|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7644|lr = 0.00020\n",
      "Epoch: 2306|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7724|lr = 0.00020\n",
      "Epoch: 2306|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7481|lr = 0.00020\n",
      "Epoch: 2307|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7488|lr = 0.00020\n",
      "Epoch: 2307|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7370|lr = 0.00020\n",
      "Epoch: 2308|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7552|lr = 0.00020\n",
      "Epoch: 2308|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7382|lr = 0.00020\n",
      "Epoch: 2309|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7415|lr = 0.00020\n",
      "Epoch: 2309|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7567|lr = 0.00020\n",
      "Epoch: 2310|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7422|lr = 0.00020\n",
      "Epoch: 2310|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7370|lr = 0.00020\n",
      "Epoch: 2311|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7627|lr = 0.00020\n",
      "Epoch: 2311|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7267|lr = 0.00020\n",
      "Epoch: 2312|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7501|lr = 0.00020\n",
      "Epoch: 2312|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7462|lr = 0.00020\n",
      "Epoch: 2313|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7531|lr = 0.00020\n",
      "Epoch: 2313|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7739|lr = 0.00020\n",
      "Epoch: 2314|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7618|lr = 0.00020\n",
      "Epoch: 2314|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7446|lr = 0.00020\n",
      "Epoch: 2315|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7561|lr = 0.00020\n",
      "Epoch: 2315|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7715|lr = 0.00020\n",
      "Epoch: 2316|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7273|lr = 0.00019\n",
      "Epoch: 2316|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7691|lr = 0.00019\n",
      "Epoch: 2317|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7421|lr = 0.00019\n",
      "Epoch: 2317|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7702|lr = 0.00019\n",
      "Epoch: 2318|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7784|lr = 0.00019\n",
      "Epoch: 2318|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7371|lr = 0.00019\n",
      "Epoch: 2319|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7152|lr = 0.00019\n",
      "Epoch: 2319|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7543|lr = 0.00019\n",
      "Epoch: 2320|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7424|lr = 0.00019\n",
      "Epoch: 2320|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7366|lr = 0.00019\n",
      "Epoch: 2321|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7387|lr = 0.00019\n",
      "Epoch: 2321|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7775|lr = 0.00019\n",
      "Epoch: 2322|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7541|lr = 0.00019\n",
      "Epoch: 2322|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7898|lr = 0.00019\n",
      "Epoch: 2323|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7663|lr = 0.00019\n",
      "Epoch: 2323|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7141|lr = 0.00019\n",
      "Epoch: 2324|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7319|lr = 0.00019\n",
      "Epoch: 2324|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7476|lr = 0.00019\n",
      "Epoch: 2325|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7153|lr = 0.00019\n",
      "Epoch: 2325|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7468|lr = 0.00019\n",
      "Epoch: 2326|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7482|lr = 0.00019\n",
      "Epoch: 2326|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7499|lr = 0.00019\n",
      "Epoch: 2327|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7167|lr = 0.00019\n",
      "Epoch: 2327|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7336|lr = 0.00019\n",
      "Epoch: 2328|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7342|lr = 0.00019\n",
      "Epoch: 2328|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7320|lr = 0.00019\n",
      "Epoch: 2329|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7346|lr = 0.00019\n",
      "Epoch: 2329|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7292|lr = 0.00019\n",
      "Epoch: 2330|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7433|lr = 0.00019\n",
      "Epoch: 2330|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7322|lr = 0.00019\n",
      "Epoch: 2331|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7951|lr = 0.00019\n",
      "Epoch: 2331|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7533|lr = 0.00019\n",
      "Epoch: 2332|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7732|lr = 0.00019\n",
      "Epoch: 2332|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7699|lr = 0.00019\n",
      "Epoch: 2333|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7255|lr = 0.00019\n",
      "Epoch: 2333|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7279|lr = 0.00019\n",
      "Epoch: 2334|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7376|lr = 0.00019\n",
      "Epoch: 2334|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7554|lr = 0.00019\n",
      "Epoch: 2335|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7550|lr = 0.00019\n",
      "Epoch: 2335|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7314|lr = 0.00019\n",
      "Epoch: 2336|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7452|lr = 0.00019\n",
      "Epoch: 2336|steps:   60|Train Avg Loss: 0.0055 |Test Loss: 1.7306|lr = 0.00019\n",
      "Epoch: 2337|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7367|lr = 0.00019\n",
      "Epoch: 2337|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7404|lr = 0.00019\n",
      "Epoch: 2338|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7450|lr = 0.00019\n",
      "Epoch: 2338|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7346|lr = 0.00019\n",
      "Epoch: 2339|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7458|lr = 0.00019\n",
      "Epoch: 2339|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7439|lr = 0.00019\n",
      "Epoch: 2340|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7359|lr = 0.00019\n",
      "Epoch: 2340|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7226|lr = 0.00019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2341|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7465|lr = 0.00019\n",
      "Epoch: 2341|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7423|lr = 0.00019\n",
      "Epoch: 2342|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7821|lr = 0.00019\n",
      "Epoch: 2342|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7498|lr = 0.00019\n",
      "Epoch: 2343|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7356|lr = 0.00019\n",
      "Epoch: 2343|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7498|lr = 0.00019\n",
      "Epoch: 2344|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7538|lr = 0.00019\n",
      "Epoch: 2344|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7805|lr = 0.00019\n",
      "Epoch: 2345|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7152|lr = 0.00019\n",
      "Epoch: 2345|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7444|lr = 0.00019\n",
      "Epoch: 2346|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7718|lr = 0.00019\n",
      "Epoch: 2346|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7252|lr = 0.00019\n",
      "Epoch: 2347|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7513|lr = 0.00019\n",
      "Epoch: 2347|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7564|lr = 0.00019\n",
      "Epoch: 2348|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7157|lr = 0.00019\n",
      "Epoch: 2348|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7357|lr = 0.00019\n",
      "Epoch: 2349|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7509|lr = 0.00018\n",
      "Epoch: 2349|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7481|lr = 0.00018\n",
      "Epoch: 2350|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7436|lr = 0.00018\n",
      "Epoch: 2350|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7273|lr = 0.00018\n",
      "Epoch: 2351|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7069|lr = 0.00018\n",
      "Epoch: 2351|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7599|lr = 0.00018\n",
      "Epoch: 2352|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7434|lr = 0.00018\n",
      "Epoch: 2352|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7388|lr = 0.00018\n",
      "Epoch: 2353|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7262|lr = 0.00018\n",
      "Epoch: 2353|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7334|lr = 0.00018\n",
      "Epoch: 2354|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7472|lr = 0.00018\n",
      "Epoch: 2354|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7481|lr = 0.00018\n",
      "Epoch: 2355|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7299|lr = 0.00018\n",
      "Epoch: 2355|steps:   60|Train Avg Loss: 0.0052 |Test Loss: 1.7295|lr = 0.00018\n",
      "Epoch: 2356|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7269|lr = 0.00018\n",
      "Epoch: 2356|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7413|lr = 0.00018\n",
      "Epoch: 2357|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.6855|lr = 0.00018\n",
      "Epoch: 2357|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7628|lr = 0.00018\n",
      "Epoch: 2358|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7096|lr = 0.00018\n",
      "Epoch: 2358|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.6932|lr = 0.00018\n",
      "Epoch: 2359|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7437|lr = 0.00018\n",
      "Epoch: 2359|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7439|lr = 0.00018\n",
      "Epoch: 2360|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7461|lr = 0.00018\n",
      "Epoch: 2360|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7461|lr = 0.00018\n",
      "Epoch: 2361|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7316|lr = 0.00018\n",
      "Epoch: 2361|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7466|lr = 0.00018\n",
      "Epoch: 2362|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7268|lr = 0.00018\n",
      "Epoch: 2362|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7342|lr = 0.00018\n",
      "Epoch: 2363|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.6867|lr = 0.00018\n",
      "Epoch: 2363|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7315|lr = 0.00018\n",
      "Epoch: 2364|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7480|lr = 0.00018\n",
      "Epoch: 2364|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7570|lr = 0.00018\n",
      "Epoch: 2365|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7349|lr = 0.00018\n",
      "Epoch: 2365|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7226|lr = 0.00018\n",
      "Epoch: 2366|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7431|lr = 0.00018\n",
      "Epoch: 2366|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7289|lr = 0.00018\n",
      "Epoch: 2367|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7389|lr = 0.00018\n",
      "Epoch: 2367|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7479|lr = 0.00018\n",
      "Epoch: 2368|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7590|lr = 0.00018\n",
      "Epoch: 2368|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7491|lr = 0.00018\n",
      "Epoch: 2369|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7380|lr = 0.00018\n",
      "Epoch: 2369|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7625|lr = 0.00018\n",
      "Epoch: 2370|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7174|lr = 0.00018\n",
      "Epoch: 2370|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7321|lr = 0.00018\n",
      "Epoch: 2371|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7365|lr = 0.00018\n",
      "Epoch: 2371|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7356|lr = 0.00018\n",
      "Epoch: 2372|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7679|lr = 0.00018\n",
      "Epoch: 2372|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7253|lr = 0.00018\n",
      "Epoch: 2373|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7053|lr = 0.00018\n",
      "Epoch: 2373|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.6885|lr = 0.00018\n",
      "Epoch: 2374|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7377|lr = 0.00018\n",
      "Epoch: 2374|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7565|lr = 0.00018\n",
      "Epoch: 2375|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7018|lr = 0.00018\n",
      "Epoch: 2375|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7265|lr = 0.00018\n",
      "Epoch: 2376|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7766|lr = 0.00018\n",
      "Epoch: 2376|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7474|lr = 0.00018\n",
      "Epoch: 2377|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7307|lr = 0.00018\n",
      "Epoch: 2377|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7049|lr = 0.00018\n",
      "Epoch: 2378|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7316|lr = 0.00018\n",
      "Epoch: 2378|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7357|lr = 0.00018\n",
      "Epoch: 2379|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7250|lr = 0.00018\n",
      "Epoch: 2379|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7222|lr = 0.00018\n",
      "Epoch: 2380|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7282|lr = 0.00018\n",
      "Epoch: 2380|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7675|lr = 0.00018\n",
      "Epoch: 2381|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7265|lr = 0.00018\n",
      "Epoch: 2381|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7343|lr = 0.00018\n",
      "Epoch: 2382|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7382|lr = 0.00017\n",
      "Epoch: 2382|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7157|lr = 0.00017\n",
      "Epoch: 2383|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7359|lr = 0.00017\n",
      "Epoch: 2383|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7389|lr = 0.00017\n",
      "Epoch: 2384|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7385|lr = 0.00017\n",
      "Epoch: 2384|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7037|lr = 0.00017\n",
      "Epoch: 2385|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7136|lr = 0.00017\n",
      "Epoch: 2385|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7089|lr = 0.00017\n",
      "Epoch: 2386|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7219|lr = 0.00017\n",
      "Epoch: 2386|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7055|lr = 0.00017\n",
      "Epoch: 2387|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7515|lr = 0.00017\n",
      "Epoch: 2387|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7047|lr = 0.00017\n",
      "Epoch: 2388|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7124|lr = 0.00017\n",
      "Epoch: 2388|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7353|lr = 0.00017\n",
      "Epoch: 2389|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7364|lr = 0.00017\n",
      "Epoch: 2389|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7173|lr = 0.00017\n",
      "Epoch: 2390|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7186|lr = 0.00017\n",
      "Epoch: 2390|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.6842|lr = 0.00017\n",
      "Epoch: 2391|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7384|lr = 0.00017\n",
      "Epoch: 2391|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7346|lr = 0.00017\n",
      "Epoch: 2392|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7170|lr = 0.00017\n",
      "Epoch: 2392|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7552|lr = 0.00017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2393|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7344|lr = 0.00017\n",
      "Epoch: 2393|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7291|lr = 0.00017\n",
      "Epoch: 2394|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7143|lr = 0.00017\n",
      "Epoch: 2394|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7571|lr = 0.00017\n",
      "Epoch: 2395|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7207|lr = 0.00017\n",
      "Epoch: 2395|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7518|lr = 0.00017\n",
      "Epoch: 2396|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7052|lr = 0.00017\n",
      "Epoch: 2396|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7261|lr = 0.00017\n",
      "Epoch: 2397|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7558|lr = 0.00017\n",
      "Epoch: 2397|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7377|lr = 0.00017\n",
      "Epoch: 2398|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7536|lr = 0.00017\n",
      "Epoch: 2398|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7041|lr = 0.00017\n",
      "Epoch: 2399|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7455|lr = 0.00017\n",
      "Epoch: 2399|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7115|lr = 0.00017\n",
      "Epoch: 2400|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7498|lr = 0.00017\n",
      "Epoch: 2400|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7410|lr = 0.00017\n",
      "Epoch: 2401|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7638|lr = 0.00017\n",
      "Epoch: 2401|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7512|lr = 0.00017\n",
      "Epoch: 2402|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7280|lr = 0.00017\n",
      "Epoch: 2402|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7248|lr = 0.00017\n",
      "Epoch: 2403|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7419|lr = 0.00017\n",
      "Epoch: 2403|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7254|lr = 0.00017\n",
      "Epoch: 2404|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7123|lr = 0.00017\n",
      "Epoch: 2404|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7179|lr = 0.00017\n",
      "Epoch: 2405|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7285|lr = 0.00017\n",
      "Epoch: 2405|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7091|lr = 0.00017\n",
      "Epoch: 2406|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7583|lr = 0.00017\n",
      "Epoch: 2406|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7448|lr = 0.00017\n",
      "Epoch: 2407|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7661|lr = 0.00017\n",
      "Epoch: 2407|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7579|lr = 0.00017\n",
      "Epoch: 2408|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7180|lr = 0.00017\n",
      "Epoch: 2408|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7184|lr = 0.00017\n",
      "Epoch: 2409|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7602|lr = 0.00017\n",
      "Epoch: 2409|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7153|lr = 0.00017\n",
      "Epoch: 2410|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7233|lr = 0.00017\n",
      "Epoch: 2410|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7284|lr = 0.00017\n",
      "Epoch: 2411|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7222|lr = 0.00017\n",
      "Epoch: 2411|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.6905|lr = 0.00017\n",
      "Epoch: 2412|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7283|lr = 0.00017\n",
      "Epoch: 2412|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7419|lr = 0.00017\n",
      "Epoch: 2413|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7245|lr = 0.00017\n",
      "Epoch: 2413|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7422|lr = 0.00017\n",
      "Epoch: 2414|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7299|lr = 0.00017\n",
      "Epoch: 2414|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7311|lr = 0.00017\n",
      "Epoch: 2415|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7533|lr = 0.00016\n",
      "Epoch: 2415|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7588|lr = 0.00016\n",
      "Epoch: 2416|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7450|lr = 0.00016\n",
      "Epoch: 2416|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7169|lr = 0.00016\n",
      "Epoch: 2417|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7517|lr = 0.00016\n",
      "Epoch: 2417|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7617|lr = 0.00016\n",
      "Epoch: 2418|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7393|lr = 0.00016\n",
      "Epoch: 2418|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7470|lr = 0.00016\n",
      "Epoch: 2419|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7044|lr = 0.00016\n",
      "Epoch: 2419|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7254|lr = 0.00016\n",
      "Epoch: 2420|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7491|lr = 0.00016\n",
      "Epoch: 2420|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7344|lr = 0.00016\n",
      "Epoch: 2421|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7361|lr = 0.00016\n",
      "Epoch: 2421|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7370|lr = 0.00016\n",
      "Epoch: 2422|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7289|lr = 0.00016\n",
      "Epoch: 2422|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7637|lr = 0.00016\n",
      "Epoch: 2423|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7568|lr = 0.00016\n",
      "Epoch: 2423|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7500|lr = 0.00016\n",
      "Epoch: 2424|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7400|lr = 0.00016\n",
      "Epoch: 2424|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7277|lr = 0.00016\n",
      "Epoch: 2425|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7592|lr = 0.00016\n",
      "Epoch: 2425|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7350|lr = 0.00016\n",
      "Epoch: 2426|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7478|lr = 0.00016\n",
      "Epoch: 2426|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7549|lr = 0.00016\n",
      "Epoch: 2427|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7367|lr = 0.00016\n",
      "Epoch: 2427|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7195|lr = 0.00016\n",
      "Epoch: 2428|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.6975|lr = 0.00016\n",
      "Epoch: 2428|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7565|lr = 0.00016\n",
      "Epoch: 2429|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7486|lr = 0.00016\n",
      "Epoch: 2429|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7313|lr = 0.00016\n",
      "Epoch: 2430|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7671|lr = 0.00016\n",
      "Epoch: 2430|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7659|lr = 0.00016\n",
      "Epoch: 2431|steps:   30|Train Avg Loss: 0.0059 |Test Loss: 1.7427|lr = 0.00016\n",
      "Epoch: 2431|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7799|lr = 0.00016\n",
      "Epoch: 2432|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7367|lr = 0.00016\n",
      "Epoch: 2432|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7693|lr = 0.00016\n",
      "Epoch: 2433|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7519|lr = 0.00016\n",
      "Epoch: 2433|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7499|lr = 0.00016\n",
      "Epoch: 2434|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7101|lr = 0.00016\n",
      "Epoch: 2434|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7322|lr = 0.00016\n",
      "Epoch: 2435|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7485|lr = 0.00016\n",
      "Epoch: 2435|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7492|lr = 0.00016\n",
      "Epoch: 2436|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7704|lr = 0.00016\n",
      "Epoch: 2436|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7596|lr = 0.00016\n",
      "Epoch: 2437|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7419|lr = 0.00016\n",
      "Epoch: 2437|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7481|lr = 0.00016\n",
      "Epoch: 2438|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7477|lr = 0.00016\n",
      "Epoch: 2438|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7530|lr = 0.00016\n",
      "Epoch: 2439|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7651|lr = 0.00016\n",
      "Epoch: 2439|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7757|lr = 0.00016\n",
      "Epoch: 2440|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7499|lr = 0.00016\n",
      "Epoch: 2440|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7648|lr = 0.00016\n",
      "Epoch: 2441|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7564|lr = 0.00016\n",
      "Epoch: 2441|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7281|lr = 0.00016\n",
      "Epoch: 2442|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7653|lr = 0.00016\n",
      "Epoch: 2442|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7679|lr = 0.00016\n",
      "Epoch: 2443|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7438|lr = 0.00016\n",
      "Epoch: 2443|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7581|lr = 0.00016\n",
      "Epoch: 2444|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7218|lr = 0.00016\n",
      "Epoch: 2444|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7482|lr = 0.00016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2445|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7328|lr = 0.00016\n",
      "Epoch: 2445|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7489|lr = 0.00016\n",
      "Epoch: 2446|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7327|lr = 0.00016\n",
      "Epoch: 2446|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7275|lr = 0.00016\n",
      "Epoch: 2447|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7458|lr = 0.00016\n",
      "Epoch: 2447|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7476|lr = 0.00016\n",
      "Epoch: 2448|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7339|lr = 0.00015\n",
      "Epoch: 2448|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7517|lr = 0.00015\n",
      "Epoch: 2449|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7463|lr = 0.00015\n",
      "Epoch: 2449|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7683|lr = 0.00015\n",
      "Epoch: 2450|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7182|lr = 0.00015\n",
      "Epoch: 2450|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7270|lr = 0.00015\n",
      "Epoch: 2451|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7430|lr = 0.00015\n",
      "Epoch: 2451|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7270|lr = 0.00015\n",
      "Epoch: 2452|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7322|lr = 0.00015\n",
      "Epoch: 2452|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7220|lr = 0.00015\n",
      "Epoch: 2453|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7415|lr = 0.00015\n",
      "Epoch: 2453|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7212|lr = 0.00015\n",
      "Epoch: 2454|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7365|lr = 0.00015\n",
      "Epoch: 2454|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7258|lr = 0.00015\n",
      "Epoch: 2455|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7460|lr = 0.00015\n",
      "Epoch: 2455|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7429|lr = 0.00015\n",
      "Epoch: 2456|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7560|lr = 0.00015\n",
      "Epoch: 2456|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7520|lr = 0.00015\n",
      "Epoch: 2457|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7183|lr = 0.00015\n",
      "Epoch: 2457|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7604|lr = 0.00015\n",
      "Epoch: 2458|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7363|lr = 0.00015\n",
      "Epoch: 2458|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7427|lr = 0.00015\n",
      "Epoch: 2459|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7668|lr = 0.00015\n",
      "Epoch: 2459|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7379|lr = 0.00015\n",
      "Epoch: 2460|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7366|lr = 0.00015\n",
      "Epoch: 2460|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7201|lr = 0.00015\n",
      "Epoch: 2461|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7404|lr = 0.00015\n",
      "Epoch: 2461|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7382|lr = 0.00015\n",
      "Epoch: 2462|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7442|lr = 0.00015\n",
      "Epoch: 2462|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7251|lr = 0.00015\n",
      "Epoch: 2463|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7534|lr = 0.00015\n",
      "Epoch: 2463|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7157|lr = 0.00015\n",
      "Epoch: 2464|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7309|lr = 0.00015\n",
      "Epoch: 2464|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7395|lr = 0.00015\n",
      "Epoch: 2465|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7045|lr = 0.00015\n",
      "Epoch: 2465|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7225|lr = 0.00015\n",
      "Epoch: 2466|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7481|lr = 0.00015\n",
      "Epoch: 2466|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7406|lr = 0.00015\n",
      "Epoch: 2467|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7507|lr = 0.00015\n",
      "Epoch: 2467|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7822|lr = 0.00015\n",
      "Epoch: 2468|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7349|lr = 0.00015\n",
      "Epoch: 2468|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7369|lr = 0.00015\n",
      "Epoch: 2469|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7129|lr = 0.00015\n",
      "Epoch: 2469|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7300|lr = 0.00015\n",
      "Epoch: 2470|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7155|lr = 0.00015\n",
      "Epoch: 2470|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7161|lr = 0.00015\n",
      "Epoch: 2471|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7171|lr = 0.00015\n",
      "Epoch: 2471|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7283|lr = 0.00015\n",
      "Epoch: 2472|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7080|lr = 0.00015\n",
      "Epoch: 2472|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7127|lr = 0.00015\n",
      "Epoch: 2473|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7409|lr = 0.00015\n",
      "Epoch: 2473|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7220|lr = 0.00015\n",
      "Epoch: 2474|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7301|lr = 0.00015\n",
      "Epoch: 2474|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7094|lr = 0.00015\n",
      "Epoch: 2475|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7096|lr = 0.00015\n",
      "Epoch: 2475|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7334|lr = 0.00015\n",
      "Epoch: 2476|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.6957|lr = 0.00015\n",
      "Epoch: 2476|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7499|lr = 0.00015\n",
      "Epoch: 2477|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7321|lr = 0.00015\n",
      "Epoch: 2477|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7253|lr = 0.00015\n",
      "Epoch: 2478|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7597|lr = 0.00015\n",
      "Epoch: 2478|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7337|lr = 0.00015\n",
      "Epoch: 2479|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7676|lr = 0.00015\n",
      "Epoch: 2479|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7349|lr = 0.00015\n",
      "Epoch: 2480|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7775|lr = 0.00015\n",
      "Epoch: 2480|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7586|lr = 0.00015\n",
      "Epoch: 2481|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7530|lr = 0.00014\n",
      "Epoch: 2481|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7432|lr = 0.00014\n",
      "Epoch: 2482|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7529|lr = 0.00014\n",
      "Epoch: 2482|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7240|lr = 0.00014\n",
      "Epoch: 2483|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7516|lr = 0.00014\n",
      "Epoch: 2483|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7309|lr = 0.00014\n",
      "Epoch: 2484|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.6921|lr = 0.00014\n",
      "Epoch: 2484|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7587|lr = 0.00014\n",
      "Epoch: 2485|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7249|lr = 0.00014\n",
      "Epoch: 2485|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7312|lr = 0.00014\n",
      "Epoch: 2486|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7632|lr = 0.00014\n",
      "Epoch: 2486|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7456|lr = 0.00014\n",
      "Epoch: 2487|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7449|lr = 0.00014\n",
      "Epoch: 2487|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7347|lr = 0.00014\n",
      "Epoch: 2488|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7277|lr = 0.00014\n",
      "Epoch: 2488|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7150|lr = 0.00014\n",
      "Epoch: 2489|steps:   30|Train Avg Loss: 0.0051 |Test Loss: 1.7503|lr = 0.00014\n",
      "Epoch: 2489|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7340|lr = 0.00014\n",
      "Epoch: 2490|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7243|lr = 0.00014\n",
      "Epoch: 2490|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7475|lr = 0.00014\n",
      "Epoch: 2491|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7354|lr = 0.00014\n",
      "Epoch: 2491|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7426|lr = 0.00014\n",
      "Epoch: 2492|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7318|lr = 0.00014\n",
      "Epoch: 2492|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7380|lr = 0.00014\n",
      "Epoch: 2493|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7535|lr = 0.00014\n",
      "Epoch: 2493|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7545|lr = 0.00014\n",
      "Epoch: 2494|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7505|lr = 0.00014\n",
      "Epoch: 2494|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7491|lr = 0.00014\n",
      "Epoch: 2495|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7387|lr = 0.00014\n",
      "Epoch: 2495|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7290|lr = 0.00014\n",
      "Epoch: 2496|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7491|lr = 0.00014\n",
      "Epoch: 2496|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7788|lr = 0.00014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2497|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7422|lr = 0.00014\n",
      "Epoch: 2497|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7391|lr = 0.00014\n",
      "Epoch: 2498|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7247|lr = 0.00014\n",
      "Epoch: 2498|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7176|lr = 0.00014\n",
      "Epoch: 2499|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7316|lr = 0.00014\n",
      "Epoch: 2499|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7412|lr = 0.00014\n",
      "Epoch: 2500|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7231|lr = 0.00014\n",
      "Epoch: 2500|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7419|lr = 0.00014\n",
      "Epoch: 2501|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7547|lr = 0.00014\n",
      "Epoch: 2501|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7489|lr = 0.00014\n",
      "Epoch: 2502|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7384|lr = 0.00014\n",
      "Epoch: 2502|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7256|lr = 0.00014\n",
      "Epoch: 2503|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7581|lr = 0.00014\n",
      "Epoch: 2503|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7312|lr = 0.00014\n",
      "Epoch: 2504|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7011|lr = 0.00014\n",
      "Epoch: 2504|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7513|lr = 0.00014\n",
      "Epoch: 2505|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7208|lr = 0.00014\n",
      "Epoch: 2505|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7613|lr = 0.00014\n",
      "Epoch: 2506|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7480|lr = 0.00014\n",
      "Epoch: 2506|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7373|lr = 0.00014\n",
      "Epoch: 2507|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7617|lr = 0.00014\n",
      "Epoch: 2507|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7476|lr = 0.00014\n",
      "Epoch: 2508|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7779|lr = 0.00014\n",
      "Epoch: 2508|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7630|lr = 0.00014\n",
      "Epoch: 2509|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7732|lr = 0.00014\n",
      "Epoch: 2509|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7610|lr = 0.00014\n",
      "Epoch: 2510|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7451|lr = 0.00014\n",
      "Epoch: 2510|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7517|lr = 0.00014\n",
      "Epoch: 2511|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7437|lr = 0.00014\n",
      "Epoch: 2511|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7300|lr = 0.00014\n",
      "Epoch: 2512|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7393|lr = 0.00014\n",
      "Epoch: 2512|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7071|lr = 0.00014\n",
      "Epoch: 2513|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7222|lr = 0.00014\n",
      "Epoch: 2513|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7385|lr = 0.00014\n",
      "Epoch: 2514|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7590|lr = 0.00014\n",
      "Epoch: 2514|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7532|lr = 0.00014\n",
      "Epoch: 2515|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7414|lr = 0.00014\n",
      "Epoch: 2515|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7326|lr = 0.00014\n",
      "Epoch: 2516|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7268|lr = 0.00014\n",
      "Epoch: 2516|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7571|lr = 0.00014\n",
      "Epoch: 2517|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7070|lr = 0.00014\n",
      "Epoch: 2517|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7343|lr = 0.00014\n",
      "Epoch: 2518|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7451|lr = 0.00014\n",
      "Epoch: 2518|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.6989|lr = 0.00014\n",
      "Epoch: 2519|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7546|lr = 0.00014\n",
      "Epoch: 2519|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7240|lr = 0.00014\n",
      "Epoch: 2520|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7434|lr = 0.00014\n",
      "Epoch: 2520|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7678|lr = 0.00014\n",
      "Epoch: 2521|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7622|lr = 0.00014\n",
      "Epoch: 2521|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7510|lr = 0.00014\n",
      "Epoch: 2522|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7368|lr = 0.00014\n",
      "Epoch: 2522|steps:   60|Train Avg Loss: 0.0048 |Test Loss: 1.7117|lr = 0.00014\n",
      "Epoch: 2523|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7409|lr = 0.00014\n",
      "Epoch: 2523|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7421|lr = 0.00014\n",
      "Epoch: 2524|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7245|lr = 0.00014\n",
      "Epoch: 2524|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7460|lr = 0.00014\n",
      "Epoch: 2525|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7600|lr = 0.00013\n",
      "Epoch: 2525|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7499|lr = 0.00013\n",
      "Epoch: 2526|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7147|lr = 0.00013\n",
      "Epoch: 2526|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7350|lr = 0.00013\n",
      "Epoch: 2527|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7339|lr = 0.00013\n",
      "Epoch: 2527|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7456|lr = 0.00013\n",
      "Epoch: 2528|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7402|lr = 0.00013\n",
      "Epoch: 2528|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7212|lr = 0.00013\n",
      "Epoch: 2529|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7557|lr = 0.00013\n",
      "Epoch: 2529|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7426|lr = 0.00013\n",
      "Epoch: 2530|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7412|lr = 0.00013\n",
      "Epoch: 2530|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7314|lr = 0.00013\n",
      "Epoch: 2531|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7527|lr = 0.00013\n",
      "Epoch: 2531|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7541|lr = 0.00013\n",
      "Epoch: 2532|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7459|lr = 0.00013\n",
      "Epoch: 2532|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7526|lr = 0.00013\n",
      "Epoch: 2533|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7491|lr = 0.00013\n",
      "Epoch: 2533|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.6919|lr = 0.00013\n",
      "Epoch: 2534|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7888|lr = 0.00013\n",
      "Epoch: 2534|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7334|lr = 0.00013\n",
      "Epoch: 2535|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7736|lr = 0.00013\n",
      "Epoch: 2535|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7727|lr = 0.00013\n",
      "Epoch: 2536|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7328|lr = 0.00013\n",
      "Epoch: 2536|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7523|lr = 0.00013\n",
      "Epoch: 2537|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7330|lr = 0.00013\n",
      "Epoch: 2537|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7396|lr = 0.00013\n",
      "Epoch: 2538|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7336|lr = 0.00013\n",
      "Epoch: 2538|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7419|lr = 0.00013\n",
      "Epoch: 2539|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7399|lr = 0.00013\n",
      "Epoch: 2539|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.6997|lr = 0.00013\n",
      "Epoch: 2540|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7336|lr = 0.00013\n",
      "Epoch: 2540|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7031|lr = 0.00013\n",
      "Epoch: 2541|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7338|lr = 0.00013\n",
      "Epoch: 2541|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7162|lr = 0.00013\n",
      "Epoch: 2542|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7172|lr = 0.00013\n",
      "Epoch: 2542|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7223|lr = 0.00013\n",
      "Epoch: 2543|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7565|lr = 0.00013\n",
      "Epoch: 2543|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7108|lr = 0.00013\n",
      "Epoch: 2544|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7457|lr = 0.00013\n",
      "Epoch: 2544|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7387|lr = 0.00013\n",
      "Epoch: 2545|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7248|lr = 0.00013\n",
      "Epoch: 2545|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7412|lr = 0.00013\n",
      "Epoch: 2546|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7573|lr = 0.00013\n",
      "Epoch: 2546|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7277|lr = 0.00013\n",
      "Epoch: 2547|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7199|lr = 0.00013\n",
      "Epoch: 2547|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7651|lr = 0.00013\n",
      "Epoch: 2548|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7431|lr = 0.00013\n",
      "Epoch: 2548|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7543|lr = 0.00013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2549|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7778|lr = 0.00013\n",
      "Epoch: 2549|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7672|lr = 0.00013\n",
      "Epoch: 2550|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7582|lr = 0.00013\n",
      "Epoch: 2550|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7227|lr = 0.00013\n",
      "Epoch: 2551|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7496|lr = 0.00013\n",
      "Epoch: 2551|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7451|lr = 0.00013\n",
      "Epoch: 2552|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7424|lr = 0.00013\n",
      "Epoch: 2552|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7631|lr = 0.00013\n",
      "Epoch: 2553|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7681|lr = 0.00013\n",
      "Epoch: 2553|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7563|lr = 0.00013\n",
      "Epoch: 2554|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7584|lr = 0.00013\n",
      "Epoch: 2554|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7150|lr = 0.00013\n",
      "Epoch: 2555|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7442|lr = 0.00013\n",
      "Epoch: 2555|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7243|lr = 0.00013\n",
      "Epoch: 2556|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7501|lr = 0.00013\n",
      "Epoch: 2556|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7164|lr = 0.00013\n",
      "Epoch: 2557|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7876|lr = 0.00013\n",
      "Epoch: 2557|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7403|lr = 0.00013\n",
      "Epoch: 2558|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7664|lr = 0.00012\n",
      "Epoch: 2558|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7580|lr = 0.00012\n",
      "Epoch: 2559|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7243|lr = 0.00012\n",
      "Epoch: 2559|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7486|lr = 0.00012\n",
      "Epoch: 2560|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7591|lr = 0.00012\n",
      "Epoch: 2560|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7497|lr = 0.00012\n",
      "Epoch: 2561|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7341|lr = 0.00012\n",
      "Epoch: 2561|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7436|lr = 0.00012\n",
      "Epoch: 2562|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7396|lr = 0.00012\n",
      "Epoch: 2562|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7495|lr = 0.00012\n",
      "Epoch: 2563|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7304|lr = 0.00012\n",
      "Epoch: 2563|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7841|lr = 0.00012\n",
      "Epoch: 2564|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7230|lr = 0.00012\n",
      "Epoch: 2564|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7551|lr = 0.00012\n",
      "Epoch: 2565|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7438|lr = 0.00012\n",
      "Epoch: 2565|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7402|lr = 0.00012\n",
      "Epoch: 2566|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.6979|lr = 0.00012\n",
      "Epoch: 2566|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7469|lr = 0.00012\n",
      "Epoch: 2567|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7307|lr = 0.00012\n",
      "Epoch: 2567|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7462|lr = 0.00012\n",
      "Epoch: 2568|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7703|lr = 0.00012\n",
      "Epoch: 2568|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7699|lr = 0.00012\n",
      "Epoch: 2569|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7416|lr = 0.00012\n",
      "Epoch: 2569|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7804|lr = 0.00012\n",
      "Epoch: 2570|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7206|lr = 0.00012\n",
      "Epoch: 2570|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7440|lr = 0.00012\n",
      "Epoch: 2571|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7719|lr = 0.00012\n",
      "Epoch: 2571|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7377|lr = 0.00012\n",
      "Epoch: 2572|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7570|lr = 0.00012\n",
      "Epoch: 2572|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7389|lr = 0.00012\n",
      "Epoch: 2573|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7573|lr = 0.00012\n",
      "Epoch: 2573|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7367|lr = 0.00012\n",
      "Epoch: 2574|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7556|lr = 0.00012\n",
      "Epoch: 2574|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7647|lr = 0.00012\n",
      "Epoch: 2575|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7492|lr = 0.00012\n",
      "Epoch: 2575|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7471|lr = 0.00012\n",
      "Epoch: 2576|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7672|lr = 0.00012\n",
      "Epoch: 2576|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7585|lr = 0.00012\n",
      "Epoch: 2577|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7450|lr = 0.00012\n",
      "Epoch: 2577|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7562|lr = 0.00012\n",
      "Epoch: 2578|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7319|lr = 0.00012\n",
      "Epoch: 2578|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7538|lr = 0.00012\n",
      "Epoch: 2579|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7286|lr = 0.00012\n",
      "Epoch: 2579|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7147|lr = 0.00012\n",
      "Epoch: 2580|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7310|lr = 0.00012\n",
      "Epoch: 2580|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7416|lr = 0.00012\n",
      "Epoch: 2581|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7084|lr = 0.00012\n",
      "Epoch: 2581|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7286|lr = 0.00012\n",
      "Epoch: 2582|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7515|lr = 0.00012\n",
      "Epoch: 2582|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7604|lr = 0.00012\n",
      "Epoch: 2583|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7571|lr = 0.00012\n",
      "Epoch: 2583|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7735|lr = 0.00012\n",
      "Epoch: 2584|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7661|lr = 0.00012\n",
      "Epoch: 2584|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7121|lr = 0.00012\n",
      "Epoch: 2585|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7485|lr = 0.00012\n",
      "Epoch: 2585|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7512|lr = 0.00012\n",
      "Epoch: 2586|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7597|lr = 0.00012\n",
      "Epoch: 2586|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7388|lr = 0.00012\n",
      "Epoch: 2587|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7256|lr = 0.00012\n",
      "Epoch: 2587|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7631|lr = 0.00012\n",
      "Epoch: 2588|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7653|lr = 0.00012\n",
      "Epoch: 2588|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7812|lr = 0.00012\n",
      "Epoch: 2589|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7524|lr = 0.00012\n",
      "Epoch: 2589|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7685|lr = 0.00012\n",
      "Epoch: 2590|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7429|lr = 0.00012\n",
      "Epoch: 2590|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7889|lr = 0.00012\n",
      "Epoch: 2591|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7495|lr = 0.00012\n",
      "Epoch: 2591|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7525|lr = 0.00012\n",
      "Epoch: 2592|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7499|lr = 0.00012\n",
      "Epoch: 2592|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7189|lr = 0.00012\n",
      "Epoch: 2593|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7489|lr = 0.00012\n",
      "Epoch: 2593|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7516|lr = 0.00012\n",
      "Epoch: 2594|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7613|lr = 0.00012\n",
      "Epoch: 2594|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7404|lr = 0.00012\n",
      "Epoch: 2595|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7339|lr = 0.00012\n",
      "Epoch: 2595|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7522|lr = 0.00012\n",
      "Epoch: 2596|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7738|lr = 0.00012\n",
      "Epoch: 2596|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7447|lr = 0.00012\n",
      "Epoch: 2597|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7304|lr = 0.00012\n",
      "Epoch: 2597|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7187|lr = 0.00012\n",
      "Epoch: 2598|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7486|lr = 0.00012\n",
      "Epoch: 2598|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7744|lr = 0.00012\n",
      "Epoch: 2599|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7433|lr = 0.00012\n",
      "Epoch: 2599|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7446|lr = 0.00012\n",
      "Epoch: 2600|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7599|lr = 0.00012\n",
      "Epoch: 2600|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7568|lr = 0.00012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2601|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7554|lr = 0.00012\n",
      "Epoch: 2601|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7259|lr = 0.00012\n",
      "Epoch: 2602|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7542|lr = 0.00012\n",
      "Epoch: 2602|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7336|lr = 0.00012\n",
      "Epoch: 2603|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7393|lr = 0.00012\n",
      "Epoch: 2603|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7585|lr = 0.00012\n",
      "Epoch: 2604|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7543|lr = 0.00012\n",
      "Epoch: 2604|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7446|lr = 0.00012\n",
      "Epoch: 2605|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7426|lr = 0.00012\n",
      "Epoch: 2605|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7499|lr = 0.00012\n",
      "Epoch: 2606|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7675|lr = 0.00012\n",
      "Epoch: 2606|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7517|lr = 0.00012\n",
      "Epoch: 2607|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7441|lr = 0.00012\n",
      "Epoch: 2607|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7468|lr = 0.00012\n",
      "Epoch: 2608|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7433|lr = 0.00012\n",
      "Epoch: 2608|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7241|lr = 0.00012\n",
      "Epoch: 2609|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7670|lr = 0.00012\n",
      "Epoch: 2609|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7393|lr = 0.00012\n",
      "Epoch: 2610|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7788|lr = 0.00012\n",
      "Epoch: 2610|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7464|lr = 0.00012\n",
      "Epoch: 2611|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7763|lr = 0.00012\n",
      "Epoch: 2611|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7533|lr = 0.00012\n",
      "Epoch: 2612|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7171|lr = 0.00012\n",
      "Epoch: 2612|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7235|lr = 0.00012\n",
      "Epoch: 2613|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7527|lr = 0.00011\n",
      "Epoch: 2613|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7552|lr = 0.00011\n",
      "Epoch: 2614|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7642|lr = 0.00011\n",
      "Epoch: 2614|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7588|lr = 0.00011\n",
      "Epoch: 2615|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7517|lr = 0.00011\n",
      "Epoch: 2615|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7510|lr = 0.00011\n",
      "Epoch: 2616|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7449|lr = 0.00011\n",
      "Epoch: 2616|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7342|lr = 0.00011\n",
      "Epoch: 2617|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7484|lr = 0.00011\n",
      "Epoch: 2617|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7485|lr = 0.00011\n",
      "Epoch: 2618|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7793|lr = 0.00011\n",
      "Epoch: 2618|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7680|lr = 0.00011\n",
      "Epoch: 2619|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7344|lr = 0.00011\n",
      "Epoch: 2619|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7530|lr = 0.00011\n",
      "Epoch: 2620|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7290|lr = 0.00011\n",
      "Epoch: 2620|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7735|lr = 0.00011\n",
      "Epoch: 2621|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7347|lr = 0.00011\n",
      "Epoch: 2621|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7435|lr = 0.00011\n",
      "Epoch: 2622|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7556|lr = 0.00011\n",
      "Epoch: 2622|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7703|lr = 0.00011\n",
      "Epoch: 2623|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7652|lr = 0.00011\n",
      "Epoch: 2623|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7379|lr = 0.00011\n",
      "Epoch: 2624|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7610|lr = 0.00011\n",
      "Epoch: 2624|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.6992|lr = 0.00011\n",
      "Epoch: 2625|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7278|lr = 0.00011\n",
      "Epoch: 2625|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7564|lr = 0.00011\n",
      "Epoch: 2626|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7598|lr = 0.00011\n",
      "Epoch: 2626|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7537|lr = 0.00011\n",
      "Epoch: 2627|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7511|lr = 0.00011\n",
      "Epoch: 2627|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7223|lr = 0.00011\n",
      "Epoch: 2628|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7551|lr = 0.00011\n",
      "Epoch: 2628|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7523|lr = 0.00011\n",
      "Epoch: 2629|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7425|lr = 0.00011\n",
      "Epoch: 2629|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7508|lr = 0.00011\n",
      "Epoch: 2630|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7710|lr = 0.00011\n",
      "Epoch: 2630|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7413|lr = 0.00011\n",
      "Epoch: 2631|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7278|lr = 0.00011\n",
      "Epoch: 2631|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7486|lr = 0.00011\n",
      "Epoch: 2632|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7459|lr = 0.00011\n",
      "Epoch: 2632|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7477|lr = 0.00011\n",
      "Epoch: 2633|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7681|lr = 0.00011\n",
      "Epoch: 2633|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7632|lr = 0.00011\n",
      "Epoch: 2634|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7620|lr = 0.00011\n",
      "Epoch: 2634|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7570|lr = 0.00011\n",
      "Epoch: 2635|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7348|lr = 0.00011\n",
      "Epoch: 2635|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7775|lr = 0.00011\n",
      "Epoch: 2636|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7264|lr = 0.00011\n",
      "Epoch: 2636|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7478|lr = 0.00011\n",
      "Epoch: 2637|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7585|lr = 0.00011\n",
      "Epoch: 2637|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7527|lr = 0.00011\n",
      "Epoch: 2638|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7850|lr = 0.00011\n",
      "Epoch: 2638|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7472|lr = 0.00011\n",
      "Epoch: 2639|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7583|lr = 0.00011\n",
      "Epoch: 2639|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7450|lr = 0.00011\n",
      "Epoch: 2640|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7482|lr = 0.00011\n",
      "Epoch: 2640|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7547|lr = 0.00011\n",
      "Epoch: 2641|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7324|lr = 0.00011\n",
      "Epoch: 2641|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7449|lr = 0.00011\n",
      "Epoch: 2642|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7426|lr = 0.00011\n",
      "Epoch: 2642|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7409|lr = 0.00011\n",
      "Epoch: 2643|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7629|lr = 0.00011\n",
      "Epoch: 2643|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7537|lr = 0.00011\n",
      "Epoch: 2644|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7467|lr = 0.00011\n",
      "Epoch: 2644|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7568|lr = 0.00011\n",
      "Epoch: 2645|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7322|lr = 0.00011\n",
      "Epoch: 2645|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7552|lr = 0.00011\n",
      "Epoch: 2646|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7461|lr = 0.00011\n",
      "Epoch: 2646|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7445|lr = 0.00011\n",
      "Epoch: 2647|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7347|lr = 0.00011\n",
      "Epoch: 2647|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7335|lr = 0.00011\n",
      "Epoch: 2648|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7233|lr = 0.00011\n",
      "Epoch: 2648|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7458|lr = 0.00011\n",
      "Epoch: 2649|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7036|lr = 0.00011\n",
      "Epoch: 2649|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7355|lr = 0.00011\n",
      "Epoch: 2650|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7549|lr = 0.00011\n",
      "Epoch: 2650|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7284|lr = 0.00011\n",
      "Epoch: 2651|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7642|lr = 0.00011\n",
      "Epoch: 2651|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7682|lr = 0.00011\n",
      "Epoch: 2652|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7560|lr = 0.00011\n",
      "Epoch: 2652|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7553|lr = 0.00011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2653|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7118|lr = 0.00011\n",
      "Epoch: 2653|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7516|lr = 0.00011\n",
      "Epoch: 2654|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7111|lr = 0.00011\n",
      "Epoch: 2654|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7147|lr = 0.00011\n",
      "Epoch: 2655|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7432|lr = 0.00011\n",
      "Epoch: 2655|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7271|lr = 0.00011\n",
      "Epoch: 2656|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7535|lr = 0.00011\n",
      "Epoch: 2656|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7827|lr = 0.00011\n",
      "Epoch: 2657|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 2657|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7242|lr = 0.00010\n",
      "Epoch: 2658|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 2658|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 2659|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 2659|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 2660|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 2660|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 2661|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 2661|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 2662|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7245|lr = 0.00010\n",
      "Epoch: 2662|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7266|lr = 0.00010\n",
      "Epoch: 2663|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7157|lr = 0.00010\n",
      "Epoch: 2663|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7215|lr = 0.00010\n",
      "Epoch: 2664|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 2664|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7356|lr = 0.00010\n",
      "Epoch: 2665|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 2665|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 2666|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 2666|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7222|lr = 0.00010\n",
      "Epoch: 2667|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7357|lr = 0.00010\n",
      "Epoch: 2667|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7339|lr = 0.00010\n",
      "Epoch: 2668|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7300|lr = 0.00010\n",
      "Epoch: 2668|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 2669|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 2669|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 2670|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 2670|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7181|lr = 0.00010\n",
      "Epoch: 2671|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7081|lr = 0.00010\n",
      "Epoch: 2671|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7247|lr = 0.00010\n",
      "Epoch: 2672|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 2672|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7137|lr = 0.00010\n",
      "Epoch: 2673|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7364|lr = 0.00010\n",
      "Epoch: 2673|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7364|lr = 0.00010\n",
      "Epoch: 2674|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 2674|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 2675|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7318|lr = 0.00010\n",
      "Epoch: 2675|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 2676|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 2676|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7372|lr = 0.00010\n",
      "Epoch: 2677|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7370|lr = 0.00010\n",
      "Epoch: 2677|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 2678|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 2678|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 2679|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7331|lr = 0.00010\n",
      "Epoch: 2679|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7406|lr = 0.00010\n",
      "Epoch: 2680|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 2680|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7249|lr = 0.00010\n",
      "Epoch: 2681|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 2681|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7350|lr = 0.00010\n",
      "Epoch: 2682|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 2682|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 2683|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 2683|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 2684|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7352|lr = 0.00010\n",
      "Epoch: 2684|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 2685|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7365|lr = 0.00010\n",
      "Epoch: 2685|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 2686|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 2686|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 2687|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 2687|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 2688|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 2688|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 2689|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 2689|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7176|lr = 0.00010\n",
      "Epoch: 2690|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 2690|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7235|lr = 0.00010\n",
      "Epoch: 2691|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7283|lr = 0.00010\n",
      "Epoch: 2691|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 2692|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7340|lr = 0.00010\n",
      "Epoch: 2692|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7350|lr = 0.00010\n",
      "Epoch: 2693|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 2693|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 2694|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7454|lr = 0.00010\n",
      "Epoch: 2694|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 2695|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 2695|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 2696|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7416|lr = 0.00010\n",
      "Epoch: 2696|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7236|lr = 0.00010\n",
      "Epoch: 2697|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 2697|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 2698|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7374|lr = 0.00010\n",
      "Epoch: 2698|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7377|lr = 0.00010\n",
      "Epoch: 2699|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 2699|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 2700|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 2700|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 2701|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 2701|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 2702|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 2702|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7315|lr = 0.00010\n",
      "Epoch: 2703|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 2703|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7023|lr = 0.00010\n",
      "Epoch: 2704|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7346|lr = 0.00010\n",
      "Epoch: 2704|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7280|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2705|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 2705|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 2706|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 2706|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 2707|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7225|lr = 0.00010\n",
      "Epoch: 2707|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 2708|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 2708|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7371|lr = 0.00010\n",
      "Epoch: 2709|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 2709|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 2710|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7333|lr = 0.00010\n",
      "Epoch: 2710|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 2711|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 2711|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7342|lr = 0.00010\n",
      "Epoch: 2712|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 2712|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7292|lr = 0.00010\n",
      "Epoch: 2713|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 2713|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7290|lr = 0.00010\n",
      "Epoch: 2714|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7215|lr = 0.00010\n",
      "Epoch: 2714|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 2715|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 2715|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7243|lr = 0.00010\n",
      "Epoch: 2716|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 2716|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 2717|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7359|lr = 0.00010\n",
      "Epoch: 2717|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 2718|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 2718|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7324|lr = 0.00010\n",
      "Epoch: 2719|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7286|lr = 0.00010\n",
      "Epoch: 2719|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 2720|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 2720|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 2721|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7335|lr = 0.00010\n",
      "Epoch: 2721|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7307|lr = 0.00010\n",
      "Epoch: 2722|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 2722|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7454|lr = 0.00010\n",
      "Epoch: 2723|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7452|lr = 0.00010\n",
      "Epoch: 2723|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 2724|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 2724|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 2725|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7451|lr = 0.00010\n",
      "Epoch: 2725|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7355|lr = 0.00010\n",
      "Epoch: 2726|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 2726|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7308|lr = 0.00010\n",
      "Epoch: 2727|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 2727|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7205|lr = 0.00010\n",
      "Epoch: 2728|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 2728|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 2729|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7411|lr = 0.00010\n",
      "Epoch: 2729|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 2730|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7305|lr = 0.00010\n",
      "Epoch: 2730|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 2731|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 2731|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 2732|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 2732|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 2733|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7416|lr = 0.00010\n",
      "Epoch: 2733|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 2734|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 2734|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 2735|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 2735|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 2736|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7311|lr = 0.00010\n",
      "Epoch: 2736|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 2737|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 2737|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 2738|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 2738|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7132|lr = 0.00010\n",
      "Epoch: 2739|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 2739|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7349|lr = 0.00010\n",
      "Epoch: 2740|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 2740|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 2741|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7194|lr = 0.00010\n",
      "Epoch: 2741|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7343|lr = 0.00010\n",
      "Epoch: 2742|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 2742|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 2743|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 2743|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 2744|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7317|lr = 0.00010\n",
      "Epoch: 2744|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 2745|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7281|lr = 0.00010\n",
      "Epoch: 2745|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7257|lr = 0.00010\n",
      "Epoch: 2746|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7256|lr = 0.00010\n",
      "Epoch: 2746|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7416|lr = 0.00010\n",
      "Epoch: 2747|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 2747|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 2748|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 2748|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7358|lr = 0.00010\n",
      "Epoch: 2749|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7383|lr = 0.00010\n",
      "Epoch: 2749|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 2750|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7163|lr = 0.00010\n",
      "Epoch: 2750|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7384|lr = 0.00010\n",
      "Epoch: 2751|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 2751|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7350|lr = 0.00010\n",
      "Epoch: 2752|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 2752|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 2753|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 2753|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 2754|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 2754|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 2755|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7289|lr = 0.00010\n",
      "Epoch: 2755|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7246|lr = 0.00010\n",
      "Epoch: 2756|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 2756|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7491|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2757|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7396|lr = 0.00010\n",
      "Epoch: 2757|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 2758|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7340|lr = 0.00010\n",
      "Epoch: 2758|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 2759|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7226|lr = 0.00010\n",
      "Epoch: 2759|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 2760|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7234|lr = 0.00010\n",
      "Epoch: 2760|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 2761|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7434|lr = 0.00010\n",
      "Epoch: 2761|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 2762|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 2762|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 2763|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 2763|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 2764|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7426|lr = 0.00010\n",
      "Epoch: 2764|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 2765|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7179|lr = 0.00010\n",
      "Epoch: 2765|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 2766|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 2766|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7351|lr = 0.00010\n",
      "Epoch: 2767|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 2767|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 2768|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7467|lr = 0.00010\n",
      "Epoch: 2768|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7374|lr = 0.00010\n",
      "Epoch: 2769|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 2769|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 2770|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 2770|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7251|lr = 0.00010\n",
      "Epoch: 2771|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7284|lr = 0.00010\n",
      "Epoch: 2771|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 2772|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7351|lr = 0.00010\n",
      "Epoch: 2772|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 2773|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 2773|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 2774|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7374|lr = 0.00010\n",
      "Epoch: 2774|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 2775|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 2775|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 2776|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 2776|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 2777|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 2777|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 2778|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 2778|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 2779|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 2779|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 2780|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 2780|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7343|lr = 0.00010\n",
      "Epoch: 2781|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 2781|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7311|lr = 0.00010\n",
      "Epoch: 2782|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7453|lr = 0.00010\n",
      "Epoch: 2782|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7293|lr = 0.00010\n",
      "Epoch: 2783|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 2783|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7406|lr = 0.00010\n",
      "Epoch: 2784|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 2784|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 2785|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 2785|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 2786|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 2786|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 2787|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 2787|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 2788|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 2788|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7384|lr = 0.00010\n",
      "Epoch: 2789|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7348|lr = 0.00010\n",
      "Epoch: 2789|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 2790|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7363|lr = 0.00010\n",
      "Epoch: 2790|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 2791|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7124|lr = 0.00010\n",
      "Epoch: 2791|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 2792|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7361|lr = 0.00010\n",
      "Epoch: 2792|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 2793|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7216|lr = 0.00010\n",
      "Epoch: 2793|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 2794|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7289|lr = 0.00010\n",
      "Epoch: 2794|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 2795|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 2795|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7323|lr = 0.00010\n",
      "Epoch: 2796|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7298|lr = 0.00010\n",
      "Epoch: 2796|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7165|lr = 0.00010\n",
      "Epoch: 2797|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 2797|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7307|lr = 0.00010\n",
      "Epoch: 2798|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 2798|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 2799|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 2799|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7198|lr = 0.00010\n",
      "Epoch: 2800|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 2800|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 2801|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7366|lr = 0.00010\n",
      "Epoch: 2801|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7190|lr = 0.00010\n",
      "Epoch: 2802|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 2802|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 2803|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 2803|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7291|lr = 0.00010\n",
      "Epoch: 2804|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 2804|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 2805|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 2805|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 2806|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 2806|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 2807|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7363|lr = 0.00010\n",
      "Epoch: 2807|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 2808|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 2808|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7614|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2809|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7432|lr = 0.00010\n",
      "Epoch: 2809|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 2810|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 2810|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 2811|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 2811|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 2812|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 2812|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 2813|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 2813|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7222|lr = 0.00010\n",
      "Epoch: 2814|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 2814|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 2815|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 2815|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 2816|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 2816|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7298|lr = 0.00010\n",
      "Epoch: 2817|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 2817|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 2818|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 2818|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 2819|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 2819|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 2820|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7309|lr = 0.00010\n",
      "Epoch: 2820|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 2821|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 2821|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7334|lr = 0.00010\n",
      "Epoch: 2822|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 2822|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7301|lr = 0.00010\n",
      "Epoch: 2823|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7340|lr = 0.00010\n",
      "Epoch: 2823|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 2824|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 2824|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7236|lr = 0.00010\n",
      "Epoch: 2825|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7290|lr = 0.00010\n",
      "Epoch: 2825|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 2826|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7355|lr = 0.00010\n",
      "Epoch: 2826|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 2827|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7417|lr = 0.00010\n",
      "Epoch: 2827|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7362|lr = 0.00010\n",
      "Epoch: 2828|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 2828|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 2829|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 2829|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 2830|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7347|lr = 0.00010\n",
      "Epoch: 2830|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 2831|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 2831|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 2832|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7319|lr = 0.00010\n",
      "Epoch: 2832|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7098|lr = 0.00010\n",
      "Epoch: 2833|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 2833|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 2834|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 2834|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 2835|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 2835|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 2836|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 2836|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 2837|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 2837|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 2838|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7267|lr = 0.00010\n",
      "Epoch: 2838|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 2839|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 2839|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 2840|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 2840|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7341|lr = 0.00010\n",
      "Epoch: 2841|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 2841|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7225|lr = 0.00010\n",
      "Epoch: 2842|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 2842|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 2843|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 2843|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 2844|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 2844|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 2845|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 2845|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 2846|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7958|lr = 0.00010\n",
      "Epoch: 2846|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 2847|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 2847|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 2848|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7407|lr = 0.00010\n",
      "Epoch: 2848|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 2849|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7175|lr = 0.00010\n",
      "Epoch: 2849|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 2850|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7449|lr = 0.00010\n",
      "Epoch: 2850|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 2851|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 2851|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 2852|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7264|lr = 0.00010\n",
      "Epoch: 2852|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 2853|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 2853|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 2854|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 2854|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 2855|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7453|lr = 0.00010\n",
      "Epoch: 2855|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 2856|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7402|lr = 0.00010\n",
      "Epoch: 2856|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 2857|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7384|lr = 0.00010\n",
      "Epoch: 2857|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 2858|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 2858|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 2859|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 2859|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 2860|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 2860|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7503|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2861|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 2861|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7334|lr = 0.00010\n",
      "Epoch: 2862|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7406|lr = 0.00010\n",
      "Epoch: 2862|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 2863|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7339|lr = 0.00010\n",
      "Epoch: 2863|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 2864|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7468|lr = 0.00010\n",
      "Epoch: 2864|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7388|lr = 0.00010\n",
      "Epoch: 2865|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7351|lr = 0.00010\n",
      "Epoch: 2865|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7473|lr = 0.00010\n",
      "Epoch: 2866|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 2866|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 2867|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 2867|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 2868|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7126|lr = 0.00010\n",
      "Epoch: 2868|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 2869|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7314|lr = 0.00010\n",
      "Epoch: 2869|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 2870|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7222|lr = 0.00010\n",
      "Epoch: 2870|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 2871|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 2871|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 2872|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 2872|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7426|lr = 0.00010\n",
      "Epoch: 2873|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 2873|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 2874|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 2874|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 2875|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 2875|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 2876|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 2876|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7235|lr = 0.00010\n",
      "Epoch: 2877|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 2877|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 2878|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 2878|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 2879|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7224|lr = 0.00010\n",
      "Epoch: 2879|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7519|lr = 0.00010\n",
      "Epoch: 2880|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 2880|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 2881|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 2881|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 2882|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 2882|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 2883|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7372|lr = 0.00010\n",
      "Epoch: 2883|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 2884|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 2884|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 2885|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 2885|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 2886|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 2886|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7467|lr = 0.00010\n",
      "Epoch: 2887|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 2887|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 2888|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 2888|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 2889|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 2889|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7406|lr = 0.00010\n",
      "Epoch: 2890|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 2890|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 2891|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 2891|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7451|lr = 0.00010\n",
      "Epoch: 2892|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 2892|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7293|lr = 0.00010\n",
      "Epoch: 2893|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 2893|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 2894|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 2894|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7075|lr = 0.00010\n",
      "Epoch: 2895|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7335|lr = 0.00010\n",
      "Epoch: 2895|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7306|lr = 0.00010\n",
      "Epoch: 2896|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 2896|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 2897|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7361|lr = 0.00010\n",
      "Epoch: 2897|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 2898|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 2898|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 2899|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7259|lr = 0.00010\n",
      "Epoch: 2899|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 2900|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 2900|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 2901|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7263|lr = 0.00010\n",
      "Epoch: 2901|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 2902|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 2902|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 2903|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 2903|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7378|lr = 0.00010\n",
      "Epoch: 2904|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 2904|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 2905|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 2905|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 2906|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 2906|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7358|lr = 0.00010\n",
      "Epoch: 2907|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 2907|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 2908|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 2908|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 2909|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 2909|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 2910|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 2910|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 2911|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 2911|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 2912|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 2912|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7197|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2913|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 2913|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 2914|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 2914|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 2915|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 2915|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 2916|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 2916|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 2917|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 2917|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7474|lr = 0.00010\n",
      "Epoch: 2918|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 2918|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 2919|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 2919|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 2920|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 2920|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7328|lr = 0.00010\n",
      "Epoch: 2921|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 2921|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 2922|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 2922|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 2923|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7392|lr = 0.00010\n",
      "Epoch: 2923|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 2924|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 2924|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 2925|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 2925|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 2926|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 2926|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 2927|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 2927|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 2928|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7474|lr = 0.00010\n",
      "Epoch: 2928|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 2929|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 2929|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 2930|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 2930|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 2931|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 2931|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7171|lr = 0.00010\n",
      "Epoch: 2932|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 2932|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 2933|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 2933|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 2934|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 2934|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7440|lr = 0.00010\n",
      "Epoch: 2935|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 2935|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7353|lr = 0.00010\n",
      "Epoch: 2936|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 2936|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 2937|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 2937|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 2938|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 2938|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7468|lr = 0.00010\n",
      "Epoch: 2939|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 2939|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7467|lr = 0.00010\n",
      "Epoch: 2940|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 2940|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 2941|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 2941|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 2942|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7399|lr = 0.00010\n",
      "Epoch: 2942|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7406|lr = 0.00010\n",
      "Epoch: 2943|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 2943|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7352|lr = 0.00010\n",
      "Epoch: 2944|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 2944|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 2945|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 2945|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 2946|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 2946|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 2947|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 2947|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 2948|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 2948|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7256|lr = 0.00010\n",
      "Epoch: 2949|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 2949|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 2950|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 2950|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 2951|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 2951|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 2952|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 2952|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7411|lr = 0.00010\n",
      "Epoch: 2953|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 2953|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 2954|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 2954|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 2955|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7250|lr = 0.00010\n",
      "Epoch: 2955|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 2956|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7352|lr = 0.00010\n",
      "Epoch: 2956|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7273|lr = 0.00010\n",
      "Epoch: 2957|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 2957|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7338|lr = 0.00010\n",
      "Epoch: 2958|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 2958|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 2959|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 2959|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 2960|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 2960|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 2961|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 2961|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 2962|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7080|lr = 0.00010\n",
      "Epoch: 2962|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 2963|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7166|lr = 0.00010\n",
      "Epoch: 2963|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7380|lr = 0.00010\n",
      "Epoch: 2964|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 2964|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7614|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2965|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 2965|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7467|lr = 0.00010\n",
      "Epoch: 2966|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7451|lr = 0.00010\n",
      "Epoch: 2966|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 2967|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 2967|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 2968|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 2968|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7199|lr = 0.00010\n",
      "Epoch: 2969|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 2969|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 2970|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 2970|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 2971|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 2971|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 2972|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 2972|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 2973|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 2973|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 2974|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7297|lr = 0.00010\n",
      "Epoch: 2974|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 2975|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 2975|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 2976|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 2976|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 2977|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 2977|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 2978|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7232|lr = 0.00010\n",
      "Epoch: 2978|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 2979|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 2979|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 2980|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7029|lr = 0.00010\n",
      "Epoch: 2980|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 2981|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 2981|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 2982|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7395|lr = 0.00010\n",
      "Epoch: 2982|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7267|lr = 0.00010\n",
      "Epoch: 2983|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7342|lr = 0.00010\n",
      "Epoch: 2983|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 2984|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 2984|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7352|lr = 0.00010\n",
      "Epoch: 2985|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7263|lr = 0.00010\n",
      "Epoch: 2985|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 2986|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7223|lr = 0.00010\n",
      "Epoch: 2986|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 2987|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7364|lr = 0.00010\n",
      "Epoch: 2987|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 2988|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 2988|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 2989|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 2989|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 2990|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7342|lr = 0.00010\n",
      "Epoch: 2990|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 2991|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 2991|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 2992|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 2992|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 2993|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 2993|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 2994|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 2994|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 2995|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 2995|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 2996|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 2996|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 2997|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 2997|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7417|lr = 0.00010\n",
      "Epoch: 2998|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 2998|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7452|lr = 0.00010\n",
      "Epoch: 2999|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 2999|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7348|lr = 0.00010\n",
      "Epoch: 3000|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 3000|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 3001|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 3001|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7297|lr = 0.00010\n",
      "Epoch: 3002|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 3002|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3003|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7273|lr = 0.00010\n",
      "Epoch: 3003|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7268|lr = 0.00010\n",
      "Epoch: 3004|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7219|lr = 0.00010\n",
      "Epoch: 3004|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 3005|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 3005|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7172|lr = 0.00010\n",
      "Epoch: 3006|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 3006|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3007|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7379|lr = 0.00010\n",
      "Epoch: 3007|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7400|lr = 0.00010\n",
      "Epoch: 3008|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7429|lr = 0.00010\n",
      "Epoch: 3008|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7467|lr = 0.00010\n",
      "Epoch: 3009|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7350|lr = 0.00010\n",
      "Epoch: 3009|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 3010|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 3010|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 3011|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3011|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 3012|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 3012|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7245|lr = 0.00010\n",
      "Epoch: 3013|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3013|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7211|lr = 0.00010\n",
      "Epoch: 3014|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 3014|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7283|lr = 0.00010\n",
      "Epoch: 3015|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7324|lr = 0.00010\n",
      "Epoch: 3015|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 3016|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 3016|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7616|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3017|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 3017|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 3018|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7278|lr = 0.00010\n",
      "Epoch: 3018|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7303|lr = 0.00010\n",
      "Epoch: 3019|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 3019|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7173|lr = 0.00010\n",
      "Epoch: 3020|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7400|lr = 0.00010\n",
      "Epoch: 3020|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7361|lr = 0.00010\n",
      "Epoch: 3021|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 3021|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 3022|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7283|lr = 0.00010\n",
      "Epoch: 3022|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7279|lr = 0.00010\n",
      "Epoch: 3023|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 3023|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7362|lr = 0.00010\n",
      "Epoch: 3024|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 3024|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 3025|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3025|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 3026|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7402|lr = 0.00010\n",
      "Epoch: 3026|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 3027|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 3027|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7245|lr = 0.00010\n",
      "Epoch: 3028|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 3028|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 3029|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7264|lr = 0.00010\n",
      "Epoch: 3029|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 3030|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3030|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 3031|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3031|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7285|lr = 0.00010\n",
      "Epoch: 3032|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 3032|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 3033|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7338|lr = 0.00010\n",
      "Epoch: 3033|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 3034|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 3034|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7183|lr = 0.00010\n",
      "Epoch: 3035|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 3035|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 3036|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 3036|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 3037|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7321|lr = 0.00010\n",
      "Epoch: 3037|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7377|lr = 0.00010\n",
      "Epoch: 3038|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7381|lr = 0.00010\n",
      "Epoch: 3038|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7279|lr = 0.00010\n",
      "Epoch: 3039|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 3039|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7297|lr = 0.00010\n",
      "Epoch: 3040|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3040|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3041|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 3041|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 3042|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 3042|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 3043|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7383|lr = 0.00010\n",
      "Epoch: 3043|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7345|lr = 0.00010\n",
      "Epoch: 3044|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 3044|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 3045|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3045|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 3046|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3046|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 3047|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7309|lr = 0.00010\n",
      "Epoch: 3047|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7095|lr = 0.00010\n",
      "Epoch: 3048|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 3048|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 3049|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3049|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7186|lr = 0.00010\n",
      "Epoch: 3050|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 3050|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7378|lr = 0.00010\n",
      "Epoch: 3051|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 3051|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7355|lr = 0.00010\n",
      "Epoch: 3052|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7267|lr = 0.00010\n",
      "Epoch: 3052|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 3053|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 3053|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7193|lr = 0.00010\n",
      "Epoch: 3054|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 3054|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7350|lr = 0.00010\n",
      "Epoch: 3055|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 3055|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7194|lr = 0.00010\n",
      "Epoch: 3056|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3056|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 3057|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 3057|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7269|lr = 0.00010\n",
      "Epoch: 3058|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7098|lr = 0.00010\n",
      "Epoch: 3058|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 3059|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7343|lr = 0.00010\n",
      "Epoch: 3059|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 3060|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 3060|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 3061|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3061|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 3062|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 3062|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3063|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7360|lr = 0.00010\n",
      "Epoch: 3063|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 3064|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7188|lr = 0.00010\n",
      "Epoch: 3064|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 3065|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7285|lr = 0.00010\n",
      "Epoch: 3065|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7381|lr = 0.00010\n",
      "Epoch: 3066|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 3066|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 3067|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 3067|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 3068|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3068|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7401|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3069|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 3069|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 3070|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7292|lr = 0.00010\n",
      "Epoch: 3070|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 3071|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7253|lr = 0.00010\n",
      "Epoch: 3071|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7323|lr = 0.00010\n",
      "Epoch: 3072|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7294|lr = 0.00010\n",
      "Epoch: 3072|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 3073|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3073|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 3074|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7177|lr = 0.00010\n",
      "Epoch: 3074|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7199|lr = 0.00010\n",
      "Epoch: 3075|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7440|lr = 0.00010\n",
      "Epoch: 3075|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 3076|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 3076|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 3077|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3077|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 3078|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7143|lr = 0.00010\n",
      "Epoch: 3078|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 3079|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 3079|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 3080|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 3080|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7392|lr = 0.00010\n",
      "Epoch: 3081|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 3081|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 3082|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 3082|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 3083|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 3083|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7186|lr = 0.00010\n",
      "Epoch: 3084|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3084|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3085|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 3085|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 3086|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7369|lr = 0.00010\n",
      "Epoch: 3086|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 3087|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7409|lr = 0.00010\n",
      "Epoch: 3087|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 3088|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7243|lr = 0.00010\n",
      "Epoch: 3088|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 3089|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 3089|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 3090|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7352|lr = 0.00010\n",
      "Epoch: 3090|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7180|lr = 0.00010\n",
      "Epoch: 3091|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 3091|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 3092|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3092|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 3093|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 3093|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 3094|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 3094|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7259|lr = 0.00010\n",
      "Epoch: 3095|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7345|lr = 0.00010\n",
      "Epoch: 3095|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7357|lr = 0.00010\n",
      "Epoch: 3096|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3096|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7273|lr = 0.00010\n",
      "Epoch: 3097|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7349|lr = 0.00010\n",
      "Epoch: 3097|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 3098|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7404|lr = 0.00010\n",
      "Epoch: 3098|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 3099|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3099|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7388|lr = 0.00010\n",
      "Epoch: 3100|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 3100|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 3101|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 3101|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3102|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3102|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 3103|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7432|lr = 0.00010\n",
      "Epoch: 3103|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3104|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 3104|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3105|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 3105|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 3106|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 3106|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 3107|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 3107|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 3108|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 3108|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3109|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7362|lr = 0.00010\n",
      "Epoch: 3109|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 3110|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3110|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3111|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 3111|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7346|lr = 0.00010\n",
      "Epoch: 3112|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 3112|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3113|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 3113|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 3114|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 3114|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7351|lr = 0.00010\n",
      "Epoch: 3115|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 3115|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 3116|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 3116|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7268|lr = 0.00010\n",
      "Epoch: 3117|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 3117|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7084|lr = 0.00010\n",
      "Epoch: 3118|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3118|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7410|lr = 0.00010\n",
      "Epoch: 3119|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7324|lr = 0.00010\n",
      "Epoch: 3119|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 3120|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7068|lr = 0.00010\n",
      "Epoch: 3120|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7242|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3121|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 3121|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 3122|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 3122|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7196|lr = 0.00010\n",
      "Epoch: 3123|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 3123|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7395|lr = 0.00010\n",
      "Epoch: 3124|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 3124|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 3125|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 3125|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 3126|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3126|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 3127|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 3127|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7350|lr = 0.00010\n",
      "Epoch: 3128|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 3128|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7300|lr = 0.00010\n",
      "Epoch: 3129|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 3129|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 3130|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 3130|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3131|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7261|lr = 0.00010\n",
      "Epoch: 3131|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7222|lr = 0.00010\n",
      "Epoch: 3132|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 3132|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 3133|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3133|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7311|lr = 0.00010\n",
      "Epoch: 3134|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3134|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 3135|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 3135|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 3136|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 3136|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 3137|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 3137|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 3138|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 3138|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7473|lr = 0.00010\n",
      "Epoch: 3139|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3139|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 3140|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 3140|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3141|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 3141|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 3142|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7374|lr = 0.00010\n",
      "Epoch: 3142|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 3143|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7288|lr = 0.00010\n",
      "Epoch: 3143|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7027|lr = 0.00010\n",
      "Epoch: 3144|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 3144|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7258|lr = 0.00010\n",
      "Epoch: 3145|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 3145|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3146|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7392|lr = 0.00010\n",
      "Epoch: 3146|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 3147|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7194|lr = 0.00010\n",
      "Epoch: 3147|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3148|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7072|lr = 0.00010\n",
      "Epoch: 3148|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3149|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 3149|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 3150|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7361|lr = 0.00010\n",
      "Epoch: 3150|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 3151|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 3151|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3152|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 3152|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 3153|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 3153|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 3154|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 3154|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3155|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3155|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 3156|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 3156|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 3157|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 3157|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 3158|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 3158|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 3159|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 3159|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7399|lr = 0.00010\n",
      "Epoch: 3160|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7380|lr = 0.00010\n",
      "Epoch: 3160|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 3161|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 3161|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7409|lr = 0.00010\n",
      "Epoch: 3162|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 3162|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 3163|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 3163|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7283|lr = 0.00010\n",
      "Epoch: 3164|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 3164|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 3165|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 3165|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 3166|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 3166|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3167|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 3167|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 3168|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 3168|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7174|lr = 0.00010\n",
      "Epoch: 3169|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7339|lr = 0.00010\n",
      "Epoch: 3169|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7384|lr = 0.00010\n",
      "Epoch: 3170|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7288|lr = 0.00010\n",
      "Epoch: 3170|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 3171|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 3171|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3172|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7372|lr = 0.00010\n",
      "Epoch: 3172|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7452|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3173|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 3173|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 3174|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7157|lr = 0.00010\n",
      "Epoch: 3174|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7426|lr = 0.00010\n",
      "Epoch: 3175|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 3175|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7365|lr = 0.00010\n",
      "Epoch: 3176|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 3176|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 3177|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 3177|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 3178|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3178|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7301|lr = 0.00010\n",
      "Epoch: 3179|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 3179|steps:   60|Train Avg Loss: 0.0060 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 3180|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7388|lr = 0.00010\n",
      "Epoch: 3180|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7317|lr = 0.00010\n",
      "Epoch: 3181|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 3181|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 3182|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 3182|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 3183|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3183|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 3184|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 3184|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 3185|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 3185|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 3186|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 3186|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 3187|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 3187|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7380|lr = 0.00010\n",
      "Epoch: 3188|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 3188|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 3189|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 3189|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7324|lr = 0.00010\n",
      "Epoch: 3190|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3190|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 3191|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 3191|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 3192|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7222|lr = 0.00010\n",
      "Epoch: 3192|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7315|lr = 0.00010\n",
      "Epoch: 3193|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7322|lr = 0.00010\n",
      "Epoch: 3193|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7329|lr = 0.00010\n",
      "Epoch: 3194|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 3194|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 3195|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 3195|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 3196|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 3196|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 3197|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 3197|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3198|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 3198|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7366|lr = 0.00010\n",
      "Epoch: 3199|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7179|lr = 0.00010\n",
      "Epoch: 3199|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 3200|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7402|lr = 0.00010\n",
      "Epoch: 3200|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 3201|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 3201|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 3202|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 3202|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7380|lr = 0.00010\n",
      "Epoch: 3203|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 3203|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 3204|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7289|lr = 0.00010\n",
      "Epoch: 3204|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7452|lr = 0.00010\n",
      "Epoch: 3205|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 3205|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 3206|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 3206|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3207|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 3207|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 3208|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 3208|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3209|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3209|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 3210|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 3210|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7388|lr = 0.00010\n",
      "Epoch: 3211|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 3211|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 3212|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 3212|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3213|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 3213|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 3214|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 3214|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 3215|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3215|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 3216|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3216|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7227|lr = 0.00010\n",
      "Epoch: 3217|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7300|lr = 0.00010\n",
      "Epoch: 3217|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3218|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 3218|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 3219|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3219|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 3220|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 3220|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 3221|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7213|lr = 0.00010\n",
      "Epoch: 3221|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7292|lr = 0.00010\n",
      "Epoch: 3222|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 3222|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7312|lr = 0.00010\n",
      "Epoch: 3223|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3223|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 3224|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7357|lr = 0.00010\n",
      "Epoch: 3224|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7500|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3225|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 3225|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 3226|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 3226|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 3227|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7328|lr = 0.00010\n",
      "Epoch: 3227|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 3228|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7013|lr = 0.00010\n",
      "Epoch: 3228|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 3229|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 3229|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 3230|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3230|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 3231|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3231|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 3232|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7302|lr = 0.00010\n",
      "Epoch: 3232|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7322|lr = 0.00010\n",
      "Epoch: 3233|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7454|lr = 0.00010\n",
      "Epoch: 3233|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7149|lr = 0.00010\n",
      "Epoch: 3234|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3234|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 3235|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7447|lr = 0.00010\n",
      "Epoch: 3235|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 3236|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 3236|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7432|lr = 0.00010\n",
      "Epoch: 3237|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 3237|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7265|lr = 0.00010\n",
      "Epoch: 3238|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7376|lr = 0.00010\n",
      "Epoch: 3238|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7271|lr = 0.00010\n",
      "Epoch: 3239|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 3239|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 3240|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 3240|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7135|lr = 0.00010\n",
      "Epoch: 3241|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7400|lr = 0.00010\n",
      "Epoch: 3241|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 3242|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 3242|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 3243|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7349|lr = 0.00010\n",
      "Epoch: 3243|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7290|lr = 0.00010\n",
      "Epoch: 3244|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7055|lr = 0.00010\n",
      "Epoch: 3244|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 3245|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 3245|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 3246|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 3246|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 3247|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3247|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 3248|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 3248|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3249|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 3249|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7417|lr = 0.00010\n",
      "Epoch: 3250|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 3250|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 3251|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 3251|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 3252|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 3252|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 3253|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 3253|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 3254|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 3254|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7226|lr = 0.00010\n",
      "Epoch: 3255|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 3255|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 3256|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 3256|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 3257|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 3257|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3258|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3258|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7416|lr = 0.00010\n",
      "Epoch: 3259|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 3259|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 3260|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 3260|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 3261|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 3261|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 3262|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 3262|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7255|lr = 0.00010\n",
      "Epoch: 3263|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7410|lr = 0.00010\n",
      "Epoch: 3263|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 3264|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 3264|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3265|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3265|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 3266|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7411|lr = 0.00010\n",
      "Epoch: 3266|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 3267|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7331|lr = 0.00010\n",
      "Epoch: 3267|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3268|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 3268|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 3269|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3269|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 3270|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 3270|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 3271|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 3271|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7260|lr = 0.00010\n",
      "Epoch: 3272|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3272|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7230|lr = 0.00010\n",
      "Epoch: 3273|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 3273|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3274|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 3274|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 3275|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 3275|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 3276|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7354|lr = 0.00010\n",
      "Epoch: 3276|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7702|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3277|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 3277|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 3278|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 3278|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3279|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7214|lr = 0.00010\n",
      "Epoch: 3279|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 3280|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3280|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7367|lr = 0.00010\n",
      "Epoch: 3281|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7316|lr = 0.00010\n",
      "Epoch: 3281|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3282|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7411|lr = 0.00010\n",
      "Epoch: 3282|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 3283|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 3283|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 3284|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 3284|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 3285|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 3285|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 3286|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3286|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 3287|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 3287|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 3288|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 3288|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 3289|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 3289|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 3290|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7372|lr = 0.00010\n",
      "Epoch: 3290|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3291|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7345|lr = 0.00010\n",
      "Epoch: 3291|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 3292|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 3292|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 3293|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7327|lr = 0.00010\n",
      "Epoch: 3293|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 3294|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 3294|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 3295|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 3295|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7380|lr = 0.00010\n",
      "Epoch: 3296|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 3296|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 3297|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3297|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 3298|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 3298|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 3299|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 3299|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7343|lr = 0.00010\n",
      "Epoch: 3300|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7378|lr = 0.00010\n",
      "Epoch: 3300|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 3301|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 3301|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 3302|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7321|lr = 0.00010\n",
      "Epoch: 3302|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 3303|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3303|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 3304|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7054|lr = 0.00010\n",
      "Epoch: 3304|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 3305|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3305|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 3306|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 3306|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 3307|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7993|lr = 0.00010\n",
      "Epoch: 3307|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 3308|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 3308|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 3309|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 3309|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 3310|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3310|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7399|lr = 0.00010\n",
      "Epoch: 3311|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 3311|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7408|lr = 0.00010\n",
      "Epoch: 3312|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 3312|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 3313|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 3313|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 3314|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 3314|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 3315|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 3315|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7407|lr = 0.00010\n",
      "Epoch: 3316|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 3316|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7433|lr = 0.00010\n",
      "Epoch: 3317|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3317|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 3318|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 3318|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7276|lr = 0.00010\n",
      "Epoch: 3319|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 3319|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7290|lr = 0.00010\n",
      "Epoch: 3320|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 3320|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 3321|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 3321|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3322|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 3322|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 3323|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 3323|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 3324|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 3324|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7333|lr = 0.00010\n",
      "Epoch: 3325|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7306|lr = 0.00010\n",
      "Epoch: 3325|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7357|lr = 0.00010\n",
      "Epoch: 3326|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7434|lr = 0.00010\n",
      "Epoch: 3326|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 3327|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 3327|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3328|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7370|lr = 0.00010\n",
      "Epoch: 3328|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7421|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3329|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3329|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7252|lr = 0.00010\n",
      "Epoch: 3330|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 3330|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 3331|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7317|lr = 0.00010\n",
      "Epoch: 3331|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 3332|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 3332|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 3333|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3333|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7324|lr = 0.00010\n",
      "Epoch: 3334|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3334|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 3335|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 3335|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 3336|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 3336|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7236|lr = 0.00010\n",
      "Epoch: 3337|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7292|lr = 0.00010\n",
      "Epoch: 3337|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 3338|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 3338|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 3339|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7265|lr = 0.00010\n",
      "Epoch: 3339|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 3340|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7280|lr = 0.00010\n",
      "Epoch: 3340|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 3341|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 3341|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7257|lr = 0.00010\n",
      "Epoch: 3342|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 3342|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7340|lr = 0.00010\n",
      "Epoch: 3343|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7354|lr = 0.00010\n",
      "Epoch: 3343|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 3344|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7357|lr = 0.00010\n",
      "Epoch: 3344|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3345|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3345|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7429|lr = 0.00010\n",
      "Epoch: 3346|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 3346|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 3347|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7355|lr = 0.00010\n",
      "Epoch: 3347|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3348|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 3348|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 3349|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7452|lr = 0.00010\n",
      "Epoch: 3349|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 3350|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 3350|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 3351|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7063|lr = 0.00010\n",
      "Epoch: 3351|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 3352|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7379|lr = 0.00010\n",
      "Epoch: 3352|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 3353|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7304|lr = 0.00010\n",
      "Epoch: 3353|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 3354|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 3354|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 3355|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 3355|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 3356|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 3356|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 3357|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 3357|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7384|lr = 0.00010\n",
      "Epoch: 3358|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3358|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 3359|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7452|lr = 0.00010\n",
      "Epoch: 3359|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 3360|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 3360|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 3361|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3361|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3362|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 3362|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3363|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7407|lr = 0.00010\n",
      "Epoch: 3363|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 3364|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 3364|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 3365|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7429|lr = 0.00010\n",
      "Epoch: 3365|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 3366|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 3366|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 3367|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 3367|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 3368|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3368|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 3369|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 3369|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 3370|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 3370|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 3371|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 3371|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 3372|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 3372|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 3373|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 3373|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 3374|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 3374|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 3375|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 3375|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 3376|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 3376|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7130|lr = 0.00010\n",
      "Epoch: 3377|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 3377|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 3378|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7305|lr = 0.00010\n",
      "Epoch: 3378|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 3379|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 3379|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7162|lr = 0.00010\n",
      "Epoch: 3380|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 3380|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7529|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3381|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 3381|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7357|lr = 0.00010\n",
      "Epoch: 3382|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 3382|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 3383|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 3383|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 3384|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 3384|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 3385|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 3385|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 3386|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7294|lr = 0.00010\n",
      "Epoch: 3386|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 3387|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7232|lr = 0.00010\n",
      "Epoch: 3387|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 3388|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 3388|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 3389|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3389|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 3390|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3390|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 3391|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 3391|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 3392|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 3392|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7159|lr = 0.00010\n",
      "Epoch: 3393|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 3393|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7309|lr = 0.00010\n",
      "Epoch: 3394|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3394|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7384|lr = 0.00010\n",
      "Epoch: 3395|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7288|lr = 0.00010\n",
      "Epoch: 3395|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 3396|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 3396|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 3397|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 3397|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7218|lr = 0.00010\n",
      "Epoch: 3398|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 3398|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 3399|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7384|lr = 0.00010\n",
      "Epoch: 3399|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3400|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 3400|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 3401|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 3401|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 3402|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 3402|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 3403|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3403|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 3404|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 3404|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 3405|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 3405|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 3406|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3406|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 3407|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 3407|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3408|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7355|lr = 0.00010\n",
      "Epoch: 3408|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 3409|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 3409|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 3410|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3410|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 3411|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 3411|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 3412|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 3412|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7325|lr = 0.00010\n",
      "Epoch: 3413|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 3413|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 3414|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 3414|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 3415|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 3415|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7305|lr = 0.00010\n",
      "Epoch: 3416|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 3416|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 3417|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 3417|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 3418|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 3418|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7180|lr = 0.00010\n",
      "Epoch: 3419|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 3419|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7286|lr = 0.00010\n",
      "Epoch: 3420|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 3420|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3421|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7379|lr = 0.00010\n",
      "Epoch: 3421|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 3422|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3422|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 3423|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 3423|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 3424|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 3424|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 3425|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.8026|lr = 0.00010\n",
      "Epoch: 3425|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3426|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 3426|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3427|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 3427|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3428|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 3428|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 3429|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7348|lr = 0.00010\n",
      "Epoch: 3429|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 3430|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 3430|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 3431|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 3431|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3432|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7417|lr = 0.00010\n",
      "Epoch: 3432|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7626|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3433|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 3433|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 3434|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 3434|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7322|lr = 0.00010\n",
      "Epoch: 3435|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 3435|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3436|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3436|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7345|lr = 0.00010\n",
      "Epoch: 3437|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7327|lr = 0.00010\n",
      "Epoch: 3437|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 3438|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 3438|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7447|lr = 0.00010\n",
      "Epoch: 3439|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 3439|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 3440|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3440|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 3441|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7359|lr = 0.00010\n",
      "Epoch: 3441|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 3442|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7329|lr = 0.00010\n",
      "Epoch: 3442|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3443|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 3443|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3444|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 3444|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 3445|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 3445|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 3446|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 3446|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 3447|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 3447|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 3448|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 3448|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 3449|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 3449|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 3450|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7323|lr = 0.00010\n",
      "Epoch: 3450|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 3451|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 3451|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 3452|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 3452|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 3453|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 3453|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 3454|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 3454|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 3455|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 3455|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3456|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 3456|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3457|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 3457|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3458|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 3458|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 3459|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 3459|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7400|lr = 0.00010\n",
      "Epoch: 3460|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7416|lr = 0.00010\n",
      "Epoch: 3460|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7426|lr = 0.00010\n",
      "Epoch: 3461|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 3461|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 3462|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7263|lr = 0.00010\n",
      "Epoch: 3462|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7449|lr = 0.00010\n",
      "Epoch: 3463|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3463|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7230|lr = 0.00010\n",
      "Epoch: 3464|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3464|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 3465|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 3465|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3466|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 3466|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 3467|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 3467|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 3468|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 3468|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 3469|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 3469|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 3470|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 3470|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 3471|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 3471|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3472|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 3472|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 3473|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 3473|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 3474|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 3474|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 3475|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 3475|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 3476|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 3476|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 3477|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 3477|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 3478|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 3478|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 3479|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7398|lr = 0.00010\n",
      "Epoch: 3479|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 3480|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3480|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7305|lr = 0.00010\n",
      "Epoch: 3481|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 3481|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 3482|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 3482|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3483|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 3483|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 3484|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 3484|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7469|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3485|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 3485|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7207|lr = 0.00010\n",
      "Epoch: 3486|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3486|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 3487|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 3487|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 3488|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7310|lr = 0.00010\n",
      "Epoch: 3488|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 3489|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 3489|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 3490|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7440|lr = 0.00010\n",
      "Epoch: 3490|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 3491|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 3491|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7440|lr = 0.00010\n",
      "Epoch: 3492|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 3492|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7383|lr = 0.00010\n",
      "Epoch: 3493|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7313|lr = 0.00010\n",
      "Epoch: 3493|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7315|lr = 0.00010\n",
      "Epoch: 3494|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3494|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 3495|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 3495|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7233|lr = 0.00010\n",
      "Epoch: 3496|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 3496|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 3497|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 3497|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7284|lr = 0.00010\n",
      "Epoch: 3498|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7259|lr = 0.00010\n",
      "Epoch: 3498|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3499|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 3499|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 3500|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 3500|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 3501|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 3501|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 3502|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7298|lr = 0.00010\n",
      "Epoch: 3502|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3503|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 3503|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 3504|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7269|lr = 0.00010\n",
      "Epoch: 3504|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7361|lr = 0.00010\n",
      "Epoch: 3505|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3505|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 3506|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 3506|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 3507|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 3507|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 3508|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3508|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3509|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7411|lr = 0.00010\n",
      "Epoch: 3509|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7363|lr = 0.00010\n",
      "Epoch: 3510|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 3510|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 3511|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3511|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3512|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7347|lr = 0.00010\n",
      "Epoch: 3512|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 3513|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7278|lr = 0.00010\n",
      "Epoch: 3513|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 3514|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7447|lr = 0.00010\n",
      "Epoch: 3514|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 3515|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 3515|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 3516|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 3516|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 3517|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 3517|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3518|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 3518|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 3519|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 3519|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7315|lr = 0.00010\n",
      "Epoch: 3520|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 3520|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 3521|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3521|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3522|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 3522|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 3523|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3523|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7359|lr = 0.00010\n",
      "Epoch: 3524|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 3524|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 3525|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 3525|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 3526|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7201|lr = 0.00010\n",
      "Epoch: 3526|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 3527|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 3527|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 3528|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 3528|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7351|lr = 0.00010\n",
      "Epoch: 3529|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 3529|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3530|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 3530|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 3531|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 3531|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 3532|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 3532|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 3533|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7406|lr = 0.00010\n",
      "Epoch: 3533|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 3534|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7322|lr = 0.00010\n",
      "Epoch: 3534|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3535|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3535|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7377|lr = 0.00010\n",
      "Epoch: 3536|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 3536|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7454|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3537|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 3537|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3538|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3538|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3539|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 3539|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 3540|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 3540|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7281|lr = 0.00010\n",
      "Epoch: 3541|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 3541|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 3542|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 3542|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 3543|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7156|lr = 0.00010\n",
      "Epoch: 3543|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7351|lr = 0.00010\n",
      "Epoch: 3544|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 3544|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7402|lr = 0.00010\n",
      "Epoch: 3545|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7501|lr = 0.00010\n",
      "Epoch: 3545|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 3546|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 3546|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 3547|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3547|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 3548|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 3548|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 3549|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 3549|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3550|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 3550|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 3551|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3551|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3552|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3552|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3553|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 3553|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 3554|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 3554|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7343|lr = 0.00010\n",
      "Epoch: 3555|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7270|lr = 0.00010\n",
      "Epoch: 3555|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 3556|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3556|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 3557|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7108|lr = 0.00010\n",
      "Epoch: 3557|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 3558|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 3558|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 3559|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 3559|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 3560|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7404|lr = 0.00010\n",
      "Epoch: 3560|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 3561|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 3561|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3562|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 3562|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 3563|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7434|lr = 0.00010\n",
      "Epoch: 3563|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 3564|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3564|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 3565|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7409|lr = 0.00010\n",
      "Epoch: 3565|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 3566|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7240|lr = 0.00010\n",
      "Epoch: 3566|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7298|lr = 0.00010\n",
      "Epoch: 3567|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 3567|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7381|lr = 0.00010\n",
      "Epoch: 3568|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7138|lr = 0.00010\n",
      "Epoch: 3568|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7410|lr = 0.00010\n",
      "Epoch: 3569|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 3569|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 3570|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 3570|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3571|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 3571|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 3572|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3572|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 3573|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3573|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 3574|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 3574|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3575|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 3575|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7396|lr = 0.00010\n",
      "Epoch: 3576|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 3576|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 3577|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 3577|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3578|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3578|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 3579|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 3579|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 3580|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7468|lr = 0.00010\n",
      "Epoch: 3580|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 3581|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 3581|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 3582|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 3582|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3583|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 3583|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 3584|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 3584|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 3585|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 3585|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 3586|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 3586|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 3587|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 3587|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 3588|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 3588|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7542|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3589|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 3589|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3590|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 3590|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 3591|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 3591|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 3592|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7347|lr = 0.00010\n",
      "Epoch: 3592|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7380|lr = 0.00010\n",
      "Epoch: 3593|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 3593|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 3594|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3594|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7340|lr = 0.00010\n",
      "Epoch: 3595|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 3595|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 3596|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 3596|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 3597|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 3597|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 3598|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 3598|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 3599|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 3599|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 3600|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 3600|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7404|lr = 0.00010\n",
      "Epoch: 3601|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 3601|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7362|lr = 0.00010\n",
      "Epoch: 3602|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 3602|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 3603|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 3603|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 3604|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 3604|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 3605|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7281|lr = 0.00010\n",
      "Epoch: 3605|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3606|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 3606|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 3607|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 3607|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 3608|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 3608|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3609|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 3609|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7400|lr = 0.00010\n",
      "Epoch: 3610|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 3610|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 3611|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 3611|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 3612|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 3612|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 3613|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7429|lr = 0.00010\n",
      "Epoch: 3613|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 3614|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3614|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 3615|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 3615|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3616|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 3616|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7416|lr = 0.00010\n",
      "Epoch: 3617|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3617|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 3618|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 3618|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7449|lr = 0.00010\n",
      "Epoch: 3619|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 3619|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3620|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 3620|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3621|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 3621|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 3622|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 3622|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 3623|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 3623|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 3624|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 3624|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 3625|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 3625|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 3626|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 3626|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 3627|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 3627|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 3628|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 3628|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3629|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 3629|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 3630|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 3630|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 3631|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 3631|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 3632|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 3632|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 3633|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 3633|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 3634|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 3634|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 3635|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 3635|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 3636|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 3636|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 3637|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 3637|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 3638|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 3638|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 3639|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7388|lr = 0.00010\n",
      "Epoch: 3639|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 3640|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 3640|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7846|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3641|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 3641|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 3642|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 3642|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 3643|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 3643|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 3644|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 3644|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 3645|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 3645|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7357|lr = 0.00010\n",
      "Epoch: 3646|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 3646|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 3647|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3647|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7319|lr = 0.00010\n",
      "Epoch: 3648|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 3648|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 3649|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 3649|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 3650|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3650|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7378|lr = 0.00010\n",
      "Epoch: 3651|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 3651|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7209|lr = 0.00010\n",
      "Epoch: 3652|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 3652|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 3653|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 3653|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 3654|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7349|lr = 0.00010\n",
      "Epoch: 3654|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 3655|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 3655|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 3656|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 3656|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7368|lr = 0.00010\n",
      "Epoch: 3657|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 3657|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 3658|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7115|lr = 0.00010\n",
      "Epoch: 3658|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7410|lr = 0.00010\n",
      "Epoch: 3659|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 3659|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 3660|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 3660|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3661|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 3661|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3662|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3662|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7333|lr = 0.00010\n",
      "Epoch: 3663|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3663|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 3664|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7346|lr = 0.00010\n",
      "Epoch: 3664|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3665|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 3665|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 3666|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3666|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 3667|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 3667|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 3668|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3668|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 3669|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 3669|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 3670|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 3670|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3671|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 3671|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 3672|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7355|lr = 0.00010\n",
      "Epoch: 3672|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 3673|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 3673|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3674|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 3674|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 3675|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7266|lr = 0.00010\n",
      "Epoch: 3675|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 3676|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7163|lr = 0.00010\n",
      "Epoch: 3676|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 3677|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 3677|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 3678|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 3678|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 3679|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 3679|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 3680|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 3680|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 3681|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 3681|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 3682|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3682|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 3683|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 3683|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 3684|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 3684|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 3685|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 3685|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 3686|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 3686|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 3687|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3687|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3688|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 3688|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 3689|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 3689|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 3690|steps:   30|Train Avg Loss: 0.0053 |Test Loss: 1.7361|lr = 0.00010\n",
      "Epoch: 3690|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 3691|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 3691|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 3692|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 3692|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7535|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3693|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 3693|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 3694|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 3694|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 3695|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7323|lr = 0.00010\n",
      "Epoch: 3695|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 3696|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 3696|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 3697|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 3697|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3698|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 3698|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 3699|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 3699|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 3700|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 3700|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 3701|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 3701|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 3702|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 3702|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 3703|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7986|lr = 0.00010\n",
      "Epoch: 3703|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 3704|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 3704|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 3705|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7222|lr = 0.00010\n",
      "Epoch: 3705|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 3706|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 3706|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 3707|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 3707|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 3708|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3708|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 3709|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3709|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 3710|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7452|lr = 0.00010\n",
      "Epoch: 3710|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 3711|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3711|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 3712|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 3712|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 3713|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 3713|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 3714|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 3714|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 3715|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 3715|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 3716|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 3716|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 3717|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 3717|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3718|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 3718|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 3719|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 3719|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 3720|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3720|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 3721|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 3721|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 3722|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 3722|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 3723|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 3723|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 3724|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7376|lr = 0.00010\n",
      "Epoch: 3724|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 3725|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 3725|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 3726|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 3726|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3727|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3727|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 3728|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 3728|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3729|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3729|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 3730|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 3730|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 3731|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 3731|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 3732|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3732|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 3733|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 3733|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 3734|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 3734|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 3735|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 3735|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3736|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 3736|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 3737|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 3737|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 3738|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 3738|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 3739|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 3739|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 3740|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3740|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7395|lr = 0.00010\n",
      "Epoch: 3741|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 3741|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 3742|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 3742|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7376|lr = 0.00010\n",
      "Epoch: 3743|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 3743|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 3744|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 3744|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7305|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3745|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 3745|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3746|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 3746|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 3747|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7245|lr = 0.00010\n",
      "Epoch: 3747|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 3748|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 3748|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 3749|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 3749|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 3750|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 3750|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 3751|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7416|lr = 0.00010\n",
      "Epoch: 3751|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 3752|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7395|lr = 0.00010\n",
      "Epoch: 3752|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7392|lr = 0.00010\n",
      "Epoch: 3753|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 3753|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 3754|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 3754|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 3755|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 3755|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 3756|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 3756|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 3757|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 3757|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 3758|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 3758|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7437|lr = 0.00010\n",
      "Epoch: 3759|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7202|lr = 0.00010\n",
      "Epoch: 3759|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 3760|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 3760|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 3761|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3761|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 3762|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 3762|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3763|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 3763|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 3764|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 3764|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 3765|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 3765|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 3766|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 3766|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 3767|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 3767|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 3768|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 3768|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 3769|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 3769|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 3770|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 3770|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3771|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 3771|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 3772|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3772|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 3773|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 3773|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 3774|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 3774|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3775|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 3775|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 3776|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 3776|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 3777|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 3777|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 3778|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 3778|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 3779|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7351|lr = 0.00010\n",
      "Epoch: 3779|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3780|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 3780|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7433|lr = 0.00010\n",
      "Epoch: 3781|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 3781|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 3782|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 3782|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 3783|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3783|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 3784|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 3784|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 3785|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 3785|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 3786|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3786|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 3787|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 3787|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7432|lr = 0.00010\n",
      "Epoch: 3788|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 3788|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 3789|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 3789|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 3790|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 3790|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 3791|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 3791|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 3792|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 3792|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 3793|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 3793|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 3794|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 3794|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 3795|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 3795|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 3796|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 3796|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7694|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3797|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 3797|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 3798|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 3798|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 3799|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 3799|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 3800|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 3800|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 3801|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 3801|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 3802|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 3802|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 3803|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 3803|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 3804|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7319|lr = 0.00010\n",
      "Epoch: 3804|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 3805|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 3805|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 3806|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 3806|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 3807|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7519|lr = 0.00010\n",
      "Epoch: 3807|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 3808|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 3808|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 3809|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 3809|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3810|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 3810|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 3811|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 3811|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 3812|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 3812|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8071|lr = 0.00010\n",
      "Epoch: 3813|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 3813|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 3814|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7961|lr = 0.00010\n",
      "Epoch: 3814|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 3815|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 3815|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 3816|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 3816|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 3817|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 3817|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 3818|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 3818|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 3819|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 3819|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 3820|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 3820|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 3821|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 3821|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 3822|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 3822|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 3823|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 3823|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 3824|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 3824|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 3825|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 3825|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 3826|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 3826|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 3827|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 3827|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 3828|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 3828|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 3829|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 3829|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 3830|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 3830|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 3831|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3831|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 3832|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 3832|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 3833|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3833|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 3834|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 3834|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 3835|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7398|lr = 0.00010\n",
      "Epoch: 3835|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 3836|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 3836|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 3837|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 3837|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 3838|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 3838|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 3839|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 3839|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 3840|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 3840|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 3841|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 3841|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 3842|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 3842|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 3843|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 3843|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 3844|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 3844|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 3845|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 3845|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 3846|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 3846|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7519|lr = 0.00010\n",
      "Epoch: 3847|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 3847|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 3848|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 3848|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7701|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3849|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3849|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 3850|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 3850|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 3851|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 3851|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 3852|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 3852|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 3853|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 3853|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 3854|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7409|lr = 0.00010\n",
      "Epoch: 3854|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 3855|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 3855|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 3856|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3856|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 3857|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 3857|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 3858|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 3858|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 3859|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 3859|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 3860|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 3860|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 3861|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 3861|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 3862|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 3862|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 3863|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 3863|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 3864|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7306|lr = 0.00010\n",
      "Epoch: 3864|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 3865|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 3865|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 3866|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3866|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 3867|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 3867|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 3868|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 3868|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 3869|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 3869|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 3870|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 3870|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 3871|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 3871|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7356|lr = 0.00010\n",
      "Epoch: 3872|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 3872|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 3873|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 3873|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 3874|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 3874|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 3875|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 3875|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 3876|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 3876|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 3877|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 3877|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 3878|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 3878|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 3879|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 3879|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 3880|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 3880|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 3881|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 3881|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 3882|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 3882|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 3883|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 3883|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 3884|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 3884|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 3885|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 3885|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 3886|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 3886|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 3887|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 3887|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 3888|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 3888|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 3889|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 3889|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7434|lr = 0.00010\n",
      "Epoch: 3890|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 3890|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 3891|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 3891|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 3892|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 3892|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 3893|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7379|lr = 0.00010\n",
      "Epoch: 3893|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 3894|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7356|lr = 0.00010\n",
      "Epoch: 3894|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 3895|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 3895|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 3896|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 3896|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 3897|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 3897|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 3898|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 3898|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 3899|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 3899|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 3900|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 3900|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7666|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3901|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 3901|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 3902|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 3902|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 3903|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 3903|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 3904|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7383|lr = 0.00010\n",
      "Epoch: 3904|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 3905|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 3905|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3906|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 3906|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 3907|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 3907|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 3908|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7369|lr = 0.00010\n",
      "Epoch: 3908|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 3909|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3909|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 3910|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 3910|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3911|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 3911|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 3912|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 3912|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 3913|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 3913|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 3914|steps:   30|Train Avg Loss: 0.0063 |Test Loss: 1.7399|lr = 0.00010\n",
      "Epoch: 3914|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 3915|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 3915|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 3916|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 3916|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 3917|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 3917|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 3918|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7361|lr = 0.00010\n",
      "Epoch: 3918|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 3919|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 3919|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 3920|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 3920|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 3921|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 3921|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 3922|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 3922|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 3923|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 3923|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 3924|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3924|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7245|lr = 0.00010\n",
      "Epoch: 3925|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 3925|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 3926|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 3926|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 3927|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 3927|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 3928|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 3928|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 3929|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 3929|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 3930|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 3930|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 3931|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 3931|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3932|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 3932|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 3933|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 3933|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 3934|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 3934|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 3935|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 3935|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 3936|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 3936|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 3937|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 3937|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 3938|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 3938|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 3939|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 3939|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 3940|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 3940|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 3941|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 3941|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 3942|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 3942|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 3943|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 3943|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 3944|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 3944|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 3945|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 3945|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 3946|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3946|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 3947|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 3947|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 3948|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 3948|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 3949|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 3949|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 3950|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8075|lr = 0.00010\n",
      "Epoch: 3950|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 3951|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 3951|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 3952|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 3952|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7799|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3953|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 3953|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7276|lr = 0.00010\n",
      "Epoch: 3954|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 3954|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 3955|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 3955|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 3956|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 3956|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 3957|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 3957|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3958|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3958|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 3959|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 3959|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 3960|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 3960|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 3961|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 3961|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 3962|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 3962|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7400|lr = 0.00010\n",
      "Epoch: 3963|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 3963|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 3964|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 3964|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 3965|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 3965|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 3966|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 3966|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 3967|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 3967|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 3968|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 3968|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 3969|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 3969|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 3970|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 3970|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 3971|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 3971|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 3972|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 3972|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 3973|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 3973|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 3974|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 3974|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 3975|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 3975|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 3976|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 3976|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7300|lr = 0.00010\n",
      "Epoch: 3977|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 3977|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 3978|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 3978|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 3979|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 3979|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 3980|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 3980|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 3981|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 3981|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 3982|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3982|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 3983|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 3983|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 3984|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 3984|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 3985|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 3985|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 3986|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7359|lr = 0.00010\n",
      "Epoch: 3986|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 3987|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 3987|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7319|lr = 0.00010\n",
      "Epoch: 3988|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 3988|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 3989|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 3989|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 3990|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 3990|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 3991|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7398|lr = 0.00010\n",
      "Epoch: 3991|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 3992|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 3992|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 3993|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 3993|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 3994|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 3994|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 3995|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 3995|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 3996|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 3996|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 3997|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 3997|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 3998|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7368|lr = 0.00010\n",
      "Epoch: 3998|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8058|lr = 0.00010\n",
      "Epoch: 3999|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 3999|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 4000|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 4000|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 4001|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 4001|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 4002|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 4002|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 4003|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4003|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 4004|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4004|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7776|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4005|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 4005|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7352|lr = 0.00010\n",
      "Epoch: 4006|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 4006|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 4007|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 4007|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 4008|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4008|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 4009|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 4009|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8042|lr = 0.00010\n",
      "Epoch: 4010|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7434|lr = 0.00010\n",
      "Epoch: 4010|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4011|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4011|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 4012|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 4012|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 4013|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 4013|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 4014|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 4014|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4015|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 4015|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4016|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 4016|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 4017|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 4017|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 4018|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7948|lr = 0.00010\n",
      "Epoch: 4018|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4019|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 4019|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7454|lr = 0.00010\n",
      "Epoch: 4020|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 4020|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 4021|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 4021|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 4022|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 4022|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 4023|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 4023|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4024|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 4024|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 4025|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 4025|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 4026|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 4026|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 4027|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 4027|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 4028|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 4028|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 4029|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4029|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 4030|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 4030|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 4031|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 4031|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 4032|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 4032|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 4033|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 4033|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 4034|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 4034|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 4035|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4035|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7372|lr = 0.00010\n",
      "Epoch: 4036|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 4036|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 4037|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 4037|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 4038|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4038|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 4039|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 4039|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7353|lr = 0.00010\n",
      "Epoch: 4040|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 4040|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7454|lr = 0.00010\n",
      "Epoch: 4041|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7335|lr = 0.00010\n",
      "Epoch: 4041|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7215|lr = 0.00010\n",
      "Epoch: 4042|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4042|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 4043|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 4043|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7373|lr = 0.00010\n",
      "Epoch: 4044|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7353|lr = 0.00010\n",
      "Epoch: 4044|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 4045|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 4045|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 4046|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 4046|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 4047|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 4047|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7363|lr = 0.00010\n",
      "Epoch: 4048|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 4048|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 4049|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 4049|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 4050|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 4050|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 4051|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4051|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 4052|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 4052|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 4053|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 4053|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 4054|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 4054|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 4055|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 4055|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 4056|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 4056|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7629|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4057|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4057|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 4058|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 4058|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 4059|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 4059|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 4060|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 4060|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 4061|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 4061|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 4062|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 4062|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 4063|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 4063|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 4064|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7911|lr = 0.00010\n",
      "Epoch: 4064|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 4065|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 4065|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 4066|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4066|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 4067|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 4067|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 4068|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 4068|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 4069|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 4069|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 4070|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7383|lr = 0.00010\n",
      "Epoch: 4070|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4071|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4071|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 4072|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7910|lr = 0.00010\n",
      "Epoch: 4072|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 4073|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 4073|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 4074|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4074|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4075|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 4075|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 4076|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4076|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 4077|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 4077|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 4078|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 4078|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 4079|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 4079|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 4080|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 4080|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 4081|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 4081|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 4082|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4082|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 4083|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 4083|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 4084|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 4084|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 4085|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 4085|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 4086|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4086|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4087|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 4087|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 4088|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 4088|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 4089|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 4089|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 4090|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4090|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 4091|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 4091|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.8047|lr = 0.00010\n",
      "Epoch: 4092|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 4092|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 4093|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4093|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 4094|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 4094|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 4095|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 4095|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 4096|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 4096|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 4097|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 4097|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 4098|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 4098|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7930|lr = 0.00010\n",
      "Epoch: 4099|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 4099|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 4100|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 4100|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 4101|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7453|lr = 0.00010\n",
      "Epoch: 4101|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 4102|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 4102|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4103|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 4103|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 4104|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 4104|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 4105|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 4105|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 4106|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4106|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 4107|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 4107|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 4108|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 4108|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7462|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4109|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 4109|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 4110|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 4110|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 4111|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4111|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 4112|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 4112|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 4113|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 4113|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 4114|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 4114|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 4115|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4115|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 4116|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 4116|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4117|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4117|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 4118|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 4118|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 4119|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 4119|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 4120|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 4120|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 4121|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 4121|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7381|lr = 0.00010\n",
      "Epoch: 4122|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 4122|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 4123|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7388|lr = 0.00010\n",
      "Epoch: 4123|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 4124|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 4124|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 4125|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 4125|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 4126|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4126|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 4127|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 4127|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 4128|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 4128|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 4129|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4129|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 4130|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 4130|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7270|lr = 0.00010\n",
      "Epoch: 4131|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 4131|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7429|lr = 0.00010\n",
      "Epoch: 4132|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 4132|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 4133|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 4133|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4134|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 4134|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 4135|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4135|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 4136|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4136|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 4137|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 4137|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 4138|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 4138|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 4139|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 4139|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 4140|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 4140|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 4141|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 4141|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 4142|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 4142|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 4143|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 4143|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 4144|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7271|lr = 0.00010\n",
      "Epoch: 4144|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 4145|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 4145|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 4146|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 4146|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 4147|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4147|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 4148|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4148|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4149|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 4149|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4150|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 4150|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7305|lr = 0.00010\n",
      "Epoch: 4151|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 4151|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 4152|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 4152|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 4153|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4153|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 4154|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 4154|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4155|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 4155|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 4156|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 4156|steps:   60|Train Avg Loss: 0.0050 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 4157|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 4157|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 4158|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 4158|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 4159|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 4159|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 4160|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4160|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7869|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4161|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 4161|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 4162|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 4162|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 4163|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 4163|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 4164|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 4164|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4165|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 4165|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 4166|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4166|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 4167|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 4167|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 4168|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 4168|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 4169|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4169|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7304|lr = 0.00010\n",
      "Epoch: 4170|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 4170|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 4171|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 4171|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4172|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 4172|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 4173|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 4173|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 4174|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 4174|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 4175|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 4175|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 4176|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 4176|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8118|lr = 0.00010\n",
      "Epoch: 4177|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 4177|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 4178|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 4178|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 4179|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 4179|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 4180|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 4180|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 4181|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 4181|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 4182|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 4182|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 4183|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 4183|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 4184|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4184|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 4185|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 4185|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 4186|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 4186|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 4187|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 4187|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 4188|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 4188|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 4189|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 4189|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 4190|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7987|lr = 0.00010\n",
      "Epoch: 4190|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4191|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 4191|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 4192|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 4192|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 4193|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.8069|lr = 0.00010\n",
      "Epoch: 4193|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4194|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 4194|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4195|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 4195|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 4196|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 4196|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7977|lr = 0.00010\n",
      "Epoch: 4197|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 4197|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 4198|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 4198|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 4199|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 4199|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 4200|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 4200|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4201|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 4201|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 4202|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 4202|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 4203|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.8099|lr = 0.00010\n",
      "Epoch: 4203|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 4204|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 4204|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 4205|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 4205|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 4206|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4206|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 4207|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 4207|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 4208|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 4208|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 4209|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 4209|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 4210|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 4210|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 4211|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 4211|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 4212|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 4212|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7756|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4213|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 4213|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 4214|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 4214|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4215|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4215|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 4216|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 4216|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 4217|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 4217|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7334|lr = 0.00010\n",
      "Epoch: 4218|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7342|lr = 0.00010\n",
      "Epoch: 4218|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 4219|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 4219|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 4220|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 4220|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 4221|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 4221|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.8003|lr = 0.00010\n",
      "Epoch: 4222|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 4222|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 4223|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4223|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 4224|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 4224|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 4225|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 4225|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8045|lr = 0.00010\n",
      "Epoch: 4226|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 4226|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 4227|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 4227|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 4228|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 4228|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 4229|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 4229|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 4230|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 4230|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 4231|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 4231|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4232|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4232|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 4233|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 4233|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7398|lr = 0.00010\n",
      "Epoch: 4234|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 4234|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4235|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 4235|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 4236|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 4236|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 4237|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 4237|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 4238|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 4238|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4239|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 4239|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 4240|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 4240|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4241|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 4241|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 4242|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7998|lr = 0.00010\n",
      "Epoch: 4242|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 4243|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 4243|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 4244|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4244|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 4245|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 4245|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 4246|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 4246|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 4247|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 4247|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 4248|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 4248|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4249|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 4249|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4250|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 4250|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 4251|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 4251|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 4252|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 4252|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 4253|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 4253|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 4254|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 4254|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4255|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 4255|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 4256|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 4256|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 4257|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 4257|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 4258|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 4258|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 4259|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 4259|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 4260|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 4260|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 4261|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 4261|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4262|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4262|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4263|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7433|lr = 0.00010\n",
      "Epoch: 4263|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 4264|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 4264|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7617|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4265|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 4265|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 4266|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 4266|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 4267|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 4267|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 4268|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 4268|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 4269|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 4269|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7434|lr = 0.00010\n",
      "Epoch: 4270|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 4270|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 4271|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 4271|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 4272|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 4272|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 4273|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 4273|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 4274|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 4274|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 4275|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 4275|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 4276|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 4276|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 4277|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 4277|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4278|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 4278|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 4279|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 4279|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7427|lr = 0.00010\n",
      "Epoch: 4280|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 4280|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 4281|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 4281|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7151|lr = 0.00010\n",
      "Epoch: 4282|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 4282|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 4283|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 4283|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 4284|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 4284|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4285|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 4285|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4286|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 4286|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 4287|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 4287|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 4288|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 4288|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 4289|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 4289|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 4290|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 4290|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 4291|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 4291|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 4292|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 4292|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 4293|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 4293|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 4294|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 4294|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4295|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 4295|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7395|lr = 0.00010\n",
      "Epoch: 4296|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 4296|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 4297|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 4297|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 4298|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 4298|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7449|lr = 0.00010\n",
      "Epoch: 4299|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 4299|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 4300|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 4300|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 4301|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4301|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 4302|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4302|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 4303|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 4303|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 4304|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 4304|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 4305|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 4305|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 4306|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 4306|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 4307|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 4307|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 4308|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 4308|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7331|lr = 0.00010\n",
      "Epoch: 4309|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 4309|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 4310|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 4310|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 4311|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 4311|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7429|lr = 0.00010\n",
      "Epoch: 4312|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 4312|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4313|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 4313|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 4314|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 4314|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 4315|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 4315|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 4316|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 4316|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7678|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4317|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 4317|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 4318|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 4318|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 4319|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 4319|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 4320|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 4320|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 4321|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 4321|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 4322|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 4322|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 4323|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 4323|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 4324|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 4324|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 4325|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 4325|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4326|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 4326|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 4327|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 4327|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 4328|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 4328|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 4329|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 4329|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4330|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 4330|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 4331|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 4331|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 4332|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4332|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 4333|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 4333|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 4334|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 4334|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 4335|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 4335|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 4336|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 4336|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 4337|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 4337|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 4338|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 4338|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 4339|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 4339|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 4340|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 4340|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7445|lr = 0.00010\n",
      "Epoch: 4341|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 4341|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 4342|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 4342|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 4343|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 4343|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4344|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 4344|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.8027|lr = 0.00010\n",
      "Epoch: 4345|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 4345|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 4346|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 4346|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 4347|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 4347|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 4348|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 4348|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.8070|lr = 0.00010\n",
      "Epoch: 4349|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8028|lr = 0.00010\n",
      "Epoch: 4349|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 4350|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7939|lr = 0.00010\n",
      "Epoch: 4350|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4351|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 4351|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 4352|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 4352|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 4353|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 4353|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8157|lr = 0.00010\n",
      "Epoch: 4354|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4354|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4355|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 4355|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 4356|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8020|lr = 0.00010\n",
      "Epoch: 4356|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 4357|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 4357|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 4358|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 4358|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 4359|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 4359|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 4360|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 4360|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 4361|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 4361|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 4362|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 4362|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 4363|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 4363|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 4364|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 4364|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7382|lr = 0.00010\n",
      "Epoch: 4365|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 4365|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 4366|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 4366|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 4367|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4367|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7989|lr = 0.00010\n",
      "Epoch: 4368|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4368|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7650|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4369|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 4369|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 4370|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 4370|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 4371|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 4371|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 4372|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7395|lr = 0.00010\n",
      "Epoch: 4372|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.8020|lr = 0.00010\n",
      "Epoch: 4373|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 4373|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4374|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 4374|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7468|lr = 0.00010\n",
      "Epoch: 4375|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 4375|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4376|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 4376|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7391|lr = 0.00010\n",
      "Epoch: 4377|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4377|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.8067|lr = 0.00010\n",
      "Epoch: 4378|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 4378|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 4379|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 4379|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 4380|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7392|lr = 0.00010\n",
      "Epoch: 4380|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4381|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 4381|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 4382|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4382|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 4383|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4383|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 4384|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 4384|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 4385|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 4385|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 4386|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4386|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 4387|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 4387|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 4388|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 4388|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8008|lr = 0.00010\n",
      "Epoch: 4389|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 4389|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4390|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 4390|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4391|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 4391|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 4392|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4392|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 4393|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 4393|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 4394|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 4394|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 4395|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 4395|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 4396|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 4396|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 4397|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 4397|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7426|lr = 0.00010\n",
      "Epoch: 4398|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 4398|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 4399|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 4399|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 4400|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 4400|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 4401|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 4401|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7930|lr = 0.00010\n",
      "Epoch: 4402|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 4402|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 4403|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 4403|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 4404|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8035|lr = 0.00010\n",
      "Epoch: 4404|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 4405|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 4405|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 4406|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 4406|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 4407|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 4407|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 4408|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 4408|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 4409|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 4409|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 4410|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 4410|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 4411|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.8212|lr = 0.00010\n",
      "Epoch: 4411|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 4412|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 4412|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 4413|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 4413|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7959|lr = 0.00010\n",
      "Epoch: 4414|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 4414|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4415|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 4415|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 4416|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 4416|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 4417|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 4417|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 4418|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7977|lr = 0.00010\n",
      "Epoch: 4418|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 4419|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 4419|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 4420|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.8020|lr = 0.00010\n",
      "Epoch: 4420|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7867|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4421|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 4421|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 4422|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7956|lr = 0.00010\n",
      "Epoch: 4422|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4423|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 4423|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 4424|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 4424|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 4425|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 4425|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 4426|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 4426|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 4427|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 4427|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 4428|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4428|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.8000|lr = 0.00010\n",
      "Epoch: 4429|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4429|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 4430|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4430|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 4431|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 4431|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 4432|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 4432|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 4433|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 4433|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 4434|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 4434|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 4435|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4435|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 4436|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 4436|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 4437|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 4437|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 4438|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 4438|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.8042|lr = 0.00010\n",
      "Epoch: 4439|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7355|lr = 0.00010\n",
      "Epoch: 4439|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 4440|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 4440|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4441|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 4441|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 4442|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 4442|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 4443|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 4443|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 4444|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4444|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 4445|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 4445|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7955|lr = 0.00010\n",
      "Epoch: 4446|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 4446|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 4447|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 4447|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 4448|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 4448|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 4449|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7980|lr = 0.00010\n",
      "Epoch: 4449|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 4450|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 4450|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 4451|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 4451|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 4452|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 4452|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 4453|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 4453|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 4454|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 4454|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 4455|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 4455|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4456|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 4456|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4457|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 4457|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 4458|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 4458|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 4459|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4459|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 4460|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4460|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 4461|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 4461|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4462|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 4462|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4463|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 4463|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 4464|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 4464|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 4465|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4465|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 4466|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 4466|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 4467|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 4467|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 4468|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4468|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 4469|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4469|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 4470|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 4470|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4471|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 4471|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 4472|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 4472|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7661|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4473|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 4473|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8012|lr = 0.00010\n",
      "Epoch: 4474|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 4474|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4475|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 4475|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4476|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 4476|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 4477|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 4477|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 4478|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4478|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 4479|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 4479|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7911|lr = 0.00010\n",
      "Epoch: 4480|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 4480|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4481|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4481|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 4482|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 4482|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 4483|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 4483|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4484|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 4484|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 4485|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 4485|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 4486|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4486|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 4487|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 4487|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 4488|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 4488|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4489|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 4489|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 4490|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 4490|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 4491|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 4491|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 4492|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 4492|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 4493|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 4493|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 4494|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 4494|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 4495|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 4495|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4496|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 4496|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 4497|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 4497|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 4498|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7440|lr = 0.00010\n",
      "Epoch: 4498|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4499|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 4499|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 4500|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4500|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 4501|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 4501|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 4502|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4502|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 4503|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 4503|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 4504|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 4504|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 4505|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 4505|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 4506|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 4506|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 4507|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 4507|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 4508|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 4508|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 4509|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4509|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 4510|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 4510|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4511|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 4511|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 4512|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 4512|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 4513|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7372|lr = 0.00010\n",
      "Epoch: 4513|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 4514|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 4514|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4515|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 4515|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 4516|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 4516|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 4517|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 4517|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 4518|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 4518|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 4519|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 4519|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 4520|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 4520|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 4521|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 4521|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7395|lr = 0.00010\n",
      "Epoch: 4522|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 4522|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 4523|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 4523|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 4524|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 4524|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7949|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4525|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 4525|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 4526|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 4526|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 4527|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 4527|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 4528|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 4528|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 4529|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 4529|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4530|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4530|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 4531|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4531|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7474|lr = 0.00010\n",
      "Epoch: 4532|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 4532|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 4533|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 4533|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 4534|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 4534|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7961|lr = 0.00010\n",
      "Epoch: 4535|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 4535|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4536|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 4536|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 4537|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 4537|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 4538|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 4538|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 4539|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 4539|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 4540|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 4540|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 4541|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 4541|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 4542|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 4542|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 4543|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 4543|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 4544|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8094|lr = 0.00010\n",
      "Epoch: 4544|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 4545|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 4545|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 4546|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 4546|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 4547|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 4547|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 4548|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 4548|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 4549|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 4549|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 4550|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 4550|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 4551|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 4551|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4552|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4552|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 4553|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 4553|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 4554|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 4554|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 4555|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 4555|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 4556|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 4556|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 4557|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 4557|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 4558|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 4558|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 4559|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 4559|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 4560|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 4560|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4561|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7429|lr = 0.00010\n",
      "Epoch: 4561|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 4562|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 4562|steps:   60|Train Avg Loss: 0.0049 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4563|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 4563|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 4564|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 4564|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 4565|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 4565|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 4566|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7961|lr = 0.00010\n",
      "Epoch: 4566|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 4567|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 4567|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4568|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 4568|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 4569|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 4569|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 4570|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 4570|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 4571|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 4571|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 4572|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 4572|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7952|lr = 0.00010\n",
      "Epoch: 4573|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 4573|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 4574|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 4574|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 4575|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 4575|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 4576|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 4576|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7851|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4577|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 4577|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 4578|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 4578|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 4579|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4579|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4580|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 4580|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 4581|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 4581|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 4582|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 4582|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 4583|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7985|lr = 0.00010\n",
      "Epoch: 4583|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 4584|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 4584|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 4585|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 4585|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4586|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 4586|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 4587|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4587|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7992|lr = 0.00010\n",
      "Epoch: 4588|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 4588|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 4589|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 4589|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 4590|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 4590|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 4591|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 4591|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 4592|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 4592|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 4593|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 4593|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 4594|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 4594|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 4595|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 4595|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4596|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7251|lr = 0.00010\n",
      "Epoch: 4596|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 4597|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 4597|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.8058|lr = 0.00010\n",
      "Epoch: 4598|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7978|lr = 0.00010\n",
      "Epoch: 4598|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.8014|lr = 0.00010\n",
      "Epoch: 4599|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 4599|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 4600|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7983|lr = 0.00010\n",
      "Epoch: 4600|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 4601|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 4601|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4602|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 4602|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4603|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4603|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 4604|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 4604|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 4605|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 4605|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 4606|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 4606|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7959|lr = 0.00010\n",
      "Epoch: 4607|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 4607|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 4608|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4608|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 4609|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.8027|lr = 0.00010\n",
      "Epoch: 4609|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 4610|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 4610|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 4611|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 4611|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 4612|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4612|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 4613|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7988|lr = 0.00010\n",
      "Epoch: 4613|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 4614|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 4614|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4615|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4615|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7998|lr = 0.00010\n",
      "Epoch: 4616|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 4616|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 4617|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 4617|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 4618|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 4618|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8049|lr = 0.00010\n",
      "Epoch: 4619|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 4619|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.8055|lr = 0.00010\n",
      "Epoch: 4620|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 4620|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 4621|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 4621|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 4622|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 4622|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 4623|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 4623|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 4624|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 4624|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 4625|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 4625|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7956|lr = 0.00010\n",
      "Epoch: 4626|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.8003|lr = 0.00010\n",
      "Epoch: 4626|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 4627|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 4627|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 4628|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7897|lr = 0.00010\n",
      "Epoch: 4628|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.8007|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4629|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 4629|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 4630|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 4630|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 4631|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7955|lr = 0.00010\n",
      "Epoch: 4631|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 4632|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 4632|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 4633|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 4633|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4634|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 4634|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 4635|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 4635|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 4636|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 4636|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 4637|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 4637|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 4638|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 4638|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 4639|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 4639|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7930|lr = 0.00010\n",
      "Epoch: 4640|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 4640|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 4641|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 4641|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 4642|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 4642|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 4643|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 4643|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4644|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 4644|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4645|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 4645|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 4646|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 4646|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 4647|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 4647|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 4648|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 4648|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 4649|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 4649|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 4650|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 4650|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8087|lr = 0.00010\n",
      "Epoch: 4651|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 4651|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 4652|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 4652|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 4653|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 4653|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 4654|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 4654|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 4655|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4655|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 4656|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 4656|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4657|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 4657|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 4658|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 4658|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 4659|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 4659|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 4660|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 4660|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 4661|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 4661|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 4662|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 4662|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 4663|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4663|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 4664|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 4664|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7905|lr = 0.00010\n",
      "Epoch: 4665|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 4665|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 4666|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 4666|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4667|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 4667|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 4668|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7910|lr = 0.00010\n",
      "Epoch: 4668|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 4669|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 4669|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 4670|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7930|lr = 0.00010\n",
      "Epoch: 4670|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 4671|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 4671|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 4672|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 4672|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 4673|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 4673|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 4674|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 4674|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 4675|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 4675|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 4676|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 4676|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 4677|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 4677|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 4678|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 4678|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 4679|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7982|lr = 0.00010\n",
      "Epoch: 4679|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4680|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 4680|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7895|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4681|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 4681|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 4682|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 4682|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 4683|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 4683|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 4684|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 4684|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4685|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4685|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 4686|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 4686|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 4687|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 4687|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 4688|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 4688|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7955|lr = 0.00010\n",
      "Epoch: 4689|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 4689|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 4690|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 4690|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8002|lr = 0.00010\n",
      "Epoch: 4691|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 4691|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4692|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 4692|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 4693|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 4693|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4694|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 4694|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 4695|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 4695|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 4696|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 4696|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 4697|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 4697|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 4698|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 4698|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 4699|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 4699|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 4700|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 4700|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 4701|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 4701|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 4702|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 4702|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 4703|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4703|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 4704|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 4704|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 4705|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 4705|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 4706|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4706|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 4707|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 4707|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4708|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 4708|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 4709|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 4709|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 4710|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 4710|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 4711|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 4711|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 4712|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4712|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 4713|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 4713|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 4714|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 4714|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4715|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4715|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4716|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4716|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4717|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 4717|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 4718|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 4718|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 4719|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4719|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 4720|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 4720|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 4721|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 4721|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 4722|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 4722|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 4723|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 4723|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 4724|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 4724|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 4725|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 4725|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.8094|lr = 0.00010\n",
      "Epoch: 4726|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4726|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4727|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 4727|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 4728|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 4728|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 4729|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 4729|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 4730|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7986|lr = 0.00010\n",
      "Epoch: 4730|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8093|lr = 0.00010\n",
      "Epoch: 4731|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7980|lr = 0.00010\n",
      "Epoch: 4731|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 4732|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.8021|lr = 0.00010\n",
      "Epoch: 4732|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7955|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4733|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 4733|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 4734|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.8016|lr = 0.00010\n",
      "Epoch: 4734|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 4735|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 4735|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 4736|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 4736|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 4737|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 4737|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7397|lr = 0.00010\n",
      "Epoch: 4738|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7400|lr = 0.00010\n",
      "Epoch: 4738|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 4739|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 4739|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 4740|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4740|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 4741|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4741|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 4742|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.8034|lr = 0.00010\n",
      "Epoch: 4742|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 4743|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 4743|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 4744|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 4744|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 4745|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 4745|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4746|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 4746|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 4747|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8090|lr = 0.00010\n",
      "Epoch: 4747|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 4748|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 4748|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4749|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 4749|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 4750|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 4750|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 4751|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7977|lr = 0.00010\n",
      "Epoch: 4751|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 4752|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 4752|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 4753|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 4753|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 4754|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 4754|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 4755|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 4755|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 4756|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 4756|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 4757|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 4757|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 4758|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 4758|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 4759|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 4759|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4760|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 4760|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 4761|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 4761|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4762|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 4762|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 4763|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 4763|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 4764|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4764|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 4765|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 4765|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 4766|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7374|lr = 0.00010\n",
      "Epoch: 4766|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 4767|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4767|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7943|lr = 0.00010\n",
      "Epoch: 4768|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7969|lr = 0.00010\n",
      "Epoch: 4768|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4769|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4769|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 4770|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 4770|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 4771|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4771|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 4772|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 4772|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 4773|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 4773|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 4774|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 4774|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8021|lr = 0.00010\n",
      "Epoch: 4775|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 4775|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 4776|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 4776|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 4777|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4777|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 4778|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 4778|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 4779|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 4779|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 4780|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 4780|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 4781|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 4781|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 4782|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 4782|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 4783|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 4783|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7970|lr = 0.00010\n",
      "Epoch: 4784|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4784|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7816|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4785|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4785|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 4786|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 4786|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 4787|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4787|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4788|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7973|lr = 0.00010\n",
      "Epoch: 4788|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 4789|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 4789|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 4790|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7937|lr = 0.00010\n",
      "Epoch: 4790|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 4791|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 4791|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 4792|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 4792|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4793|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 4793|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 4794|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 4794|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 4795|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 4795|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 4796|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 4796|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 4797|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 4797|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 4798|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.8031|lr = 0.00010\n",
      "Epoch: 4798|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 4799|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7973|lr = 0.00010\n",
      "Epoch: 4799|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7993|lr = 0.00010\n",
      "Epoch: 4800|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7911|lr = 0.00010\n",
      "Epoch: 4800|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 4801|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 4801|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 4802|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 4802|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 4803|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7937|lr = 0.00010\n",
      "Epoch: 4803|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 4804|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8034|lr = 0.00010\n",
      "Epoch: 4804|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7945|lr = 0.00010\n",
      "Epoch: 4805|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 4805|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 4806|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 4806|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7983|lr = 0.00010\n",
      "Epoch: 4807|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4807|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 4808|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 4808|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 4809|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 4809|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 4810|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7988|lr = 0.00010\n",
      "Epoch: 4810|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 4811|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4811|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8039|lr = 0.00010\n",
      "Epoch: 4812|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 4812|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 4813|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7380|lr = 0.00010\n",
      "Epoch: 4813|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 4814|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 4814|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 4815|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 4815|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 4816|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 4816|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 4817|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.8034|lr = 0.00010\n",
      "Epoch: 4817|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 4818|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 4818|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4819|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 4819|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.8111|lr = 0.00010\n",
      "Epoch: 4820|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 4820|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8009|lr = 0.00010\n",
      "Epoch: 4821|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 4821|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7939|lr = 0.00010\n",
      "Epoch: 4822|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.8006|lr = 0.00010\n",
      "Epoch: 4822|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 4823|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 4823|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8112|lr = 0.00010\n",
      "Epoch: 4824|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4824|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.8399|lr = 0.00010\n",
      "Epoch: 4825|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8055|lr = 0.00010\n",
      "Epoch: 4825|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 4826|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 4826|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8038|lr = 0.00010\n",
      "Epoch: 4827|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 4827|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8083|lr = 0.00010\n",
      "Epoch: 4828|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 4828|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 4829|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.8051|lr = 0.00010\n",
      "Epoch: 4829|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8049|lr = 0.00010\n",
      "Epoch: 4830|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4830|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 4831|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 4831|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 4832|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 4832|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 4833|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 4833|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7984|lr = 0.00010\n",
      "Epoch: 4834|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4834|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 4835|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 4835|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8043|lr = 0.00010\n",
      "Epoch: 4836|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 4836|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7663|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4837|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 4837|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 4838|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4838|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 4839|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.8171|lr = 0.00010\n",
      "Epoch: 4839|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.8139|lr = 0.00010\n",
      "Epoch: 4840|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 4840|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 4841|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8125|lr = 0.00010\n",
      "Epoch: 4841|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8129|lr = 0.00010\n",
      "Epoch: 4842|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 4842|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 4843|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 4843|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7978|lr = 0.00010\n",
      "Epoch: 4844|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 4844|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 4845|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 4845|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 4846|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 4846|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 4847|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 4847|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 4848|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 4848|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 4849|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 4849|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 4850|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 4850|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8016|lr = 0.00010\n",
      "Epoch: 4851|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.8011|lr = 0.00010\n",
      "Epoch: 4851|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 4852|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8004|lr = 0.00010\n",
      "Epoch: 4852|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.8060|lr = 0.00010\n",
      "Epoch: 4853|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 4853|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 4854|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 4854|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 4855|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 4855|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 4856|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 4856|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 4857|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7247|lr = 0.00010\n",
      "Epoch: 4857|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 4858|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 4858|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 4859|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 4859|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 4860|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 4860|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 4861|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 4861|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 4862|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4862|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 4863|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 4863|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 4864|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 4864|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 4865|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 4865|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 4866|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4866|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 4867|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 4867|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 4868|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 4868|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 4869|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 4869|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 4870|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 4870|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 4871|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 4871|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 4872|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 4872|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 4873|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 4873|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 4874|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 4874|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 4875|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4875|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 4876|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.8123|lr = 0.00010\n",
      "Epoch: 4876|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7945|lr = 0.00010\n",
      "Epoch: 4877|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 4877|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7945|lr = 0.00010\n",
      "Epoch: 4878|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 4878|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8054|lr = 0.00010\n",
      "Epoch: 4879|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 4879|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 4880|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 4880|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 4881|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 4881|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8029|lr = 0.00010\n",
      "Epoch: 4882|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 4882|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 4883|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7934|lr = 0.00010\n",
      "Epoch: 4883|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 4884|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8034|lr = 0.00010\n",
      "Epoch: 4884|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 4885|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 4885|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.8037|lr = 0.00010\n",
      "Epoch: 4886|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7948|lr = 0.00010\n",
      "Epoch: 4886|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 4887|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 4887|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 4888|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 4888|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7818|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4889|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 4889|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 4890|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.8024|lr = 0.00010\n",
      "Epoch: 4890|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 4891|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4891|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 4892|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 4892|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 4893|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 4893|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4894|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4894|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 4895|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7937|lr = 0.00010\n",
      "Epoch: 4895|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 4896|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 4896|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7981|lr = 0.00010\n",
      "Epoch: 4897|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 4897|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 4898|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4898|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 4899|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 4899|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 4900|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4900|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 4901|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 4901|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 4902|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 4902|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 4903|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 4903|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 4904|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7367|lr = 0.00010\n",
      "Epoch: 4904|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 4905|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7968|lr = 0.00010\n",
      "Epoch: 4905|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 4906|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 4906|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 4907|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4907|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 4908|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 4908|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 4909|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 4909|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 4910|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 4910|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 4911|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 4911|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7934|lr = 0.00010\n",
      "Epoch: 4912|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 4912|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 4913|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 4913|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 4914|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7378|lr = 0.00010\n",
      "Epoch: 4914|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 4915|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 4915|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 4916|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 4916|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 4917|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 4917|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7970|lr = 0.00010\n",
      "Epoch: 4918|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 4918|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 4919|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 4919|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 4920|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4920|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 4921|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 4921|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 4922|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 4922|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 4923|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 4923|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 4924|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 4924|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 4925|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 4925|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 4926|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 4926|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.8003|lr = 0.00010\n",
      "Epoch: 4927|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.8236|lr = 0.00010\n",
      "Epoch: 4927|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 4928|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7988|lr = 0.00010\n",
      "Epoch: 4928|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 4929|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 4929|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 4930|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 4930|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 4931|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 4931|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.8006|lr = 0.00010\n",
      "Epoch: 4932|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 4932|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 4933|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 4933|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 4934|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 4934|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 4935|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 4935|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.8120|lr = 0.00010\n",
      "Epoch: 4936|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 4936|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 4937|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4937|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 4938|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 4938|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 4939|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 4939|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 4940|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 4940|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7980|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4941|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 4941|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.8106|lr = 0.00010\n",
      "Epoch: 4942|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 4942|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 4943|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 4943|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 4944|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 4944|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 4945|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 4945|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 4946|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 4946|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7969|lr = 0.00010\n",
      "Epoch: 4947|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 4947|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 4948|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4948|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 4949|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 4949|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8148|lr = 0.00010\n",
      "Epoch: 4950|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 4950|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 4951|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 4951|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 4952|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4952|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 4953|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 4953|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4954|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 4954|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 4955|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 4955|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 4956|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 4956|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 4957|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7914|lr = 0.00010\n",
      "Epoch: 4957|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 4958|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 4958|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 4959|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 4959|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 4960|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7959|lr = 0.00010\n",
      "Epoch: 4960|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7973|lr = 0.00010\n",
      "Epoch: 4961|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 4961|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7958|lr = 0.00010\n",
      "Epoch: 4962|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 4962|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 4963|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 4963|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 4964|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 4964|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 4965|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 4965|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 4966|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8100|lr = 0.00010\n",
      "Epoch: 4966|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7987|lr = 0.00010\n",
      "Epoch: 4967|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 4967|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 4968|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7911|lr = 0.00010\n",
      "Epoch: 4968|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 4969|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 4969|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 4970|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8111|lr = 0.00010\n",
      "Epoch: 4970|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 4971|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 4971|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 4972|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 4972|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7905|lr = 0.00010\n",
      "Epoch: 4973|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 4973|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 4974|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 4974|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 4975|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 4975|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 4976|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 4976|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.8126|lr = 0.00010\n",
      "Epoch: 4977|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7978|lr = 0.00010\n",
      "Epoch: 4977|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 4978|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 4978|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 4979|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 4979|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 4980|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 4980|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 4981|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 4981|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 4982|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 4982|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 4983|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 4983|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 4984|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4984|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 4985|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 4985|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 4986|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 4986|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 4987|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8020|lr = 0.00010\n",
      "Epoch: 4987|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 4988|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 4988|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 4989|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8065|lr = 0.00010\n",
      "Epoch: 4989|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 4990|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 4990|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 4991|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 4991|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 4992|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 4992|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7551|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4993|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.8019|lr = 0.00010\n",
      "Epoch: 4993|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 4994|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 4994|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 4995|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 4995|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 4996|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 4996|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 4997|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 4997|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 4998|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 4998|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 4999|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 4999|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 5000|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 5000|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 5001|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5001|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 5002|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 5002|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 5003|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 5003|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 5004|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5004|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5005|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7970|lr = 0.00010\n",
      "Epoch: 5005|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 5006|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 5006|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5007|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5007|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 5008|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 5008|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 5009|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 5009|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5010|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 5010|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 5011|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.8183|lr = 0.00010\n",
      "Epoch: 5011|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 5012|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 5012|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5013|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 5013|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 5014|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7937|lr = 0.00010\n",
      "Epoch: 5014|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 5015|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 5015|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 5016|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5016|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 5017|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5017|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 5018|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 5018|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5019|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 5019|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 5020|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 5020|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 5021|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 5021|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 5022|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5022|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 5023|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 5023|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5024|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 5024|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 5025|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 5025|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8049|lr = 0.00010\n",
      "Epoch: 5026|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 5026|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 5027|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 5027|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 5028|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.8070|lr = 0.00010\n",
      "Epoch: 5028|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 5029|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7956|lr = 0.00010\n",
      "Epoch: 5029|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 5030|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 5030|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 5031|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5031|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5032|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 5032|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7948|lr = 0.00010\n",
      "Epoch: 5033|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 5033|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 5034|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5034|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 5035|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7914|lr = 0.00010\n",
      "Epoch: 5035|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.8129|lr = 0.00010\n",
      "Epoch: 5036|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5036|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 5037|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 5037|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 5038|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 5038|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 5039|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 5039|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 5040|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5040|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 5041|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 5041|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 5042|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7972|lr = 0.00010\n",
      "Epoch: 5042|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 5043|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5043|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 5044|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7977|lr = 0.00010\n",
      "Epoch: 5044|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7842|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5045|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 5045|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 5046|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 5046|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 5047|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 5047|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 5048|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 5048|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 5049|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7910|lr = 0.00010\n",
      "Epoch: 5049|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 5050|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 5050|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 5051|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 5051|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 5052|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7910|lr = 0.00010\n",
      "Epoch: 5052|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 5053|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 5053|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 5054|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 5054|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 5055|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 5055|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 5056|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7934|lr = 0.00010\n",
      "Epoch: 5056|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5057|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8080|lr = 0.00010\n",
      "Epoch: 5057|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 5058|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 5058|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 5059|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 5059|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 5060|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 5060|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 5061|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 5061|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 5062|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5062|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 5063|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.8137|lr = 0.00010\n",
      "Epoch: 5063|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 5064|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 5064|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 5065|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5065|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5066|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 5066|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 5067|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 5067|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 5068|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 5068|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 5069|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 5069|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 5070|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 5070|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 5071|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 5071|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 5072|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 5072|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 5073|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.8091|lr = 0.00010\n",
      "Epoch: 5073|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5074|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 5074|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 5075|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 5075|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8078|lr = 0.00010\n",
      "Epoch: 5076|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 5076|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 5077|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 5077|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7989|lr = 0.00010\n",
      "Epoch: 5078|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 5078|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 5079|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5079|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 5080|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8027|lr = 0.00010\n",
      "Epoch: 5080|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.8044|lr = 0.00010\n",
      "Epoch: 5081|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7960|lr = 0.00010\n",
      "Epoch: 5081|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 5082|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7981|lr = 0.00010\n",
      "Epoch: 5082|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 5083|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8056|lr = 0.00010\n",
      "Epoch: 5083|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 5084|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 5084|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8104|lr = 0.00010\n",
      "Epoch: 5085|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8044|lr = 0.00010\n",
      "Epoch: 5085|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 5086|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 5086|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 5087|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 5087|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 5088|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 5088|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 5089|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 5089|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 5090|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 5090|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5091|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5091|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7961|lr = 0.00010\n",
      "Epoch: 5092|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 5092|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 5093|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 5093|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 5094|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 5094|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 5095|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8028|lr = 0.00010\n",
      "Epoch: 5095|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 5096|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 5096|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7673|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5097|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 5097|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 5098|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.8058|lr = 0.00010\n",
      "Epoch: 5098|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 5099|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 5099|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 5100|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 5100|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7972|lr = 0.00010\n",
      "Epoch: 5101|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 5101|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 5102|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5102|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 5103|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8109|lr = 0.00010\n",
      "Epoch: 5103|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 5104|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 5104|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 5105|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5105|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 5106|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 5106|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7994|lr = 0.00010\n",
      "Epoch: 5107|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5107|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5108|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 5108|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 5109|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 5109|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5110|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 5110|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 5111|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7999|lr = 0.00010\n",
      "Epoch: 5111|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 5112|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 5112|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 5113|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 5113|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 5114|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 5114|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 5115|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 5115|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 5116|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5116|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 5117|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5117|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 5118|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 5118|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7963|lr = 0.00010\n",
      "Epoch: 5119|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 5119|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8028|lr = 0.00010\n",
      "Epoch: 5120|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5120|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 5121|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 5121|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 5122|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5122|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 5123|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 5123|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5124|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 5124|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.8024|lr = 0.00010\n",
      "Epoch: 5125|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 5125|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 5126|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 5126|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 5127|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 5127|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 5128|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5128|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 5129|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 5129|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 5130|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 5130|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 5131|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 5131|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 5132|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 5132|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 5133|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 5133|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 5134|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 5134|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 5135|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 5135|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.8103|lr = 0.00010\n",
      "Epoch: 5136|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 5136|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 5137|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 5137|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 5138|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 5138|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 5139|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 5139|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 5140|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 5140|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5141|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 5141|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 5142|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5142|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5143|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5143|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 5144|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 5144|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5145|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 5145|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7966|lr = 0.00010\n",
      "Epoch: 5146|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5146|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 5147|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 5147|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 5148|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 5148|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7681|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5149|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5149|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 5150|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 5150|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 5151|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 5151|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 5152|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 5152|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 5153|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 5153|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5154|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 5154|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 5155|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 5155|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 5156|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 5156|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 5157|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 5157|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 5158|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 5158|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 5159|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 5159|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 5160|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.8028|lr = 0.00010\n",
      "Epoch: 5160|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 5161|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 5161|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 5162|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 5162|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5163|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 5163|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 5164|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 5164|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 5165|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 5165|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5166|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8058|lr = 0.00010\n",
      "Epoch: 5166|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 5167|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5167|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.8119|lr = 0.00010\n",
      "Epoch: 5168|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 5168|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7905|lr = 0.00010\n",
      "Epoch: 5169|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 5169|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 5170|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 5170|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8019|lr = 0.00010\n",
      "Epoch: 5171|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7952|lr = 0.00010\n",
      "Epoch: 5171|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5172|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 5172|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5173|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 5173|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 5174|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5174|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 5175|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 5175|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 5176|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 5176|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 5177|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 5177|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7968|lr = 0.00010\n",
      "Epoch: 5178|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 5178|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 5179|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5179|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 5180|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 5180|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 5181|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 5181|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 5182|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 5182|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 5183|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.8037|lr = 0.00010\n",
      "Epoch: 5183|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5184|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 5184|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 5185|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5185|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 5186|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 5186|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 5187|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 5187|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 5188|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 5188|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 5189|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5189|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 5190|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 5190|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7262|lr = 0.00010\n",
      "Epoch: 5191|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 5191|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 5192|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5192|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5193|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 5193|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5194|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 5194|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 5195|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 5195|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 5196|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 5196|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7417|lr = 0.00010\n",
      "Epoch: 5197|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7980|lr = 0.00010\n",
      "Epoch: 5197|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 5198|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 5198|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 5199|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 5199|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 5200|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5200|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7872|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5201|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 5201|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 5202|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 5202|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 5203|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 5203|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 5204|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 5204|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 5205|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8020|lr = 0.00010\n",
      "Epoch: 5205|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 5206|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 5206|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 5207|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5207|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 5208|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5208|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 5209|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 5209|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 5210|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 5210|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5211|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 5211|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7966|lr = 0.00010\n",
      "Epoch: 5212|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5212|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 5213|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 5213|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 5214|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 5214|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 5215|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 5215|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.8063|lr = 0.00010\n",
      "Epoch: 5216|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 5216|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 5217|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 5217|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 5218|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5218|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 5219|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 5219|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 5220|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 5220|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 5221|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 5221|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7949|lr = 0.00010\n",
      "Epoch: 5222|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 5222|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 5223|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5223|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 5224|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5224|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5225|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 5225|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 5226|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 5226|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 5227|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7961|lr = 0.00010\n",
      "Epoch: 5227|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 5228|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 5228|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 5229|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5229|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 5230|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 5230|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 5231|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 5231|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 5232|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5232|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.8122|lr = 0.00010\n",
      "Epoch: 5233|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 5233|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 5234|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 5234|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 5235|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7989|lr = 0.00010\n",
      "Epoch: 5235|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5236|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5236|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5237|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7981|lr = 0.00010\n",
      "Epoch: 5237|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 5238|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 5238|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 5239|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 5239|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 5240|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 5240|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 5241|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5241|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5242|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 5242|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 5243|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5243|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 5244|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 5244|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5245|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 5245|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5246|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5246|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 5247|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 5247|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 5248|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 5248|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 5249|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 5249|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 5250|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 5250|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 5251|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5251|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 5252|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7981|lr = 0.00010\n",
      "Epoch: 5252|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7426|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5253|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 5253|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7365|lr = 0.00010\n",
      "Epoch: 5254|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5254|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 5255|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 5255|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5256|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5256|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5257|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 5257|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 5258|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5258|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 5259|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.8025|lr = 0.00010\n",
      "Epoch: 5259|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 5260|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 5260|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 5261|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5261|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5262|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 5262|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 5263|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 5263|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5264|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 5264|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5265|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 5265|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 5266|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7984|lr = 0.00010\n",
      "Epoch: 5266|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7959|lr = 0.00010\n",
      "Epoch: 5267|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5267|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7974|lr = 0.00010\n",
      "Epoch: 5268|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 5268|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5269|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 5269|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 5270|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5270|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 5271|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5271|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 5272|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5272|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 5273|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 5273|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 5274|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 5274|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 5275|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7398|lr = 0.00010\n",
      "Epoch: 5275|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 5276|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 5276|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 5277|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 5277|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 5278|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 5278|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 5279|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5279|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 5280|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 5280|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 5281|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 5281|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7334|lr = 0.00010\n",
      "Epoch: 5282|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 5282|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 5283|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 5283|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 5284|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5284|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 5285|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 5285|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 5286|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5286|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 5287|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 5287|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 5288|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7980|lr = 0.00010\n",
      "Epoch: 5288|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5289|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 5289|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 5290|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 5290|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 5291|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 5291|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 5292|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5292|steps:   60|Train Avg Loss: 0.0053 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 5293|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 5293|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 5294|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 5294|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.8121|lr = 0.00010\n",
      "Epoch: 5295|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 5295|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 5296|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5296|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5297|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 5297|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 5298|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 5298|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 5299|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 5299|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 5300|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 5300|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 5301|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 5301|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 5302|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 5302|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 5303|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 5303|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 5304|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 5304|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7855|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5305|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 5305|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5306|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7949|lr = 0.00010\n",
      "Epoch: 5306|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5307|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5307|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5308|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7963|lr = 0.00010\n",
      "Epoch: 5308|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 5309|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 5309|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 5310|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7955|lr = 0.00010\n",
      "Epoch: 5310|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 5311|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 5311|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7998|lr = 0.00010\n",
      "Epoch: 5312|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 5312|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5313|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7958|lr = 0.00010\n",
      "Epoch: 5313|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 5314|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5314|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5315|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 5315|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 5316|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 5316|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 5317|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 5317|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 5318|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 5318|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.8035|lr = 0.00010\n",
      "Epoch: 5319|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 5319|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5320|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 5320|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 5321|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 5321|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 5322|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 5322|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 5323|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5323|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 5324|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 5324|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 5325|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5325|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 5326|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 5326|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5327|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 5327|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5328|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5328|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 5329|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 5329|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 5330|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 5330|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 5331|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5331|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 5332|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 5332|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7986|lr = 0.00010\n",
      "Epoch: 5333|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7974|lr = 0.00010\n",
      "Epoch: 5333|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 5334|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 5334|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 5335|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 5335|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 5336|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 5336|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 5337|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 5337|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 5338|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 5338|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5339|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 5339|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 5340|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 5340|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 5341|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 5341|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 5342|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 5342|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 5343|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5343|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5344|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 5344|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 5345|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 5345|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 5346|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 5346|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7943|lr = 0.00010\n",
      "Epoch: 5347|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 5347|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5348|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 5348|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 5349|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 5349|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 5350|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 5350|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5351|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 5351|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5352|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 5352|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 5353|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 5353|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 5354|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 5354|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 5355|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 5355|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 5356|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 5356|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7537|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5357|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 5357|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 5358|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 5358|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 5359|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 5359|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 5360|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 5360|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 5361|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 5361|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 5362|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 5362|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 5363|steps:   30|Train Avg Loss: 0.0048 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 5363|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 5364|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7973|lr = 0.00010\n",
      "Epoch: 5364|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5365|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 5365|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 5366|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 5366|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 5367|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 5367|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5368|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7993|lr = 0.00010\n",
      "Epoch: 5368|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 5369|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 5369|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 5370|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5370|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 5371|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5371|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5372|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5372|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 5373|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 5373|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 5374|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5374|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 5375|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 5375|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 5376|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5376|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 5377|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 5377|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 5378|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 5378|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 5379|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 5379|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 5380|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 5380|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.8190|lr = 0.00010\n",
      "Epoch: 5381|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 5381|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 5382|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 5382|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 5383|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8013|lr = 0.00010\n",
      "Epoch: 5383|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 5384|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 5384|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 5385|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 5385|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 5386|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 5386|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5387|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 5387|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 5388|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 5388|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7993|lr = 0.00010\n",
      "Epoch: 5389|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 5389|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8032|lr = 0.00010\n",
      "Epoch: 5390|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 5390|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5391|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 5391|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5392|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 5392|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5393|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 5393|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5394|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 5394|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 5395|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 5395|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 5396|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 5396|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 5397|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 5397|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 5398|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 5398|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 5399|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 5399|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 5400|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 5400|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 5401|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8004|lr = 0.00010\n",
      "Epoch: 5401|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 5402|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 5402|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 5403|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5403|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 5404|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 5404|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 5405|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 5405|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8033|lr = 0.00010\n",
      "Epoch: 5406|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 5406|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5407|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 5407|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 5408|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5408|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7548|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5409|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 5409|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 5410|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 5410|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 5411|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 5411|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5412|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 5412|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 5413|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7468|lr = 0.00010\n",
      "Epoch: 5413|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 5414|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 5414|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 5415|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5415|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 5416|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 5416|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 5417|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 5417|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 5418|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5418|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 5419|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 5419|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 5420|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 5420|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 5421|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 5421|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 5422|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 5422|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 5423|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 5423|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 5424|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 5424|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5425|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5425|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 5426|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5426|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5427|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 5427|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 5428|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5428|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 5429|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 5429|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 5430|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 5430|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 5431|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5431|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 5432|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 5432|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 5433|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 5433|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5434|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 5434|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 5435|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 5435|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 5436|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 5436|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 5437|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 5437|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5438|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 5438|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 5439|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 5439|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 5440|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 5440|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 5441|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5441|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5442|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 5442|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 5443|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 5443|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5444|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 5444|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5445|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 5445|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 5446|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 5446|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 5447|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 5447|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 5448|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 5448|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 5449|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 5449|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8043|lr = 0.00010\n",
      "Epoch: 5450|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 5450|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 5451|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5451|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8098|lr = 0.00010\n",
      "Epoch: 5452|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 5452|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 5453|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 5453|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 5454|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.8006|lr = 0.00010\n",
      "Epoch: 5454|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5455|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 5455|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 5456|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 5456|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 5457|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5457|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 5458|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5458|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5459|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 5459|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 5460|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 5460|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7724|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5461|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 5461|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 5462|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 5462|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5463|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 5463|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 5464|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 5464|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5465|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5465|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7997|lr = 0.00010\n",
      "Epoch: 5466|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5466|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 5467|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5467|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5468|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 5468|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 5469|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 5469|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5470|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5470|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 5471|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7963|lr = 0.00010\n",
      "Epoch: 5471|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 5472|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 5472|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 5473|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 5473|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 5474|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 5474|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 5475|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 5475|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.8169|lr = 0.00010\n",
      "Epoch: 5476|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.8034|lr = 0.00010\n",
      "Epoch: 5476|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 5477|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 5477|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 5478|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 5478|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 5479|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 5479|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 5480|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5480|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 5481|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 5481|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 5482|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 5482|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7257|lr = 0.00010\n",
      "Epoch: 5483|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 5483|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7353|lr = 0.00010\n",
      "Epoch: 5484|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 5484|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 5485|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 5485|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7417|lr = 0.00010\n",
      "Epoch: 5486|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5486|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7344|lr = 0.00010\n",
      "Epoch: 5487|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 5487|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 5488|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 5488|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5489|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 5489|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 5490|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 5490|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5491|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5491|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5492|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 5492|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 5493|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 5493|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 5494|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 5494|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7989|lr = 0.00010\n",
      "Epoch: 5495|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 5495|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 5496|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 5496|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7914|lr = 0.00010\n",
      "Epoch: 5497|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 5497|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 5498|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 5498|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 5499|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 5499|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 5500|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 5500|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 5501|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 5501|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.8016|lr = 0.00010\n",
      "Epoch: 5502|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8047|lr = 0.00010\n",
      "Epoch: 5502|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 5503|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 5503|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 5504|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 5504|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5505|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5505|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 5506|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7949|lr = 0.00010\n",
      "Epoch: 5506|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 5507|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 5507|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 5508|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 5508|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5509|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 5509|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 5510|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 5510|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.8020|lr = 0.00010\n",
      "Epoch: 5511|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5511|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 5512|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7401|lr = 0.00010\n",
      "Epoch: 5512|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7728|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5513|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 5513|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 5514|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 5514|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 5515|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 5515|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8024|lr = 0.00010\n",
      "Epoch: 5516|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 5516|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5517|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5517|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5518|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.8024|lr = 0.00010\n",
      "Epoch: 5518|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 5519|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 5519|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5520|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 5520|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 5521|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 5521|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 5522|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 5522|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 5523|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 5523|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 5524|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 5524|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 5525|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 5525|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 5526|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5526|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5527|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 5527|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 5528|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.8022|lr = 0.00010\n",
      "Epoch: 5528|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 5529|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5529|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 5530|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 5530|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 5531|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 5531|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 5532|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 5532|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 5533|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 5533|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8083|lr = 0.00010\n",
      "Epoch: 5534|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7948|lr = 0.00010\n",
      "Epoch: 5534|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7966|lr = 0.00010\n",
      "Epoch: 5535|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 5535|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8068|lr = 0.00010\n",
      "Epoch: 5536|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 5536|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5537|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 5537|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 5538|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 5538|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 5539|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5539|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 5540|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 5540|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 5541|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 5541|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 5542|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 5542|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5543|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5543|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 5544|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 5544|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 5545|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 5545|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 5546|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 5546|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 5547|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5547|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 5548|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 5548|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 5549|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7352|lr = 0.00010\n",
      "Epoch: 5549|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 5550|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 5550|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 5551|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7370|lr = 0.00010\n",
      "Epoch: 5551|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 5552|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5552|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 5553|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 5553|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 5554|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 5554|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 5555|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.8096|lr = 0.00010\n",
      "Epoch: 5555|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 5556|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 5556|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 5557|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 5557|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 5558|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5558|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 5559|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 5559|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 5560|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5560|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 5561|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 5561|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 5562|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 5562|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 5563|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 5563|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 5564|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 5564|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7509|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5565|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 5565|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 5566|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8052|lr = 0.00010\n",
      "Epoch: 5566|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 5567|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 5567|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5568|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5568|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8009|lr = 0.00010\n",
      "Epoch: 5569|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 5569|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 5570|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 5570|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 5571|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 5571|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 5572|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5572|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 5573|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5573|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5574|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 5574|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 5575|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 5575|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 5576|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 5576|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 5577|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 5577|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 5578|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 5578|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5579|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 5579|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 5580|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 5580|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 5581|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 5581|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 5582|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5582|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 5583|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 5583|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 5584|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7939|lr = 0.00010\n",
      "Epoch: 5584|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 5585|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5585|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 5586|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 5586|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 5587|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.8013|lr = 0.00010\n",
      "Epoch: 5587|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5588|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 5588|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 5589|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 5589|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 5590|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 5590|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 5591|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5591|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 5592|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 5592|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5593|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 5593|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 5594|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5594|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 5595|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5595|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 5596|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5596|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5597|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 5597|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 5598|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 5598|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5599|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5599|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 5600|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 5600|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 5601|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 5601|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 5602|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 5602|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 5603|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 5603|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 5604|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 5604|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 5605|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 5605|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 5606|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 5606|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 5607|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 5607|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 5608|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 5608|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5609|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 5609|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 5610|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5610|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 5611|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 5611|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 5612|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 5612|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7394|lr = 0.00010\n",
      "Epoch: 5613|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 5613|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 5614|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 5614|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5615|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 5615|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7447|lr = 0.00010\n",
      "Epoch: 5616|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 5616|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7609|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5617|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 5617|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 5618|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 5618|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5619|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5619|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 5620|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 5620|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5621|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 5621|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 5622|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 5622|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 5623|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 5623|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 5624|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5624|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5625|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 5625|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 5626|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 5626|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 5627|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 5627|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 5628|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 5628|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 5629|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7928|lr = 0.00010\n",
      "Epoch: 5629|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 5630|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 5630|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 5631|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 5631|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 5632|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5632|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 5633|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8005|lr = 0.00010\n",
      "Epoch: 5633|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 5634|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 5634|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5635|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8051|lr = 0.00010\n",
      "Epoch: 5635|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 5636|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 5636|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 5637|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 5637|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 5638|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7996|lr = 0.00010\n",
      "Epoch: 5638|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 5639|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 5639|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5640|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 5640|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 5641|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 5641|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 5642|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5642|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 5643|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5643|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8071|lr = 0.00010\n",
      "Epoch: 5644|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 5644|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 5645|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5645|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5646|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 5646|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 5647|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 5647|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 5648|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 5648|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 5649|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 5649|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 5650|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 5650|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 5651|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 5651|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5652|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 5652|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 5653|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 5653|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 5654|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 5654|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 5655|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 5655|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 5656|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 5656|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5657|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5657|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 5658|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 5658|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 5659|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 5659|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 5660|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 5660|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 5661|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 5661|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 5662|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 5662|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 5663|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 5663|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 5664|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 5664|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 5665|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 5665|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 5666|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 5666|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 5667|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 5667|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 5668|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 5668|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7724|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5669|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 5669|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 5670|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 5670|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 5671|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 5671|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 5672|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 5672|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 5673|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 5673|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 5674|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5674|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7501|lr = 0.00010\n",
      "Epoch: 5675|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5675|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 5676|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 5676|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 5677|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 5677|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 5678|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 5678|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 5679|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 5679|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 5680|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 5680|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 5681|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 5681|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 5682|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 5682|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 5683|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 5683|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 5684|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5684|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 5685|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5685|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 5686|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 5686|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 5687|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 5687|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 5688|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 5688|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5689|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 5689|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 5690|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 5690|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 5691|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 5691|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5692|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 5692|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 5693|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5693|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 5694|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 5694|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 5695|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.8019|lr = 0.00010\n",
      "Epoch: 5695|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 5696|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5696|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5697|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5697|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 5698|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7999|lr = 0.00010\n",
      "Epoch: 5698|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7996|lr = 0.00010\n",
      "Epoch: 5699|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 5699|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 5700|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 5700|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 5701|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 5701|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 5702|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 5702|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 5703|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 5703|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 5704|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7905|lr = 0.00010\n",
      "Epoch: 5704|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 5705|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 5705|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 5706|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 5706|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 5707|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 5707|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 5708|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 5708|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 5709|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5709|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5710|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5710|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 5711|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7455|lr = 0.00010\n",
      "Epoch: 5711|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.8118|lr = 0.00010\n",
      "Epoch: 5712|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 5712|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 5713|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 5713|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5714|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 5714|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 5715|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 5715|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8083|lr = 0.00010\n",
      "Epoch: 5716|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 5716|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 5717|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5717|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 5718|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 5718|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 5719|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 5719|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 5720|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 5720|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7722|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5721|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 5721|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 5722|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 5722|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 5723|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 5723|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8033|lr = 0.00010\n",
      "Epoch: 5724|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.8048|lr = 0.00010\n",
      "Epoch: 5724|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7934|lr = 0.00010\n",
      "Epoch: 5725|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 5725|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 5726|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 5726|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7369|lr = 0.00010\n",
      "Epoch: 5727|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7437|lr = 0.00010\n",
      "Epoch: 5727|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 5728|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5728|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 5729|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 5729|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 5730|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 5730|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7951|lr = 0.00010\n",
      "Epoch: 5731|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 5731|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 5732|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 5732|steps:   60|Train Avg Loss: 0.0054 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 5733|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 5733|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 5734|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 5734|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 5735|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 5735|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 5736|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 5736|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 5737|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 5737|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 5738|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 5738|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 5739|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 5739|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 5740|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 5740|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 5741|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 5741|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 5742|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 5742|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7946|lr = 0.00010\n",
      "Epoch: 5743|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 5743|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 5744|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 5744|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 5745|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5745|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 5746|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5746|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 5747|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 5747|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5748|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5748|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 5749|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 5749|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 5750|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 5750|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 5751|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 5751|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5752|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5752|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 5753|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 5753|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 5754|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 5754|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 5755|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 5755|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 5756|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 5756|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 5757|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 5757|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 5758|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 5758|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 5759|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 5759|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5760|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 5760|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 5761|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 5761|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 5762|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 5762|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 5763|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 5763|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 5764|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 5764|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 5765|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 5765|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7377|lr = 0.00010\n",
      "Epoch: 5766|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 5766|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 5767|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 5767|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 5768|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 5768|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 5769|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 5769|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 5770|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 5770|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 5771|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 5771|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 5772|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 5772|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7831|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5773|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.8067|lr = 0.00010\n",
      "Epoch: 5773|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.8037|lr = 0.00010\n",
      "Epoch: 5774|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 5774|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 5775|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5775|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5776|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8111|lr = 0.00010\n",
      "Epoch: 5776|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 5777|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5777|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 5778|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 5778|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5779|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 5779|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 5780|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 5780|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 5781|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 5781|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 5782|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 5782|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 5783|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 5783|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 5784|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 5784|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 5785|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 5785|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 5786|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 5786|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 5787|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 5787|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 5788|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 5788|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 5789|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 5789|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 5790|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 5790|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 5791|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 5791|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 5792|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 5792|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 5793|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 5793|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7501|lr = 0.00010\n",
      "Epoch: 5794|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 5794|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 5795|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 5795|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 5796|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 5796|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 5797|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 5797|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 5798|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 5798|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5799|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 5799|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 5800|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 5800|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 5801|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 5801|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.8087|lr = 0.00010\n",
      "Epoch: 5802|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5802|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 5803|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 5803|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 5804|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 5804|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 5805|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 5805|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 5806|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 5806|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.8046|lr = 0.00010\n",
      "Epoch: 5807|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7928|lr = 0.00010\n",
      "Epoch: 5807|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 5808|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 5808|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 5809|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 5809|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5810|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 5810|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 5811|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 5811|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 5812|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5812|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 5813|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 5813|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 5814|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 5814|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 5815|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5815|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 5816|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 5816|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8042|lr = 0.00010\n",
      "Epoch: 5817|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 5817|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 5818|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 5818|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 5819|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 5819|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 5820|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 5820|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 5821|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8004|lr = 0.00010\n",
      "Epoch: 5821|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 5822|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 5822|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8253|lr = 0.00010\n",
      "Epoch: 5823|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5823|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8070|lr = 0.00010\n",
      "Epoch: 5824|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 5824|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7892|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5825|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 5825|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 5826|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 5826|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 5827|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5827|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7975|lr = 0.00010\n",
      "Epoch: 5828|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 5828|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 5829|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 5829|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5830|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5830|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 5831|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5831|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 5832|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.8035|lr = 0.00010\n",
      "Epoch: 5832|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 5833|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 5833|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 5834|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 5834|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5835|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 5835|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5836|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 5836|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7992|lr = 0.00010\n",
      "Epoch: 5837|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 5837|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 5838|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 5838|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 5839|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 5839|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 5840|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.8035|lr = 0.00010\n",
      "Epoch: 5840|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5841|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 5841|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 5842|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.8063|lr = 0.00010\n",
      "Epoch: 5842|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 5843|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 5843|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 5844|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 5844|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5845|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 5845|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7939|lr = 0.00010\n",
      "Epoch: 5846|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5846|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 5847|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 5847|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7993|lr = 0.00010\n",
      "Epoch: 5848|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 5848|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7987|lr = 0.00010\n",
      "Epoch: 5849|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 5849|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 5850|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 5850|steps:   60|Train Avg Loss: 0.0058 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 5851|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 5851|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7437|lr = 0.00010\n",
      "Epoch: 5852|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 5852|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 5853|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 5853|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 5854|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 5854|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 5855|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 5855|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 5856|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 5856|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5857|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 5857|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5858|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5858|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 5859|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 5859|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 5860|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 5860|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 5861|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 5861|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 5862|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5862|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 5863|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 5863|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 5864|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.8071|lr = 0.00010\n",
      "Epoch: 5864|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 5865|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 5865|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 5866|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 5866|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7996|lr = 0.00010\n",
      "Epoch: 5867|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 5867|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 5868|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 5868|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 5869|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 5869|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 5870|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 5870|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 5871|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5871|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 5872|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 5872|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5873|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 5873|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 5874|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 5874|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5875|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5875|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 5876|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 5876|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7620|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5877|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 5877|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 5878|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 5878|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 5879|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5879|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 5880|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5880|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 5881|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5881|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 5882|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 5882|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 5883|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 5883|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 5884|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 5884|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 5885|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 5885|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 5886|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 5886|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 5887|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5887|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 5888|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 5888|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5889|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 5889|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 5890|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 5890|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 5891|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 5891|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5892|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 5892|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 5893|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 5893|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 5894|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 5894|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 5895|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 5895|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 5896|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 5896|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7981|lr = 0.00010\n",
      "Epoch: 5897|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 5897|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 5898|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 5898|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 5899|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 5899|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 5900|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 5900|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 5901|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 5901|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5902|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5902|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 5903|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 5903|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.8044|lr = 0.00010\n",
      "Epoch: 5904|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 5904|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 5905|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 5905|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 5906|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 5906|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 5907|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 5907|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 5908|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 5908|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5909|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 5909|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 5910|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 5910|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 5911|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 5911|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 5912|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 5912|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 5913|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 5913|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 5914|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 5914|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5915|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 5915|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 5916|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 5916|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 5917|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 5917|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 5918|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 5918|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 5919|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 5919|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 5920|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 5920|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 5921|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 5921|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 5922|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 5922|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 5923|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 5923|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 5924|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 5924|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 5925|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 5925|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 5926|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 5926|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8013|lr = 0.00010\n",
      "Epoch: 5927|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 5927|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 5928|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 5928|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7773|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5929|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 5929|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 5930|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 5930|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5931|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5931|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 5932|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 5932|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 5933|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 5933|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7949|lr = 0.00010\n",
      "Epoch: 5934|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 5934|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 5935|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 5935|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 5936|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 5936|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 5937|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 5937|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7989|lr = 0.00010\n",
      "Epoch: 5938|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 5938|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 5939|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 5939|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 5940|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 5940|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 5941|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 5941|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8018|lr = 0.00010\n",
      "Epoch: 5942|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 5942|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 5943|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 5943|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 5944|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 5944|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 5945|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 5945|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 5946|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 5946|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 5947|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7945|lr = 0.00010\n",
      "Epoch: 5947|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 5948|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8004|lr = 0.00010\n",
      "Epoch: 5948|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.8370|lr = 0.00010\n",
      "Epoch: 5949|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 5949|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7939|lr = 0.00010\n",
      "Epoch: 5950|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.8033|lr = 0.00010\n",
      "Epoch: 5950|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 5951|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 5951|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 5952|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 5952|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5953|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 5953|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 5954|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 5954|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 5955|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 5955|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 5956|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 5956|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 5957|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 5957|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 5958|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 5958|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 5959|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 5959|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8102|lr = 0.00010\n",
      "Epoch: 5960|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 5960|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 5961|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 5961|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 5962|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 5962|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7989|lr = 0.00010\n",
      "Epoch: 5963|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 5963|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 5964|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 5964|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 5965|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 5965|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.8012|lr = 0.00010\n",
      "Epoch: 5966|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 5966|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5967|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 5967|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 5968|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 5968|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.8094|lr = 0.00010\n",
      "Epoch: 5969|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8033|lr = 0.00010\n",
      "Epoch: 5969|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 5970|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 5970|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 5971|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7945|lr = 0.00010\n",
      "Epoch: 5971|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7910|lr = 0.00010\n",
      "Epoch: 5972|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.8009|lr = 0.00010\n",
      "Epoch: 5972|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 5973|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5973|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 5974|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 5974|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 5975|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 5975|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5976|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 5976|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 5977|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 5977|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 5978|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 5978|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 5979|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 5979|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 5980|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 5980|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7528|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5981|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 5981|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 5982|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7353|lr = 0.00010\n",
      "Epoch: 5982|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 5983|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 5983|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 5984|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 5984|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7960|lr = 0.00010\n",
      "Epoch: 5985|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 5985|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 5986|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 5986|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 5987|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 5987|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5988|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 5988|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 5989|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 5989|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 5990|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 5990|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 5991|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 5991|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 5992|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 5992|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 5993|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 5993|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 5994|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 5994|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 5995|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8064|lr = 0.00010\n",
      "Epoch: 5995|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 5996|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 5996|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 5997|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 5997|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 5998|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 5998|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 5999|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 5999|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 6000|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 6000|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 6001|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 6001|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 6002|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 6002|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 6003|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 6003|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 6004|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 6004|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 6005|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 6005|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6006|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 6006|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6007|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 6007|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 6008|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 6008|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 6009|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 6009|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 6010|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 6010|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 6011|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 6011|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7990|lr = 0.00010\n",
      "Epoch: 6012|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 6012|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 6013|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 6013|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 6014|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6014|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 6015|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 6015|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6016|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6016|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 6017|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 6017|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 6018|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 6018|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 6019|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 6019|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 6020|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 6020|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6021|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 6021|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 6022|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 6022|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 6023|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 6023|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 6024|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 6024|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 6025|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6025|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 6026|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6026|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 6027|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 6027|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 6028|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 6028|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 6029|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 6029|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 6030|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 6030|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 6031|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 6031|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6032|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 6032|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7937|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6033|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 6033|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7399|lr = 0.00010\n",
      "Epoch: 6034|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 6034|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 6035|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 6035|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6036|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 6036|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 6037|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 6037|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 6038|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 6038|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 6039|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 6039|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 6040|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6040|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 6041|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 6041|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 6042|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 6042|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 6043|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 6043|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7473|lr = 0.00010\n",
      "Epoch: 6044|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 6044|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 6045|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 6045|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 6046|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 6046|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6047|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 6047|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6048|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6048|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 6049|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 6049|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 6050|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 6050|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6051|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6051|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 6052|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 6052|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 6053|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 6053|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 6054|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 6054|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6055|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 6055|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6056|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 6056|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7368|lr = 0.00010\n",
      "Epoch: 6057|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 6057|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6058|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 6058|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6059|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6059|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6060|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6060|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 6061|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 6061|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 6062|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 6062|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 6063|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 6063|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 6064|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 6064|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 6065|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 6065|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6066|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.8065|lr = 0.00010\n",
      "Epoch: 6066|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 6067|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 6067|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 6068|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 6068|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.8024|lr = 0.00010\n",
      "Epoch: 6069|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 6069|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7940|lr = 0.00010\n",
      "Epoch: 6070|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 6070|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 6071|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 6071|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 6072|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6072|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 6073|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 6073|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6074|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 6074|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 6075|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7985|lr = 0.00010\n",
      "Epoch: 6075|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 6076|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 6076|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 6077|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6077|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 6078|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7968|lr = 0.00010\n",
      "Epoch: 6078|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7928|lr = 0.00010\n",
      "Epoch: 6079|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 6079|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 6080|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 6080|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 6081|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 6081|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7453|lr = 0.00010\n",
      "Epoch: 6082|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 6082|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 6083|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6083|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 6084|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6084|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7777|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6085|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 6085|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 6086|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 6086|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6087|steps:   30|Train Avg Loss: 0.0050 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 6087|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 6088|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7934|lr = 0.00010\n",
      "Epoch: 6088|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 6089|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.8002|lr = 0.00010\n",
      "Epoch: 6089|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 6090|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6090|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 6091|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 6091|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 6092|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 6092|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.8056|lr = 0.00010\n",
      "Epoch: 6093|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 6093|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6094|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 6094|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 6095|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 6095|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 6096|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 6096|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 6097|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 6097|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 6098|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 6098|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 6099|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 6099|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 6100|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 6100|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 6101|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 6101|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 6102|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 6102|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7977|lr = 0.00010\n",
      "Epoch: 6103|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 6103|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6104|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6104|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 6105|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 6105|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 6106|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 6106|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 6107|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7442|lr = 0.00010\n",
      "Epoch: 6107|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.8001|lr = 0.00010\n",
      "Epoch: 6108|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 6108|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 6109|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 6109|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 6110|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8005|lr = 0.00010\n",
      "Epoch: 6110|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 6111|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 6111|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 6112|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6112|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 6113|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 6113|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6114|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 6114|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 6115|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 6115|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 6116|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 6116|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 6117|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 6117|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 6118|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 6118|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 6119|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 6119|steps:   60|Train Avg Loss: 0.0057 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 6120|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 6120|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 6121|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 6121|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6122|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 6122|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 6123|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 6123|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6124|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 6124|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 6125|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 6125|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 6126|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 6126|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 6127|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 6127|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 6128|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 6128|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 6129|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 6129|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6130|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 6130|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 6131|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 6131|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 6132|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 6132|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 6133|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7952|lr = 0.00010\n",
      "Epoch: 6133|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6134|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 6134|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 6135|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 6135|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6136|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 6136|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7844|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6137|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 6137|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6138|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8035|lr = 0.00010\n",
      "Epoch: 6138|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.8021|lr = 0.00010\n",
      "Epoch: 6139|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8056|lr = 0.00010\n",
      "Epoch: 6139|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 6140|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 6140|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.8027|lr = 0.00010\n",
      "Epoch: 6141|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 6141|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 6142|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 6142|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.8081|lr = 0.00010\n",
      "Epoch: 6143|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 6143|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 6144|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 6144|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 6145|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6145|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 6146|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7980|lr = 0.00010\n",
      "Epoch: 6146|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 6147|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 6147|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6148|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 6148|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 6149|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 6149|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.8129|lr = 0.00010\n",
      "Epoch: 6150|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 6150|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6151|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 6151|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7928|lr = 0.00010\n",
      "Epoch: 6152|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 6152|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 6153|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 6153|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 6154|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 6154|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 6155|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6155|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8005|lr = 0.00010\n",
      "Epoch: 6156|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6156|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.8045|lr = 0.00010\n",
      "Epoch: 6157|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 6157|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 6158|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 6158|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 6159|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 6159|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 6160|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 6160|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 6161|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 6161|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 6162|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6162|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 6163|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8032|lr = 0.00010\n",
      "Epoch: 6163|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 6164|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 6164|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 6165|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 6165|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 6166|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6166|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 6167|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 6167|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 6168|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 6168|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 6169|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 6169|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 6170|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 6170|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 6171|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 6171|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 6172|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 6172|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 6173|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 6173|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 6174|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 6174|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 6175|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7981|lr = 0.00010\n",
      "Epoch: 6175|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 6176|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 6176|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7966|lr = 0.00010\n",
      "Epoch: 6177|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7952|lr = 0.00010\n",
      "Epoch: 6177|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 6178|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 6178|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 6179|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 6179|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 6180|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 6180|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 6181|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 6181|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 6182|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7238|lr = 0.00010\n",
      "Epoch: 6182|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6183|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 6183|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 6184|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7928|lr = 0.00010\n",
      "Epoch: 6184|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 6185|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6185|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 6186|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 6186|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 6187|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 6187|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 6188|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 6188|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7709|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6189|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7910|lr = 0.00010\n",
      "Epoch: 6189|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 6190|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.8020|lr = 0.00010\n",
      "Epoch: 6190|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 6191|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 6191|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 6192|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 6192|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 6193|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 6193|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 6194|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 6194|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 6195|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6195|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 6196|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 6196|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 6197|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 6197|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 6198|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 6198|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 6199|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 6199|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 6200|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 6200|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 6201|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 6201|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6202|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 6202|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 6203|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 6203|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 6204|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 6204|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 6205|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 6205|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 6206|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 6206|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6207|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 6207|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 6208|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6208|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 6209|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 6209|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 6210|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 6210|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 6211|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 6211|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 6212|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6212|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 6213|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6213|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 6214|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 6214|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 6215|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7467|lr = 0.00010\n",
      "Epoch: 6215|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 6216|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 6216|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7501|lr = 0.00010\n",
      "Epoch: 6217|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 6217|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7994|lr = 0.00010\n",
      "Epoch: 6218|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 6218|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 6219|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 6219|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 6220|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 6220|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 6221|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 6221|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 6222|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 6222|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 6223|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 6223|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 6224|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 6224|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 6225|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 6225|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 6226|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 6226|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 6227|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 6227|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 6228|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.8147|lr = 0.00010\n",
      "Epoch: 6228|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 6229|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 6229|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6230|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6230|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 6231|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6231|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 6232|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 6232|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 6233|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 6233|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 6234|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 6234|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8040|lr = 0.00010\n",
      "Epoch: 6235|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7911|lr = 0.00010\n",
      "Epoch: 6235|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6236|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 6236|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6237|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 6237|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 6238|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 6238|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 6239|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6239|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6240|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 6240|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7823|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6241|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 6241|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 6242|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 6242|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 6243|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 6243|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 6244|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 6244|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 6245|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6245|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 6246|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6246|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 6247|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 6247|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 6248|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 6248|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 6249|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 6249|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 6250|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 6250|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 6251|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 6251|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 6252|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 6252|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 6253|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 6253|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 6254|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 6254|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 6255|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 6255|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 6256|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6256|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6257|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6257|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 6258|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 6258|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 6259|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 6259|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 6260|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 6260|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 6261|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 6261|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 6262|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 6262|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 6263|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 6263|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 6264|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 6264|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 6265|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 6265|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 6266|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 6266|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 6267|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 6267|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 6268|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 6268|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 6269|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 6269|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 6270|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 6270|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 6271|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 6271|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 6272|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6272|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 6273|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 6273|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 6274|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 6274|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 6275|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 6275|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 6276|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 6276|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6277|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 6277|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 6278|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 6278|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 6279|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 6279|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 6280|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 6280|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 6281|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 6281|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 6282|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6282|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 6283|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 6283|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 6284|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 6284|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 6285|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 6285|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 6286|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 6286|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 6287|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7390|lr = 0.00010\n",
      "Epoch: 6287|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 6288|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 6288|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 6289|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6289|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 6290|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 6290|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 6291|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 6291|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.8064|lr = 0.00010\n",
      "Epoch: 6292|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 6292|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7703|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6293|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 6293|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 6294|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 6294|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 6295|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 6295|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 6296|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 6296|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7440|lr = 0.00010\n",
      "Epoch: 6297|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6297|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 6298|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 6298|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 6299|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6299|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 6300|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6300|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 6301|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 6301|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 6302|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 6302|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 6303|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 6303|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 6304|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 6304|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 6305|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 6305|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 6306|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 6306|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 6307|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 6307|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 6308|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6308|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 6309|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 6309|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 6310|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 6310|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6311|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 6311|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 6312|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6312|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 6313|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 6313|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 6314|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 6314|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6315|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 6315|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 6316|steps:   30|Train Avg Loss: 0.0052 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 6316|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 6317|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7437|lr = 0.00010\n",
      "Epoch: 6317|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 6318|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 6318|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 6319|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6319|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6320|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 6320|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 6321|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 6321|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 6322|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 6322|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 6323|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 6323|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 6324|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 6324|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 6325|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7265|lr = 0.00010\n",
      "Epoch: 6325|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 6326|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6326|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 6327|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 6327|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7467|lr = 0.00010\n",
      "Epoch: 6328|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6328|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7410|lr = 0.00010\n",
      "Epoch: 6329|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 6329|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 6330|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 6330|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 6331|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 6331|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7381|lr = 0.00010\n",
      "Epoch: 6332|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 6332|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 6333|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6333|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 6334|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 6334|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6335|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 6335|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 6336|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 6336|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 6337|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 6337|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 6338|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6338|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 6339|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 6339|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 6340|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 6340|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 6341|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 6341|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 6342|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 6342|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6343|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6343|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 6344|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 6344|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7730|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6345|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 6345|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 6346|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 6346|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 6347|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 6347|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6348|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 6348|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 6349|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 6349|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 6350|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6350|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 6351|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 6351|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 6352|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6352|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 6353|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7312|lr = 0.00010\n",
      "Epoch: 6353|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 6354|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 6354|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 6355|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 6355|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 6356|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 6356|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 6357|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 6357|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 6358|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 6358|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 6359|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 6359|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8076|lr = 0.00010\n",
      "Epoch: 6360|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 6360|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 6361|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 6361|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 6362|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 6362|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 6363|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 6363|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 6364|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 6364|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 6365|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6365|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 6366|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 6366|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 6367|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 6367|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 6368|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 6368|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6369|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 6369|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 6370|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6370|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 6371|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 6371|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 6372|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6372|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 6373|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 6373|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6374|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8024|lr = 0.00010\n",
      "Epoch: 6374|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 6375|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6375|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 6376|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 6376|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 6377|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 6377|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 6378|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 6378|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6379|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 6379|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 6380|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6380|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7952|lr = 0.00010\n",
      "Epoch: 6381|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 6381|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7997|lr = 0.00010\n",
      "Epoch: 6382|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6382|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 6383|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6383|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 6384|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 6384|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6385|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 6385|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7969|lr = 0.00010\n",
      "Epoch: 6386|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 6386|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 6387|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8031|lr = 0.00010\n",
      "Epoch: 6387|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 6388|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8008|lr = 0.00010\n",
      "Epoch: 6388|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.8003|lr = 0.00010\n",
      "Epoch: 6389|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 6389|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 6390|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6390|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 6391|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7977|lr = 0.00010\n",
      "Epoch: 6391|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7958|lr = 0.00010\n",
      "Epoch: 6392|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6392|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 6393|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 6393|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 6394|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 6394|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 6395|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 6395|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 6396|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 6396|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7850|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6397|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 6397|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 6398|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 6398|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 6399|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 6399|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 6400|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 6400|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 6401|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6401|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 6402|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 6402|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 6403|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6403|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 6404|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6404|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6405|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 6405|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 6406|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 6406|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6407|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 6407|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 6408|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 6408|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 6409|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6409|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6410|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 6410|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 6411|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 6411|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 6412|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 6412|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 6413|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6413|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 6414|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 6414|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 6415|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 6415|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 6416|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 6416|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 6417|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 6417|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 6418|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 6418|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 6419|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 6419|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7326|lr = 0.00010\n",
      "Epoch: 6420|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 6420|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 6421|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 6421|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 6422|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 6422|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 6423|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 6423|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7390|lr = 0.00010\n",
      "Epoch: 6424|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 6424|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 6425|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 6425|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 6426|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 6426|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 6427|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 6427|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6428|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 6428|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 6429|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 6429|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 6430|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 6430|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 6431|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 6431|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 6432|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 6432|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 6433|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 6433|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 6434|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6434|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 6435|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 6435|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 6436|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 6436|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 6437|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 6437|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 6438|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 6438|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 6439|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 6439|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 6440|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 6440|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7461|lr = 0.00010\n",
      "Epoch: 6441|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7364|lr = 0.00010\n",
      "Epoch: 6441|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7433|lr = 0.00010\n",
      "Epoch: 6442|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 6442|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6443|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6443|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 6444|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 6444|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 6445|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 6445|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6446|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 6446|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7387|lr = 0.00010\n",
      "Epoch: 6447|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 6447|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 6448|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 6448|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7737|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6449|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 6449|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 6450|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 6450|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 6451|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 6451|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 6452|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8014|lr = 0.00010\n",
      "Epoch: 6452|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6453|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6453|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 6454|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 6454|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 6455|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 6455|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6456|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 6456|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 6457|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6457|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 6458|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 6458|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 6459|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 6459|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 6460|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 6460|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 6461|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6461|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7910|lr = 0.00010\n",
      "Epoch: 6462|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 6462|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 6463|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7998|lr = 0.00010\n",
      "Epoch: 6463|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6464|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 6464|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 6465|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 6465|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 6466|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 6466|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 6467|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 6467|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 6468|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 6468|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 6469|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7977|lr = 0.00010\n",
      "Epoch: 6469|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 6470|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 6470|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 6471|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 6471|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 6472|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7923|lr = 0.00010\n",
      "Epoch: 6472|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 6473|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 6473|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 6474|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 6474|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 6475|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 6475|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6476|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 6476|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 6477|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 6477|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 6478|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 6478|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6479|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 6479|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 6480|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 6480|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 6481|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6481|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 6482|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 6482|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7374|lr = 0.00010\n",
      "Epoch: 6483|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 6483|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6484|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 6484|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 6485|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 6485|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 6486|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6486|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 6487|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 6487|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 6488|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 6488|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 6489|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 6489|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 6490|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 6490|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 6491|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6491|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 6492|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 6492|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 6493|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 6493|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 6494|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 6494|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 6495|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 6495|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 6496|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 6496|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 6497|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 6497|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 6498|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 6498|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7510|lr = 0.00010\n",
      "Epoch: 6499|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 6499|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 6500|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7453|lr = 0.00010\n",
      "Epoch: 6500|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7729|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6501|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 6501|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 6502|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 6502|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 6503|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8029|lr = 0.00010\n",
      "Epoch: 6503|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 6504|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6504|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 6505|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 6505|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 6506|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 6506|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 6507|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6507|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6508|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6508|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 6509|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 6509|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 6510|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 6510|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 6511|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 6511|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 6512|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 6512|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 6513|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 6513|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 6514|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 6514|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 6515|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 6515|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 6516|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 6516|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 6517|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 6517|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 6518|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 6518|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6519|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 6519|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 6520|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6520|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7990|lr = 0.00010\n",
      "Epoch: 6521|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6521|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 6522|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 6522|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 6523|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 6523|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6524|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 6524|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 6525|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 6525|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 6526|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 6526|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 6527|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 6527|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 6528|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 6528|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7972|lr = 0.00010\n",
      "Epoch: 6529|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 6529|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 6530|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7930|lr = 0.00010\n",
      "Epoch: 6530|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 6531|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 6531|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 6532|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 6532|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6533|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.8108|lr = 0.00010\n",
      "Epoch: 6533|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6534|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 6534|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 6535|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 6535|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 6536|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 6536|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 6537|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 6537|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 6538|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 6538|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 6539|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 6539|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 6540|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 6540|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 6541|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6541|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8137|lr = 0.00010\n",
      "Epoch: 6542|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 6542|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 6543|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6543|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8046|lr = 0.00010\n",
      "Epoch: 6544|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 6544|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6545|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6545|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 6546|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 6546|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 6547|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 6547|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6548|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6548|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 6549|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 6549|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 6550|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 6550|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 6551|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 6551|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 6552|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6552|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7860|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6553|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 6553|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 6554|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 6554|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 6555|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 6555|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 6556|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 6556|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 6557|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 6557|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.8084|lr = 0.00010\n",
      "Epoch: 6558|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 6558|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 6559|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 6559|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 6560|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 6560|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 6561|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 6561|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6562|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 6562|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6563|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 6563|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 6564|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6564|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6565|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6565|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 6566|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 6566|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7426|lr = 0.00010\n",
      "Epoch: 6567|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6567|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 6568|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 6568|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 6569|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 6569|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 6570|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 6570|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 6571|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 6571|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 6572|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 6572|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 6573|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 6573|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 6574|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 6574|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 6575|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 6575|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 6576|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 6576|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 6577|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6577|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 6578|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 6578|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 6579|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 6579|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 6580|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 6580|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 6581|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 6581|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 6582|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 6582|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7404|lr = 0.00010\n",
      "Epoch: 6583|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 6583|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6584|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.8058|lr = 0.00010\n",
      "Epoch: 6584|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 6585|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6585|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 6586|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 6586|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 6587|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 6587|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 6588|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 6588|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 6589|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 6589|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 6590|steps:   30|Train Avg Loss: 0.0054 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 6590|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 6591|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6591|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 6592|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6592|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 6593|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6593|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 6594|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 6594|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6595|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6595|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 6596|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 6596|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6597|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 6597|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 6598|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.8051|lr = 0.00010\n",
      "Epoch: 6598|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 6599|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6599|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 6600|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 6600|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 6601|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6601|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 6602|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 6602|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 6603|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 6603|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 6604|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6604|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7400|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6605|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6605|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 6606|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 6606|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7948|lr = 0.00010\n",
      "Epoch: 6607|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 6607|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6608|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 6608|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 6609|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 6609|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 6610|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 6610|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6611|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 6611|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 6612|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 6612|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 6613|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 6613|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 6614|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 6614|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6615|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 6615|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8014|lr = 0.00010\n",
      "Epoch: 6616|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 6616|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 6617|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 6617|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 6618|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 6618|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 6619|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 6619|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 6620|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 6620|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 6621|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6621|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 6622|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6622|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 6623|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 6623|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6624|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 6624|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6625|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 6625|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 6626|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 6626|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 6627|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 6627|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 6628|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 6628|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 6629|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 6629|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 6630|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 6630|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 6631|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 6631|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 6632|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 6632|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 6633|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 6633|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 6634|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 6634|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 6635|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 6635|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 6636|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 6636|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 6637|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 6637|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 6638|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 6638|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 6639|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 6639|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 6640|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 6640|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 6641|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 6641|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6642|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 6642|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 6643|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 6643|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 6644|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 6644|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 6645|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 6645|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 6646|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6646|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 6647|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 6647|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 6648|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 6648|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 6649|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 6649|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6650|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6650|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6651|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 6651|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 6652|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7992|lr = 0.00010\n",
      "Epoch: 6652|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 6653|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 6653|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 6654|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6654|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 6655|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 6655|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 6656|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 6656|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7593|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6657|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6657|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 6658|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 6658|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6659|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 6659|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 6660|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 6660|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 6661|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 6661|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 6662|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 6662|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 6663|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6663|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 6664|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 6664|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 6665|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 6665|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 6666|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 6666|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 6667|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 6667|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 6668|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 6668|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6669|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 6669|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 6670|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 6670|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 6671|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6671|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 6672|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6672|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 6673|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 6673|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 6674|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6674|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 6675|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 6675|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6676|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6676|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 6677|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 6677|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 6678|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6678|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6679|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 6679|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 6680|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 6680|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 6681|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 6681|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 6682|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6682|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 6683|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6683|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6684|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 6684|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 6685|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6685|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 6686|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 6686|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 6687|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 6687|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 6688|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 6688|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 6689|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6689|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6690|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 6690|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 6691|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 6691|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6692|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 6692|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.8025|lr = 0.00010\n",
      "Epoch: 6693|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 6693|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 6694|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6694|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 6695|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6695|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 6696|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7362|lr = 0.00010\n",
      "Epoch: 6696|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7260|lr = 0.00010\n",
      "Epoch: 6697|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6697|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 6698|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 6698|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6699|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 6699|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7984|lr = 0.00010\n",
      "Epoch: 6700|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 6700|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 6701|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 6701|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 6702|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 6702|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 6703|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 6703|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 6704|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7968|lr = 0.00010\n",
      "Epoch: 6704|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6705|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6705|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 6706|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 6706|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 6707|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 6707|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 6708|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 6708|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7912|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6709|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8045|lr = 0.00010\n",
      "Epoch: 6709|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6710|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 6710|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 6711|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 6711|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 6712|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 6712|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7989|lr = 0.00010\n",
      "Epoch: 6713|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 6713|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 6714|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 6714|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 6715|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 6715|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 6716|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6716|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 6717|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 6717|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 6718|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 6718|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 6719|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 6719|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 6720|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 6720|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 6721|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 6721|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 6722|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 6722|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 6723|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 6723|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 6724|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6724|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 6725|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6725|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 6726|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 6726|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 6727|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 6727|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 6728|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6728|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 6729|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 6729|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 6730|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 6730|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 6731|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 6731|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 6732|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6732|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 6733|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 6733|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 6734|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 6734|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 6735|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6735|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 6736|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 6736|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 6737|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 6737|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 6738|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 6738|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 6739|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6739|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 6740|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 6740|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 6741|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 6741|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6742|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 6742|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 6743|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 6743|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 6744|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 6744|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6745|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 6745|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 6746|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 6746|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 6747|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7368|lr = 0.00010\n",
      "Epoch: 6747|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 6748|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 6748|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 6749|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 6749|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 6750|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 6750|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 6751|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6751|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 6752|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 6752|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 6753|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 6753|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 6754|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 6754|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 6755|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 6755|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 6756|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 6756|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 6757|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 6757|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 6758|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 6758|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 6759|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 6759|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 6760|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 6760|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7640|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6761|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6761|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 6762|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 6762|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 6763|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 6763|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7178|lr = 0.00010\n",
      "Epoch: 6764|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 6764|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 6765|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6765|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 6766|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 6766|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 6767|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 6767|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 6768|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 6768|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 6769|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6769|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 6770|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6770|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 6771|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 6771|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 6772|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 6772|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 6773|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 6773|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 6774|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 6774|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 6775|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 6775|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 6776|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 6776|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 6777|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 6777|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 6778|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6778|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 6779|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6779|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 6780|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 6780|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 6781|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 6781|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 6782|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6782|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 6783|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 6783|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 6784|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 6784|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 6785|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7391|lr = 0.00010\n",
      "Epoch: 6785|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7436|lr = 0.00010\n",
      "Epoch: 6786|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 6786|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 6787|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7492|lr = 0.00010\n",
      "Epoch: 6787|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 6788|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 6788|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 6789|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 6789|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6790|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 6790|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 6791|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 6791|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 6792|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7965|lr = 0.00010\n",
      "Epoch: 6792|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 6793|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 6793|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6794|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6794|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6795|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 6795|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 6796|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 6796|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 6797|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 6797|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 6798|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 6798|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 6799|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 6799|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 6800|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 6800|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 6801|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 6801|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 6802|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 6802|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.8103|lr = 0.00010\n",
      "Epoch: 6803|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 6803|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 6804|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 6804|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 6805|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 6805|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 6806|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 6806|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7968|lr = 0.00010\n",
      "Epoch: 6807|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 6807|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 6808|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 6808|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 6809|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 6809|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 6810|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 6810|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6811|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6811|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 6812|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6812|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7850|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6813|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 6813|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 6814|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8135|lr = 0.00010\n",
      "Epoch: 6814|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6815|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6815|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 6816|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 6816|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 6817|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 6817|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 6818|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 6818|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 6819|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 6819|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6820|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 6820|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 6821|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 6821|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 6822|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7473|lr = 0.00010\n",
      "Epoch: 6822|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 6823|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6823|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 6824|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 6824|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 6825|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 6825|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6826|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 6826|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 6827|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 6827|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 6828|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 6828|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 6829|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 6829|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6830|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 6830|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 6831|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7976|lr = 0.00010\n",
      "Epoch: 6831|steps:   60|Train Avg Loss: 0.0047 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 6832|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 6832|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 6833|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 6833|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6834|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 6834|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 6835|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 6835|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6836|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 6836|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 6837|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6837|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 6838|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 6838|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 6839|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 6839|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 6840|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 6840|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 6841|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 6841|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6842|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 6842|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 6843|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 6843|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 6844|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6844|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7992|lr = 0.00010\n",
      "Epoch: 6845|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 6845|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 6846|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 6846|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 6847|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 6847|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 6848|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 6848|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 6849|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 6849|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 6850|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 6850|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7501|lr = 0.00010\n",
      "Epoch: 6851|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 6851|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 6852|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 6852|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 6853|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 6853|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 6854|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 6854|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 6855|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 6855|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6856|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8032|lr = 0.00010\n",
      "Epoch: 6856|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.8064|lr = 0.00010\n",
      "Epoch: 6857|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 6857|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 6858|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 6858|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 6859|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 6859|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6860|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6860|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 6861|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6861|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 6862|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 6862|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 6863|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 6863|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 6864|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 6864|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7826|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6865|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 6865|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7975|lr = 0.00010\n",
      "Epoch: 6866|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7973|lr = 0.00010\n",
      "Epoch: 6866|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 6867|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 6867|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 6868|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 6868|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 6869|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 6869|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 6870|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 6870|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 6871|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 6871|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 6872|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 6872|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 6873|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 6873|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 6874|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 6874|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7956|lr = 0.00010\n",
      "Epoch: 6875|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 6875|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 6876|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 6876|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6877|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 6877|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 6878|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 6878|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 6879|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 6879|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 6880|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 6880|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 6881|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 6881|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 6882|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7878|lr = 0.00010\n",
      "Epoch: 6882|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.8109|lr = 0.00010\n",
      "Epoch: 6883|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 6883|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 6884|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 6884|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 6885|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 6885|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 6886|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 6886|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 6887|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 6887|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 6888|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 6888|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 6889|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 6889|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 6890|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 6890|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 6891|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 6891|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 6892|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 6892|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 6893|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6893|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 6894|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 6894|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 6895|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 6895|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6896|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 6896|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 6897|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 6897|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 6898|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 6898|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 6899|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6899|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 6900|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7954|lr = 0.00010\n",
      "Epoch: 6900|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.8141|lr = 0.00010\n",
      "Epoch: 6901|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 6901|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.8031|lr = 0.00010\n",
      "Epoch: 6902|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6902|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 6903|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 6903|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 6904|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8060|lr = 0.00010\n",
      "Epoch: 6904|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 6905|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6905|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 6906|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 6906|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 6907|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6907|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 6908|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8009|lr = 0.00010\n",
      "Epoch: 6908|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 6909|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.8054|lr = 0.00010\n",
      "Epoch: 6909|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7937|lr = 0.00010\n",
      "Epoch: 6910|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 6910|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6911|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 6911|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 6912|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 6912|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 6913|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 6913|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 6914|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6914|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6915|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 6915|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 6916|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 6916|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7888|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6917|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 6917|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.8037|lr = 0.00010\n",
      "Epoch: 6918|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 6918|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 6919|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 6919|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 6920|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 6920|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 6921|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 6921|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 6922|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 6922|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 6923|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 6923|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 6924|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 6924|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 6925|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 6925|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 6926|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 6926|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 6927|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6927|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 6928|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7276|lr = 0.00010\n",
      "Epoch: 6928|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6929|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 6929|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 6930|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 6930|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 6931|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 6931|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 6932|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 6932|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 6933|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 6933|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6934|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 6934|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 6935|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 6935|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 6936|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 6936|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 6937|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 6937|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8008|lr = 0.00010\n",
      "Epoch: 6938|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 6938|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8127|lr = 0.00010\n",
      "Epoch: 6939|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 6939|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6940|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 6940|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.8075|lr = 0.00010\n",
      "Epoch: 6941|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 6941|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 6942|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 6942|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 6943|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 6943|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 6944|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 6944|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 6945|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 6945|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 6946|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 6946|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 6947|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 6947|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6948|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8017|lr = 0.00010\n",
      "Epoch: 6948|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 6949|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7964|lr = 0.00010\n",
      "Epoch: 6949|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 6950|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 6950|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 6951|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 6951|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7993|lr = 0.00010\n",
      "Epoch: 6952|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 6952|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 6953|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 6953|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 6954|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6954|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 6955|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 6955|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 6956|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 6956|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 6957|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8023|lr = 0.00010\n",
      "Epoch: 6957|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7978|lr = 0.00010\n",
      "Epoch: 6958|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 6958|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 6959|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 6959|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 6960|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 6960|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6961|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 6961|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 6962|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 6962|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7983|lr = 0.00010\n",
      "Epoch: 6963|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 6963|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 6964|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 6964|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 6965|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 6965|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 6966|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7995|lr = 0.00010\n",
      "Epoch: 6966|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 6967|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 6967|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 6968|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 6968|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7929|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6969|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 6969|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 6970|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 6970|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 6971|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 6971|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 6972|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 6972|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 6973|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 6973|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 6974|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 6974|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 6975|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 6975|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 6976|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 6976|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 6977|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 6977|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 6978|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 6978|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 6979|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 6979|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.8035|lr = 0.00010\n",
      "Epoch: 6980|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 6980|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 6981|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 6981|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 6982|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 6982|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 6983|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 6983|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 6984|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8029|lr = 0.00010\n",
      "Epoch: 6984|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 6985|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 6985|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 6986|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 6986|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 6987|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 6987|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 6988|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 6988|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 6989|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 6989|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 6990|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 6990|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7961|lr = 0.00010\n",
      "Epoch: 6991|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7976|lr = 0.00010\n",
      "Epoch: 6991|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 6992|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.8099|lr = 0.00010\n",
      "Epoch: 6992|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 6993|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 6993|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.8006|lr = 0.00010\n",
      "Epoch: 6994|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.8041|lr = 0.00010\n",
      "Epoch: 6994|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 6995|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 6995|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.8053|lr = 0.00010\n",
      "Epoch: 6996|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 6996|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 6997|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 6997|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 6998|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 6998|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 6999|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 6999|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 7000|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 7000|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 7001|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 7001|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 7002|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 7002|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 7003|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 7003|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7004|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7004|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7005|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7428|lr = 0.00010\n",
      "Epoch: 7005|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 7006|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7006|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 7007|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 7007|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 7008|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7008|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 7009|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 7009|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7010|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7376|lr = 0.00010\n",
      "Epoch: 7010|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 7011|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 7011|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 7012|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 7012|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7013|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 7013|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 7014|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7997|lr = 0.00010\n",
      "Epoch: 7014|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 7015|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 7015|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 7016|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 7016|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7017|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 7017|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 7018|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 7018|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 7019|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 7019|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7453|lr = 0.00010\n",
      "Epoch: 7020|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 7020|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7582|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7021|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 7021|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7022|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 7022|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7454|lr = 0.00010\n",
      "Epoch: 7023|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7023|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 7024|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 7024|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 7025|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 7025|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 7026|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 7026|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7027|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 7027|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7912|lr = 0.00010\n",
      "Epoch: 7028|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 7028|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 7029|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 7029|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 7030|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 7030|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 7031|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 7031|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 7032|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 7032|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 7033|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 7033|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 7034|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.8098|lr = 0.00010\n",
      "Epoch: 7034|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 7035|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 7035|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 7036|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 7036|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7037|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 7037|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 7038|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 7038|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 7039|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 7039|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 7040|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 7040|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 7041|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 7041|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7042|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 7042|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7043|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 7043|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 7044|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7044|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 7045|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 7045|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7046|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 7046|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 7047|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 7047|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 7048|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 7048|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7049|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 7049|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 7050|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 7050|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 7051|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 7051|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7974|lr = 0.00010\n",
      "Epoch: 7052|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 7052|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7053|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8038|lr = 0.00010\n",
      "Epoch: 7053|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 7054|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7054|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7055|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 7055|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 7056|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 7056|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 7057|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7996|lr = 0.00010\n",
      "Epoch: 7057|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 7058|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 7058|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7059|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7059|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7896|lr = 0.00010\n",
      "Epoch: 7060|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7060|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7061|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7061|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 7062|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 7062|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 7063|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7933|lr = 0.00010\n",
      "Epoch: 7063|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 7064|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8026|lr = 0.00010\n",
      "Epoch: 7064|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 7065|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 7065|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 7066|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7066|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7966|lr = 0.00010\n",
      "Epoch: 7067|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 7067|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 7068|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 7068|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 7069|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 7069|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.8075|lr = 0.00010\n",
      "Epoch: 7070|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 7070|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7994|lr = 0.00010\n",
      "Epoch: 7071|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 7071|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 7072|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.8106|lr = 0.00010\n",
      "Epoch: 7072|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7847|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7073|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 7073|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 7074|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 7074|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 7075|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7075|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7955|lr = 0.00010\n",
      "Epoch: 7076|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 7076|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 7077|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 7077|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7078|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7078|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7959|lr = 0.00010\n",
      "Epoch: 7079|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 7079|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 7080|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 7080|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7081|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.8120|lr = 0.00010\n",
      "Epoch: 7081|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7082|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7082|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7083|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7976|lr = 0.00010\n",
      "Epoch: 7083|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 7084|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 7084|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7938|lr = 0.00010\n",
      "Epoch: 7085|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 7085|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 7086|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 7086|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7087|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 7087|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 7088|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 7088|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 7089|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 7089|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7968|lr = 0.00010\n",
      "Epoch: 7090|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.8011|lr = 0.00010\n",
      "Epoch: 7090|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7091|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 7091|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7092|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 7092|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 7093|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 7093|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 7094|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 7094|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 7095|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 7095|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7967|lr = 0.00010\n",
      "Epoch: 7096|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 7096|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 7097|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7097|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 7098|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 7098|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 7099|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 7099|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 7100|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 7100|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 7101|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 7101|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 7102|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7102|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 7103|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 7103|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 7104|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 7104|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 7105|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7105|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7106|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7106|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7908|lr = 0.00010\n",
      "Epoch: 7107|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 7107|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 7108|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 7108|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 7109|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 7109|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 7110|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 7110|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 7111|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 7111|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7112|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 7112|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 7113|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 7113|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 7114|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 7114|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 7115|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 7115|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 7116|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 7116|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 7117|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 7117|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 7118|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7943|lr = 0.00010\n",
      "Epoch: 7118|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7119|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 7119|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 7120|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 7120|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 7121|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 7121|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.8006|lr = 0.00010\n",
      "Epoch: 7122|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8046|lr = 0.00010\n",
      "Epoch: 7122|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 7123|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 7123|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7124|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 7124|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7870|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7125|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7125|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7126|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8052|lr = 0.00010\n",
      "Epoch: 7126|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7959|lr = 0.00010\n",
      "Epoch: 7127|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 7127|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 7128|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 7128|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 7129|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 7129|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7885|lr = 0.00010\n",
      "Epoch: 7130|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 7130|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 7131|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 7131|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 7132|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 7132|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 7133|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 7133|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7134|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 7134|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 7135|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 7135|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 7136|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 7136|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 7137|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 7137|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 7138|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 7138|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7944|lr = 0.00010\n",
      "Epoch: 7139|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 7139|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7140|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 7140|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 7141|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7141|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 7142|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 7142|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 7143|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7867|lr = 0.00010\n",
      "Epoch: 7143|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 7144|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 7144|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 7145|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 7145|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 7146|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 7146|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 7147|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 7147|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7148|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 7148|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 7149|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 7149|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 7150|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 7150|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 7151|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7151|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 7152|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 7152|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 7153|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 7153|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 7154|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 7154|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 7155|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 7155|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 7156|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 7156|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 7157|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 7157|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 7158|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 7158|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 7159|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7159|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 7160|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 7160|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 7161|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 7161|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7162|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 7162|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7163|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 7163|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 7164|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 7164|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 7165|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 7165|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 7166|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 7166|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 7167|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7167|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7168|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 7168|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7473|lr = 0.00010\n",
      "Epoch: 7169|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 7169|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 7170|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8137|lr = 0.00010\n",
      "Epoch: 7170|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 7171|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7171|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 7172|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 7172|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 7173|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 7173|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 7174|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 7174|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7175|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 7175|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 7176|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 7176|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7811|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7177|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 7177|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 7178|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7178|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7179|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 7179|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 7180|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 7180|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7939|lr = 0.00010\n",
      "Epoch: 7181|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7181|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7379|lr = 0.00010\n",
      "Epoch: 7182|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 7182|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 7183|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 7183|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 7184|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 7184|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 7185|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8006|lr = 0.00010\n",
      "Epoch: 7185|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 7186|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7186|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 7187|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 7187|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 7188|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 7188|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 7189|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 7189|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 7190|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 7190|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 7191|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 7191|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 7192|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 7192|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7918|lr = 0.00010\n",
      "Epoch: 7193|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 7193|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7924|lr = 0.00010\n",
      "Epoch: 7194|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 7194|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7195|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7195|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 7196|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7196|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 7197|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7197|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 7198|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 7198|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 7199|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7199|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 7200|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 7200|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 7201|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 7201|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 7202|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 7202|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 7203|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 7203|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 7204|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 7204|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 7205|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 7205|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 7206|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 7206|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 7207|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7207|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 7208|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 7208|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 7209|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 7209|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 7210|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 7210|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.8029|lr = 0.00010\n",
      "Epoch: 7211|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 7211|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7212|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 7212|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 7213|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 7213|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 7214|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7214|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 7215|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 7215|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 7216|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7216|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7217|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 7217|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 7218|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 7218|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 7219|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 7219|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 7220|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 7220|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 7221|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 7221|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 7222|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 7222|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 7223|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 7223|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 7224|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 7224|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 7225|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 7225|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 7226|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7226|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 7227|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 7227|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 7228|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 7228|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7612|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7229|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 7229|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7230|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 7230|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7231|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7231|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 7232|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 7232|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 7233|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 7233|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7234|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 7234|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 7235|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 7235|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 7236|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 7236|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 7237|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 7237|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.8091|lr = 0.00010\n",
      "Epoch: 7238|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 7238|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7239|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 7239|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 7240|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 7240|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 7241|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7241|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 7242|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7872|lr = 0.00010\n",
      "Epoch: 7242|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.8047|lr = 0.00010\n",
      "Epoch: 7243|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 7243|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 7244|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 7244|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7972|lr = 0.00010\n",
      "Epoch: 7245|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 7245|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.8009|lr = 0.00010\n",
      "Epoch: 7246|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 7246|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 7247|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 7247|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 7248|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 7248|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7868|lr = 0.00010\n",
      "Epoch: 7249|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7249|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7937|lr = 0.00010\n",
      "Epoch: 7250|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 7250|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 7251|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 7251|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8064|lr = 0.00010\n",
      "Epoch: 7252|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 7252|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 7253|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7253|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 7254|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 7254|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 7255|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 7255|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 7256|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 7256|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 7257|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7257|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 7258|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 7258|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7259|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 7259|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 7260|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8010|lr = 0.00010\n",
      "Epoch: 7260|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 7261|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7261|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 7262|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 7262|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 7263|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 7263|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 7264|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 7264|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 7265|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 7265|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 7266|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 7266|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7267|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7960|lr = 0.00010\n",
      "Epoch: 7267|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 7268|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 7268|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 7269|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7269|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7270|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 7270|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 7271|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7271|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7272|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 7272|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7273|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 7273|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 7274|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 7274|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 7275|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7961|lr = 0.00010\n",
      "Epoch: 7275|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7276|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7981|lr = 0.00010\n",
      "Epoch: 7276|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 7277|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 7277|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 7278|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 7278|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 7279|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 7279|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7280|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 7280|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7751|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7281|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 7281|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 7282|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 7282|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7283|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 7283|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7284|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 7284|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 7285|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7285|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 7286|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 7286|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 7287|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.8013|lr = 0.00010\n",
      "Epoch: 7287|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 7288|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7963|lr = 0.00010\n",
      "Epoch: 7288|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7289|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7973|lr = 0.00010\n",
      "Epoch: 7289|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 7290|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 7290|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 7291|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.8038|lr = 0.00010\n",
      "Epoch: 7291|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7909|lr = 0.00010\n",
      "Epoch: 7292|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 7292|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 7293|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 7293|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 7294|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 7294|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 7295|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 7295|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 7296|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 7296|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 7297|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 7297|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 7298|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7298|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 7299|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7931|lr = 0.00010\n",
      "Epoch: 7299|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 7300|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 7300|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 7301|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7301|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 7302|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 7302|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7303|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 7303|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7304|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 7304|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7305|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 7305|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7306|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7306|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.8003|lr = 0.00010\n",
      "Epoch: 7307|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7958|lr = 0.00010\n",
      "Epoch: 7307|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 7308|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7308|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 7309|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 7309|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7310|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 7310|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 7311|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 7311|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 7312|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 7312|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 7313|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 7313|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 7314|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 7314|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 7315|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8014|lr = 0.00010\n",
      "Epoch: 7315|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 7316|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 7316|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 7317|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 7317|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7318|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 7318|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 7319|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 7319|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 7320|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 7320|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 7321|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 7321|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 7322|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 7322|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 7323|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 7323|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 7324|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 7324|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 7325|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 7325|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 7326|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7326|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 7327|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 7327|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7328|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 7328|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 7329|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 7329|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 7330|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 7330|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 7331|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 7331|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7332|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 7332|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7993|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7333|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 7333|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 7334|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 7334|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 7335|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 7335|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 7336|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7336|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 7337|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 7337|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 7338|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 7338|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 7339|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 7339|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 7340|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 7340|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.8040|lr = 0.00010\n",
      "Epoch: 7341|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 7341|steps:   60|Train Avg Loss: 0.0043 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 7342|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 7342|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 7343|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 7343|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 7344|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 7344|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7345|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 7345|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7346|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 7346|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.8044|lr = 0.00010\n",
      "Epoch: 7347|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 7347|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 7348|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7348|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 7349|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 7349|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7350|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 7350|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 7351|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7351|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 7352|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7352|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 7353|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 7353|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7354|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 7354|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 7355|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 7355|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7932|lr = 0.00010\n",
      "Epoch: 7356|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 7356|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 7357|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 7357|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 7358|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 7358|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 7359|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 7359|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7360|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 7360|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 7361|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 7361|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 7362|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 7362|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 7363|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7363|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7364|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 7364|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 7365|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7365|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 7366|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 7366|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 7367|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 7367|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7969|lr = 0.00010\n",
      "Epoch: 7368|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 7368|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 7369|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7369|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 7370|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7370|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7371|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 7371|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 7372|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 7372|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7373|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 7373|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 7374|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 7374|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 7375|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 7375|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7376|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 7376|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 7377|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7859|lr = 0.00010\n",
      "Epoch: 7377|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7378|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 7378|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7379|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 7379|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 7380|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7380|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 7381|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 7381|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 7382|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7382|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 7383|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7383|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7384|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 7384|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7415|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7385|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 7385|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 7386|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 7386|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 7387|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7387|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 7388|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 7388|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 7389|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7928|lr = 0.00010\n",
      "Epoch: 7389|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7390|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 7390|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 7391|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 7391|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 7392|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 7392|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7393|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 7393|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7394|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7936|lr = 0.00010\n",
      "Epoch: 7394|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 7395|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 7395|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 7396|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 7396|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7312|lr = 0.00010\n",
      "Epoch: 7397|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7397|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 7398|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 7398|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 7399|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7399|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 7400|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 7400|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 7401|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7401|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7391|lr = 0.00010\n",
      "Epoch: 7402|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 7402|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 7403|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 7403|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 7404|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 7404|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 7405|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 7405|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 7406|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 7406|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 7407|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7407|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 7408|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 7408|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 7409|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 7409|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7410|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7410|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 7411|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 7411|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 7412|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7412|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7413|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 7413|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 7414|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 7414|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 7415|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 7415|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 7416|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 7416|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7417|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 7417|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 7418|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 7418|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 7419|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 7419|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7420|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 7420|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7421|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 7421|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7422|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 7422|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 7423|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 7423|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7424|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 7424|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 7425|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 7425|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 7426|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 7426|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7427|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7427|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 7428|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 7428|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 7429|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 7429|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 7430|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 7430|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 7431|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 7431|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7432|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 7432|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 7433|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7451|lr = 0.00010\n",
      "Epoch: 7433|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 7434|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 7434|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 7435|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7435|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 7436|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 7436|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7777|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7437|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7437|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 7438|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 7438|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 7439|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7791|lr = 0.00010\n",
      "Epoch: 7439|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 7440|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 7440|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 7441|steps:   30|Train Avg Loss: 0.0045 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 7441|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 7442|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 7442|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 7443|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 7443|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7444|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 7444|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7445|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7957|lr = 0.00010\n",
      "Epoch: 7445|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 7446|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 7446|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7447|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 7447|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 7448|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 7448|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7449|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 7449|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 7450|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 7450|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7451|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 7451|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 7452|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 7452|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7453|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 7453|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 7454|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 7454|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7501|lr = 0.00010\n",
      "Epoch: 7455|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 7455|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 7456|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7456|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 7457|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 7457|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 7458|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7458|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 7459|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 7459|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 7460|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 7460|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7461|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7461|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 7462|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 7462|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 7463|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 7463|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 7464|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7464|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 7465|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 7465|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 7466|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 7466|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7921|lr = 0.00010\n",
      "Epoch: 7467|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7467|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7468|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 7468|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 7469|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 7469|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 7470|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 7470|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 7471|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7471|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 7472|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 7472|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 7473|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 7473|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 7474|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 7474|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7982|lr = 0.00010\n",
      "Epoch: 7475|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.8011|lr = 0.00010\n",
      "Epoch: 7475|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 7476|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 7476|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7477|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 7477|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 7478|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 7478|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 7479|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7479|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 7480|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7480|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7481|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7481|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 7482|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7482|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 7483|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 7483|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7484|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 7484|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7895|lr = 0.00010\n",
      "Epoch: 7485|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 7485|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 7486|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 7486|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 7487|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7487|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7488|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 7488|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7740|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7489|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 7489|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 7490|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7959|lr = 0.00010\n",
      "Epoch: 7490|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 7491|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 7491|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 7492|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 7492|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 7493|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 7493|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 7494|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 7494|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7495|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 7495|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 7496|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 7496|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 7497|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7497|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7498|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 7498|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 7499|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 7499|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 7500|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7566|lr = 0.00010\n",
      "Epoch: 7500|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 7501|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7421|lr = 0.00010\n",
      "Epoch: 7501|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 7502|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7502|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 7503|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 7503|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 7504|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 7504|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 7505|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 7505|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 7506|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 7506|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 7507|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7507|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7900|lr = 0.00010\n",
      "Epoch: 7508|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 7508|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 7509|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 7509|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 7510|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 7510|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7511|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 7511|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 7512|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 7512|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 7513|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 7513|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 7514|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7514|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 7515|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 7515|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 7516|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7516|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 7517|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7517|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 7518|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 7518|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 7519|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7519|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7520|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7520|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 7521|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 7521|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 7522|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7870|lr = 0.00010\n",
      "Epoch: 7522|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 7523|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 7523|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7524|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 7524|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7525|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 7525|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 7526|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 7526|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 7527|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7527|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 7528|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7528|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 7529|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 7529|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 7530|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 7530|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 7531|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 7531|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 7532|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7532|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 7533|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 7533|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 7534|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7534|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 7535|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 7535|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7891|lr = 0.00010\n",
      "Epoch: 7536|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 7536|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7414|lr = 0.00010\n",
      "Epoch: 7537|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7537|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7974|lr = 0.00010\n",
      "Epoch: 7538|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 7538|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 7539|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 7539|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 7540|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 7540|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7727|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7541|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7541|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7542|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 7542|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 7543|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 7543|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7544|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 7544|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 7545|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 7545|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 7546|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7546|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 7547|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7547|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7548|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7548|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 7549|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 7549|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7916|lr = 0.00010\n",
      "Epoch: 7550|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7550|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 7551|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 7551|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 7552|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 7552|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 7553|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 7553|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 7554|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 7554|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 7555|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7451|lr = 0.00010\n",
      "Epoch: 7555|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 7556|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 7556|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 7557|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 7557|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 7558|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 7558|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 7559|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 7559|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7560|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 7560|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7561|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 7561|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7562|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 7562|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 7563|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7905|lr = 0.00010\n",
      "Epoch: 7563|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 7564|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 7564|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7565|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 7565|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7566|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 7566|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 7567|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7927|lr = 0.00010\n",
      "Epoch: 7567|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 7568|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7568|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 7569|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7569|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 7570|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 7570|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 7571|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7571|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7572|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7572|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 7573|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 7573|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 7574|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 7574|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7575|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7575|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7576|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 7576|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 7577|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7577|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 7578|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 7578|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 7579|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 7579|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 7580|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 7580|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 7581|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8007|lr = 0.00010\n",
      "Epoch: 7581|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 7582|steps:   30|Train Avg Loss: 0.0043 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 7582|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 7583|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 7583|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 7584|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7894|lr = 0.00010\n",
      "Epoch: 7584|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 7585|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 7585|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 7586|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 7586|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 7587|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 7587|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7588|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7588|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 7589|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 7589|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7590|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 7590|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 7591|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 7591|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7592|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 7592|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7637|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7593|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7593|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 7594|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 7594|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 7595|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 7595|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 7596|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 7596|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 7597|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 7597|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 7598|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 7598|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7599|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 7599|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 7600|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 7600|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 7601|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 7601|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7602|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 7602|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 7603|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 7603|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 7604|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8088|lr = 0.00010\n",
      "Epoch: 7604|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7605|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 7605|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7606|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 7606|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7607|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 7607|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7990|lr = 0.00010\n",
      "Epoch: 7608|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7608|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7609|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 7609|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 7610|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 7610|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7611|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7969|lr = 0.00010\n",
      "Epoch: 7611|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 7612|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 7612|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7613|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 7613|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 7614|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 7614|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 7615|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 7615|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 7616|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 7616|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 7617|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 7617|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 7618|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 7618|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 7619|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 7619|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 7620|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 7620|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7409|lr = 0.00010\n",
      "Epoch: 7621|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7621|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 7622|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 7622|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 7623|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 7623|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 7624|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7624|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 7625|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 7625|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 7626|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 7626|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 7627|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 7627|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 7628|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 7628|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 7629|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 7629|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7630|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7630|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 7631|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 7631|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 7632|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 7632|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 7633|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 7633|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 7634|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7883|lr = 0.00010\n",
      "Epoch: 7634|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7635|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 7635|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7636|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 7636|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 7637|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 7637|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 7638|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7978|lr = 0.00010\n",
      "Epoch: 7638|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 7639|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7437|lr = 0.00010\n",
      "Epoch: 7639|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 7640|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 7640|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 7641|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 7641|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 7642|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 7642|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 7643|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 7643|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7644|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7982|lr = 0.00010\n",
      "Epoch: 7644|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7892|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7645|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7645|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7646|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7646|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 7647|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7647|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 7648|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 7648|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 7649|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 7649|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7650|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 7650|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 7651|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 7651|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 7652|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 7652|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 7653|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7653|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 7654|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 7654|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 7655|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 7655|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7656|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7656|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 7657|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 7657|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7658|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 7658|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 7659|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 7659|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 7660|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 7660|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7661|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 7661|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7662|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 7662|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 7663|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7663|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7919|lr = 0.00010\n",
      "Epoch: 7664|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 7664|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 7665|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7665|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 7666|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 7666|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 7667|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 7667|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 7668|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7668|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 7669|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7669|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 7670|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7374|lr = 0.00010\n",
      "Epoch: 7670|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 7671|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 7671|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 7672|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 7672|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 7673|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 7673|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7674|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 7674|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 7675|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7675|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 7676|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 7676|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 7677|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 7677|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 7678|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 7678|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 7679|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 7679|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 7680|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 7680|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 7681|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7681|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 7682|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 7682|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7683|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7856|lr = 0.00010\n",
      "Epoch: 7683|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 7684|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 7684|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 7685|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 7685|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 7686|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 7686|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 7687|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 7687|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 7688|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 7688|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 7689|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 7689|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 7690|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7996|lr = 0.00010\n",
      "Epoch: 7690|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 7691|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 7691|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 7692|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 7692|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 7693|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7693|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 7694|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 7694|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 7695|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 7695|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 7696|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7696|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7568|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7697|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 7697|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7698|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7698|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7699|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 7699|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 7700|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 7700|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 7701|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 7701|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 7702|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7702|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 7703|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 7703|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 7704|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 7704|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 7705|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 7705|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 7706|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7364|lr = 0.00010\n",
      "Epoch: 7706|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7402|lr = 0.00010\n",
      "Epoch: 7707|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 7707|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 7708|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 7708|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 7709|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 7709|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 7710|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 7710|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 7711|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 7711|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 7712|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 7712|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 7713|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 7713|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 7714|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 7714|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 7715|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7450|lr = 0.00010\n",
      "Epoch: 7715|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 7716|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7716|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 7717|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 7717|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 7718|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 7718|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 7719|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 7719|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 7720|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 7720|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 7721|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 7721|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7722|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 7722|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 7723|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 7723|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 7724|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 7724|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7725|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 7725|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7439|lr = 0.00010\n",
      "Epoch: 7726|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 7726|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7727|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 7727|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 7728|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7728|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 7729|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 7729|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 7730|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 7730|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7731|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.8042|lr = 0.00010\n",
      "Epoch: 7731|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7732|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 7732|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 7733|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 7733|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 7734|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8060|lr = 0.00010\n",
      "Epoch: 7734|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7735|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7920|lr = 0.00010\n",
      "Epoch: 7735|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 7736|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 7736|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 7737|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 7737|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 7738|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 7738|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7739|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 7739|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 7740|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7740|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7491|lr = 0.00010\n",
      "Epoch: 7741|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7741|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 7742|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 7742|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 7743|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 7743|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 7744|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 7744|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 7745|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7745|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 7746|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 7746|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7911|lr = 0.00010\n",
      "Epoch: 7747|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 7747|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 7748|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7852|lr = 0.00010\n",
      "Epoch: 7748|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7663|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7749|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 7749|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 7750|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 7750|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7751|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7751|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 7752|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 7752|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7753|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 7753|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7754|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 7754|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7888|lr = 0.00010\n",
      "Epoch: 7755|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 7755|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 7756|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 7756|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7757|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 7757|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7758|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 7758|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 7759|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 7759|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 7760|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 7760|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 7761|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7761|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 7762|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 7762|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 7763|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 7763|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7764|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 7764|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 7765|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 7765|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 7766|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7766|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 7767|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7935|lr = 0.00010\n",
      "Epoch: 7767|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 7768|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 7768|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 7769|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 7769|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 7770|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 7770|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 7771|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 7771|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 7772|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 7772|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 7773|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7773|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 7774|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 7774|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 7775|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 7775|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 7776|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 7776|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 7777|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 7777|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 7778|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7778|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 7779|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7866|lr = 0.00010\n",
      "Epoch: 7779|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 7780|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7945|lr = 0.00010\n",
      "Epoch: 7780|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 7781|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7970|lr = 0.00010\n",
      "Epoch: 7781|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 7782|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7782|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 7783|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7783|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 7784|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 7784|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 7785|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7785|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 7786|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 7786|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 7787|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 7787|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 7788|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7788|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7882|lr = 0.00010\n",
      "Epoch: 7789|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 7789|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 7790|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 7790|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7970|lr = 0.00010\n",
      "Epoch: 7791|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 7791|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 7792|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 7792|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 7793|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7430|lr = 0.00010\n",
      "Epoch: 7793|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 7794|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7794|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 7795|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 7795|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 7796|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7796|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 7797|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 7797|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 7798|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 7798|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7897|lr = 0.00010\n",
      "Epoch: 7799|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 7799|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 7800|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 7800|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7598|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7801|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 7801|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 7802|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 7802|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 7803|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 7803|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 7804|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 7804|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7805|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 7805|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 7806|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 7806|steps:   60|Train Avg Loss: 0.0051 |Test Loss: 1.7974|lr = 0.00010\n",
      "Epoch: 7807|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 7807|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7808|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7808|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 7809|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.8032|lr = 0.00010\n",
      "Epoch: 7809|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 7810|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7810|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7417|lr = 0.00010\n",
      "Epoch: 7811|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7599|lr = 0.00010\n",
      "Epoch: 7811|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 7812|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 7812|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7813|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 7813|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 7814|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 7814|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 7815|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7994|lr = 0.00010\n",
      "Epoch: 7815|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7941|lr = 0.00010\n",
      "Epoch: 7816|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 7816|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 7817|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 7817|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 7818|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 7818|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 7819|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7379|lr = 0.00010\n",
      "Epoch: 7819|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 7820|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 7820|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 7821|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7821|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 7822|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 7822|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 7823|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7823|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7824|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7449|lr = 0.00010\n",
      "Epoch: 7824|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 7825|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 7825|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 7826|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 7826|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 7827|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 7827|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 7828|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 7828|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7829|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 7829|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 7830|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 7830|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7831|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 7831|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 7832|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 7832|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 7833|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7955|lr = 0.00010\n",
      "Epoch: 7833|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 7834|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 7834|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7835|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7835|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7836|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 7836|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 7837|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 7837|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7405|lr = 0.00010\n",
      "Epoch: 7838|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7838|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 7839|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 7839|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7840|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 7840|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 7841|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 7841|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7472|lr = 0.00010\n",
      "Epoch: 7842|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 7842|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7843|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 7843|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 7844|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7449|lr = 0.00010\n",
      "Epoch: 7844|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 7845|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 7845|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 7846|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 7846|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 7847|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 7847|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 7848|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7848|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 7849|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 7849|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 7850|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7850|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 7851|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 7851|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 7852|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 7852|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7599|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7853|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7496|lr = 0.00010\n",
      "Epoch: 7853|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7432|lr = 0.00010\n",
      "Epoch: 7854|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 7854|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 7855|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7855|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 7856|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 7856|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7857|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7857|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 7858|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 7858|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 7859|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 7859|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 7860|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 7860|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 7861|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 7861|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7862|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 7862|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 7863|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 7863|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 7864|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 7864|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 7865|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 7865|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 7866|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 7866|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 7867|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 7867|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 7868|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7868|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7869|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 7869|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 7870|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 7870|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 7871|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 7871|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 7872|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 7872|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 7873|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 7873|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 7874|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 7874|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 7875|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7875|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7876|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 7876|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 7877|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 7877|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 7878|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 7878|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 7879|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 7879|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7793|lr = 0.00010\n",
      "Epoch: 7880|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7880|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 7881|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 7881|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 7882|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 7882|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7883|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 7883|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 7884|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 7884|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7386|lr = 0.00010\n",
      "Epoch: 7885|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 7885|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 7886|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 7886|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 7887|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7890|lr = 0.00010\n",
      "Epoch: 7887|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 7888|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 7888|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 7889|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7468|lr = 0.00010\n",
      "Epoch: 7889|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 7890|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 7890|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7528|lr = 0.00010\n",
      "Epoch: 7891|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 7891|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 7892|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 7892|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 7893|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 7893|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7894|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 7894|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 7895|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 7895|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 7896|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 7896|steps:   60|Train Avg Loss: 0.0046 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7897|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 7897|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 7898|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 7898|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7899|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7899|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7900|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 7900|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 7901|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 7901|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7943|lr = 0.00010\n",
      "Epoch: 7902|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 7902|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 7903|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 7903|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7316|lr = 0.00010\n",
      "Epoch: 7904|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7929|lr = 0.00010\n",
      "Epoch: 7904|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7690|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7905|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 7905|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 7906|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7906|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 7907|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 7907|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7908|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 7908|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 7909|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 7909|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7910|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7375|lr = 0.00010\n",
      "Epoch: 7910|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 7911|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 7911|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 7912|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 7912|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7402|lr = 0.00010\n",
      "Epoch: 7913|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 7913|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 7914|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 7914|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 7915|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 7915|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 7916|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 7916|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 7917|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 7917|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 7918|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7918|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 7919|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 7919|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 7920|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 7920|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7887|lr = 0.00010\n",
      "Epoch: 7921|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7399|lr = 0.00010\n",
      "Epoch: 7921|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7922|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 7922|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 7923|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 7923|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 7924|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 7924|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 7925|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7726|lr = 0.00010\n",
      "Epoch: 7925|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 7926|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 7926|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 7927|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 7927|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 7928|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7928|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 7929|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 7929|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 7930|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 7930|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 7931|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 7931|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 7932|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 7932|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 7933|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 7933|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 7934|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 7934|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 7935|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 7935|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 7936|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 7936|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 7937|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 7937|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 7938|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 7938|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 7939|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 7939|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 7940|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 7940|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 7941|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 7941|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7947|lr = 0.00010\n",
      "Epoch: 7942|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.8001|lr = 0.00010\n",
      "Epoch: 7942|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 7943|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 7943|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 7944|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 7944|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7810|lr = 0.00010\n",
      "Epoch: 7945|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 7945|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 7946|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7946|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7834|lr = 0.00010\n",
      "Epoch: 7947|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7947|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 7948|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7832|lr = 0.00010\n",
      "Epoch: 7948|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 7949|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 7949|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 7950|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 7950|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 7951|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7951|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 7952|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 7952|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 7953|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7953|lr = 0.00010\n",
      "Epoch: 7953|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 7954|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 7954|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 7955|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 7955|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 7956|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 7956|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7585|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7957|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 7957|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 7958|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 7958|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 7959|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 7959|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 7960|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 7960|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 7961|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7830|lr = 0.00010\n",
      "Epoch: 7961|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 7962|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 7962|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 7963|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 7963|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7764|lr = 0.00010\n",
      "Epoch: 7964|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 7964|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 7965|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 7965|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 7966|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 7966|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 7967|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 7967|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 7968|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 7968|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 7969|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 7969|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 7970|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 7970|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 7971|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 7971|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 7972|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7925|lr = 0.00010\n",
      "Epoch: 7972|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 7973|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7835|lr = 0.00010\n",
      "Epoch: 7973|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 7974|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7869|lr = 0.00010\n",
      "Epoch: 7974|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7979|lr = 0.00010\n",
      "Epoch: 7975|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 7975|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 7976|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 7976|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 7977|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 7977|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 7978|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 7978|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7880|lr = 0.00010\n",
      "Epoch: 7979|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 7979|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 7980|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 7980|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 7981|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 7981|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7447|lr = 0.00010\n",
      "Epoch: 7982|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 7982|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 7983|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 7983|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 7984|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 7984|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 7985|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 7985|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 7986|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 7986|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 7987|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 7987|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 7988|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 7988|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7497|lr = 0.00010\n",
      "Epoch: 7989|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 7989|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 7990|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 7990|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 7991|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 7991|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 7992|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 7992|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 7993|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 7993|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 7994|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 7994|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 7995|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 7995|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7996|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 7996|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 7997|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 7997|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 7998|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 7998|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 7999|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 7999|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 8000|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 8000|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7531|lr = 0.00010\n",
      "Epoch: 8001|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7638|lr = 0.00010\n",
      "Epoch: 8001|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 8002|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 8002|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8003|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 8003|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 8004|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 8004|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 8005|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 8005|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 8006|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 8006|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 8007|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 8007|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7466|lr = 0.00010\n",
      "Epoch: 8008|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8008|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7572|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8009|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 8009|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 8010|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 8010|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7581|lr = 0.00010\n",
      "Epoch: 8011|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 8011|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 8012|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 8012|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 8013|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7408|lr = 0.00010\n",
      "Epoch: 8013|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 8014|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 8014|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7516|lr = 0.00010\n",
      "Epoch: 8015|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 8015|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7423|lr = 0.00010\n",
      "Epoch: 8016|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 8016|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7382|lr = 0.00010\n",
      "Epoch: 8017|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7468|lr = 0.00010\n",
      "Epoch: 8017|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 8018|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 8018|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 8019|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 8019|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 8020|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 8020|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7673|lr = 0.00010\n",
      "Epoch: 8021|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 8021|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 8022|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 8022|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 8023|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 8023|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 8024|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 8024|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7651|lr = 0.00010\n",
      "Epoch: 8025|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 8025|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 8026|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 8026|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 8027|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 8027|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 8028|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8028|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 8029|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 8029|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8030|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 8030|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 8031|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 8031|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 8032|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 8032|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 8033|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 8033|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 8034|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 8034|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 8035|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 8035|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 8036|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 8036|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7884|lr = 0.00010\n",
      "Epoch: 8037|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 8037|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7550|lr = 0.00010\n",
      "Epoch: 8038|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 8038|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 8039|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 8039|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8040|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 8040|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 8041|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 8041|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 8042|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 8042|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7864|lr = 0.00010\n",
      "Epoch: 8043|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8043|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 8044|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7677|lr = 0.00010\n",
      "Epoch: 8044|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 8045|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 8045|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 8046|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 8046|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8047|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 8047|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 8048|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.8032|lr = 0.00010\n",
      "Epoch: 8048|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 8049|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 8049|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7946|lr = 0.00010\n",
      "Epoch: 8050|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 8050|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 8051|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 8051|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7903|lr = 0.00010\n",
      "Epoch: 8052|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8052|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 8053|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 8053|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 8054|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 8054|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 8055|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7797|lr = 0.00010\n",
      "Epoch: 8055|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8056|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 8056|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 8057|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 8057|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7527|lr = 0.00010\n",
      "Epoch: 8058|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8058|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 8059|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 8059|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 8060|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7336|lr = 0.00010\n",
      "Epoch: 8060|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7620|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8061|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 8061|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 8062|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 8062|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8063|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 8063|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7424|lr = 0.00010\n",
      "Epoch: 8064|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 8064|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 8065|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7451|lr = 0.00010\n",
      "Epoch: 8065|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7482|lr = 0.00010\n",
      "Epoch: 8066|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 8066|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 8067|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 8067|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 8068|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 8068|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 8069|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8069|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 8070|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 8070|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8071|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 8071|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 8072|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 8072|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7893|lr = 0.00010\n",
      "Epoch: 8073|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 8073|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 8074|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 8074|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 8075|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 8075|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 8076|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8076|steps:   60|Train Avg Loss: 0.0063 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 8077|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 8077|steps:   60|Train Avg Loss: 0.0065 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 8078|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 8078|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 8079|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 8079|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8080|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 8080|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 8081|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 8081|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 8082|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 8082|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 8083|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 8083|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 8084|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7829|lr = 0.00010\n",
      "Epoch: 8084|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 8085|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 8085|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 8086|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 8086|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 8087|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 8087|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 8088|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 8088|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 8089|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 8089|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 8090|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 8090|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 8091|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 8091|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 8092|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 8092|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 8093|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 8093|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 8094|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7716|lr = 0.00010\n",
      "Epoch: 8094|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8095|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7825|lr = 0.00010\n",
      "Epoch: 8095|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 8096|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 8096|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 8097|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 8097|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 8098|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 8098|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7839|lr = 0.00010\n",
      "Epoch: 8099|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 8099|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 8100|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 8100|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 8101|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 8101|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 8102|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 8102|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 8103|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 8103|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 8104|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 8104|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7862|lr = 0.00010\n",
      "Epoch: 8105|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8105|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 8106|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8106|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8107|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8107|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7874|lr = 0.00010\n",
      "Epoch: 8108|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 8108|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7815|lr = 0.00010\n",
      "Epoch: 8109|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 8109|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 8110|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 8110|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 8111|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 8111|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 8112|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 8112|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7900|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8113|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 8113|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 8114|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 8114|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7337|lr = 0.00010\n",
      "Epoch: 8115|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 8115|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7519|lr = 0.00010\n",
      "Epoch: 8116|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 8116|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 8117|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 8117|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 8118|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 8118|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 8119|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 8119|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 8120|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8120|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 8121|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 8121|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 8122|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 8122|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7939|lr = 0.00010\n",
      "Epoch: 8123|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 8123|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7816|lr = 0.00010\n",
      "Epoch: 8124|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 8124|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 8125|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 8125|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 8126|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8126|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 8127|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 8127|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7422|lr = 0.00010\n",
      "Epoch: 8128|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 8128|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 8129|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 8129|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 8130|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 8130|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 8131|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 8131|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8132|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 8132|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 8133|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 8133|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 8134|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 8134|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 8135|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 8135|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 8136|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 8136|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7537|lr = 0.00010\n",
      "Epoch: 8137|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 8137|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 8138|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 8138|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 8139|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8139|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 8140|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 8140|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 8141|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 8141|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7535|lr = 0.00010\n",
      "Epoch: 8142|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 8142|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 8143|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 8143|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 8144|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7819|lr = 0.00010\n",
      "Epoch: 8144|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7502|lr = 0.00010\n",
      "Epoch: 8145|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8145|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 8146|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 8146|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 8147|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 8147|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 8148|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 8148|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 8149|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 8149|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 8150|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 8150|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 8151|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 8151|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 8152|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 8152|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7621|lr = 0.00010\n",
      "Epoch: 8153|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 8153|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 8154|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 8154|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 8155|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 8155|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 8156|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 8156|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 8157|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8157|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8158|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 8158|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7485|lr = 0.00010\n",
      "Epoch: 8159|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 8159|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7525|lr = 0.00010\n",
      "Epoch: 8160|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 8160|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 8161|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 8161|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 8162|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 8162|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 8163|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 8163|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7409|lr = 0.00010\n",
      "Epoch: 8164|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8164|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7691|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8165|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8165|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 8166|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 8166|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 8167|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 8167|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7456|lr = 0.00010\n",
      "Epoch: 8168|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7477|lr = 0.00010\n",
      "Epoch: 8168|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 8169|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7252|lr = 0.00010\n",
      "Epoch: 8169|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7549|lr = 0.00010\n",
      "Epoch: 8170|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 8170|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 8171|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7459|lr = 0.00010\n",
      "Epoch: 8171|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 8172|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 8172|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8173|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 8173|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7811|lr = 0.00010\n",
      "Epoch: 8174|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7676|lr = 0.00010\n",
      "Epoch: 8174|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 8175|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 8175|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7904|lr = 0.00010\n",
      "Epoch: 8176|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7973|lr = 0.00010\n",
      "Epoch: 8176|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7624|lr = 0.00010\n",
      "Epoch: 8177|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 8177|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 8178|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 8178|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7517|lr = 0.00010\n",
      "Epoch: 8179|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 8179|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7786|lr = 0.00010\n",
      "Epoch: 8180|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 8180|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 8181|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 8181|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 8182|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 8182|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 8183|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 8183|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 8184|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 8184|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8185|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 8185|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 8186|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 8186|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7750|lr = 0.00010\n",
      "Epoch: 8187|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 8187|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 8188|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 8188|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 8189|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7553|lr = 0.00010\n",
      "Epoch: 8189|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 8190|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 8190|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8191|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 8191|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 8192|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 8192|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 8193|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 8193|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 8194|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 8194|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 8195|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7846|lr = 0.00010\n",
      "Epoch: 8195|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7848|lr = 0.00010\n",
      "Epoch: 8196|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7674|lr = 0.00010\n",
      "Epoch: 8196|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 8197|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 8197|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8198|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 8198|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 8199|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 8199|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8200|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 8200|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8201|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8201|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 8202|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 8202|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 8203|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 8203|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 8204|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 8204|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 8205|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 8205|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 8206|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 8206|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8207|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 8207|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8208|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7545|lr = 0.00010\n",
      "Epoch: 8208|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 8209|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 8209|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 8210|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 8210|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 8211|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 8211|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 8212|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8212|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 8213|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8213|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 8214|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8214|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 8215|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 8215|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 8216|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 8216|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7768|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8217|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 8217|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7728|lr = 0.00010\n",
      "Epoch: 8218|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 8218|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7608|lr = 0.00010\n",
      "Epoch: 8219|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7660|lr = 0.00010\n",
      "Epoch: 8219|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 8220|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 8220|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 8221|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 8221|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 8222|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 8222|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8223|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 8223|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7732|lr = 0.00010\n",
      "Epoch: 8224|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7454|lr = 0.00010\n",
      "Epoch: 8224|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 8225|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7803|lr = 0.00010\n",
      "Epoch: 8225|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 8226|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 8226|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 8227|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 8227|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 8228|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 8228|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 8229|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7827|lr = 0.00010\n",
      "Epoch: 8229|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 8230|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 8230|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 8231|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 8231|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 8232|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8232|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 8233|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7875|lr = 0.00010\n",
      "Epoch: 8233|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 8234|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 8234|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 8235|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7711|lr = 0.00010\n",
      "Epoch: 8235|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8236|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 8236|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8237|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7659|lr = 0.00010\n",
      "Epoch: 8237|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7593|lr = 0.00010\n",
      "Epoch: 8238|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7584|lr = 0.00010\n",
      "Epoch: 8238|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 8239|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 8239|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 8240|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 8240|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7844|lr = 0.00010\n",
      "Epoch: 8241|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 8241|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7758|lr = 0.00010\n",
      "Epoch: 8242|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7524|lr = 0.00010\n",
      "Epoch: 8242|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 8243|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7383|lr = 0.00010\n",
      "Epoch: 8243|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 8244|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 8244|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 8245|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 8245|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 8246|steps:   30|Train Avg Loss: 0.0046 |Test Loss: 1.7708|lr = 0.00010\n",
      "Epoch: 8246|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 8247|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 8247|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 8248|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 8248|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 8249|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 8249|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7569|lr = 0.00010\n",
      "Epoch: 8250|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 8250|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8251|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8251|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8252|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 8252|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 8253|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 8253|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 8254|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 8254|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 8255|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 8255|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 8256|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 8256|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 8257|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 8257|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 8258|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7446|lr = 0.00010\n",
      "Epoch: 8258|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7504|lr = 0.00010\n",
      "Epoch: 8259|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 8259|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 8260|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 8260|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 8261|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 8261|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 8262|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 8262|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 8263|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 8263|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 8264|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 8264|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 8265|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 8265|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7415|lr = 0.00010\n",
      "Epoch: 8266|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 8266|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 8267|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 8267|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 8268|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 8268|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7647|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8269|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 8269|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7505|lr = 0.00010\n",
      "Epoch: 8270|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 8270|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 8271|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 8271|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 8272|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7571|lr = 0.00010\n",
      "Epoch: 8272|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 8273|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 8273|steps:   60|Train Avg Loss: 0.0042 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 8274|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8274|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 8275|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 8275|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 8276|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 8276|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 8277|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8277|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8278|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7385|lr = 0.00010\n",
      "Epoch: 8278|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7404|lr = 0.00010\n",
      "Epoch: 8279|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 8279|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 8280|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 8280|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 8281|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 8281|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 8282|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 8282|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 8283|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7523|lr = 0.00010\n",
      "Epoch: 8283|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 8284|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7365|lr = 0.00010\n",
      "Epoch: 8284|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7547|lr = 0.00010\n",
      "Epoch: 8285|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 8285|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 8286|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 8286|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 8287|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7632|lr = 0.00010\n",
      "Epoch: 8287|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 8288|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 8288|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 8289|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 8289|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 8290|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 8290|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 8291|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8291|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 8292|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 8292|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8293|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 8293|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 8294|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7546|lr = 0.00010\n",
      "Epoch: 8294|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7425|lr = 0.00010\n",
      "Epoch: 8295|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 8295|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 8296|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7892|lr = 0.00010\n",
      "Epoch: 8296|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8297|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 8297|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 8298|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7596|lr = 0.00010\n",
      "Epoch: 8298|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8299|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 8299|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7586|lr = 0.00010\n",
      "Epoch: 8300|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8300|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 8301|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 8301|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 8302|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 8302|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 8303|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 8303|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7647|lr = 0.00010\n",
      "Epoch: 8304|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8304|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 8305|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7469|lr = 0.00010\n",
      "Epoch: 8305|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7538|lr = 0.00010\n",
      "Epoch: 8306|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7443|lr = 0.00010\n",
      "Epoch: 8306|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 8307|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 8307|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 8308|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7648|lr = 0.00010\n",
      "Epoch: 8308|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7860|lr = 0.00010\n",
      "Epoch: 8309|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 8309|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 8310|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 8310|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 8311|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7754|lr = 0.00010\n",
      "Epoch: 8311|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 8312|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 8312|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 8313|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 8313|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 8314|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8314|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 8315|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7316|lr = 0.00010\n",
      "Epoch: 8315|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 8316|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 8316|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7565|lr = 0.00010\n",
      "Epoch: 8317|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 8317|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 8318|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 8318|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 8319|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 8319|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7713|lr = 0.00010\n",
      "Epoch: 8320|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 8320|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7494|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8321|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 8321|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8322|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7412|lr = 0.00010\n",
      "Epoch: 8322|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 8323|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 8323|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7687|lr = 0.00010\n",
      "Epoch: 8324|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 8324|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 8325|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 8325|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7534|lr = 0.00010\n",
      "Epoch: 8326|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7419|lr = 0.00010\n",
      "Epoch: 8326|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 8327|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 8327|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 8328|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8328|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7499|lr = 0.00010\n",
      "Epoch: 8329|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 8329|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 8330|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7686|lr = 0.00010\n",
      "Epoch: 8330|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 8331|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 8331|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 8332|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 8332|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 8333|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 8333|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7841|lr = 0.00010\n",
      "Epoch: 8334|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7564|lr = 0.00010\n",
      "Epoch: 8334|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7763|lr = 0.00010\n",
      "Epoch: 8335|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7379|lr = 0.00010\n",
      "Epoch: 8335|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 8336|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 8336|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 8337|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 8337|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 8338|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 8338|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 8339|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7645|lr = 0.00010\n",
      "Epoch: 8339|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 8340|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8340|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 8341|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 8341|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7378|lr = 0.00010\n",
      "Epoch: 8342|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 8342|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 8343|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 8343|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 8344|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7639|lr = 0.00010\n",
      "Epoch: 8344|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 8345|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 8345|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 8346|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 8346|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 8347|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 8347|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 8348|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 8348|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 8349|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 8349|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 8350|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 8350|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8351|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 8351|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 8352|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7500|lr = 0.00010\n",
      "Epoch: 8352|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 8353|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7533|lr = 0.00010\n",
      "Epoch: 8353|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7555|lr = 0.00010\n",
      "Epoch: 8354|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 8354|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 8355|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 8355|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 8356|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8356|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 8357|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 8357|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 8358|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 8358|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 8359|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7559|lr = 0.00010\n",
      "Epoch: 8359|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 8360|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 8360|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8361|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 8361|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8362|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 8362|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8363|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 8363|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 8364|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7483|lr = 0.00010\n",
      "Epoch: 8364|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 8365|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 8365|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 8366|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7331|lr = 0.00010\n",
      "Epoch: 8366|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7828|lr = 0.00010\n",
      "Epoch: 8367|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 8367|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 8368|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7577|lr = 0.00010\n",
      "Epoch: 8368|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 8369|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 8369|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 8370|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7836|lr = 0.00010\n",
      "Epoch: 8370|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 8371|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 8371|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 8372|steps:   30|Train Avg Loss: 0.0038 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 8372|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7740|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8373|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 8373|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 8374|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 8374|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8375|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 8375|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 8376|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7653|lr = 0.00010\n",
      "Epoch: 8376|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8377|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 8377|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7418|lr = 0.00010\n",
      "Epoch: 8378|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 8378|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 8379|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7712|lr = 0.00010\n",
      "Epoch: 8379|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 8380|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 8380|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7543|lr = 0.00010\n",
      "Epoch: 8381|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 8381|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 8382|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 8382|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 8383|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 8383|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8384|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 8384|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 8385|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 8385|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7749|lr = 0.00010\n",
      "Epoch: 8386|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 8386|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8387|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 8387|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8388|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7889|lr = 0.00010\n",
      "Epoch: 8388|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7710|lr = 0.00010\n",
      "Epoch: 8389|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7614|lr = 0.00010\n",
      "Epoch: 8389|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 8390|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 8390|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7655|lr = 0.00010\n",
      "Epoch: 8391|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 8391|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 8392|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7746|lr = 0.00010\n",
      "Epoch: 8392|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 8393|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 8393|steps:   60|Train Avg Loss: 0.0039 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 8394|steps:   30|Train Avg Loss: 0.0047 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 8394|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 8395|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7773|lr = 0.00010\n",
      "Epoch: 8395|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 8396|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 8396|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7413|lr = 0.00010\n",
      "Epoch: 8397|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7780|lr = 0.00010\n",
      "Epoch: 8397|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 8398|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 8398|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8399|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7568|lr = 0.00010\n",
      "Epoch: 8399|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7595|lr = 0.00010\n",
      "Epoch: 8400|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7725|lr = 0.00010\n",
      "Epoch: 8400|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 8401|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8401|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7529|lr = 0.00010\n",
      "Epoch: 8402|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7544|lr = 0.00010\n",
      "Epoch: 8402|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8403|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 8403|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 8404|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7630|lr = 0.00010\n",
      "Epoch: 8404|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 8405|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7806|lr = 0.00010\n",
      "Epoch: 8405|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 8406|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7463|lr = 0.00010\n",
      "Epoch: 8406|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7438|lr = 0.00010\n",
      "Epoch: 8407|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7871|lr = 0.00010\n",
      "Epoch: 8407|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 8408|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 8408|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 8409|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7406|lr = 0.00010\n",
      "Epoch: 8409|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7521|lr = 0.00010\n",
      "Epoch: 8410|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7341|lr = 0.00010\n",
      "Epoch: 8410|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7530|lr = 0.00010\n",
      "Epoch: 8411|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 8411|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7507|lr = 0.00010\n",
      "Epoch: 8412|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 8412|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 8413|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 8413|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 8414|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 8414|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 8415|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 8415|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7772|lr = 0.00010\n",
      "Epoch: 8416|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 8416|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7631|lr = 0.00010\n",
      "Epoch: 8417|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 8417|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 8418|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 8418|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 8419|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 8419|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 8420|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7616|lr = 0.00010\n",
      "Epoch: 8420|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7486|lr = 0.00010\n",
      "Epoch: 8421|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 8421|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7470|lr = 0.00010\n",
      "Epoch: 8422|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 8422|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 8423|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7743|lr = 0.00010\n",
      "Epoch: 8423|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7580|lr = 0.00010\n",
      "Epoch: 8424|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 8424|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7710|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8425|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 8425|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 8426|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 8426|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 8427|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8427|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 8428|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 8428|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7552|lr = 0.00010\n",
      "Epoch: 8429|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7691|lr = 0.00010\n",
      "Epoch: 8429|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 8430|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 8430|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 8431|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7665|lr = 0.00010\n",
      "Epoch: 8431|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7579|lr = 0.00010\n",
      "Epoch: 8432|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 8432|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 8433|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7403|lr = 0.00010\n",
      "Epoch: 8433|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 8434|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8434|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 8435|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 8435|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7646|lr = 0.00010\n",
      "Epoch: 8436|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 8436|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 8437|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 8437|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 8438|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 8438|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7696|lr = 0.00010\n",
      "Epoch: 8439|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7494|lr = 0.00010\n",
      "Epoch: 8439|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 8440|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7489|lr = 0.00010\n",
      "Epoch: 8440|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 8441|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8441|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7837|lr = 0.00010\n",
      "Epoch: 8442|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 8442|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 8443|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 8443|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7915|lr = 0.00010\n",
      "Epoch: 8444|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 8444|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 8445|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 8445|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7640|lr = 0.00010\n",
      "Epoch: 8446|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 8446|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 8447|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7822|lr = 0.00010\n",
      "Epoch: 8447|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 8448|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7558|lr = 0.00010\n",
      "Epoch: 8448|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 8449|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 8449|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 8450|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7578|lr = 0.00010\n",
      "Epoch: 8450|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7809|lr = 0.00010\n",
      "Epoch: 8451|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 8451|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7706|lr = 0.00010\n",
      "Epoch: 8452|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7518|lr = 0.00010\n",
      "Epoch: 8452|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7724|lr = 0.00010\n",
      "Epoch: 8453|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7512|lr = 0.00010\n",
      "Epoch: 8453|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 8454|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 8454|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7604|lr = 0.00010\n",
      "Epoch: 8455|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 8455|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 8456|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 8456|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 8457|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 8457|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7747|lr = 0.00010\n",
      "Epoch: 8458|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8458|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8459|steps:   30|Train Avg Loss: 0.0061 |Test Loss: 1.7702|lr = 0.00010\n",
      "Epoch: 8459|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7962|lr = 0.00010\n",
      "Epoch: 8460|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7877|lr = 0.00010\n",
      "Epoch: 8460|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8461|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 8461|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7490|lr = 0.00010\n",
      "Epoch: 8462|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 8462|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7801|lr = 0.00010\n",
      "Epoch: 8463|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 8463|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 8464|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 8464|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7515|lr = 0.00010\n",
      "Epoch: 8465|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 8465|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7789|lr = 0.00010\n",
      "Epoch: 8466|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7701|lr = 0.00010\n",
      "Epoch: 8466|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 8467|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 8467|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 8468|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7377|lr = 0.00010\n",
      "Epoch: 8468|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7842|lr = 0.00010\n",
      "Epoch: 8469|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7679|lr = 0.00010\n",
      "Epoch: 8469|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 8470|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 8470|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7808|lr = 0.00010\n",
      "Epoch: 8471|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7798|lr = 0.00010\n",
      "Epoch: 8471|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7926|lr = 0.00010\n",
      "Epoch: 8472|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7576|lr = 0.00010\n",
      "Epoch: 8472|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7769|lr = 0.00010\n",
      "Epoch: 8473|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7654|lr = 0.00010\n",
      "Epoch: 8473|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 8474|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 1.7506|lr = 0.00010\n",
      "Epoch: 8474|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 8475|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7574|lr = 0.00010\n",
      "Epoch: 8475|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7625|lr = 0.00010\n",
      "Epoch: 8476|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7690|lr = 0.00010\n",
      "Epoch: 8476|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7605|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8477|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8477|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 8478|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 8478|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 8479|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 8479|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 8480|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 8480|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 8481|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7508|lr = 0.00010\n",
      "Epoch: 8481|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8482|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7554|lr = 0.00010\n",
      "Epoch: 8482|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7756|lr = 0.00010\n",
      "Epoch: 8483|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 8483|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7656|lr = 0.00010\n",
      "Epoch: 8484|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8484|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 8485|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7733|lr = 0.00010\n",
      "Epoch: 8485|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 8486|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7602|lr = 0.00010\n",
      "Epoch: 8486|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7628|lr = 0.00010\n",
      "Epoch: 8487|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7714|lr = 0.00010\n",
      "Epoch: 8487|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7335|lr = 0.00010\n",
      "Epoch: 8488|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 8488|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8489|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 8489|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8490|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 8490|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7573|lr = 0.00010\n",
      "Epoch: 8491|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 8491|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7551|lr = 0.00010\n",
      "Epoch: 8492|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8492|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7649|lr = 0.00010\n",
      "Epoch: 8493|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7675|lr = 0.00010\n",
      "Epoch: 8493|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7541|lr = 0.00010\n",
      "Epoch: 8494|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 8494|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7717|lr = 0.00010\n",
      "Epoch: 8495|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7597|lr = 0.00010\n",
      "Epoch: 8495|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7685|lr = 0.00010\n",
      "Epoch: 8496|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 8496|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7611|lr = 0.00010\n",
      "Epoch: 8497|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 8497|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7514|lr = 0.00010\n",
      "Epoch: 8498|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7501|lr = 0.00010\n",
      "Epoch: 8498|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7562|lr = 0.00010\n",
      "Epoch: 8499|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 8499|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7465|lr = 0.00010\n",
      "Epoch: 8500|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7441|lr = 0.00010\n",
      "Epoch: 8500|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 8501|steps:   30|Train Avg Loss: 0.0037 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 8501|steps:   60|Train Avg Loss: 0.0040 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8502|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 8502|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 8503|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 8503|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7637|lr = 0.00010\n",
      "Epoch: 8504|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7838|lr = 0.00010\n",
      "Epoch: 8504|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 8505|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7734|lr = 0.00010\n",
      "Epoch: 8505|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 8506|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7548|lr = 0.00010\n",
      "Epoch: 8506|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 8507|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 8507|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 8508|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7694|lr = 0.00010\n",
      "Epoch: 8508|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7757|lr = 0.00010\n",
      "Epoch: 8509|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8509|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7650|lr = 0.00010\n",
      "Epoch: 8510|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 8510|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7790|lr = 0.00010\n",
      "Epoch: 8511|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7950|lr = 0.00010\n",
      "Epoch: 8511|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7802|lr = 0.00010\n",
      "Epoch: 8512|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 8512|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7532|lr = 0.00010\n",
      "Epoch: 8513|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 8513|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7635|lr = 0.00010\n",
      "Epoch: 8514|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 8514|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7503|lr = 0.00010\n",
      "Epoch: 8515|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7590|lr = 0.00010\n",
      "Epoch: 8515|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 8516|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 8516|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 8517|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7478|lr = 0.00010\n",
      "Epoch: 8517|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7481|lr = 0.00010\n",
      "Epoch: 8518|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 8518|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 8519|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7476|lr = 0.00010\n",
      "Epoch: 8519|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7760|lr = 0.00010\n",
      "Epoch: 8520|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7460|lr = 0.00010\n",
      "Epoch: 8520|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7458|lr = 0.00010\n",
      "Epoch: 8521|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8521|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7563|lr = 0.00010\n",
      "Epoch: 8522|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7585|lr = 0.00010\n",
      "Epoch: 8522|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 8523|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7770|lr = 0.00010\n",
      "Epoch: 8523|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7526|lr = 0.00010\n",
      "Epoch: 8524|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7513|lr = 0.00010\n",
      "Epoch: 8524|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7557|lr = 0.00010\n",
      "Epoch: 8525|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7540|lr = 0.00010\n",
      "Epoch: 8525|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7718|lr = 0.00010\n",
      "Epoch: 8526|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 8526|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 8527|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7433|lr = 0.00010\n",
      "Epoch: 8527|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7709|lr = 0.00010\n",
      "Epoch: 8528|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7633|lr = 0.00010\n",
      "Epoch: 8528|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7694|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8529|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 8529|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7794|lr = 0.00010\n",
      "Epoch: 8530|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7511|lr = 0.00010\n",
      "Epoch: 8530|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7682|lr = 0.00010\n",
      "Epoch: 8531|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 8531|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7741|lr = 0.00010\n",
      "Epoch: 8532|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 8532|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 1.7735|lr = 0.00010\n",
      "Epoch: 8533|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7560|lr = 0.00010\n",
      "Epoch: 8533|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8534|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7509|lr = 0.00010\n",
      "Epoch: 8534|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 8535|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 8535|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7567|lr = 0.00010\n",
      "Epoch: 8536|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8536|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 8537|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 8537|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7692|lr = 0.00010\n",
      "Epoch: 8538|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7730|lr = 0.00010\n",
      "Epoch: 8538|steps:   60|Train Avg Loss: 0.0037 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 8539|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 8539|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7681|lr = 0.00010\n",
      "Epoch: 8540|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7591|lr = 0.00010\n",
      "Epoch: 8540|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 8541|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 8541|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7782|lr = 0.00010\n",
      "Epoch: 8542|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 8542|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7719|lr = 0.00010\n",
      "Epoch: 8543|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 8543|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7879|lr = 0.00010\n",
      "Epoch: 8544|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7767|lr = 0.00010\n",
      "Epoch: 8544|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7899|lr = 0.00010\n",
      "Epoch: 8545|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 8545|steps:   60|Train Avg Loss: 0.0041 |Test Loss: 1.7695|lr = 0.00010\n",
      "Epoch: 8546|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7613|lr = 0.00010\n",
      "Epoch: 8546|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 8547|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7606|lr = 0.00010\n",
      "Epoch: 8547|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7759|lr = 0.00010\n",
      "Epoch: 8548|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7795|lr = 0.00010\n",
      "Epoch: 8548|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7618|lr = 0.00010\n",
      "Epoch: 8549|steps:   30|Train Avg Loss: 0.0044 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 8549|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7619|lr = 0.00010\n",
      "Epoch: 8550|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 8550|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 8551|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 8551|steps:   60|Train Avg Loss: 0.0044 |Test Loss: 1.7966|lr = 0.00010\n",
      "Epoch: 8552|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 8552|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 8553|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7873|lr = 0.00010\n",
      "Epoch: 8553|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7556|lr = 0.00010\n",
      "Epoch: 8554|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7813|lr = 0.00010\n",
      "Epoch: 8554|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7851|lr = 0.00010\n",
      "Epoch: 8555|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 8555|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7898|lr = 0.00010\n",
      "Epoch: 8556|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8556|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7777|lr = 0.00010\n",
      "Epoch: 8557|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 8557|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7752|lr = 0.00010\n",
      "Epoch: 8558|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7629|lr = 0.00010\n",
      "Epoch: 8558|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7800|lr = 0.00010\n",
      "Epoch: 8559|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8559|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7700|lr = 0.00010\n",
      "Epoch: 8560|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7768|lr = 0.00010\n",
      "Epoch: 8560|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7831|lr = 0.00010\n",
      "Epoch: 8561|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7755|lr = 0.00010\n",
      "Epoch: 8561|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7582|lr = 0.00010\n",
      "Epoch: 8562|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7922|lr = 0.00010\n",
      "Epoch: 8562|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 8563|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 8563|steps:   60|Train Avg Loss: 0.0036 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 8564|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7643|lr = 0.00010\n",
      "Epoch: 8564|steps:   60|Train Avg Loss: 0.0033 |Test Loss: 1.7826|lr = 0.00010\n",
      "Epoch: 8565|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 8565|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7855|lr = 0.00010\n",
      "Epoch: 8566|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7796|lr = 0.00010\n",
      "Epoch: 8566|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 8567|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7881|lr = 0.00010\n",
      "Epoch: 8567|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 8568|steps:   30|Train Avg Loss: 0.0040 |Test Loss: 1.7986|lr = 0.00010\n",
      "Epoch: 8568|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7731|lr = 0.00010\n",
      "Epoch: 8569|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7592|lr = 0.00010\n",
      "Epoch: 8569|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7865|lr = 0.00010\n",
      "Epoch: 8570|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 8570|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 8571|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 8571|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7833|lr = 0.00010\n",
      "Epoch: 8572|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7693|lr = 0.00010\n",
      "Epoch: 8572|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7774|lr = 0.00010\n",
      "Epoch: 8573|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7603|lr = 0.00010\n",
      "Epoch: 8573|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 8574|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7672|lr = 0.00010\n",
      "Epoch: 8574|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 8575|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7818|lr = 0.00010\n",
      "Epoch: 8575|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7937|lr = 0.00010\n",
      "Epoch: 8576|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7840|lr = 0.00010\n",
      "Epoch: 8576|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7781|lr = 0.00010\n",
      "Epoch: 8577|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 8577|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7765|lr = 0.00010\n",
      "Epoch: 8578|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 8578|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7943|lr = 0.00010\n",
      "Epoch: 8579|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8579|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7699|lr = 0.00010\n",
      "Epoch: 8580|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8580|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7894|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8581|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7762|lr = 0.00010\n",
      "Epoch: 8581|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7671|lr = 0.00010\n",
      "Epoch: 8582|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 8582|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7817|lr = 0.00010\n",
      "Epoch: 8583|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7705|lr = 0.00010\n",
      "Epoch: 8583|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 8584|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7976|lr = 0.00010\n",
      "Epoch: 8584|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7901|lr = 0.00010\n",
      "Epoch: 8585|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 8585|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7570|lr = 0.00010\n",
      "Epoch: 8586|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8586|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7748|lr = 0.00010\n",
      "Epoch: 8587|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 8587|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 8588|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7991|lr = 0.00010\n",
      "Epoch: 8588|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8589|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7753|lr = 0.00010\n",
      "Epoch: 8589|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7435|lr = 0.00010\n",
      "Epoch: 8590|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7609|lr = 0.00010\n",
      "Epoch: 8590|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7667|lr = 0.00010\n",
      "Epoch: 8591|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7678|lr = 0.00010\n",
      "Epoch: 8591|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7779|lr = 0.00010\n",
      "Epoch: 8592|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8592|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.8059|lr = 0.00010\n",
      "Epoch: 8593|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7727|lr = 0.00010\n",
      "Epoch: 8593|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7914|lr = 0.00010\n",
      "Epoch: 8594|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7853|lr = 0.00010\n",
      "Epoch: 8594|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 8595|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7799|lr = 0.00010\n",
      "Epoch: 8595|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7792|lr = 0.00010\n",
      "Epoch: 8596|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 8596|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7761|lr = 0.00010\n",
      "Epoch: 8597|steps:   30|Train Avg Loss: 0.0042 |Test Loss: 1.7627|lr = 0.00010\n",
      "Epoch: 8597|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7723|lr = 0.00010\n",
      "Epoch: 8598|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7778|lr = 0.00010\n",
      "Epoch: 8598|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7849|lr = 0.00010\n",
      "Epoch: 8599|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7821|lr = 0.00010\n",
      "Epoch: 8599|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8600|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7751|lr = 0.00010\n",
      "Epoch: 8600|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7664|lr = 0.00010\n",
      "Epoch: 8601|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7905|lr = 0.00010\n",
      "Epoch: 8601|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7620|lr = 0.00010\n",
      "Epoch: 8602|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7600|lr = 0.00010\n",
      "Epoch: 8602|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7857|lr = 0.00010\n",
      "Epoch: 8603|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7522|lr = 0.00010\n",
      "Epoch: 8603|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7736|lr = 0.00010\n",
      "Epoch: 8604|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7784|lr = 0.00010\n",
      "Epoch: 8604|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7729|lr = 0.00010\n",
      "Epoch: 8605|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7572|lr = 0.00010\n",
      "Epoch: 8605|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7805|lr = 0.00010\n",
      "Epoch: 8606|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7740|lr = 0.00010\n",
      "Epoch: 8606|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7775|lr = 0.00010\n",
      "Epoch: 8607|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7986|lr = 0.00010\n",
      "Epoch: 8607|steps:   60|Train Avg Loss: 0.0024 |Test Loss: 1.7575|lr = 0.00010\n",
      "Epoch: 8608|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7850|lr = 0.00010\n",
      "Epoch: 8608|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 8609|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 8609|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 1.7812|lr = 0.00010\n",
      "Epoch: 8610|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7689|lr = 0.00010\n",
      "Epoch: 8610|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7480|lr = 0.00010\n",
      "Epoch: 8611|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 8611|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8612|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7703|lr = 0.00010\n",
      "Epoch: 8612|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 8613|steps:   30|Train Avg Loss: 0.0025 |Test Loss: 1.7788|lr = 0.00010\n",
      "Epoch: 8613|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7704|lr = 0.00010\n",
      "Epoch: 8614|steps:   30|Train Avg Loss: 0.0031 |Test Loss: 1.7663|lr = 0.00010\n",
      "Epoch: 8614|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7783|lr = 0.00010\n",
      "Epoch: 8615|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7807|lr = 0.00010\n",
      "Epoch: 8615|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7615|lr = 0.00010\n",
      "Epoch: 8616|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7876|lr = 0.00010\n",
      "Epoch: 8616|steps:   60|Train Avg Loss: 0.0030 |Test Loss: 1.7913|lr = 0.00010\n",
      "Epoch: 8617|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7683|lr = 0.00010\n",
      "Epoch: 8617|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 8618|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7636|lr = 0.00010\n",
      "Epoch: 8618|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7858|lr = 0.00010\n",
      "Epoch: 8619|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7644|lr = 0.00010\n",
      "Epoch: 8619|steps:   60|Train Avg Loss: 0.0032 |Test Loss: 1.7668|lr = 0.00010\n",
      "Epoch: 8620|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 8620|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7684|lr = 0.00010\n",
      "Epoch: 8621|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7657|lr = 0.00010\n",
      "Epoch: 8621|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7707|lr = 0.00010\n",
      "Epoch: 8622|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7498|lr = 0.00010\n",
      "Epoch: 8622|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7854|lr = 0.00010\n",
      "Epoch: 8623|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 1.7845|lr = 0.00010\n",
      "Epoch: 8623|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7744|lr = 0.00010\n",
      "Epoch: 8624|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7787|lr = 0.00010\n",
      "Epoch: 8624|steps:   60|Train Avg Loss: 0.0026 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 8625|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7617|lr = 0.00010\n",
      "Epoch: 8625|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7561|lr = 0.00010\n",
      "Epoch: 8626|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8626|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7475|lr = 0.00010\n",
      "Epoch: 8627|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7942|lr = 0.00010\n",
      "Epoch: 8627|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7776|lr = 0.00010\n",
      "Epoch: 8628|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 8628|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7722|lr = 0.00010\n",
      "Epoch: 8629|steps:   30|Train Avg Loss: 0.0034 |Test Loss: 1.7698|lr = 0.00010\n",
      "Epoch: 8629|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 8630|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7662|lr = 0.00010\n",
      "Epoch: 8630|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7697|lr = 0.00010\n",
      "Epoch: 8631|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7598|lr = 0.00010\n",
      "Epoch: 8631|steps:   60|Train Avg Loss: 0.0034 |Test Loss: 1.7738|lr = 0.00010\n",
      "Epoch: 8632|steps:   30|Train Avg Loss: 0.0033 |Test Loss: 1.7823|lr = 0.00010\n",
      "Epoch: 8632|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7530|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8633|steps:   30|Train Avg Loss: 0.0026 |Test Loss: 1.7745|lr = 0.00010\n",
      "Epoch: 8633|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7448|lr = 0.00010\n",
      "Epoch: 8634|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 1.7484|lr = 0.00010\n",
      "Epoch: 8634|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7610|lr = 0.00010\n",
      "Epoch: 8635|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 8635|steps:   60|Train Avg Loss: 0.0025 |Test Loss: 1.7244|lr = 0.00010\n",
      "Epoch: 8636|steps:   30|Train Avg Loss: 0.0039 |Test Loss: 1.7843|lr = 0.00010\n",
      "Epoch: 8636|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7539|lr = 0.00010\n",
      "Epoch: 8637|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7605|lr = 0.00010\n",
      "Epoch: 8637|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 8638|steps:   30|Train Avg Loss: 0.0032 |Test Loss: 1.7587|lr = 0.00010\n",
      "Epoch: 8638|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7688|lr = 0.00010\n",
      "Epoch: 8639|steps:   30|Train Avg Loss: 0.0035 |Test Loss: 1.7666|lr = 0.00010\n",
      "Epoch: 8639|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7641|lr = 0.00010\n",
      "Epoch: 8640|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7588|lr = 0.00010\n",
      "Epoch: 8640|steps:   60|Train Avg Loss: 0.0023 |Test Loss: 1.7457|lr = 0.00010\n",
      "Epoch: 8641|steps:   30|Train Avg Loss: 0.0049 |Test Loss: 1.7479|lr = 0.00010\n",
      "Epoch: 8641|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 1.7642|lr = 0.00010\n",
      "Epoch: 8642|steps:   30|Train Avg Loss: 0.0041 |Test Loss: 1.7626|lr = 0.00010\n",
      "Epoch: 8642|steps:   60|Train Avg Loss: 0.0035 |Test Loss: 1.7536|lr = 0.00010\n",
      "Epoch: 8643|steps:   30|Train Avg Loss: 0.0027 |Test Loss: 1.7520|lr = 0.00010\n",
      "Epoch: 8643|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7715|lr = 0.00010\n",
      "Epoch: 8644|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 1.7601|lr = 0.00010\n",
      "Epoch: 8644|steps:   60|Train Avg Loss: 0.0027 |Test Loss: 1.7495|lr = 0.00010\n",
      "Epoch: 8645|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7669|lr = 0.00010\n",
      "Epoch: 8645|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 1.7814|lr = 0.00010\n",
      "Epoch: 8646|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 1.7634|lr = 0.00010\n",
      "Epoch: 8646|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 1.7583|lr = 0.00010\n",
      "Epoch: 8647|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 1.7771|lr = 0.00010\n",
      "Epoch: 8647|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 1.7721|lr = 0.00010\n",
      "Epoch: 8648|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7914|lr = 0.00010\n",
      "Epoch: 8648|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 1.7652|lr = 0.00010\n",
      "Epoch: 8649|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 1.7661|lr = 0.00010\n",
      "Epoch: 8649|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 1.7863|lr = 0.00010\n",
      "Epoch: 8650|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 1.7742|lr = 0.00010\n",
      "Epoch: 8650|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 1.7658|lr = 0.00010\n",
      "Epoch: 8651|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 1.7612|lr = 0.00010\n",
      "Epoch: 8651|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7607|lr = 0.00010\n",
      "Epoch: 8652|steps:   30|Train Avg Loss: 0.0028 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 8652|steps:   60|Train Avg Loss: 0.0031 |Test Loss: 1.7720|lr = 0.00010\n",
      "Epoch: 8653|steps:   30|Train Avg Loss: 0.0036 |Test Loss: 1.7589|lr = 0.00010\n",
      "Epoch: 8653|steps:   60|Train Avg Loss: 0.0029 |Test Loss: 1.7464|lr = 0.00010\n",
      "Epoch: 8654|steps:   30|Train Avg Loss: 0.0030 |Test Loss: 1.7820|lr = 0.00010\n",
      "Epoch: 8654|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7766|lr = 0.00010\n",
      "Epoch: 8655|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 1.7804|lr = 0.00010\n",
      "Epoch: 8655|steps:   60|Train Avg Loss: 0.0028 |Test Loss: 1.7641|lr = 0.00010\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "TEST_LOSS = []\n",
    "TEST_ACC = []\n",
    "TRAIN_ACC = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,TIME_STEP,INPUT_SIZE)\n",
    "        # start trainnig \n",
    "        output = model(inputs)\n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,targets)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        # back propagation\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 30 steps\n",
    "        if((step+1) %30 == 0):\n",
    "            # average of loss in 30 steps\n",
    "            avg = loss_total / 30\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # calculate the accuracy of training \n",
    "            '''\n",
    "            code\n",
    "            '''\n",
    "            \n",
    "            # calculate the accuracy of using testing data as inputs\n",
    "            '''\n",
    "            code\n",
    "            '''\n",
    "            test_output = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "            test_loss = loss_func(test_output,test_labels)\n",
    "            TEST_LOSS.append(test_loss.tolist())\n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f |Test Loss: %.4f|lr = %.5f\"\n",
    "                  %(epoch+1,step+1,avg,test_loss,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "test = test.detach().numpy()\n",
    "test = scaler.inverse_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.linspace(1,231,231)\n",
    "y1 = stock_df[2208:2439][\"Close\"].values\n",
    "y2 = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(labels=['Real', 'Predicted'],  loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.linspace(1,200,200)\n",
    "y1 = np.array(LOSS)\n",
    "y2 = np.array(TEST_LOSS)\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(labels=['Train loss', 'Test loss'],  loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stock_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t =stock_df[2208:2439]\n",
    "t[\"predict\"] = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t.to_csv(\"C:/Users/acer/Desktop/LAB/123.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
