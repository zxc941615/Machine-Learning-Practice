{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "D:\\anaconda\\envs\\pytorch\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "stock_path = \"C:/Users/acer/Desktop/LAB/lab2_2_3.csv\"\n",
    "stock_df = pd.read_csv(stock_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'], format=\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Change(%)</th>\n",
       "      <th>market_Change(%)</th>\n",
       "      <th>RSV</th>\n",
       "      <th>K</th>\n",
       "      <th>D</th>\n",
       "      <th>...</th>\n",
       "      <th>D(%)</th>\n",
       "      <th>SMA(%)</th>\n",
       "      <th>EMA(%)</th>\n",
       "      <th>RSI(%)</th>\n",
       "      <th>RISE</th>\n",
       "      <th>KR</th>\n",
       "      <th>DR</th>\n",
       "      <th>SMAR</th>\n",
       "      <th>EMAR</th>\n",
       "      <th>RSIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>27.514700</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>27.567301</td>\n",
       "      <td>1.158100</td>\n",
       "      <td>-0.173601</td>\n",
       "      <td>75.128048</td>\n",
       "      <td>58.376016</td>\n",
       "      <td>52.792005</td>\n",
       "      <td>...</td>\n",
       "      <td>5.584011</td>\n",
       "      <td>1.432282</td>\n",
       "      <td>1.432282</td>\n",
       "      <td>34.20926483</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>29.461300</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>29.461300</td>\n",
       "      <td>6.870455</td>\n",
       "      <td>-1.357631</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>72.250677</td>\n",
       "      <td>59.278229</td>\n",
       "      <td>...</td>\n",
       "      <td>12.286375</td>\n",
       "      <td>2.217945</td>\n",
       "      <td>3.981222</td>\n",
       "      <td>11.76528431</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-14</td>\n",
       "      <td>30.197800</td>\n",
       "      <td>31.513100</td>\n",
       "      <td>30.197800</td>\n",
       "      <td>31.513100</td>\n",
       "      <td>6.964391</td>\n",
       "      <td>1.139757</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>81.500452</td>\n",
       "      <td>66.685637</td>\n",
       "      <td>...</td>\n",
       "      <td>12.496000</td>\n",
       "      <td>4.166085</td>\n",
       "      <td>4.812679</td>\n",
       "      <td>27.04417082</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>6.844449</td>\n",
       "      <td>0.807109</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>87.666968</td>\n",
       "      <td>73.679414</td>\n",
       "      <td>...</td>\n",
       "      <td>10.487681</td>\n",
       "      <td>4.971480</td>\n",
       "      <td>5.390591</td>\n",
       "      <td>0.893127514</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-18</td>\n",
       "      <td>35.774399</td>\n",
       "      <td>35.984901</td>\n",
       "      <td>34.564400</td>\n",
       "      <td>35.984901</td>\n",
       "      <td>6.875269</td>\n",
       "      <td>-0.228187</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.777978</td>\n",
       "      <td>79.712269</td>\n",
       "      <td>...</td>\n",
       "      <td>8.187979</td>\n",
       "      <td>6.098702</td>\n",
       "      <td>5.818715</td>\n",
       "      <td>4.021770969</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>56.599998</td>\n",
       "      <td>57.400002</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.706717</td>\n",
       "      <td>0.265105</td>\n",
       "      <td>83.333302</td>\n",
       "      <td>81.904480</td>\n",
       "      <td>77.871076</td>\n",
       "      <td>...</td>\n",
       "      <td>2.658650</td>\n",
       "      <td>1.174935</td>\n",
       "      <td>0.835216</td>\n",
       "      <td>9.756151101</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>59.200001</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>58.299999</td>\n",
       "      <td>2.280700</td>\n",
       "      <td>-0.059294</td>\n",
       "      <td>85.245876</td>\n",
       "      <td>83.018279</td>\n",
       "      <td>79.586810</td>\n",
       "      <td>...</td>\n",
       "      <td>2.203301</td>\n",
       "      <td>1.264515</td>\n",
       "      <td>1.205567</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>58.599998</td>\n",
       "      <td>59.599998</td>\n",
       "      <td>58.400002</td>\n",
       "      <td>59.400002</td>\n",
       "      <td>1.886798</td>\n",
       "      <td>0.754770</td>\n",
       "      <td>96.774256</td>\n",
       "      <td>87.603605</td>\n",
       "      <td>82.259075</td>\n",
       "      <td>...</td>\n",
       "      <td>3.357673</td>\n",
       "      <td>1.401631</td>\n",
       "      <td>1.381961</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>9.932661</td>\n",
       "      <td>-0.316085</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.735736</td>\n",
       "      <td>85.417962</td>\n",
       "      <td>...</td>\n",
       "      <td>3.840169</td>\n",
       "      <td>2.814779</td>\n",
       "      <td>3.607053</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>3.369061</td>\n",
       "      <td>-0.466512</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>94.490491</td>\n",
       "      <td>88.442138</td>\n",
       "      <td>...</td>\n",
       "      <td>3.540445</td>\n",
       "      <td>2.786605</td>\n",
       "      <td>3.541341</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2283 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Open       High        Low      Close  Change(%)  \\\n",
       "0    2010-01-12  27.514700  28.830000  27.251699  27.567301   1.158100   \n",
       "1    2010-01-13  27.251699  29.461300  27.251699  29.461300   6.870455   \n",
       "2    2010-01-14  30.197800  31.513100  30.197800  31.513100   6.964391   \n",
       "3    2010-01-15  33.669998  33.669998  33.669998  33.669998   6.844449   \n",
       "4    2010-01-18  35.774399  35.984901  34.564400  35.984901   6.875269   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2278 2019-12-25  56.599998  57.400002  56.299999  57.000000   0.706717   \n",
       "2279 2019-12-26  58.000000  59.200001  57.599998  58.299999   2.280700   \n",
       "2280 2019-12-27  58.599998  59.599998  58.400002  59.400002   1.886798   \n",
       "2281 2019-12-30  61.799999  65.300003  61.799999  65.300003   9.932661   \n",
       "2282 2019-12-31  65.300003  67.500000  64.500000  67.500000   3.369061   \n",
       "\n",
       "      market_Change(%)         RSV          K          D  ...       D(%)  \\\n",
       "0            -0.173601   75.128048  58.376016  52.792005  ...   5.584011   \n",
       "1            -1.357631  100.000000  72.250677  59.278229  ...  12.286375   \n",
       "2             1.139757  100.000000  81.500452  66.685637  ...  12.496000   \n",
       "3             0.807109  100.000000  87.666968  73.679414  ...  10.487681   \n",
       "4            -0.228187  100.000000  91.777978  79.712269  ...   8.187979   \n",
       "...                ...         ...        ...        ...  ...        ...   \n",
       "2278          0.265105   83.333302  81.904480  77.871076  ...   2.658650   \n",
       "2279         -0.059294   85.245876  83.018279  79.586810  ...   2.203301   \n",
       "2280          0.754770   96.774256  87.603605  82.259075  ...   3.357673   \n",
       "2281         -0.316085  100.000000  91.735736  85.417962  ...   3.840169   \n",
       "2282         -0.466512  100.000000  94.490491  88.442138  ...   3.540445   \n",
       "\n",
       "        SMA(%)    EMA(%)       RSI(%)  RISE  KR  DR SMAR  EMAR  RSIR  \n",
       "0     1.432282  1.432282  34.20926483     1   1   1    1     1     1  \n",
       "1     2.217945  3.981222  11.76528431     1   1   1    1     1     1  \n",
       "2     4.166085  4.812679  27.04417082     1  -1   1    1     1    -1  \n",
       "3     4.971480  5.390591  0.893127514     1  -1   1    1     1    -1  \n",
       "4     6.098702  5.818715  4.021770969     1  -1   1    1     1    -1  \n",
       "...        ...       ...          ...   ...  ..  ..  ...   ...   ...  \n",
       "2278  1.174935  0.835216  9.756151101     1  -1   1    1     1    -1  \n",
       "2279  1.264515  1.205567            0     1  -1   1    1     1    -1  \n",
       "2280  1.401631  1.381961            0     1  -1  -1    1     1    -1  \n",
       "2281  2.814779  3.607053            0     1  -1  -1    1     1    -1  \n",
       "2282  2.786605  3.541341            0     1  -1  -1    1     1    -1  \n",
       "\n",
       "[2283 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.DataFrame(stock_df[[\"Open\",\"High\",\"Low\",\"Close\",\"Change(%)\",\"market_Change(%)\",\"RSV\",\"K\",\"D\",\"SMA\",\"EMA\",\"RSI\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\",\"RISE\",\"KR\",\"DR\",\"SMAR\",\"EMAR\",\"RSIR\"]].values,columns = [\"Open\",\"High\",\"Low\",\"Close\",\"Change(%)\",\"market_Change(%)\",\"RSV\",\"K\",\"D\",\"SMA\",\"EMA\",\"RSI\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\",\"RISE\",\"KR\",\"DR\",\"SMAR\",\"EMAR\",\"RSIR\"],index = stock_df[\"Date\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Change(%)</th>\n",
       "      <th>market_Change(%)</th>\n",
       "      <th>RSV</th>\n",
       "      <th>K</th>\n",
       "      <th>D</th>\n",
       "      <th>SMA</th>\n",
       "      <th>...</th>\n",
       "      <th>K(%)</th>\n",
       "      <th>D(%)</th>\n",
       "      <th>SMA(%)</th>\n",
       "      <th>EMA(%)</th>\n",
       "      <th>RISE</th>\n",
       "      <th>KR</th>\n",
       "      <th>DR</th>\n",
       "      <th>SMAR</th>\n",
       "      <th>EMAR</th>\n",
       "      <th>RSIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>27.514700</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>27.567301</td>\n",
       "      <td>1.158100</td>\n",
       "      <td>-0.173601</td>\n",
       "      <td>75.128048</td>\n",
       "      <td>58.376016</td>\n",
       "      <td>52.792005</td>\n",
       "      <td>25.414129</td>\n",
       "      <td>...</td>\n",
       "      <td>16.752032</td>\n",
       "      <td>5.584011</td>\n",
       "      <td>1.432282</td>\n",
       "      <td>1.432282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>27.251699</td>\n",
       "      <td>29.461300</td>\n",
       "      <td>27.251699</td>\n",
       "      <td>29.461300</td>\n",
       "      <td>6.870455</td>\n",
       "      <td>-1.357631</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>72.250677</td>\n",
       "      <td>59.278229</td>\n",
       "      <td>25.977800</td>\n",
       "      <td>...</td>\n",
       "      <td>23.767743</td>\n",
       "      <td>12.286375</td>\n",
       "      <td>2.217945</td>\n",
       "      <td>3.981222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>30.197800</td>\n",
       "      <td>31.513100</td>\n",
       "      <td>30.197800</td>\n",
       "      <td>31.513100</td>\n",
       "      <td>6.964391</td>\n",
       "      <td>1.139757</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>81.500452</td>\n",
       "      <td>66.685637</td>\n",
       "      <td>27.060057</td>\n",
       "      <td>...</td>\n",
       "      <td>12.802336</td>\n",
       "      <td>12.496000</td>\n",
       "      <td>4.166085</td>\n",
       "      <td>4.812679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>33.669998</td>\n",
       "      <td>6.844449</td>\n",
       "      <td>0.807109</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>87.666968</td>\n",
       "      <td>73.679414</td>\n",
       "      <td>28.405343</td>\n",
       "      <td>...</td>\n",
       "      <td>7.566236</td>\n",
       "      <td>10.487681</td>\n",
       "      <td>4.971480</td>\n",
       "      <td>5.390591</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-18</th>\n",
       "      <td>35.774399</td>\n",
       "      <td>35.984901</td>\n",
       "      <td>34.564400</td>\n",
       "      <td>35.984901</td>\n",
       "      <td>6.875269</td>\n",
       "      <td>-0.228187</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.777978</td>\n",
       "      <td>79.712269</td>\n",
       "      <td>30.137700</td>\n",
       "      <td>...</td>\n",
       "      <td>4.689350</td>\n",
       "      <td>8.187979</td>\n",
       "      <td>6.098702</td>\n",
       "      <td>5.818715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>56.599998</td>\n",
       "      <td>57.400002</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.706717</td>\n",
       "      <td>0.265105</td>\n",
       "      <td>83.333302</td>\n",
       "      <td>81.904480</td>\n",
       "      <td>77.871076</td>\n",
       "      <td>55.357143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879924</td>\n",
       "      <td>2.658650</td>\n",
       "      <td>1.174935</td>\n",
       "      <td>0.835216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>59.200001</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>58.299999</td>\n",
       "      <td>2.280700</td>\n",
       "      <td>-0.059294</td>\n",
       "      <td>85.245876</td>\n",
       "      <td>83.018279</td>\n",
       "      <td>79.586810</td>\n",
       "      <td>56.057142</td>\n",
       "      <td>...</td>\n",
       "      <td>1.359875</td>\n",
       "      <td>2.203301</td>\n",
       "      <td>1.264515</td>\n",
       "      <td>1.205567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>58.599998</td>\n",
       "      <td>59.599998</td>\n",
       "      <td>58.400002</td>\n",
       "      <td>59.400002</td>\n",
       "      <td>1.886798</td>\n",
       "      <td>0.754770</td>\n",
       "      <td>96.774256</td>\n",
       "      <td>87.603605</td>\n",
       "      <td>82.259075</td>\n",
       "      <td>56.842857</td>\n",
       "      <td>...</td>\n",
       "      <td>5.523272</td>\n",
       "      <td>3.357673</td>\n",
       "      <td>1.401631</td>\n",
       "      <td>1.381961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>61.799999</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>9.932661</td>\n",
       "      <td>-0.316085</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.735736</td>\n",
       "      <td>85.417962</td>\n",
       "      <td>58.442857</td>\n",
       "      <td>...</td>\n",
       "      <td>4.716851</td>\n",
       "      <td>3.840169</td>\n",
       "      <td>2.814779</td>\n",
       "      <td>3.607053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>65.300003</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>3.369061</td>\n",
       "      <td>-0.466512</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>94.490491</td>\n",
       "      <td>88.442138</td>\n",
       "      <td>60.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>3.002924</td>\n",
       "      <td>3.540445</td>\n",
       "      <td>2.786605</td>\n",
       "      <td>3.541341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2283 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Change(%)  \\\n",
       "2010-01-12  27.514700  28.830000  27.251699  27.567301   1.158100   \n",
       "2010-01-13  27.251699  29.461300  27.251699  29.461300   6.870455   \n",
       "2010-01-14  30.197800  31.513100  30.197800  31.513100   6.964391   \n",
       "2010-01-15  33.669998  33.669998  33.669998  33.669998   6.844449   \n",
       "2010-01-18  35.774399  35.984901  34.564400  35.984901   6.875269   \n",
       "...               ...        ...        ...        ...        ...   \n",
       "2019-12-25  56.599998  57.400002  56.299999  57.000000   0.706717   \n",
       "2019-12-26  58.000000  59.200001  57.599998  58.299999   2.280700   \n",
       "2019-12-27  58.599998  59.599998  58.400002  59.400002   1.886798   \n",
       "2019-12-30  61.799999  65.300003  61.799999  65.300003   9.932661   \n",
       "2019-12-31  65.300003  67.500000  64.500000  67.500000   3.369061   \n",
       "\n",
       "            market_Change(%)         RSV          K          D        SMA  \\\n",
       "2010-01-12         -0.173601   75.128048  58.376016  52.792005  25.414129   \n",
       "2010-01-13         -1.357631  100.000000  72.250677  59.278229  25.977800   \n",
       "2010-01-14          1.139757  100.000000  81.500452  66.685637  27.060057   \n",
       "2010-01-15          0.807109  100.000000  87.666968  73.679414  28.405343   \n",
       "2010-01-18         -0.228187  100.000000  91.777978  79.712269  30.137700   \n",
       "...                      ...         ...        ...        ...        ...   \n",
       "2019-12-25          0.265105   83.333302  81.904480  77.871076  55.357143   \n",
       "2019-12-26         -0.059294   85.245876  83.018279  79.586810  56.057142   \n",
       "2019-12-27          0.754770   96.774256  87.603605  82.259075  56.842857   \n",
       "2019-12-30         -0.316085  100.000000  91.735736  85.417962  58.442857   \n",
       "2019-12-31         -0.466512  100.000000  94.490491  88.442138  60.071429   \n",
       "\n",
       "            ...       K(%)       D(%)    SMA(%)    EMA(%)  RISE   KR   DR  \\\n",
       "2010-01-12  ...  16.752032   5.584011  1.432282  1.432282   1.0  1.0  1.0   \n",
       "2010-01-13  ...  23.767743  12.286375  2.217945  3.981222   1.0  1.0  1.0   \n",
       "2010-01-14  ...  12.802336  12.496000  4.166085  4.812679   1.0 -1.0  1.0   \n",
       "2010-01-15  ...   7.566236  10.487681  4.971480  5.390591   1.0 -1.0  1.0   \n",
       "2010-01-18  ...   4.689350   8.187979  6.098702  5.818715   1.0 -1.0  1.0   \n",
       "...         ...        ...        ...       ...       ...   ...  ...  ...   \n",
       "2019-12-25  ...   0.879924   2.658650  1.174935  0.835216   1.0 -1.0  1.0   \n",
       "2019-12-26  ...   1.359875   2.203301  1.264515  1.205567   1.0 -1.0  1.0   \n",
       "2019-12-27  ...   5.523272   3.357673  1.401631  1.381961   1.0 -1.0 -1.0   \n",
       "2019-12-30  ...   4.716851   3.840169  2.814779  3.607053   1.0 -1.0 -1.0   \n",
       "2019-12-31  ...   3.002924   3.540445  2.786605  3.541341   1.0 -1.0 -1.0   \n",
       "\n",
       "            SMAR  EMAR  RSIR  \n",
       "2010-01-12   1.0   1.0   1.0  \n",
       "2010-01-13   1.0   1.0   1.0  \n",
       "2010-01-14   1.0   1.0  -1.0  \n",
       "2010-01-15   1.0   1.0  -1.0  \n",
       "2010-01-18   1.0   1.0  -1.0  \n",
       "...          ...   ...   ...  \n",
       "2019-12-25   1.0   1.0  -1.0  \n",
       "2019-12-26   1.0   1.0  -1.0  \n",
       "2019-12-27   1.0   1.0  -1.0  \n",
       "2019-12-30   1.0   1.0  -1.0  \n",
       "2019-12-31   1.0   1.0  -1.0  \n",
       "\n",
       "[2283 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr = stock_df[:2282].corr()\n",
    "sns.heatmap(corr,cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler(feature_range = (-1,1))\n",
    "minmax_df = scaler.fit_transform(stock_df[[\"Change(%)\",\"market_Change(%)\",\"RSV\",\"RSI\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\"]])\n",
    "minmax_df = pd.DataFrame(minmax_df,columns = [\"Change(%)\",\"market_Change(%)\",\"RSV\",\"RSI\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\"],index = stock_df.index)\n",
    "minmax_df[\"RISE\"] = stock_df[\"RISE\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Change(%)</th>\n",
       "      <th>market_Change(%)</th>\n",
       "      <th>RSV</th>\n",
       "      <th>RSI</th>\n",
       "      <th>K(%)</th>\n",
       "      <th>D(%)</th>\n",
       "      <th>SMA(%)</th>\n",
       "      <th>EMA(%)</th>\n",
       "      <th>RISE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>0.117873</td>\n",
       "      <td>0.129217</td>\n",
       "      <td>0.502561</td>\n",
       "      <td>0.342093</td>\n",
       "      <td>-0.619633</td>\n",
       "      <td>-0.354385</td>\n",
       "      <td>0.082550</td>\n",
       "      <td>0.140285</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>0.690164</td>\n",
       "      <td>-0.088579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>-0.566353</td>\n",
       "      <td>-0.209309</td>\n",
       "      <td>0.237017</td>\n",
       "      <td>0.639862</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>0.699575</td>\n",
       "      <td>0.370801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905655</td>\n",
       "      <td>-0.649628</td>\n",
       "      <td>-0.204772</td>\n",
       "      <td>0.620034</td>\n",
       "      <td>0.802823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>0.687559</td>\n",
       "      <td>0.309613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922674</td>\n",
       "      <td>-0.689393</td>\n",
       "      <td>-0.248243</td>\n",
       "      <td>0.778381</td>\n",
       "      <td>0.916090</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-18</th>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.119176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.711241</td>\n",
       "      <td>-0.298021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>0.072651</td>\n",
       "      <td>0.209914</td>\n",
       "      <td>0.666666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.740172</td>\n",
       "      <td>-0.417706</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>0.230340</td>\n",
       "      <td>0.150243</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.736527</td>\n",
       "      <td>-0.427562</td>\n",
       "      <td>0.049566</td>\n",
       "      <td>0.095850</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.190877</td>\n",
       "      <td>0.299985</td>\n",
       "      <td>0.935485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.704908</td>\n",
       "      <td>-0.402575</td>\n",
       "      <td>0.076524</td>\n",
       "      <td>0.130423</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.996951</td>\n",
       "      <td>0.103008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.711033</td>\n",
       "      <td>-0.392131</td>\n",
       "      <td>0.354358</td>\n",
       "      <td>0.566528</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.339378</td>\n",
       "      <td>0.075337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.724049</td>\n",
       "      <td>-0.398619</td>\n",
       "      <td>0.348819</td>\n",
       "      <td>0.553648</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2283 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Change(%)  market_Change(%)       RSV       RSI      K(%)  \\\n",
       "2010-01-12   0.117873          0.129217  0.502561  0.342093 -0.619633   \n",
       "2010-01-13   0.690164         -0.088579  1.000000  0.499994 -0.566353   \n",
       "2010-01-14   0.699575          0.370801  1.000000  0.905655 -0.649628   \n",
       "2010-01-15   0.687559          0.309613  1.000000  0.922674 -0.689393   \n",
       "2010-01-18   0.690647          0.119176  1.000000  1.000000 -0.711241   \n",
       "...               ...               ...       ...       ...       ...   \n",
       "2019-12-25   0.072651          0.209914  0.666666  1.000000 -0.740172   \n",
       "2019-12-26   0.230340          0.150243  0.704918  1.000000 -0.736527   \n",
       "2019-12-27   0.190877          0.299985  0.935485  1.000000 -0.704908   \n",
       "2019-12-30   0.996951          0.103008  1.000000  1.000000 -0.711033   \n",
       "2019-12-31   0.339378          0.075337  1.000000  1.000000 -0.724049   \n",
       "\n",
       "                D(%)    SMA(%)    EMA(%)  RISE  \n",
       "2010-01-12 -0.354385  0.082550  0.140285   1.0  \n",
       "2010-01-13 -0.209309  0.237017  0.639862   1.0  \n",
       "2010-01-14 -0.204772  0.620034  0.802823   1.0  \n",
       "2010-01-15 -0.248243  0.778381  0.916090   1.0  \n",
       "2010-01-18 -0.298021  1.000000  1.000000   1.0  \n",
       "...              ...       ...       ...   ...  \n",
       "2019-12-25 -0.417706  0.031954  0.023264   1.0  \n",
       "2019-12-26 -0.427562  0.049566  0.095850   1.0  \n",
       "2019-12-27 -0.402575  0.076524  0.130423   1.0  \n",
       "2019-12-30 -0.392131  0.354358  0.566528   1.0  \n",
       "2019-12-31 -0.398619  0.348819  0.553648   1.0  \n",
       "\n",
       "[2283 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "EPOCH = 10000\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEP = 30\n",
    "INPUT_SIZE = 9\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of feature:  torch.Size([2253, 30, 9])\n",
      "size of label:  torch.Size([2253])\n"
     ]
    }
   ],
   "source": [
    "# declear training features data\n",
    "features = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i-TIME_STEP:i][[\"Change(%)\",\"market_Change(%)\",\"RSV\",\"RSI\",\"K(%)\",\"D(%)\",\"SMA(%)\",\"EMA(%)\",\"RISE\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = torch.FloatTensor(features)\n",
    "print(\"size of feature: \",features.size())\n",
    "\n",
    "# declear trainging labels data\n",
    "labels = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i:i+1][\"RISE\"]\n",
    "    labels.append(x.tolist())\n",
    "labels = torch.LongTensor(labels).view(-1)\n",
    "print(\"size of label: \",labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random select 90% training set index and 10% testing set index \n",
    "\n",
    "x = np.linspace(0,2252,2253).tolist()\n",
    "for i in range(0,2253):\n",
    "    x[i] = int(x[i])\n",
    "\n",
    "np.random.shuffle(x)\n",
    "training, test = x[:2000], x[2000:]\n",
    "test.sort()\n",
    "training.sort()\n",
    "train_features = features[training]\n",
    "train_labels = labels[training]\n",
    "test_features = features[test]\n",
    "test_labels = labels[test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(train_features,train_labels)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=128,         \n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm2 = torch.nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=32,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        self.dropout=torch.nn.Dropout(p=0.2)\n",
    "        self.out = torch.nn.Linear(32,2)\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "    def forward(self, x):\n",
    "        lstm1_out,_ = self.lstm1(x,None)\n",
    "        lstm1_out = self.dropout(lstm1_out)\n",
    "        lstm1_out = self.relu(lstm1_out)\n",
    "        lstm2_out,_ = self.lstm2(lstm1_out,None)\n",
    "        lstm2_out = self.dropout(lstm2_out)\n",
    "        lstm2_out = self.relu(lstm2_out)\n",
    "        out = self.out(lstm2_out[:,-1,:])\n",
    "        return self.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm1): LSTM(9, 128, batch_first=True)\n",
      "  (lstm2): LSTM(128, 32, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (out): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.00001)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.9,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   10|Train Avg Loss: 0.6933|Accuracy: 0.5312 | Test loss:0.6933| test accuracy :0.5257\n",
      "Epoch:    1|steps:   20|Train Avg Loss: 0.6945|Accuracy: 0.3750 | Test loss:0.6931| test accuracy :0.4941\n",
      "Epoch:    1|steps:   30|Train Avg Loss: 0.6935|Accuracy: 0.4375 | Test loss:0.6928| test accuracy :0.5178\n",
      "Epoch:    1|steps:   40|Train Avg Loss: 0.6929|Accuracy: 0.6250 | Test loss:0.6939| test accuracy :0.4980\n",
      "Epoch:    1|steps:   50|Train Avg Loss: 0.6935|Accuracy: 0.5312 | Test loss:0.6930| test accuracy :0.4901\n",
      "Epoch:    1|steps:   60|Train Avg Loss: 0.6934|Accuracy: 0.4688 | Test loss:0.6932| test accuracy :0.4783\n",
      "Epoch:    2|steps:   10|Train Avg Loss: 0.6928|Accuracy: 0.5625 | Test loss:0.6927| test accuracy :0.5257\n",
      "Epoch:    2|steps:   20|Train Avg Loss: 0.6936|Accuracy: 0.5625 | Test loss:0.6930| test accuracy :0.5178\n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.6931|Accuracy: 0.5000 | Test loss:0.6931| test accuracy :0.5138\n",
      "Epoch:    2|steps:   40|Train Avg Loss: 0.6941|Accuracy: 0.5625 | Test loss:0.6932| test accuracy :0.4783\n",
      "Epoch:    2|steps:   50|Train Avg Loss: 0.6949|Accuracy: 0.5000 | Test loss:0.6931| test accuracy :0.4862\n",
      "Epoch:    2|steps:   60|Train Avg Loss: 0.6929|Accuracy: 0.5938 | Test loss:0.6927| test accuracy :0.5375\n",
      "Epoch:    3|steps:   10|Train Avg Loss: 0.6936|Accuracy: 0.5000 | Test loss:0.6931| test accuracy :0.4980\n",
      "Epoch:    3|steps:   20|Train Avg Loss: 0.6935|Accuracy: 0.4688 | Test loss:0.6930| test accuracy :0.5257\n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.6930|Accuracy: 0.4375 | Test loss:0.6928| test accuracy :0.5415\n",
      "Epoch:    3|steps:   40|Train Avg Loss: 0.6929|Accuracy: 0.5000 | Test loss:0.6929| test accuracy :0.5534\n",
      "Epoch:    3|steps:   50|Train Avg Loss: 0.6933|Accuracy: 0.5000 | Test loss:0.6934| test accuracy :0.4704\n",
      "Epoch:    3|steps:   60|Train Avg Loss: 0.6937|Accuracy: 0.5000 | Test loss:0.6931| test accuracy :0.4783\n",
      "Epoch:    4|steps:   10|Train Avg Loss: 0.6934|Accuracy: 0.5312 | Test loss:0.6929| test accuracy :0.5020\n",
      "Epoch:    4|steps:   20|Train Avg Loss: 0.6928|Accuracy: 0.6875 | Test loss:0.6932| test accuracy :0.4783\n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.6933|Accuracy: 0.4375 | Test loss:0.6931| test accuracy :0.4901\n",
      "Epoch:    4|steps:   40|Train Avg Loss: 0.6927|Accuracy: 0.5938 | Test loss:0.6927| test accuracy :0.5415\n",
      "Epoch:    4|steps:   50|Train Avg Loss: 0.6936|Accuracy: 0.4375 | Test loss:0.6932| test accuracy :0.4941\n",
      "Epoch:    4|steps:   60|Train Avg Loss: 0.6933|Accuracy: 0.5000 | Test loss:0.6924| test accuracy :0.5296\n",
      "Epoch:    5|steps:   10|Train Avg Loss: 0.6926|Accuracy: 0.6250 | Test loss:0.6922| test accuracy :0.5415\n",
      "Epoch:    5|steps:   20|Train Avg Loss: 0.6935|Accuracy: 0.2812 | Test loss:0.6931| test accuracy :0.4901\n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.6933|Accuracy: 0.5000 | Test loss:0.6924| test accuracy :0.5415\n",
      "Epoch:    5|steps:   40|Train Avg Loss: 0.6935|Accuracy: 0.3750 | Test loss:0.6927| test accuracy :0.5375\n",
      "Epoch:    5|steps:   50|Train Avg Loss: 0.6928|Accuracy: 0.5625 | Test loss:0.6928| test accuracy :0.5059\n",
      "Epoch:    5|steps:   60|Train Avg Loss: 0.6934|Accuracy: 0.3750 | Test loss:0.6927| test accuracy :0.5217\n",
      "Epoch:    6|steps:   10|Train Avg Loss: 0.6920|Accuracy: 0.5312 | Test loss:0.6922| test accuracy :0.5059\n",
      "Epoch:    6|steps:   20|Train Avg Loss: 0.6940|Accuracy: 0.5625 | Test loss:0.6924| test accuracy :0.5099\n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.6917|Accuracy: 0.5000 | Test loss:0.6930| test accuracy :0.5020\n",
      "Epoch:    6|steps:   40|Train Avg Loss: 0.6934|Accuracy: 0.5625 | Test loss:0.6929| test accuracy :0.5217\n",
      "Epoch:    6|steps:   50|Train Avg Loss: 0.6936|Accuracy: 0.6250 | Test loss:0.6924| test accuracy :0.5652\n",
      "Epoch:    6|steps:   60|Train Avg Loss: 0.6928|Accuracy: 0.6562 | Test loss:0.6928| test accuracy :0.5059\n",
      "Epoch:    7|steps:   10|Train Avg Loss: 0.6939|Accuracy: 0.5312 | Test loss:0.6938| test accuracy :0.5138\n",
      "Epoch:    7|steps:   20|Train Avg Loss: 0.6925|Accuracy: 0.6250 | Test loss:0.6932| test accuracy :0.4862\n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.6927|Accuracy: 0.5000 | Test loss:0.6943| test accuracy :0.4743\n",
      "Epoch:    7|steps:   40|Train Avg Loss: 0.6939|Accuracy: 0.5312 | Test loss:0.6933| test accuracy :0.4980\n",
      "Epoch:    7|steps:   50|Train Avg Loss: 0.6928|Accuracy: 0.4688 | Test loss:0.6927| test accuracy :0.5178\n",
      "Epoch:    7|steps:   60|Train Avg Loss: 0.6920|Accuracy: 0.5312 | Test loss:0.6922| test accuracy :0.5296\n",
      "Epoch:    8|steps:   10|Train Avg Loss: 0.6919|Accuracy: 0.6562 | Test loss:0.6917| test accuracy :0.5257\n",
      "Epoch:    8|steps:   20|Train Avg Loss: 0.6922|Accuracy: 0.4375 | Test loss:0.6916| test accuracy :0.5059\n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.6912|Accuracy: 0.4375 | Test loss:0.6920| test accuracy :0.5099\n",
      "Epoch:    8|steps:   40|Train Avg Loss: 0.6940|Accuracy: 0.5312 | Test loss:0.6931| test accuracy :0.4941\n",
      "Epoch:    8|steps:   50|Train Avg Loss: 0.6935|Accuracy: 0.3438 | Test loss:0.6922| test accuracy :0.5178\n",
      "Epoch:    8|steps:   60|Train Avg Loss: 0.6936|Accuracy: 0.4688 | Test loss:0.6926| test accuracy :0.5178\n",
      "Epoch:    9|steps:   10|Train Avg Loss: 0.6920|Accuracy: 0.5000 | Test loss:0.6930| test accuracy :0.5178\n",
      "Epoch:    9|steps:   20|Train Avg Loss: 0.6937|Accuracy: 0.6562 | Test loss:0.6931| test accuracy :0.5217\n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.6935|Accuracy: 0.4688 | Test loss:0.6919| test accuracy :0.5692\n",
      "Epoch:    9|steps:   40|Train Avg Loss: 0.6927|Accuracy: 0.5312 | Test loss:0.6913| test accuracy :0.5336\n",
      "Epoch:    9|steps:   50|Train Avg Loss: 0.6934|Accuracy: 0.4688 | Test loss:0.6926| test accuracy :0.5257\n",
      "Epoch:    9|steps:   60|Train Avg Loss: 0.6929|Accuracy: 0.4688 | Test loss:0.6919| test accuracy :0.5573\n",
      "Epoch:   10|steps:   10|Train Avg Loss: 0.6938|Accuracy: 0.5000 | Test loss:0.6919| test accuracy :0.5020\n",
      "Epoch:   10|steps:   20|Train Avg Loss: 0.6919|Accuracy: 0.3750 | Test loss:0.6922| test accuracy :0.5138\n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.6923|Accuracy: 0.5625 | Test loss:0.6911| test accuracy :0.5296\n",
      "Epoch:   10|steps:   40|Train Avg Loss: 0.6934|Accuracy: 0.5000 | Test loss:0.6907| test accuracy :0.5692\n",
      "Epoch:   10|steps:   50|Train Avg Loss: 0.6934|Accuracy: 0.5000 | Test loss:0.6929| test accuracy :0.5257\n",
      "Epoch:   10|steps:   60|Train Avg Loss: 0.6925|Accuracy: 0.5000 | Test loss:0.6922| test accuracy :0.5652\n",
      "Epoch:   11|steps:   10|Train Avg Loss: 0.6939|Accuracy: 0.5625 | Test loss:0.6917| test accuracy :0.5415\n",
      "Epoch:   11|steps:   20|Train Avg Loss: 0.6895|Accuracy: 0.4375 | Test loss:0.6932| test accuracy :0.4941\n",
      "Epoch:   11|steps:   30|Train Avg Loss: 0.6922|Accuracy: 0.4062 | Test loss:0.6913| test accuracy :0.5494\n",
      "Epoch:   11|steps:   40|Train Avg Loss: 0.6907|Accuracy: 0.5625 | Test loss:0.6912| test accuracy :0.5336\n",
      "Epoch:   11|steps:   50|Train Avg Loss: 0.6922|Accuracy: 0.5312 | Test loss:0.6914| test accuracy :0.5296\n",
      "Epoch:   11|steps:   60|Train Avg Loss: 0.6926|Accuracy: 0.4375 | Test loss:0.6922| test accuracy :0.5257\n",
      "Epoch:   12|steps:   10|Train Avg Loss: 0.6900|Accuracy: 0.4688 | Test loss:0.6934| test accuracy :0.5059\n",
      "Epoch:   12|steps:   20|Train Avg Loss: 0.6924|Accuracy: 0.5625 | Test loss:0.6945| test accuracy :0.4980\n",
      "Epoch:   12|steps:   30|Train Avg Loss: 0.6916|Accuracy: 0.5625 | Test loss:0.6930| test accuracy :0.5296\n",
      "Epoch:   12|steps:   40|Train Avg Loss: 0.6934|Accuracy: 0.5625 | Test loss:0.6914| test accuracy :0.5375\n",
      "Epoch:   12|steps:   50|Train Avg Loss: 0.6914|Accuracy: 0.5312 | Test loss:0.6912| test accuracy :0.5296\n",
      "Epoch:   12|steps:   60|Train Avg Loss: 0.6932|Accuracy: 0.3750 | Test loss:0.6917| test accuracy :0.5296\n",
      "Epoch:   13|steps:   10|Train Avg Loss: 0.6907|Accuracy: 0.4375 | Test loss:0.6928| test accuracy :0.5296\n",
      "Epoch:   13|steps:   20|Train Avg Loss: 0.6893|Accuracy: 0.6250 | Test loss:0.6940| test accuracy :0.5059\n",
      "Epoch:   13|steps:   30|Train Avg Loss: 0.6912|Accuracy: 0.5000 | Test loss:0.6933| test accuracy :0.5296\n",
      "Epoch:   13|steps:   40|Train Avg Loss: 0.6902|Accuracy: 0.6250 | Test loss:0.6915| test accuracy :0.5257\n",
      "Epoch:   13|steps:   50|Train Avg Loss: 0.6967|Accuracy: 0.5000 | Test loss:0.6924| test accuracy :0.5217\n",
      "Epoch:   13|steps:   60|Train Avg Loss: 0.6896|Accuracy: 0.5000 | Test loss:0.6915| test accuracy :0.5375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   14|steps:   10|Train Avg Loss: 0.6968|Accuracy: 0.2500 | Test loss:0.6911| test accuracy :0.5415\n",
      "Epoch:   14|steps:   20|Train Avg Loss: 0.6916|Accuracy: 0.6250 | Test loss:0.6918| test accuracy :0.5375\n",
      "Epoch:   14|steps:   30|Train Avg Loss: 0.6913|Accuracy: 0.5625 | Test loss:0.6924| test accuracy :0.5217\n",
      "Epoch:   14|steps:   40|Train Avg Loss: 0.6926|Accuracy: 0.3750 | Test loss:0.6919| test accuracy :0.5415\n",
      "Epoch:   14|steps:   50|Train Avg Loss: 0.6911|Accuracy: 0.5312 | Test loss:0.6927| test accuracy :0.5059\n",
      "Epoch:   14|steps:   60|Train Avg Loss: 0.6925|Accuracy: 0.3750 | Test loss:0.6912| test accuracy :0.5415\n",
      "Epoch:   15|steps:   10|Train Avg Loss: 0.6911|Accuracy: 0.4062 | Test loss:0.6911| test accuracy :0.5178\n",
      "Epoch:   15|steps:   20|Train Avg Loss: 0.6937|Accuracy: 0.6250 | Test loss:0.6913| test accuracy :0.5059\n",
      "Epoch:   15|steps:   30|Train Avg Loss: 0.6914|Accuracy: 0.5312 | Test loss:0.6917| test accuracy :0.5217\n",
      "Epoch:   15|steps:   40|Train Avg Loss: 0.6924|Accuracy: 0.5938 | Test loss:0.6920| test accuracy :0.4822\n",
      "Epoch:   15|steps:   50|Train Avg Loss: 0.6892|Accuracy: 0.4688 | Test loss:0.6931| test accuracy :0.5375\n",
      "Epoch:   15|steps:   60|Train Avg Loss: 0.6920|Accuracy: 0.5625 | Test loss:0.6932| test accuracy :0.4862\n",
      "Epoch:   16|steps:   10|Train Avg Loss: 0.6935|Accuracy: 0.4062 | Test loss:0.6932| test accuracy :0.5020\n",
      "Epoch:   16|steps:   20|Train Avg Loss: 0.6882|Accuracy: 0.4688 | Test loss:0.6924| test accuracy :0.5138\n",
      "Epoch:   16|steps:   30|Train Avg Loss: 0.6956|Accuracy: 0.5312 | Test loss:0.6915| test accuracy :0.5217\n",
      "Epoch:   16|steps:   40|Train Avg Loss: 0.6898|Accuracy: 0.5312 | Test loss:0.6915| test accuracy :0.5257\n",
      "Epoch:   16|steps:   50|Train Avg Loss: 0.6913|Accuracy: 0.4375 | Test loss:0.6927| test accuracy :0.5059\n",
      "Epoch:   16|steps:   60|Train Avg Loss: 0.6889|Accuracy: 0.5625 | Test loss:0.6909| test accuracy :0.5138\n",
      "Epoch:   17|steps:   10|Train Avg Loss: 0.6921|Accuracy: 0.6875 | Test loss:0.6912| test accuracy :0.5217\n",
      "Epoch:   17|steps:   20|Train Avg Loss: 0.6890|Accuracy: 0.6250 | Test loss:0.6909| test accuracy :0.5375\n",
      "Epoch:   17|steps:   30|Train Avg Loss: 0.6875|Accuracy: 0.5938 | Test loss:0.6928| test accuracy :0.4862\n",
      "Epoch:   17|steps:   40|Train Avg Loss: 0.6910|Accuracy: 0.4688 | Test loss:0.6953| test accuracy :0.5099\n",
      "Epoch:   17|steps:   50|Train Avg Loss: 0.6939|Accuracy: 0.4688 | Test loss:0.6937| test accuracy :0.4862\n",
      "Epoch:   17|steps:   60|Train Avg Loss: 0.6894|Accuracy: 0.6562 | Test loss:0.6911| test accuracy :0.5059\n",
      "Epoch:   18|steps:   10|Train Avg Loss: 0.6895|Accuracy: 0.6562 | Test loss:0.6924| test accuracy :0.4980\n",
      "Epoch:   18|steps:   20|Train Avg Loss: 0.6856|Accuracy: 0.6875 | Test loss:0.6925| test accuracy :0.5217\n",
      "Epoch:   18|steps:   30|Train Avg Loss: 0.6881|Accuracy: 0.7188 | Test loss:0.6930| test accuracy :0.5099\n",
      "Epoch:   18|steps:   40|Train Avg Loss: 0.7024|Accuracy: 0.5938 | Test loss:0.6936| test accuracy :0.5296\n",
      "Epoch:   18|steps:   50|Train Avg Loss: 0.6902|Accuracy: 0.5625 | Test loss:0.6967| test accuracy :0.4901\n",
      "Epoch:   18|steps:   60|Train Avg Loss: 0.6888|Accuracy: 0.5000 | Test loss:0.6933| test accuracy :0.4941\n",
      "Epoch:   19|steps:   10|Train Avg Loss: 0.6904|Accuracy: 0.4688 | Test loss:0.6946| test accuracy :0.5178\n",
      "Epoch:   19|steps:   20|Train Avg Loss: 0.6926|Accuracy: 0.4688 | Test loss:0.6945| test accuracy :0.4862\n",
      "Epoch:   19|steps:   30|Train Avg Loss: 0.6889|Accuracy: 0.4062 | Test loss:0.6905| test accuracy :0.5217\n",
      "Epoch:   19|steps:   40|Train Avg Loss: 0.6892|Accuracy: 0.5000 | Test loss:0.6936| test accuracy :0.4822\n",
      "Epoch:   19|steps:   50|Train Avg Loss: 0.6929|Accuracy: 0.6562 | Test loss:0.6914| test accuracy :0.5217\n",
      "Epoch:   19|steps:   60|Train Avg Loss: 0.6885|Accuracy: 0.6875 | Test loss:0.6928| test accuracy :0.4980\n",
      "Epoch:   20|steps:   10|Train Avg Loss: 0.6905|Accuracy: 0.4375 | Test loss:0.6961| test accuracy :0.4941\n",
      "Epoch:   20|steps:   20|Train Avg Loss: 0.6913|Accuracy: 0.5000 | Test loss:0.6954| test accuracy :0.4585\n",
      "Epoch:   20|steps:   30|Train Avg Loss: 0.6843|Accuracy: 0.6875 | Test loss:0.6930| test accuracy :0.4980\n",
      "Epoch:   20|steps:   40|Train Avg Loss: 0.6909|Accuracy: 0.5625 | Test loss:0.6878| test accuracy :0.5336\n",
      "Epoch:   20|steps:   50|Train Avg Loss: 0.6858|Accuracy: 0.7812 | Test loss:0.6910| test accuracy :0.5178\n",
      "Epoch:   20|steps:   60|Train Avg Loss: 0.6928|Accuracy: 0.4375 | Test loss:0.6907| test accuracy :0.5217\n",
      "Epoch:   21|steps:   10|Train Avg Loss: 0.6933|Accuracy: 0.4062 | Test loss:0.6955| test accuracy :0.4704\n",
      "Epoch:   21|steps:   20|Train Avg Loss: 0.6896|Accuracy: 0.5312 | Test loss:0.6908| test accuracy :0.5217\n",
      "Epoch:   21|steps:   30|Train Avg Loss: 0.6908|Accuracy: 0.5000 | Test loss:0.6936| test accuracy :0.4941\n",
      "Epoch:   21|steps:   40|Train Avg Loss: 0.6863|Accuracy: 0.5938 | Test loss:0.6943| test accuracy :0.4862\n",
      "Epoch:   21|steps:   50|Train Avg Loss: 0.6921|Accuracy: 0.5000 | Test loss:0.6918| test accuracy :0.5099\n",
      "Epoch:   21|steps:   60|Train Avg Loss: 0.6846|Accuracy: 0.6875 | Test loss:0.6916| test accuracy :0.5099\n",
      "Epoch:   22|steps:   10|Train Avg Loss: 0.6857|Accuracy: 0.4688 | Test loss:0.6975| test accuracy :0.4783\n",
      "Epoch:   22|steps:   20|Train Avg Loss: 0.6897|Accuracy: 0.5625 | Test loss:0.6997| test accuracy :0.4704\n",
      "Epoch:   22|steps:   30|Train Avg Loss: 0.6784|Accuracy: 0.7188 | Test loss:0.6970| test accuracy :0.4743\n",
      "Epoch:   22|steps:   40|Train Avg Loss: 0.6893|Accuracy: 0.5312 | Test loss:0.6971| test accuracy :0.4466\n",
      "Epoch:   22|steps:   50|Train Avg Loss: 0.6926|Accuracy: 0.6250 | Test loss:0.6932| test accuracy :0.4980\n",
      "Epoch:   22|steps:   60|Train Avg Loss: 0.6926|Accuracy: 0.5312 | Test loss:0.6937| test accuracy :0.5138\n",
      "Epoch:   23|steps:   10|Train Avg Loss: 0.6889|Accuracy: 0.5938 | Test loss:0.6920| test accuracy :0.5375\n",
      "Epoch:   23|steps:   20|Train Avg Loss: 0.6850|Accuracy: 0.6250 | Test loss:0.6907| test accuracy :0.5296\n",
      "Epoch:   23|steps:   30|Train Avg Loss: 0.6902|Accuracy: 0.5938 | Test loss:0.6963| test accuracy :0.4862\n",
      "Epoch:   23|steps:   40|Train Avg Loss: 0.6827|Accuracy: 0.5625 | Test loss:0.6913| test accuracy :0.5099\n",
      "Epoch:   23|steps:   50|Train Avg Loss: 0.6897|Accuracy: 0.5000 | Test loss:0.6972| test accuracy :0.5020\n",
      "Epoch:   23|steps:   60|Train Avg Loss: 0.6939|Accuracy: 0.6875 | Test loss:0.6979| test accuracy :0.5020\n",
      "Epoch:   24|steps:   10|Train Avg Loss: 0.6798|Accuracy: 0.6562 | Test loss:0.6986| test accuracy :0.4704\n",
      "Epoch:   24|steps:   20|Train Avg Loss: 0.6908|Accuracy: 0.6250 | Test loss:0.7003| test accuracy :0.4901\n",
      "Epoch:   24|steps:   30|Train Avg Loss: 0.6904|Accuracy: 0.5000 | Test loss:0.6961| test accuracy :0.5217\n",
      "Epoch:   24|steps:   40|Train Avg Loss: 0.6893|Accuracy: 0.5312 | Test loss:0.6905| test accuracy :0.5573\n",
      "Epoch:   24|steps:   50|Train Avg Loss: 0.6905|Accuracy: 0.5625 | Test loss:0.6885| test accuracy :0.5059\n",
      "Epoch:   24|steps:   60|Train Avg Loss: 0.6849|Accuracy: 0.7500 | Test loss:0.6958| test accuracy :0.4980\n",
      "Epoch:   25|steps:   10|Train Avg Loss: 0.6906|Accuracy: 0.3750 | Test loss:0.6971| test accuracy :0.4980\n",
      "Epoch:   25|steps:   20|Train Avg Loss: 0.6790|Accuracy: 0.5625 | Test loss:0.6983| test accuracy :0.4901\n",
      "Epoch:   25|steps:   30|Train Avg Loss: 0.6980|Accuracy: 0.6250 | Test loss:0.6987| test accuracy :0.5059\n",
      "Epoch:   25|steps:   40|Train Avg Loss: 0.6711|Accuracy: 0.5625 | Test loss:0.6969| test accuracy :0.4980\n",
      "Epoch:   25|steps:   50|Train Avg Loss: 0.6891|Accuracy: 0.6562 | Test loss:0.6948| test accuracy :0.5099\n",
      "Epoch:   25|steps:   60|Train Avg Loss: 0.6909|Accuracy: 0.4688 | Test loss:0.6951| test accuracy :0.5059\n",
      "Epoch:   26|steps:   10|Train Avg Loss: 0.6898|Accuracy: 0.6250 | Test loss:0.6927| test accuracy :0.5178\n",
      "Epoch:   26|steps:   20|Train Avg Loss: 0.6914|Accuracy: 0.5000 | Test loss:0.6896| test accuracy :0.5415\n",
      "Epoch:   26|steps:   30|Train Avg Loss: 0.6831|Accuracy: 0.4688 | Test loss:0.6923| test accuracy :0.5059\n",
      "Epoch:   26|steps:   40|Train Avg Loss: 0.6821|Accuracy: 0.4375 | Test loss:0.6990| test accuracy :0.4941\n",
      "Epoch:   26|steps:   50|Train Avg Loss: 0.6746|Accuracy: 0.6562 | Test loss:0.6971| test accuracy :0.5138\n",
      "Epoch:   26|steps:   60|Train Avg Loss: 0.6838|Accuracy: 0.5312 | Test loss:0.6986| test accuracy :0.5099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   27|steps:   10|Train Avg Loss: 0.6848|Accuracy: 0.6250 | Test loss:0.6975| test accuracy :0.4862\n",
      "Epoch:   27|steps:   20|Train Avg Loss: 0.6925|Accuracy: 0.4688 | Test loss:0.6944| test accuracy :0.5099\n",
      "Epoch:   27|steps:   30|Train Avg Loss: 0.6871|Accuracy: 0.5625 | Test loss:0.7001| test accuracy :0.4743\n",
      "Epoch:   27|steps:   40|Train Avg Loss: 0.6800|Accuracy: 0.5625 | Test loss:0.6961| test accuracy :0.4901\n",
      "Epoch:   27|steps:   50|Train Avg Loss: 0.6819|Accuracy: 0.5625 | Test loss:0.7011| test accuracy :0.4980\n",
      "Epoch:   27|steps:   60|Train Avg Loss: 0.6901|Accuracy: 0.6250 | Test loss:0.7007| test accuracy :0.4980\n",
      "Epoch:   28|steps:   10|Train Avg Loss: 0.6875|Accuracy: 0.4375 | Test loss:0.6920| test accuracy :0.5296\n",
      "Epoch:   28|steps:   20|Train Avg Loss: 0.6872|Accuracy: 0.5625 | Test loss:0.6895| test accuracy :0.5534\n",
      "Epoch:   28|steps:   30|Train Avg Loss: 0.6847|Accuracy: 0.6562 | Test loss:0.6910| test accuracy :0.5455\n",
      "Epoch:   28|steps:   40|Train Avg Loss: 0.6824|Accuracy: 0.5625 | Test loss:0.6997| test accuracy :0.4980\n",
      "Epoch:   28|steps:   50|Train Avg Loss: 0.6897|Accuracy: 0.5000 | Test loss:0.7006| test accuracy :0.5059\n",
      "Epoch:   28|steps:   60|Train Avg Loss: 0.6839|Accuracy: 0.5625 | Test loss:0.6980| test accuracy :0.4980\n",
      "Epoch:   29|steps:   10|Train Avg Loss: 0.6899|Accuracy: 0.4062 | Test loss:0.6957| test accuracy :0.4980\n",
      "Epoch:   29|steps:   20|Train Avg Loss: 0.6720|Accuracy: 0.5625 | Test loss:0.6995| test accuracy :0.4704\n",
      "Epoch:   29|steps:   30|Train Avg Loss: 0.6819|Accuracy: 0.6250 | Test loss:0.6990| test accuracy :0.4743\n",
      "Epoch:   29|steps:   40|Train Avg Loss: 0.6825|Accuracy: 0.6250 | Test loss:0.6984| test accuracy :0.4941\n",
      "Epoch:   29|steps:   50|Train Avg Loss: 0.6758|Accuracy: 0.6875 | Test loss:0.6914| test accuracy :0.5099\n",
      "Epoch:   29|steps:   60|Train Avg Loss: 0.6966|Accuracy: 0.5312 | Test loss:0.6936| test accuracy :0.5217\n",
      "Epoch:   30|steps:   10|Train Avg Loss: 0.6698|Accuracy: 0.5938 | Test loss:0.6951| test accuracy :0.5020\n",
      "Epoch:   30|steps:   20|Train Avg Loss: 0.6869|Accuracy: 0.5000 | Test loss:0.6929| test accuracy :0.5099\n",
      "Epoch:   30|steps:   30|Train Avg Loss: 0.6829|Accuracy: 0.5625 | Test loss:0.6992| test accuracy :0.5138\n",
      "Epoch:   30|steps:   40|Train Avg Loss: 0.6829|Accuracy: 0.6250 | Test loss:0.7021| test accuracy :0.4980\n",
      "Epoch:   30|steps:   50|Train Avg Loss: 0.6779|Accuracy: 0.5938 | Test loss:0.7000| test accuracy :0.5099\n",
      "Epoch:   30|steps:   60|Train Avg Loss: 0.6977|Accuracy: 0.4688 | Test loss:0.6895| test accuracy :0.5178\n",
      "Epoch:   31|steps:   10|Train Avg Loss: 0.6867|Accuracy: 0.6250 | Test loss:0.6913| test accuracy :0.4980\n",
      "Epoch:   31|steps:   20|Train Avg Loss: 0.6795|Accuracy: 0.4375 | Test loss:0.6981| test accuracy :0.4625\n",
      "Epoch:   31|steps:   30|Train Avg Loss: 0.6843|Accuracy: 0.6562 | Test loss:0.6951| test accuracy :0.4743\n",
      "Epoch:   31|steps:   40|Train Avg Loss: 0.6882|Accuracy: 0.4375 | Test loss:0.6956| test accuracy :0.4783\n",
      "Epoch:   31|steps:   50|Train Avg Loss: 0.6826|Accuracy: 0.5312 | Test loss:0.6979| test accuracy :0.4704\n",
      "Epoch:   31|steps:   60|Train Avg Loss: 0.6809|Accuracy: 0.6562 | Test loss:0.6985| test accuracy :0.5059\n",
      "Epoch:   32|steps:   10|Train Avg Loss: 0.6863|Accuracy: 0.5625 | Test loss:0.7027| test accuracy :0.4980\n",
      "Epoch:   32|steps:   20|Train Avg Loss: 0.6823|Accuracy: 0.5000 | Test loss:0.7004| test accuracy :0.5178\n",
      "Epoch:   32|steps:   30|Train Avg Loss: 0.6853|Accuracy: 0.6250 | Test loss:0.7003| test accuracy :0.5178\n",
      "Epoch:   32|steps:   40|Train Avg Loss: 0.6744|Accuracy: 0.4688 | Test loss:0.6990| test accuracy :0.4980\n",
      "Epoch:   32|steps:   50|Train Avg Loss: 0.6826|Accuracy: 0.5625 | Test loss:0.6959| test accuracy :0.5099\n",
      "Epoch:   32|steps:   60|Train Avg Loss: 0.6850|Accuracy: 0.5625 | Test loss:0.7005| test accuracy :0.4980\n",
      "Epoch:   33|steps:   10|Train Avg Loss: 0.6893|Accuracy: 0.4688 | Test loss:0.7035| test accuracy :0.4822\n",
      "Epoch:   33|steps:   20|Train Avg Loss: 0.6803|Accuracy: 0.6562 | Test loss:0.7077| test accuracy :0.4664\n",
      "Epoch:   33|steps:   30|Train Avg Loss: 0.6697|Accuracy: 0.5312 | Test loss:0.7000| test accuracy :0.5059\n",
      "Epoch:   33|steps:   40|Train Avg Loss: 0.6724|Accuracy: 0.6250 | Test loss:0.7051| test accuracy :0.4783\n",
      "Epoch:   33|steps:   50|Train Avg Loss: 0.6766|Accuracy: 0.5625 | Test loss:0.7059| test accuracy :0.4862\n",
      "Epoch:   33|steps:   60|Train Avg Loss: 0.6968|Accuracy: 0.4375 | Test loss:0.7056| test accuracy :0.5020\n",
      "Epoch:   34|steps:   10|Train Avg Loss: 0.6821|Accuracy: 0.6250 | Test loss:0.7045| test accuracy :0.4822\n",
      "Epoch:   34|steps:   20|Train Avg Loss: 0.6760|Accuracy: 0.5938 | Test loss:0.7002| test accuracy :0.4941\n",
      "Epoch:   34|steps:   30|Train Avg Loss: 0.6838|Accuracy: 0.5625 | Test loss:0.6987| test accuracy :0.4941\n",
      "Epoch:   34|steps:   40|Train Avg Loss: 0.6723|Accuracy: 0.7188 | Test loss:0.6942| test accuracy :0.5138\n",
      "Epoch:   34|steps:   50|Train Avg Loss: 0.6766|Accuracy: 0.5000 | Test loss:0.7053| test accuracy :0.4941\n",
      "Epoch:   34|steps:   60|Train Avg Loss: 0.6797|Accuracy: 0.5625 | Test loss:0.7070| test accuracy :0.4704\n",
      "Epoch:   35|steps:   10|Train Avg Loss: 0.6707|Accuracy: 0.4688 | Test loss:0.7091| test accuracy :0.4664\n",
      "Epoch:   35|steps:   20|Train Avg Loss: 0.6809|Accuracy: 0.5938 | Test loss:0.7039| test accuracy :0.4783\n",
      "Epoch:   35|steps:   30|Train Avg Loss: 0.6747|Accuracy: 0.7188 | Test loss:0.7022| test accuracy :0.4822\n",
      "Epoch:   35|steps:   40|Train Avg Loss: 0.6745|Accuracy: 0.6875 | Test loss:0.7043| test accuracy :0.4783\n",
      "Epoch:   35|steps:   50|Train Avg Loss: 0.6878|Accuracy: 0.5625 | Test loss:0.7083| test accuracy :0.4783\n",
      "Epoch:   35|steps:   60|Train Avg Loss: 0.6752|Accuracy: 0.3438 | Test loss:0.7109| test accuracy :0.4822\n",
      "Epoch:   36|steps:   10|Train Avg Loss: 0.6708|Accuracy: 0.7500 | Test loss:0.7047| test accuracy :0.5099\n",
      "Epoch:   36|steps:   20|Train Avg Loss: 0.6818|Accuracy: 0.5938 | Test loss:0.7123| test accuracy :0.4941\n",
      "Epoch:   36|steps:   30|Train Avg Loss: 0.6736|Accuracy: 0.5938 | Test loss:0.7087| test accuracy :0.4743\n",
      "Epoch:   36|steps:   40|Train Avg Loss: 0.6731|Accuracy: 0.6250 | Test loss:0.7083| test accuracy :0.4704\n",
      "Epoch:   36|steps:   50|Train Avg Loss: 0.6870|Accuracy: 0.6562 | Test loss:0.7004| test accuracy :0.5059\n",
      "Epoch:   36|steps:   60|Train Avg Loss: 0.6836|Accuracy: 0.4688 | Test loss:0.6963| test accuracy :0.4941\n",
      "Epoch:   37|steps:   10|Train Avg Loss: 0.6917|Accuracy: 0.6250 | Test loss:0.7022| test accuracy :0.4941\n",
      "Epoch:   37|steps:   20|Train Avg Loss: 0.6698|Accuracy: 0.5312 | Test loss:0.7013| test accuracy :0.5099\n",
      "Epoch:   37|steps:   30|Train Avg Loss: 0.6716|Accuracy: 0.5938 | Test loss:0.7014| test accuracy :0.5138\n",
      "Epoch:   37|steps:   40|Train Avg Loss: 0.6827|Accuracy: 0.5000 | Test loss:0.7023| test accuracy :0.5217\n",
      "Epoch:   37|steps:   50|Train Avg Loss: 0.6741|Accuracy: 0.6250 | Test loss:0.6969| test accuracy :0.5099\n",
      "Epoch:   37|steps:   60|Train Avg Loss: 0.6743|Accuracy: 0.4375 | Test loss:0.7021| test accuracy :0.5138\n",
      "Epoch:   38|steps:   10|Train Avg Loss: 0.6772|Accuracy: 0.5625 | Test loss:0.7111| test accuracy :0.4901\n",
      "Epoch:   38|steps:   20|Train Avg Loss: 0.6686|Accuracy: 0.5312 | Test loss:0.7158| test accuracy :0.4980\n",
      "Epoch:   38|steps:   30|Train Avg Loss: 0.6794|Accuracy: 0.5938 | Test loss:0.7147| test accuracy :0.4704\n",
      "Epoch:   38|steps:   40|Train Avg Loss: 0.6862|Accuracy: 0.6875 | Test loss:0.7022| test accuracy :0.5375\n",
      "Epoch:   38|steps:   50|Train Avg Loss: 0.6782|Accuracy: 0.6250 | Test loss:0.7009| test accuracy :0.5020\n",
      "Epoch:   38|steps:   60|Train Avg Loss: 0.6696|Accuracy: 0.4375 | Test loss:0.7015| test accuracy :0.4980\n",
      "Epoch:   39|steps:   10|Train Avg Loss: 0.6544|Accuracy: 0.7500 | Test loss:0.7031| test accuracy :0.5138\n",
      "Epoch:   39|steps:   20|Train Avg Loss: 0.6603|Accuracy: 0.6875 | Test loss:0.7052| test accuracy :0.4901\n",
      "Epoch:   39|steps:   30|Train Avg Loss: 0.6549|Accuracy: 0.5000 | Test loss:0.7164| test accuracy :0.4862\n",
      "Epoch:   39|steps:   40|Train Avg Loss: 0.6730|Accuracy: 0.5000 | Test loss:0.7149| test accuracy :0.4704\n",
      "Epoch:   39|steps:   50|Train Avg Loss: 0.6904|Accuracy: 0.5625 | Test loss:0.7093| test accuracy :0.5020\n",
      "Epoch:   39|steps:   60|Train Avg Loss: 0.6927|Accuracy: 0.4375 | Test loss:0.7145| test accuracy :0.4743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   40|steps:   10|Train Avg Loss: 0.6760|Accuracy: 0.6562 | Test loss:0.7101| test accuracy :0.4901\n",
      "Epoch:   40|steps:   20|Train Avg Loss: 0.6835|Accuracy: 0.5625 | Test loss:0.7106| test accuracy :0.4743\n",
      "Epoch:   40|steps:   30|Train Avg Loss: 0.6545|Accuracy: 0.4062 | Test loss:0.7145| test accuracy :0.4743\n",
      "Epoch:   40|steps:   40|Train Avg Loss: 0.6681|Accuracy: 0.4688 | Test loss:0.7106| test accuracy :0.4941\n",
      "Epoch:   40|steps:   50|Train Avg Loss: 0.6687|Accuracy: 0.4688 | Test loss:0.7093| test accuracy :0.5020\n",
      "Epoch:   40|steps:   60|Train Avg Loss: 0.6770|Accuracy: 0.5000 | Test loss:0.7214| test accuracy :0.4625\n",
      "Epoch:   41|steps:   10|Train Avg Loss: 0.6528|Accuracy: 0.7188 | Test loss:0.7237| test accuracy :0.4545\n",
      "Epoch:   41|steps:   20|Train Avg Loss: 0.6633|Accuracy: 0.8125 | Test loss:0.7234| test accuracy :0.4901\n",
      "Epoch:   41|steps:   30|Train Avg Loss: 0.6642|Accuracy: 0.6250 | Test loss:0.7208| test accuracy :0.4862\n",
      "Epoch:   41|steps:   40|Train Avg Loss: 0.6657|Accuracy: 0.6562 | Test loss:0.7167| test accuracy :0.5138\n",
      "Epoch:   41|steps:   50|Train Avg Loss: 0.6773|Accuracy: 0.5000 | Test loss:0.7215| test accuracy :0.4862\n",
      "Epoch:   41|steps:   60|Train Avg Loss: 0.6735|Accuracy: 0.4688 | Test loss:0.7470| test accuracy :0.4783\n",
      "Epoch:   42|steps:   10|Train Avg Loss: 0.6690|Accuracy: 0.7500 | Test loss:0.7289| test accuracy :0.4387\n",
      "Epoch:   42|steps:   20|Train Avg Loss: 0.6750|Accuracy: 0.4688 | Test loss:0.7170| test accuracy :0.4822\n",
      "Epoch:   42|steps:   30|Train Avg Loss: 0.6735|Accuracy: 0.5625 | Test loss:0.7129| test accuracy :0.4901\n",
      "Epoch:   42|steps:   40|Train Avg Loss: 0.6709|Accuracy: 0.7188 | Test loss:0.7102| test accuracy :0.5099\n",
      "Epoch:   42|steps:   50|Train Avg Loss: 0.6828|Accuracy: 0.6875 | Test loss:0.7134| test accuracy :0.4783\n",
      "Epoch:   42|steps:   60|Train Avg Loss: 0.6585|Accuracy: 0.7188 | Test loss:0.7075| test accuracy :0.4901\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,TIME_STEP, INPUT_SIZE)\n",
    "        #reshape the features to (batch,time_step*input_size)\n",
    "        \n",
    "        # start trainnig \n",
    "        output = model(inputs)\n",
    "        \n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,targets)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 10 steps\n",
    "        if((step+1) %10 == 0):\n",
    "            # average of loss in 10 steps\n",
    "            avg = loss_total / 10\n",
    "            train_loss.append(avg.tolist())\n",
    "            \n",
    "            # calculate accuracy\n",
    "            accuracy = 0\n",
    "            for i in range(len(output)):\n",
    "                if(output[i][0]>0.5 and targets[i] == 0.0):\n",
    "                    accuracy  = accuracy +1\n",
    "                elif(output[i][0]<0.5 and targets[i] == 1.0):\n",
    "                    accuracy = accuracy +1\n",
    "            accuracy = accuracy / len(output)\n",
    "            train_accuracy.append(accuracy)\n",
    "            \n",
    "            test_output = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "            loss = loss_func(test_output,test_labels)\n",
    "            test_loss.append(loss)\n",
    "            \n",
    "            accuracy2 = 0\n",
    "            for i in range(len(test_output)):\n",
    "                if(test_output[i][0]>0.5 and test_labels[i] == 0.0):\n",
    "                    accuracy2  = accuracy2 +1\n",
    "                elif(test_output[i][0]<0.5 and test_labels[i] == 1.0):\n",
    "                    accuracy2 = accuracy2 +1\n",
    "            accuracy2 = accuracy2 / len(test_output)\n",
    "            test_accuracy.append(accuracy2)\n",
    "            \n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f|Accuracy: %.4f | Test loss:%.4f| test accuracy :%.4f\"\n",
    "                  %(epoch+1,step+1,avg,accuracy,loss,accuracy2))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,len(LOSS),len(LOSS))\n",
    "y = np.array(LOSS)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
