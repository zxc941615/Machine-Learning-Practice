{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "stock_path = \"C:/Users/acer/Desktop/LAB/lab2.csv\"\n",
    "stock_df = pd.read_csv(stock_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"K value\",\"D value\",\"MACD\",\"RISE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler(feature_range = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = scaler.fit_transform(stock_df[[\"Open\",\"High\",\"Low\",\"Close\",\"K value\",\"D value\",\"MACD\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13796712 0.13853436 0.13939515 ... 0.79479577 0.81382653 0.71186575]\n",
      " [0.13749311 0.13239757 0.1276979  ... 0.65791277 0.77233858 0.64977072]\n",
      " [0.12137257 0.13050986 0.12623505 ... 0.58210202 0.71724268 0.61643564]\n",
      " ...\n",
      " [0.38112413 0.38061527 0.39141134 ... 0.90992721 0.90681656 0.66360402]\n",
      " [0.4099623  0.43175519 0.42291032 ... 0.9512395  0.94049215 0.74943935]\n",
      " [0.44150406 0.45149337 0.44792425 ... 0.97878103 0.97291032 0.82052843]]\n"
     ]
    }
   ],
   "source": [
    "print(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = pd.DataFrame(minmax_df,columns = [\"Open\",\"Hign\",\"Low\",\"Close\",\"K value\",\"D value\",\"MACD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df[\"Date\"] = stock_df[\"Date\"].values\n",
    "minmax_df[\"RISE\"] = stock_df[\"RISE\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Hign</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>K value</th>\n",
       "      <th>D value</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Date</th>\n",
       "      <th>RISE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.137967</td>\n",
       "      <td>0.138534</td>\n",
       "      <td>0.139395</td>\n",
       "      <td>0.146519</td>\n",
       "      <td>0.794796</td>\n",
       "      <td>0.813827</td>\n",
       "      <td>0.711866</td>\n",
       "      <td>2010/1/20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137493</td>\n",
       "      <td>0.132398</td>\n",
       "      <td>0.127698</td>\n",
       "      <td>0.128143</td>\n",
       "      <td>0.657913</td>\n",
       "      <td>0.772339</td>\n",
       "      <td>0.649771</td>\n",
       "      <td>2010/1/21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.121373</td>\n",
       "      <td>0.130510</td>\n",
       "      <td>0.126235</td>\n",
       "      <td>0.136847</td>\n",
       "      <td>0.582102</td>\n",
       "      <td>0.717243</td>\n",
       "      <td>0.616436</td>\n",
       "      <td>2010/1/22</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.133226</td>\n",
       "      <td>0.124846</td>\n",
       "      <td>0.118437</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.442892</td>\n",
       "      <td>0.630130</td>\n",
       "      <td>0.558560</td>\n",
       "      <td>2010/1/25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.113787</td>\n",
       "      <td>0.107381</td>\n",
       "      <td>0.099916</td>\n",
       "      <td>0.098646</td>\n",
       "      <td>0.280881</td>\n",
       "      <td>0.513419</td>\n",
       "      <td>0.490480</td>\n",
       "      <td>2010/1/26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.095771</td>\n",
       "      <td>0.094637</td>\n",
       "      <td>0.090168</td>\n",
       "      <td>0.098646</td>\n",
       "      <td>0.210269</td>\n",
       "      <td>0.410057</td>\n",
       "      <td>0.448022</td>\n",
       "      <td>2010/1/27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.102883</td>\n",
       "      <td>0.104078</td>\n",
       "      <td>0.103815</td>\n",
       "      <td>0.115571</td>\n",
       "      <td>0.228640</td>\n",
       "      <td>0.347797</td>\n",
       "      <td>0.450156</td>\n",
       "      <td>2010/1/28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.111416</td>\n",
       "      <td>0.116350</td>\n",
       "      <td>0.109664</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.265860</td>\n",
       "      <td>0.319761</td>\n",
       "      <td>0.463837</td>\n",
       "      <td>2010/1/29</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.118528</td>\n",
       "      <td>0.112573</td>\n",
       "      <td>0.113563</td>\n",
       "      <td>0.119439</td>\n",
       "      <td>0.339554</td>\n",
       "      <td>0.327742</td>\n",
       "      <td>0.470706</td>\n",
       "      <td>2010/2/1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.120899</td>\n",
       "      <td>0.112573</td>\n",
       "      <td>0.105277</td>\n",
       "      <td>0.104448</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>0.323048</td>\n",
       "      <td>0.453621</td>\n",
       "      <td>2010/2/2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      Hign       Low     Close   K value   D value      MACD  \\\n",
       "0  0.137967  0.138534  0.139395  0.146519  0.794796  0.813827  0.711866   \n",
       "1  0.137493  0.132398  0.127698  0.128143  0.657913  0.772339  0.649771   \n",
       "2  0.121373  0.130510  0.126235  0.136847  0.582102  0.717243  0.616436   \n",
       "3  0.133226  0.124846  0.118437  0.117021  0.442892  0.630130  0.558560   \n",
       "4  0.113787  0.107381  0.099916  0.098646  0.280881  0.513419  0.490480   \n",
       "5  0.095771  0.094637  0.090168  0.098646  0.210269  0.410057  0.448022   \n",
       "6  0.102883  0.104078  0.103815  0.115571  0.228640  0.347797  0.450156   \n",
       "7  0.111416  0.116350  0.109664  0.121857  0.265860  0.319761  0.463837   \n",
       "8  0.118528  0.112573  0.113563  0.119439  0.339554  0.327742  0.470706   \n",
       "9  0.120899  0.112573  0.105277  0.104448  0.311884  0.323048  0.453621   \n",
       "\n",
       "        Date  RISE  \n",
       "0  2010/1/20   NaN  \n",
       "1  2010/1/21   0.0  \n",
       "2  2010/1/22   2.0  \n",
       "3  2010/1/25   0.0  \n",
       "4  2010/1/26   0.0  \n",
       "5  2010/1/27   1.0  \n",
       "6  2010/1/28   2.0  \n",
       "7  2010/1/29   2.0  \n",
       "8   2010/2/1   0.0  \n",
       "9   2010/2/2   0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = minmax_df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 9)\n",
      "(238, 9)\n"
     ]
    }
   ],
   "source": [
    "train_set = minmax_df[:2200]\n",
    "test_set = minmax_df[2200:]\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 100000\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEP = 7\n",
    "INPUT_SIZE = 7\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of feature:  torch.Size([2431, 7, 7])\n",
      "size of label:  torch.Size([2431])\n"
     ]
    }
   ],
   "source": [
    "# declear training features data\n",
    "features = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i-TIME_STEP:i][[\"Open\",\"Hign\",\"Low\",\"Close\",\"K value\",\"D value\",\"MACD\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = torch.FloatTensor(features)\n",
    "print(\"size of feature: \",features.size())\n",
    "\n",
    "# declear trainging labels data\n",
    "labels = []\n",
    "for i in range(TIME_STEP,len(minmax_df)):\n",
    "    x = minmax_df[i:i+1][\"RISE\"]\n",
    "    labels.append(x.tolist())\n",
    "labels = torch.LongTensor(labels).view(-1)\n",
    "print(\"size of label: \",labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features[:2200]\n",
    "train_labels = labels[:2200]\n",
    "test_features = features[2200:]\n",
    "test_labels = labels[2200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(train_features,train_labels)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define NN architecture\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        # define lstm layer\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=128,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        self.lstm2 = torch.nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=64,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        # dropout layer\n",
    "        self.Relu =  torch.nn.ReLU()\n",
    "        self.hidden1 = torch.nn.Linear(64,32)\n",
    "        self.out = torch.nn.Linear(64,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,(h_n,h_c) = self.lstm(x,None)\n",
    "        lstm_out,_ = self.lstm2(lstm_out,None)\n",
    "        #lstm_out = self.Relu(lstm_out[:,-1,:])\n",
    "        # only need last output of lstm layer\n",
    "        #h1_out = self.hidden1(lstm_out)\n",
    "        #h1_out = self.Relu(h1_out)\n",
    "        out = self.out(lstm_out[:,-1,:])\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(7, 128, batch_first=True)\n",
      "  (lstm2): LSTM(128, 64, batch_first=True)\n",
      "  (Relu): ReLU()\n",
      "  (hidden1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.0001)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.98,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   30|Train Avg Loss: 0.9417 |Test Loss: 0.8284|lr = 0.01000\n",
      "Epoch:    1|steps:   60|Train Avg Loss: 0.9017 |Test Loss: 0.8272|lr = 0.01000\n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.8937 |Test Loss: 0.8185|lr = 0.01000\n",
      "Epoch:    2|steps:   60|Train Avg Loss: 0.9133 |Test Loss: 0.8223|lr = 0.01000\n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.9010 |Test Loss: 0.8252|lr = 0.01000\n",
      "Epoch:    3|steps:   60|Train Avg Loss: 0.9035 |Test Loss: 0.8279|lr = 0.01000\n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.9033 |Test Loss: 0.8244|lr = 0.01000\n",
      "Epoch:    4|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8116|lr = 0.01000\n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.9003 |Test Loss: 0.8153|lr = 0.01000\n",
      "Epoch:    5|steps:   60|Train Avg Loss: 0.9029 |Test Loss: 0.8112|lr = 0.01000\n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.9085 |Test Loss: 0.8132|lr = 0.01000\n",
      "Epoch:    6|steps:   60|Train Avg Loss: 0.8912 |Test Loss: 0.8089|lr = 0.01000\n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.8841 |Test Loss: 0.8023|lr = 0.01000\n",
      "Epoch:    7|steps:   60|Train Avg Loss: 0.9122 |Test Loss: 0.8264|lr = 0.01000\n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.8939 |Test Loss: 0.8098|lr = 0.01000\n",
      "Epoch:    8|steps:   60|Train Avg Loss: 0.9148 |Test Loss: 0.8185|lr = 0.01000\n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.9169 |Test Loss: 0.8378|lr = 0.01000\n",
      "Epoch:    9|steps:   60|Train Avg Loss: 0.9007 |Test Loss: 0.8159|lr = 0.01000\n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.9197 |Test Loss: 0.8312|lr = 0.01000\n",
      "Epoch:   10|steps:   60|Train Avg Loss: 0.8876 |Test Loss: 0.8195|lr = 0.01000\n",
      "Epoch:   11|steps:   30|Train Avg Loss: 0.8889 |Test Loss: 0.8106|lr = 0.01000\n",
      "Epoch:   11|steps:   60|Train Avg Loss: 0.9066 |Test Loss: 0.8130|lr = 0.01000\n",
      "Epoch:   12|steps:   30|Train Avg Loss: 0.9188 |Test Loss: 0.8255|lr = 0.01000\n",
      "Epoch:   12|steps:   60|Train Avg Loss: 0.8907 |Test Loss: 0.8168|lr = 0.01000\n",
      "Epoch:   13|steps:   30|Train Avg Loss: 0.9000 |Test Loss: 0.8083|lr = 0.01000\n",
      "Epoch:   13|steps:   60|Train Avg Loss: 0.9146 |Test Loss: 0.8093|lr = 0.01000\n",
      "Epoch:   14|steps:   30|Train Avg Loss: 0.9042 |Test Loss: 0.8240|lr = 0.01000\n",
      "Epoch:   14|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8192|lr = 0.01000\n",
      "Epoch:   15|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8117|lr = 0.01000\n",
      "Epoch:   15|steps:   60|Train Avg Loss: 0.8989 |Test Loss: 0.8246|lr = 0.01000\n",
      "Epoch:   16|steps:   30|Train Avg Loss: 0.9237 |Test Loss: 0.8267|lr = 0.01000\n",
      "Epoch:   16|steps:   60|Train Avg Loss: 0.9148 |Test Loss: 0.8197|lr = 0.01000\n",
      "Epoch:   17|steps:   30|Train Avg Loss: 0.9022 |Test Loss: 0.8385|lr = 0.01000\n",
      "Epoch:   17|steps:   60|Train Avg Loss: 0.9023 |Test Loss: 0.8104|lr = 0.01000\n",
      "Epoch:   18|steps:   30|Train Avg Loss: 0.9097 |Test Loss: 0.8152|lr = 0.01000\n",
      "Epoch:   18|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8191|lr = 0.01000\n",
      "Epoch:   19|steps:   30|Train Avg Loss: 0.9134 |Test Loss: 0.8212|lr = 0.01000\n",
      "Epoch:   19|steps:   60|Train Avg Loss: 0.8789 |Test Loss: 0.8113|lr = 0.01000\n",
      "Epoch:   20|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8181|lr = 0.01000\n",
      "Epoch:   20|steps:   60|Train Avg Loss: 0.9006 |Test Loss: 0.8127|lr = 0.01000\n",
      "Epoch:   21|steps:   30|Train Avg Loss: 0.8962 |Test Loss: 0.8087|lr = 0.01000\n",
      "Epoch:   21|steps:   60|Train Avg Loss: 0.9121 |Test Loss: 0.8167|lr = 0.01000\n",
      "Epoch:   22|steps:   30|Train Avg Loss: 0.8977 |Test Loss: 0.8226|lr = 0.01000\n",
      "Epoch:   22|steps:   60|Train Avg Loss: 0.8983 |Test Loss: 0.8148|lr = 0.01000\n",
      "Epoch:   23|steps:   30|Train Avg Loss: 0.8929 |Test Loss: 0.8154|lr = 0.01000\n",
      "Epoch:   23|steps:   60|Train Avg Loss: 0.8947 |Test Loss: 0.8101|lr = 0.01000\n",
      "Epoch:   24|steps:   30|Train Avg Loss: 0.9051 |Test Loss: 0.8171|lr = 0.00980\n",
      "Epoch:   24|steps:   60|Train Avg Loss: 0.8977 |Test Loss: 0.8045|lr = 0.00980\n",
      "Epoch:   25|steps:   30|Train Avg Loss: 0.9026 |Test Loss: 0.8178|lr = 0.00980\n",
      "Epoch:   25|steps:   60|Train Avg Loss: 0.8970 |Test Loss: 0.8156|lr = 0.00980\n",
      "Epoch:   26|steps:   30|Train Avg Loss: 0.8992 |Test Loss: 0.8159|lr = 0.00980\n",
      "Epoch:   26|steps:   60|Train Avg Loss: 0.8921 |Test Loss: 0.8188|lr = 0.00980\n",
      "Epoch:   27|steps:   30|Train Avg Loss: 0.8978 |Test Loss: 0.8125|lr = 0.00980\n",
      "Epoch:   27|steps:   60|Train Avg Loss: 0.8952 |Test Loss: 0.8113|lr = 0.00980\n",
      "Epoch:   28|steps:   30|Train Avg Loss: 0.8998 |Test Loss: 0.8196|lr = 0.00980\n",
      "Epoch:   28|steps:   60|Train Avg Loss: 0.8950 |Test Loss: 0.8074|lr = 0.00980\n",
      "Epoch:   29|steps:   30|Train Avg Loss: 0.8919 |Test Loss: 0.8127|lr = 0.00980\n",
      "Epoch:   29|steps:   60|Train Avg Loss: 0.9007 |Test Loss: 0.8154|lr = 0.00980\n",
      "Epoch:   30|steps:   30|Train Avg Loss: 0.8922 |Test Loss: 0.8186|lr = 0.00980\n",
      "Epoch:   30|steps:   60|Train Avg Loss: 0.9014 |Test Loss: 0.8180|lr = 0.00980\n",
      "Epoch:   31|steps:   30|Train Avg Loss: 0.8739 |Test Loss: 0.8159|lr = 0.00980\n",
      "Epoch:   31|steps:   60|Train Avg Loss: 0.9033 |Test Loss: 0.8270|lr = 0.00980\n",
      "Epoch:   32|steps:   30|Train Avg Loss: 0.8949 |Test Loss: 0.8114|lr = 0.00980\n",
      "Epoch:   32|steps:   60|Train Avg Loss: 0.9006 |Test Loss: 0.8152|lr = 0.00980\n",
      "Epoch:   33|steps:   30|Train Avg Loss: 0.9047 |Test Loss: 0.8178|lr = 0.00980\n",
      "Epoch:   33|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8150|lr = 0.00980\n",
      "Epoch:   34|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8120|lr = 0.00980\n",
      "Epoch:   34|steps:   60|Train Avg Loss: 0.8996 |Test Loss: 0.8074|lr = 0.00980\n",
      "Epoch:   35|steps:   30|Train Avg Loss: 0.9040 |Test Loss: 0.8228|lr = 0.00960\n",
      "Epoch:   35|steps:   60|Train Avg Loss: 0.9061 |Test Loss: 0.8241|lr = 0.00960\n",
      "Epoch:   36|steps:   30|Train Avg Loss: 0.8915 |Test Loss: 0.8144|lr = 0.00960\n",
      "Epoch:   36|steps:   60|Train Avg Loss: 0.9040 |Test Loss: 0.8142|lr = 0.00960\n",
      "Epoch:   37|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8135|lr = 0.00960\n",
      "Epoch:   37|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8203|lr = 0.00960\n",
      "Epoch:   38|steps:   30|Train Avg Loss: 0.9033 |Test Loss: 0.8228|lr = 0.00960\n",
      "Epoch:   38|steps:   60|Train Avg Loss: 0.8919 |Test Loss: 0.8086|lr = 0.00960\n",
      "Epoch:   39|steps:   30|Train Avg Loss: 0.8886 |Test Loss: 0.8069|lr = 0.00960\n",
      "Epoch:   39|steps:   60|Train Avg Loss: 0.8955 |Test Loss: 0.8063|lr = 0.00960\n",
      "Epoch:   40|steps:   30|Train Avg Loss: 0.9125 |Test Loss: 0.8241|lr = 0.00960\n",
      "Epoch:   40|steps:   60|Train Avg Loss: 0.8914 |Test Loss: 0.8038|lr = 0.00960\n",
      "Epoch:   41|steps:   30|Train Avg Loss: 0.9043 |Test Loss: 0.8166|lr = 0.00960\n",
      "Epoch:   41|steps:   60|Train Avg Loss: 0.8992 |Test Loss: 0.8139|lr = 0.00960\n",
      "Epoch:   42|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8145|lr = 0.00960\n",
      "Epoch:   42|steps:   60|Train Avg Loss: 0.8994 |Test Loss: 0.8165|lr = 0.00960\n",
      "Epoch:   43|steps:   30|Train Avg Loss: 0.8985 |Test Loss: 0.8076|lr = 0.00960\n",
      "Epoch:   43|steps:   60|Train Avg Loss: 0.8984 |Test Loss: 0.8307|lr = 0.00960\n",
      "Epoch:   44|steps:   30|Train Avg Loss: 0.8879 |Test Loss: 0.8168|lr = 0.00960\n",
      "Epoch:   44|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8192|lr = 0.00960\n",
      "Epoch:   45|steps:   30|Train Avg Loss: 0.9012 |Test Loss: 0.8124|lr = 0.00960\n",
      "Epoch:   45|steps:   60|Train Avg Loss: 0.8961 |Test Loss: 0.8153|lr = 0.00960\n",
      "Epoch:   46|steps:   30|Train Avg Loss: 0.9064 |Test Loss: 0.8310|lr = 0.00941\n",
      "Epoch:   46|steps:   60|Train Avg Loss: 0.8901 |Test Loss: 0.8191|lr = 0.00941\n",
      "Epoch:   47|steps:   30|Train Avg Loss: 0.8899 |Test Loss: 0.8056|lr = 0.00941\n",
      "Epoch:   47|steps:   60|Train Avg Loss: 0.9128 |Test Loss: 0.8172|lr = 0.00941\n",
      "Epoch:   48|steps:   30|Train Avg Loss: 0.8782 |Test Loss: 0.8078|lr = 0.00941\n",
      "Epoch:   48|steps:   60|Train Avg Loss: 0.8956 |Test Loss: 0.8138|lr = 0.00941\n",
      "Epoch:   49|steps:   30|Train Avg Loss: 0.9031 |Test Loss: 0.8160|lr = 0.00941\n",
      "Epoch:   49|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8205|lr = 0.00941\n",
      "Epoch:   50|steps:   30|Train Avg Loss: 0.9154 |Test Loss: 0.8303|lr = 0.00941\n",
      "Epoch:   50|steps:   60|Train Avg Loss: 0.8863 |Test Loss: 0.8069|lr = 0.00941\n",
      "Epoch:   51|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8446|lr = 0.00941\n",
      "Epoch:   51|steps:   60|Train Avg Loss: 0.8937 |Test Loss: 0.8258|lr = 0.00941\n",
      "Epoch:   52|steps:   30|Train Avg Loss: 0.9018 |Test Loss: 0.8272|lr = 0.00941\n",
      "Epoch:   52|steps:   60|Train Avg Loss: 0.8922 |Test Loss: 0.8160|lr = 0.00941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   53|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8111|lr = 0.00941\n",
      "Epoch:   53|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8115|lr = 0.00941\n",
      "Epoch:   54|steps:   30|Train Avg Loss: 0.8865 |Test Loss: 0.8179|lr = 0.00941\n",
      "Epoch:   54|steps:   60|Train Avg Loss: 0.9080 |Test Loss: 0.8169|lr = 0.00941\n",
      "Epoch:   55|steps:   30|Train Avg Loss: 0.8925 |Test Loss: 0.8081|lr = 0.00941\n",
      "Epoch:   55|steps:   60|Train Avg Loss: 0.9022 |Test Loss: 0.8242|lr = 0.00941\n",
      "Epoch:   56|steps:   30|Train Avg Loss: 0.8736 |Test Loss: 0.8106|lr = 0.00941\n",
      "Epoch:   56|steps:   60|Train Avg Loss: 0.9290 |Test Loss: 0.8220|lr = 0.00941\n",
      "Epoch:   57|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8246|lr = 0.00922\n",
      "Epoch:   57|steps:   60|Train Avg Loss: 0.8989 |Test Loss: 0.8175|lr = 0.00922\n",
      "Epoch:   58|steps:   30|Train Avg Loss: 0.8869 |Test Loss: 0.8173|lr = 0.00922\n",
      "Epoch:   58|steps:   60|Train Avg Loss: 0.9240 |Test Loss: 0.8260|lr = 0.00922\n",
      "Epoch:   59|steps:   30|Train Avg Loss: 0.8900 |Test Loss: 0.8221|lr = 0.00922\n",
      "Epoch:   59|steps:   60|Train Avg Loss: 0.9091 |Test Loss: 0.8187|lr = 0.00922\n",
      "Epoch:   60|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8148|lr = 0.00922\n",
      "Epoch:   60|steps:   60|Train Avg Loss: 0.8889 |Test Loss: 0.8180|lr = 0.00922\n",
      "Epoch:   61|steps:   30|Train Avg Loss: 0.8935 |Test Loss: 0.8113|lr = 0.00922\n",
      "Epoch:   61|steps:   60|Train Avg Loss: 0.8992 |Test Loss: 0.8158|lr = 0.00922\n",
      "Epoch:   62|steps:   30|Train Avg Loss: 0.9138 |Test Loss: 0.8217|lr = 0.00922\n",
      "Epoch:   62|steps:   60|Train Avg Loss: 0.8661 |Test Loss: 0.8044|lr = 0.00922\n",
      "Epoch:   63|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8130|lr = 0.00922\n",
      "Epoch:   63|steps:   60|Train Avg Loss: 0.9127 |Test Loss: 0.8176|lr = 0.00922\n",
      "Epoch:   64|steps:   30|Train Avg Loss: 0.8980 |Test Loss: 0.8130|lr = 0.00922\n",
      "Epoch:   64|steps:   60|Train Avg Loss: 0.8999 |Test Loss: 0.8072|lr = 0.00922\n",
      "Epoch:   65|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8124|lr = 0.00922\n",
      "Epoch:   65|steps:   60|Train Avg Loss: 0.9124 |Test Loss: 0.8175|lr = 0.00922\n",
      "Epoch:   66|steps:   30|Train Avg Loss: 0.9031 |Test Loss: 0.8180|lr = 0.00922\n",
      "Epoch:   66|steps:   60|Train Avg Loss: 0.8846 |Test Loss: 0.8148|lr = 0.00922\n",
      "Epoch:   67|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8150|lr = 0.00922\n",
      "Epoch:   67|steps:   60|Train Avg Loss: 0.9019 |Test Loss: 0.8165|lr = 0.00922\n",
      "Epoch:   68|steps:   30|Train Avg Loss: 0.9013 |Test Loss: 0.8114|lr = 0.00904\n",
      "Epoch:   68|steps:   60|Train Avg Loss: 0.9055 |Test Loss: 0.8189|lr = 0.00904\n",
      "Epoch:   69|steps:   30|Train Avg Loss: 0.9065 |Test Loss: 0.8183|lr = 0.00904\n",
      "Epoch:   69|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8036|lr = 0.00904\n",
      "Epoch:   70|steps:   30|Train Avg Loss: 0.8844 |Test Loss: 0.8524|lr = 0.00904\n",
      "Epoch:   70|steps:   60|Train Avg Loss: 0.9065 |Test Loss: 0.8144|lr = 0.00904\n",
      "Epoch:   71|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8141|lr = 0.00904\n",
      "Epoch:   71|steps:   60|Train Avg Loss: 0.8995 |Test Loss: 0.8125|lr = 0.00904\n",
      "Epoch:   72|steps:   30|Train Avg Loss: 0.8811 |Test Loss: 0.8072|lr = 0.00904\n",
      "Epoch:   72|steps:   60|Train Avg Loss: 0.9146 |Test Loss: 0.8182|lr = 0.00904\n",
      "Epoch:   73|steps:   30|Train Avg Loss: 0.9057 |Test Loss: 0.8260|lr = 0.00904\n",
      "Epoch:   73|steps:   60|Train Avg Loss: 0.8906 |Test Loss: 0.8230|lr = 0.00904\n",
      "Epoch:   74|steps:   30|Train Avg Loss: 0.9069 |Test Loss: 0.8141|lr = 0.00904\n",
      "Epoch:   74|steps:   60|Train Avg Loss: 0.9070 |Test Loss: 0.8164|lr = 0.00904\n",
      "Epoch:   75|steps:   30|Train Avg Loss: 0.8967 |Test Loss: 0.8116|lr = 0.00904\n",
      "Epoch:   75|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8119|lr = 0.00904\n",
      "Epoch:   76|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8136|lr = 0.00904\n",
      "Epoch:   76|steps:   60|Train Avg Loss: 0.9014 |Test Loss: 0.8120|lr = 0.00904\n",
      "Epoch:   77|steps:   30|Train Avg Loss: 0.8789 |Test Loss: 0.8087|lr = 0.00904\n",
      "Epoch:   77|steps:   60|Train Avg Loss: 0.8970 |Test Loss: 0.8124|lr = 0.00904\n",
      "Epoch:   78|steps:   30|Train Avg Loss: 0.9025 |Test Loss: 0.8168|lr = 0.00904\n",
      "Epoch:   78|steps:   60|Train Avg Loss: 0.8949 |Test Loss: 0.8160|lr = 0.00904\n",
      "Epoch:   79|steps:   30|Train Avg Loss: 0.8877 |Test Loss: 0.8145|lr = 0.00886\n",
      "Epoch:   79|steps:   60|Train Avg Loss: 0.9081 |Test Loss: 0.8218|lr = 0.00886\n",
      "Epoch:   80|steps:   30|Train Avg Loss: 0.8842 |Test Loss: 0.8142|lr = 0.00886\n",
      "Epoch:   80|steps:   60|Train Avg Loss: 0.8966 |Test Loss: 0.8186|lr = 0.00886\n",
      "Epoch:   81|steps:   30|Train Avg Loss: 0.8994 |Test Loss: 0.8121|lr = 0.00886\n",
      "Epoch:   81|steps:   60|Train Avg Loss: 0.8871 |Test Loss: 0.8099|lr = 0.00886\n",
      "Epoch:   82|steps:   30|Train Avg Loss: 0.8837 |Test Loss: 0.8168|lr = 0.00886\n",
      "Epoch:   82|steps:   60|Train Avg Loss: 0.8961 |Test Loss: 0.8115|lr = 0.00886\n",
      "Epoch:   83|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.8106|lr = 0.00886\n",
      "Epoch:   83|steps:   60|Train Avg Loss: 0.9121 |Test Loss: 0.8189|lr = 0.00886\n",
      "Epoch:   84|steps:   30|Train Avg Loss: 0.8935 |Test Loss: 0.8127|lr = 0.00886\n",
      "Epoch:   84|steps:   60|Train Avg Loss: 0.8962 |Test Loss: 0.8049|lr = 0.00886\n",
      "Epoch:   85|steps:   30|Train Avg Loss: 0.8906 |Test Loss: 0.8051|lr = 0.00886\n",
      "Epoch:   85|steps:   60|Train Avg Loss: 0.8973 |Test Loss: 0.8142|lr = 0.00886\n",
      "Epoch:   86|steps:   30|Train Avg Loss: 0.9000 |Test Loss: 0.8172|lr = 0.00886\n",
      "Epoch:   86|steps:   60|Train Avg Loss: 0.8975 |Test Loss: 0.8216|lr = 0.00886\n",
      "Epoch:   87|steps:   30|Train Avg Loss: 0.9059 |Test Loss: 0.8211|lr = 0.00886\n",
      "Epoch:   87|steps:   60|Train Avg Loss: 0.8950 |Test Loss: 0.8132|lr = 0.00886\n",
      "Epoch:   88|steps:   30|Train Avg Loss: 0.8851 |Test Loss: 0.8149|lr = 0.00886\n",
      "Epoch:   88|steps:   60|Train Avg Loss: 0.8991 |Test Loss: 0.8155|lr = 0.00886\n",
      "Epoch:   89|steps:   30|Train Avg Loss: 0.8913 |Test Loss: 0.8094|lr = 0.00886\n",
      "Epoch:   89|steps:   60|Train Avg Loss: 0.9059 |Test Loss: 0.8171|lr = 0.00886\n",
      "Epoch:   90|steps:   30|Train Avg Loss: 0.8805 |Test Loss: 0.8131|lr = 0.00868\n",
      "Epoch:   90|steps:   60|Train Avg Loss: 0.9060 |Test Loss: 0.8131|lr = 0.00868\n",
      "Epoch:   91|steps:   30|Train Avg Loss: 0.8848 |Test Loss: 0.8089|lr = 0.00868\n",
      "Epoch:   91|steps:   60|Train Avg Loss: 0.9140 |Test Loss: 0.8186|lr = 0.00868\n",
      "Epoch:   92|steps:   30|Train Avg Loss: 0.8767 |Test Loss: 0.8098|lr = 0.00868\n",
      "Epoch:   92|steps:   60|Train Avg Loss: 0.9143 |Test Loss: 0.8241|lr = 0.00868\n",
      "Epoch:   93|steps:   30|Train Avg Loss: 0.9010 |Test Loss: 0.8198|lr = 0.00868\n",
      "Epoch:   93|steps:   60|Train Avg Loss: 0.9006 |Test Loss: 0.8176|lr = 0.00868\n",
      "Epoch:   94|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8159|lr = 0.00868\n",
      "Epoch:   94|steps:   60|Train Avg Loss: 0.9047 |Test Loss: 0.8149|lr = 0.00868\n",
      "Epoch:   95|steps:   30|Train Avg Loss: 0.9212 |Test Loss: 0.8146|lr = 0.00868\n",
      "Epoch:   95|steps:   60|Train Avg Loss: 0.8766 |Test Loss: 0.8057|lr = 0.00868\n",
      "Epoch:   96|steps:   30|Train Avg Loss: 0.8837 |Test Loss: 0.8079|lr = 0.00868\n",
      "Epoch:   96|steps:   60|Train Avg Loss: 0.9109 |Test Loss: 0.8244|lr = 0.00868\n",
      "Epoch:   97|steps:   30|Train Avg Loss: 0.9253 |Test Loss: 0.8243|lr = 0.00868\n",
      "Epoch:   97|steps:   60|Train Avg Loss: 0.8785 |Test Loss: 0.8161|lr = 0.00868\n",
      "Epoch:   98|steps:   30|Train Avg Loss: 0.8927 |Test Loss: 0.8132|lr = 0.00868\n",
      "Epoch:   98|steps:   60|Train Avg Loss: 0.8922 |Test Loss: 0.8153|lr = 0.00868\n",
      "Epoch:   99|steps:   30|Train Avg Loss: 0.9088 |Test Loss: 0.8170|lr = 0.00868\n",
      "Epoch:   99|steps:   60|Train Avg Loss: 0.8940 |Test Loss: 0.8144|lr = 0.00868\n",
      "Epoch:  100|steps:   30|Train Avg Loss: 0.8972 |Test Loss: 0.8164|lr = 0.00868\n",
      "Epoch:  100|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8141|lr = 0.00868\n",
      "Epoch:  101|steps:   30|Train Avg Loss: 0.9030 |Test Loss: 0.8210|lr = 0.00851\n",
      "Epoch:  101|steps:   60|Train Avg Loss: 0.8808 |Test Loss: 0.8109|lr = 0.00851\n",
      "Epoch:  102|steps:   30|Train Avg Loss: 0.9086 |Test Loss: 0.8167|lr = 0.00851\n",
      "Epoch:  102|steps:   60|Train Avg Loss: 0.9025 |Test Loss: 0.8209|lr = 0.00851\n",
      "Epoch:  103|steps:   30|Train Avg Loss: 0.9070 |Test Loss: 0.8114|lr = 0.00851\n",
      "Epoch:  103|steps:   60|Train Avg Loss: 0.8950 |Test Loss: 0.8122|lr = 0.00851\n",
      "Epoch:  104|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8141|lr = 0.00851\n",
      "Epoch:  104|steps:   60|Train Avg Loss: 0.8830 |Test Loss: 0.8141|lr = 0.00851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  105|steps:   30|Train Avg Loss: 0.9069 |Test Loss: 0.8132|lr = 0.00851\n",
      "Epoch:  105|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8094|lr = 0.00851\n",
      "Epoch:  106|steps:   30|Train Avg Loss: 0.8948 |Test Loss: 0.8118|lr = 0.00851\n",
      "Epoch:  106|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8142|lr = 0.00851\n",
      "Epoch:  107|steps:   30|Train Avg Loss: 0.8788 |Test Loss: 0.8105|lr = 0.00851\n",
      "Epoch:  107|steps:   60|Train Avg Loss: 0.9042 |Test Loss: 0.8146|lr = 0.00851\n",
      "Epoch:  108|steps:   30|Train Avg Loss: 0.8745 |Test Loss: 0.8087|lr = 0.00851\n",
      "Epoch:  108|steps:   60|Train Avg Loss: 0.9124 |Test Loss: 0.8251|lr = 0.00851\n",
      "Epoch:  109|steps:   30|Train Avg Loss: 0.9102 |Test Loss: 0.8160|lr = 0.00851\n",
      "Epoch:  109|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8115|lr = 0.00851\n",
      "Epoch:  110|steps:   30|Train Avg Loss: 0.9025 |Test Loss: 0.8176|lr = 0.00851\n",
      "Epoch:  110|steps:   60|Train Avg Loss: 0.8845 |Test Loss: 0.8145|lr = 0.00851\n",
      "Epoch:  111|steps:   30|Train Avg Loss: 0.9055 |Test Loss: 0.8074|lr = 0.00851\n",
      "Epoch:  111|steps:   60|Train Avg Loss: 0.8893 |Test Loss: 0.8194|lr = 0.00851\n",
      "Epoch:  112|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8111|lr = 0.00834\n",
      "Epoch:  112|steps:   60|Train Avg Loss: 0.9316 |Test Loss: 0.8237|lr = 0.00834\n",
      "Epoch:  113|steps:   30|Train Avg Loss: 0.8793 |Test Loss: 0.8098|lr = 0.00834\n",
      "Epoch:  113|steps:   60|Train Avg Loss: 0.9232 |Test Loss: 0.8202|lr = 0.00834\n",
      "Epoch:  114|steps:   30|Train Avg Loss: 0.8934 |Test Loss: 0.8076|lr = 0.00834\n",
      "Epoch:  114|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8136|lr = 0.00834\n",
      "Epoch:  115|steps:   30|Train Avg Loss: 0.8906 |Test Loss: 0.8145|lr = 0.00834\n",
      "Epoch:  115|steps:   60|Train Avg Loss: 0.8980 |Test Loss: 0.8133|lr = 0.00834\n",
      "Epoch:  116|steps:   30|Train Avg Loss: 0.8767 |Test Loss: 0.8134|lr = 0.00834\n",
      "Epoch:  116|steps:   60|Train Avg Loss: 0.9136 |Test Loss: 0.8197|lr = 0.00834\n",
      "Epoch:  117|steps:   30|Train Avg Loss: 0.9034 |Test Loss: 0.8238|lr = 0.00834\n",
      "Epoch:  117|steps:   60|Train Avg Loss: 0.8910 |Test Loss: 0.8107|lr = 0.00834\n",
      "Epoch:  118|steps:   30|Train Avg Loss: 0.8970 |Test Loss: 0.8170|lr = 0.00834\n",
      "Epoch:  118|steps:   60|Train Avg Loss: 0.8920 |Test Loss: 0.8115|lr = 0.00834\n",
      "Epoch:  119|steps:   30|Train Avg Loss: 0.8738 |Test Loss: 0.8125|lr = 0.00834\n",
      "Epoch:  119|steps:   60|Train Avg Loss: 0.9105 |Test Loss: 0.8183|lr = 0.00834\n",
      "Epoch:  120|steps:   30|Train Avg Loss: 0.8903 |Test Loss: 0.8122|lr = 0.00834\n",
      "Epoch:  120|steps:   60|Train Avg Loss: 0.8940 |Test Loss: 0.8261|lr = 0.00834\n",
      "Epoch:  121|steps:   30|Train Avg Loss: 0.8929 |Test Loss: 0.8224|lr = 0.00834\n",
      "Epoch:  121|steps:   60|Train Avg Loss: 0.9008 |Test Loss: 0.8152|lr = 0.00834\n",
      "Epoch:  122|steps:   30|Train Avg Loss: 0.8903 |Test Loss: 0.8046|lr = 0.00834\n",
      "Epoch:  122|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8167|lr = 0.00834\n",
      "Epoch:  123|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8124|lr = 0.00817\n",
      "Epoch:  123|steps:   60|Train Avg Loss: 0.9054 |Test Loss: 0.8146|lr = 0.00817\n",
      "Epoch:  124|steps:   30|Train Avg Loss: 0.8903 |Test Loss: 0.8153|lr = 0.00817\n",
      "Epoch:  124|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8105|lr = 0.00817\n",
      "Epoch:  125|steps:   30|Train Avg Loss: 0.8792 |Test Loss: 0.8148|lr = 0.00817\n",
      "Epoch:  125|steps:   60|Train Avg Loss: 0.8947 |Test Loss: 0.8127|lr = 0.00817\n",
      "Epoch:  126|steps:   30|Train Avg Loss: 0.9045 |Test Loss: 0.8229|lr = 0.00817\n",
      "Epoch:  126|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8121|lr = 0.00817\n",
      "Epoch:  127|steps:   30|Train Avg Loss: 0.8818 |Test Loss: 0.8086|lr = 0.00817\n",
      "Epoch:  127|steps:   60|Train Avg Loss: 0.9043 |Test Loss: 0.8141|lr = 0.00817\n",
      "Epoch:  128|steps:   30|Train Avg Loss: 0.9095 |Test Loss: 0.8239|lr = 0.00817\n",
      "Epoch:  128|steps:   60|Train Avg Loss: 0.8859 |Test Loss: 0.8121|lr = 0.00817\n",
      "Epoch:  129|steps:   30|Train Avg Loss: 0.8820 |Test Loss: 0.8088|lr = 0.00817\n",
      "Epoch:  129|steps:   60|Train Avg Loss: 0.9142 |Test Loss: 0.8234|lr = 0.00817\n",
      "Epoch:  130|steps:   30|Train Avg Loss: 0.8894 |Test Loss: 0.8071|lr = 0.00817\n",
      "Epoch:  130|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8233|lr = 0.00817\n",
      "Epoch:  131|steps:   30|Train Avg Loss: 0.8975 |Test Loss: 0.8068|lr = 0.00817\n",
      "Epoch:  131|steps:   60|Train Avg Loss: 0.8965 |Test Loss: 0.8193|lr = 0.00817\n",
      "Epoch:  132|steps:   30|Train Avg Loss: 0.8923 |Test Loss: 0.8126|lr = 0.00817\n",
      "Epoch:  132|steps:   60|Train Avg Loss: 0.8870 |Test Loss: 0.8094|lr = 0.00817\n",
      "Epoch:  133|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8183|lr = 0.00817\n",
      "Epoch:  133|steps:   60|Train Avg Loss: 0.8860 |Test Loss: 0.8156|lr = 0.00817\n",
      "Epoch:  134|steps:   30|Train Avg Loss: 0.9089 |Test Loss: 0.8185|lr = 0.00801\n",
      "Epoch:  134|steps:   60|Train Avg Loss: 0.8818 |Test Loss: 0.8128|lr = 0.00801\n",
      "Epoch:  135|steps:   30|Train Avg Loss: 0.8751 |Test Loss: 0.8102|lr = 0.00801\n",
      "Epoch:  135|steps:   60|Train Avg Loss: 0.9042 |Test Loss: 0.8165|lr = 0.00801\n",
      "Epoch:  136|steps:   30|Train Avg Loss: 0.8906 |Test Loss: 0.8055|lr = 0.00801\n",
      "Epoch:  136|steps:   60|Train Avg Loss: 0.8992 |Test Loss: 0.8173|lr = 0.00801\n",
      "Epoch:  137|steps:   30|Train Avg Loss: 0.9031 |Test Loss: 0.8249|lr = 0.00801\n",
      "Epoch:  137|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8147|lr = 0.00801\n",
      "Epoch:  138|steps:   30|Train Avg Loss: 0.8908 |Test Loss: 0.8137|lr = 0.00801\n",
      "Epoch:  138|steps:   60|Train Avg Loss: 0.8941 |Test Loss: 0.8192|lr = 0.00801\n",
      "Epoch:  139|steps:   30|Train Avg Loss: 0.8903 |Test Loss: 0.8160|lr = 0.00801\n",
      "Epoch:  139|steps:   60|Train Avg Loss: 0.8945 |Test Loss: 0.8159|lr = 0.00801\n",
      "Epoch:  140|steps:   30|Train Avg Loss: 0.8953 |Test Loss: 0.8088|lr = 0.00801\n",
      "Epoch:  140|steps:   60|Train Avg Loss: 0.8962 |Test Loss: 0.8194|lr = 0.00801\n",
      "Epoch:  141|steps:   30|Train Avg Loss: 0.9021 |Test Loss: 0.8205|lr = 0.00801\n",
      "Epoch:  141|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8132|lr = 0.00801\n",
      "Epoch:  142|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8191|lr = 0.00801\n",
      "Epoch:  142|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8167|lr = 0.00801\n",
      "Epoch:  143|steps:   30|Train Avg Loss: 0.8907 |Test Loss: 0.8140|lr = 0.00801\n",
      "Epoch:  143|steps:   60|Train Avg Loss: 0.9075 |Test Loss: 0.8260|lr = 0.00801\n",
      "Epoch:  144|steps:   30|Train Avg Loss: 0.9044 |Test Loss: 0.8197|lr = 0.00801\n",
      "Epoch:  144|steps:   60|Train Avg Loss: 0.8991 |Test Loss: 0.8134|lr = 0.00801\n",
      "Epoch:  145|steps:   30|Train Avg Loss: 0.9002 |Test Loss: 0.8154|lr = 0.00785\n",
      "Epoch:  145|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8107|lr = 0.00785\n",
      "Epoch:  146|steps:   30|Train Avg Loss: 0.8962 |Test Loss: 0.8175|lr = 0.00785\n",
      "Epoch:  146|steps:   60|Train Avg Loss: 0.9049 |Test Loss: 0.8166|lr = 0.00785\n",
      "Epoch:  147|steps:   30|Train Avg Loss: 0.9080 |Test Loss: 0.8153|lr = 0.00785\n",
      "Epoch:  147|steps:   60|Train Avg Loss: 0.9015 |Test Loss: 0.8138|lr = 0.00785\n",
      "Epoch:  148|steps:   30|Train Avg Loss: 0.9252 |Test Loss: 0.8276|lr = 0.00785\n",
      "Epoch:  148|steps:   60|Train Avg Loss: 0.8713 |Test Loss: 0.8117|lr = 0.00785\n",
      "Epoch:  149|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8101|lr = 0.00785\n",
      "Epoch:  149|steps:   60|Train Avg Loss: 0.9111 |Test Loss: 0.8207|lr = 0.00785\n",
      "Epoch:  150|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8141|lr = 0.00785\n",
      "Epoch:  150|steps:   60|Train Avg Loss: 0.9120 |Test Loss: 0.8186|lr = 0.00785\n",
      "Epoch:  151|steps:   30|Train Avg Loss: 0.8785 |Test Loss: 0.8084|lr = 0.00785\n",
      "Epoch:  151|steps:   60|Train Avg Loss: 0.9020 |Test Loss: 0.8179|lr = 0.00785\n",
      "Epoch:  152|steps:   30|Train Avg Loss: 0.8964 |Test Loss: 0.8165|lr = 0.00785\n",
      "Epoch:  152|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8151|lr = 0.00785\n",
      "Epoch:  153|steps:   30|Train Avg Loss: 0.8767 |Test Loss: 0.8136|lr = 0.00785\n",
      "Epoch:  153|steps:   60|Train Avg Loss: 0.9202 |Test Loss: 0.8251|lr = 0.00785\n",
      "Epoch:  154|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8163|lr = 0.00785\n",
      "Epoch:  154|steps:   60|Train Avg Loss: 0.8842 |Test Loss: 0.8136|lr = 0.00785\n",
      "Epoch:  155|steps:   30|Train Avg Loss: 0.8717 |Test Loss: 0.8149|lr = 0.00785\n",
      "Epoch:  155|steps:   60|Train Avg Loss: 0.9185 |Test Loss: 0.8162|lr = 0.00785\n",
      "Epoch:  156|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8134|lr = 0.00769\n",
      "Epoch:  156|steps:   60|Train Avg Loss: 0.8946 |Test Loss: 0.8123|lr = 0.00769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  157|steps:   30|Train Avg Loss: 0.9002 |Test Loss: 0.8183|lr = 0.00769\n",
      "Epoch:  157|steps:   60|Train Avg Loss: 0.9018 |Test Loss: 0.8147|lr = 0.00769\n",
      "Epoch:  158|steps:   30|Train Avg Loss: 0.8974 |Test Loss: 0.8134|lr = 0.00769\n",
      "Epoch:  158|steps:   60|Train Avg Loss: 0.9069 |Test Loss: 0.8207|lr = 0.00769\n",
      "Epoch:  159|steps:   30|Train Avg Loss: 0.9103 |Test Loss: 0.8157|lr = 0.00769\n",
      "Epoch:  159|steps:   60|Train Avg Loss: 0.8840 |Test Loss: 0.8111|lr = 0.00769\n",
      "Epoch:  160|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8145|lr = 0.00769\n",
      "Epoch:  160|steps:   60|Train Avg Loss: 0.9032 |Test Loss: 0.8123|lr = 0.00769\n",
      "Epoch:  161|steps:   30|Train Avg Loss: 0.8985 |Test Loss: 0.8165|lr = 0.00769\n",
      "Epoch:  161|steps:   60|Train Avg Loss: 0.8846 |Test Loss: 0.8132|lr = 0.00769\n",
      "Epoch:  162|steps:   30|Train Avg Loss: 0.8891 |Test Loss: 0.8151|lr = 0.00769\n",
      "Epoch:  162|steps:   60|Train Avg Loss: 0.8890 |Test Loss: 0.8123|lr = 0.00769\n",
      "Epoch:  163|steps:   30|Train Avg Loss: 0.9089 |Test Loss: 0.8166|lr = 0.00769\n",
      "Epoch:  163|steps:   60|Train Avg Loss: 0.8816 |Test Loss: 0.8130|lr = 0.00769\n",
      "Epoch:  164|steps:   30|Train Avg Loss: 0.8965 |Test Loss: 0.8187|lr = 0.00769\n",
      "Epoch:  164|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8146|lr = 0.00769\n",
      "Epoch:  165|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8174|lr = 0.00769\n",
      "Epoch:  165|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8167|lr = 0.00769\n",
      "Epoch:  166|steps:   30|Train Avg Loss: 0.8637 |Test Loss: 0.8033|lr = 0.00769\n",
      "Epoch:  166|steps:   60|Train Avg Loss: 0.9229 |Test Loss: 0.8288|lr = 0.00769\n",
      "Epoch:  167|steps:   30|Train Avg Loss: 0.9151 |Test Loss: 0.8235|lr = 0.00754\n",
      "Epoch:  167|steps:   60|Train Avg Loss: 0.8787 |Test Loss: 0.8093|lr = 0.00754\n",
      "Epoch:  168|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8156|lr = 0.00754\n",
      "Epoch:  168|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8162|lr = 0.00754\n",
      "Epoch:  169|steps:   30|Train Avg Loss: 0.8923 |Test Loss: 0.8083|lr = 0.00754\n",
      "Epoch:  169|steps:   60|Train Avg Loss: 0.9024 |Test Loss: 0.8177|lr = 0.00754\n",
      "Epoch:  170|steps:   30|Train Avg Loss: 0.9054 |Test Loss: 0.8182|lr = 0.00754\n",
      "Epoch:  170|steps:   60|Train Avg Loss: 0.8919 |Test Loss: 0.8186|lr = 0.00754\n",
      "Epoch:  171|steps:   30|Train Avg Loss: 0.8973 |Test Loss: 0.8112|lr = 0.00754\n",
      "Epoch:  171|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8126|lr = 0.00754\n",
      "Epoch:  172|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8099|lr = 0.00754\n",
      "Epoch:  172|steps:   60|Train Avg Loss: 0.9049 |Test Loss: 0.8190|lr = 0.00754\n",
      "Epoch:  173|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8051|lr = 0.00754\n",
      "Epoch:  173|steps:   60|Train Avg Loss: 0.8982 |Test Loss: 0.8147|lr = 0.00754\n",
      "Epoch:  174|steps:   30|Train Avg Loss: 0.9065 |Test Loss: 0.8186|lr = 0.00754\n",
      "Epoch:  174|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8191|lr = 0.00754\n",
      "Epoch:  175|steps:   30|Train Avg Loss: 0.8982 |Test Loss: 0.8120|lr = 0.00754\n",
      "Epoch:  175|steps:   60|Train Avg Loss: 0.8919 |Test Loss: 0.8149|lr = 0.00754\n",
      "Epoch:  176|steps:   30|Train Avg Loss: 0.8818 |Test Loss: 0.8078|lr = 0.00754\n",
      "Epoch:  176|steps:   60|Train Avg Loss: 0.9028 |Test Loss: 0.8207|lr = 0.00754\n",
      "Epoch:  177|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8161|lr = 0.00754\n",
      "Epoch:  177|steps:   60|Train Avg Loss: 0.8835 |Test Loss: 0.8122|lr = 0.00754\n",
      "Epoch:  178|steps:   30|Train Avg Loss: 0.8826 |Test Loss: 0.8103|lr = 0.00739\n",
      "Epoch:  178|steps:   60|Train Avg Loss: 0.9062 |Test Loss: 0.8156|lr = 0.00739\n",
      "Epoch:  179|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8115|lr = 0.00739\n",
      "Epoch:  179|steps:   60|Train Avg Loss: 0.8984 |Test Loss: 0.8135|lr = 0.00739\n",
      "Epoch:  180|steps:   30|Train Avg Loss: 0.8847 |Test Loss: 0.8144|lr = 0.00739\n",
      "Epoch:  180|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8154|lr = 0.00739\n",
      "Epoch:  181|steps:   30|Train Avg Loss: 0.8856 |Test Loss: 0.8099|lr = 0.00739\n",
      "Epoch:  181|steps:   60|Train Avg Loss: 0.9092 |Test Loss: 0.8255|lr = 0.00739\n",
      "Epoch:  182|steps:   30|Train Avg Loss: 0.9022 |Test Loss: 0.8122|lr = 0.00739\n",
      "Epoch:  182|steps:   60|Train Avg Loss: 0.8824 |Test Loss: 0.8090|lr = 0.00739\n",
      "Epoch:  183|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8130|lr = 0.00739\n",
      "Epoch:  183|steps:   60|Train Avg Loss: 0.9091 |Test Loss: 0.8174|lr = 0.00739\n",
      "Epoch:  184|steps:   30|Train Avg Loss: 0.9031 |Test Loss: 0.8173|lr = 0.00739\n",
      "Epoch:  184|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8149|lr = 0.00739\n",
      "Epoch:  185|steps:   30|Train Avg Loss: 0.8993 |Test Loss: 0.8158|lr = 0.00739\n",
      "Epoch:  185|steps:   60|Train Avg Loss: 0.8817 |Test Loss: 0.8145|lr = 0.00739\n",
      "Epoch:  186|steps:   30|Train Avg Loss: 0.9114 |Test Loss: 0.8171|lr = 0.00739\n",
      "Epoch:  186|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8120|lr = 0.00739\n",
      "Epoch:  187|steps:   30|Train Avg Loss: 0.9007 |Test Loss: 0.8171|lr = 0.00739\n",
      "Epoch:  187|steps:   60|Train Avg Loss: 0.8839 |Test Loss: 0.8165|lr = 0.00739\n",
      "Epoch:  188|steps:   30|Train Avg Loss: 0.8989 |Test Loss: 0.8159|lr = 0.00739\n",
      "Epoch:  188|steps:   60|Train Avg Loss: 0.8920 |Test Loss: 0.8160|lr = 0.00739\n",
      "Epoch:  189|steps:   30|Train Avg Loss: 0.8972 |Test Loss: 0.8166|lr = 0.00724\n",
      "Epoch:  189|steps:   60|Train Avg Loss: 0.9042 |Test Loss: 0.8206|lr = 0.00724\n",
      "Epoch:  190|steps:   30|Train Avg Loss: 0.8986 |Test Loss: 0.8175|lr = 0.00724\n",
      "Epoch:  190|steps:   60|Train Avg Loss: 0.8936 |Test Loss: 0.8106|lr = 0.00724\n",
      "Epoch:  191|steps:   30|Train Avg Loss: 0.9012 |Test Loss: 0.8128|lr = 0.00724\n",
      "Epoch:  191|steps:   60|Train Avg Loss: 0.8924 |Test Loss: 0.8125|lr = 0.00724\n",
      "Epoch:  192|steps:   30|Train Avg Loss: 0.8962 |Test Loss: 0.8145|lr = 0.00724\n",
      "Epoch:  192|steps:   60|Train Avg Loss: 0.8903 |Test Loss: 0.8119|lr = 0.00724\n",
      "Epoch:  193|steps:   30|Train Avg Loss: 0.9044 |Test Loss: 0.8153|lr = 0.00724\n",
      "Epoch:  193|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8178|lr = 0.00724\n",
      "Epoch:  194|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8134|lr = 0.00724\n",
      "Epoch:  194|steps:   60|Train Avg Loss: 0.8909 |Test Loss: 0.8153|lr = 0.00724\n",
      "Epoch:  195|steps:   30|Train Avg Loss: 0.9019 |Test Loss: 0.8220|lr = 0.00724\n",
      "Epoch:  195|steps:   60|Train Avg Loss: 0.9060 |Test Loss: 0.8216|lr = 0.00724\n",
      "Epoch:  196|steps:   30|Train Avg Loss: 0.8880 |Test Loss: 0.8156|lr = 0.00724\n",
      "Epoch:  196|steps:   60|Train Avg Loss: 0.8943 |Test Loss: 0.8137|lr = 0.00724\n",
      "Epoch:  197|steps:   30|Train Avg Loss: 0.8950 |Test Loss: 0.8190|lr = 0.00724\n",
      "Epoch:  197|steps:   60|Train Avg Loss: 0.8967 |Test Loss: 0.8137|lr = 0.00724\n",
      "Epoch:  198|steps:   30|Train Avg Loss: 0.8922 |Test Loss: 0.8161|lr = 0.00724\n",
      "Epoch:  198|steps:   60|Train Avg Loss: 0.9032 |Test Loss: 0.8148|lr = 0.00724\n",
      "Epoch:  199|steps:   30|Train Avg Loss: 0.9120 |Test Loss: 0.8217|lr = 0.00724\n",
      "Epoch:  199|steps:   60|Train Avg Loss: 0.8734 |Test Loss: 0.8106|lr = 0.00724\n",
      "Epoch:  200|steps:   30|Train Avg Loss: 0.9134 |Test Loss: 0.8113|lr = 0.00709\n",
      "Epoch:  200|steps:   60|Train Avg Loss: 0.8752 |Test Loss: 0.8143|lr = 0.00709\n",
      "Epoch:  201|steps:   30|Train Avg Loss: 0.8780 |Test Loss: 0.8077|lr = 0.00709\n",
      "Epoch:  201|steps:   60|Train Avg Loss: 0.9125 |Test Loss: 0.8277|lr = 0.00709\n",
      "Epoch:  202|steps:   30|Train Avg Loss: 0.8794 |Test Loss: 0.8085|lr = 0.00709\n",
      "Epoch:  202|steps:   60|Train Avg Loss: 0.9153 |Test Loss: 0.8193|lr = 0.00709\n",
      "Epoch:  203|steps:   30|Train Avg Loss: 0.9144 |Test Loss: 0.8205|lr = 0.00709\n",
      "Epoch:  203|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8196|lr = 0.00709\n",
      "Epoch:  204|steps:   30|Train Avg Loss: 0.9030 |Test Loss: 0.8188|lr = 0.00709\n",
      "Epoch:  204|steps:   60|Train Avg Loss: 0.9013 |Test Loss: 0.8185|lr = 0.00709\n",
      "Epoch:  205|steps:   30|Train Avg Loss: 0.9049 |Test Loss: 0.8143|lr = 0.00709\n",
      "Epoch:  205|steps:   60|Train Avg Loss: 0.8948 |Test Loss: 0.8159|lr = 0.00709\n",
      "Epoch:  206|steps:   30|Train Avg Loss: 0.8861 |Test Loss: 0.8135|lr = 0.00709\n",
      "Epoch:  206|steps:   60|Train Avg Loss: 0.9041 |Test Loss: 0.8140|lr = 0.00709\n",
      "Epoch:  207|steps:   30|Train Avg Loss: 0.8776 |Test Loss: 0.8127|lr = 0.00709\n",
      "Epoch:  207|steps:   60|Train Avg Loss: 0.9178 |Test Loss: 0.8202|lr = 0.00709\n",
      "Epoch:  208|steps:   30|Train Avg Loss: 0.8970 |Test Loss: 0.8204|lr = 0.00709\n",
      "Epoch:  208|steps:   60|Train Avg Loss: 0.9017 |Test Loss: 0.8210|lr = 0.00709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  209|steps:   30|Train Avg Loss: 0.8931 |Test Loss: 0.8239|lr = 0.00709\n",
      "Epoch:  209|steps:   60|Train Avg Loss: 0.8989 |Test Loss: 0.8153|lr = 0.00709\n",
      "Epoch:  210|steps:   30|Train Avg Loss: 0.8996 |Test Loss: 0.8174|lr = 0.00709\n",
      "Epoch:  210|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8213|lr = 0.00709\n",
      "Epoch:  211|steps:   30|Train Avg Loss: 0.8979 |Test Loss: 0.8191|lr = 0.00695\n",
      "Epoch:  211|steps:   60|Train Avg Loss: 0.9004 |Test Loss: 0.8219|lr = 0.00695\n",
      "Epoch:  212|steps:   30|Train Avg Loss: 0.9055 |Test Loss: 0.8098|lr = 0.00695\n",
      "Epoch:  212|steps:   60|Train Avg Loss: 0.8859 |Test Loss: 0.8156|lr = 0.00695\n",
      "Epoch:  213|steps:   30|Train Avg Loss: 0.8718 |Test Loss: 0.8042|lr = 0.00695\n",
      "Epoch:  213|steps:   60|Train Avg Loss: 0.9211 |Test Loss: 0.8075|lr = 0.00695\n",
      "Epoch:  214|steps:   30|Train Avg Loss: 0.8982 |Test Loss: 0.8151|lr = 0.00695\n",
      "Epoch:  214|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8145|lr = 0.00695\n",
      "Epoch:  215|steps:   30|Train Avg Loss: 0.9010 |Test Loss: 0.8177|lr = 0.00695\n",
      "Epoch:  215|steps:   60|Train Avg Loss: 0.9077 |Test Loss: 0.8158|lr = 0.00695\n",
      "Epoch:  216|steps:   30|Train Avg Loss: 0.8974 |Test Loss: 0.8155|lr = 0.00695\n",
      "Epoch:  216|steps:   60|Train Avg Loss: 0.8741 |Test Loss: 0.8124|lr = 0.00695\n",
      "Epoch:  217|steps:   30|Train Avg Loss: 0.8906 |Test Loss: 0.8126|lr = 0.00695\n",
      "Epoch:  217|steps:   60|Train Avg Loss: 0.9070 |Test Loss: 0.8273|lr = 0.00695\n",
      "Epoch:  218|steps:   30|Train Avg Loss: 0.8952 |Test Loss: 0.8172|lr = 0.00695\n",
      "Epoch:  218|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8105|lr = 0.00695\n",
      "Epoch:  219|steps:   30|Train Avg Loss: 0.9181 |Test Loss: 0.8179|lr = 0.00695\n",
      "Epoch:  219|steps:   60|Train Avg Loss: 0.8742 |Test Loss: 0.8103|lr = 0.00695\n",
      "Epoch:  220|steps:   30|Train Avg Loss: 0.8876 |Test Loss: 0.8128|lr = 0.00695\n",
      "Epoch:  220|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8134|lr = 0.00695\n",
      "Epoch:  221|steps:   30|Train Avg Loss: 0.8922 |Test Loss: 0.8144|lr = 0.00695\n",
      "Epoch:  221|steps:   60|Train Avg Loss: 0.9036 |Test Loss: 0.8195|lr = 0.00695\n",
      "Epoch:  222|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8137|lr = 0.00681\n",
      "Epoch:  222|steps:   60|Train Avg Loss: 0.8920 |Test Loss: 0.8126|lr = 0.00681\n",
      "Epoch:  223|steps:   30|Train Avg Loss: 0.9003 |Test Loss: 0.8135|lr = 0.00681\n",
      "Epoch:  223|steps:   60|Train Avg Loss: 0.8894 |Test Loss: 0.8154|lr = 0.00681\n",
      "Epoch:  224|steps:   30|Train Avg Loss: 0.8977 |Test Loss: 0.8134|lr = 0.00681\n",
      "Epoch:  224|steps:   60|Train Avg Loss: 0.8790 |Test Loss: 0.8063|lr = 0.00681\n",
      "Epoch:  225|steps:   30|Train Avg Loss: 0.8972 |Test Loss: 0.8153|lr = 0.00681\n",
      "Epoch:  225|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8126|lr = 0.00681\n",
      "Epoch:  226|steps:   30|Train Avg Loss: 0.8849 |Test Loss: 0.8143|lr = 0.00681\n",
      "Epoch:  226|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8153|lr = 0.00681\n",
      "Epoch:  227|steps:   30|Train Avg Loss: 0.9023 |Test Loss: 0.8181|lr = 0.00681\n",
      "Epoch:  227|steps:   60|Train Avg Loss: 0.8966 |Test Loss: 0.8175|lr = 0.00681\n",
      "Epoch:  228|steps:   30|Train Avg Loss: 0.8739 |Test Loss: 0.8133|lr = 0.00681\n",
      "Epoch:  228|steps:   60|Train Avg Loss: 0.9278 |Test Loss: 0.8176|lr = 0.00681\n",
      "Epoch:  229|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8152|lr = 0.00681\n",
      "Epoch:  229|steps:   60|Train Avg Loss: 0.8929 |Test Loss: 0.8188|lr = 0.00681\n",
      "Epoch:  230|steps:   30|Train Avg Loss: 0.9035 |Test Loss: 0.8133|lr = 0.00681\n",
      "Epoch:  230|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8176|lr = 0.00681\n",
      "Epoch:  231|steps:   30|Train Avg Loss: 0.8944 |Test Loss: 0.8155|lr = 0.00681\n",
      "Epoch:  231|steps:   60|Train Avg Loss: 0.8963 |Test Loss: 0.8149|lr = 0.00681\n",
      "Epoch:  232|steps:   30|Train Avg Loss: 0.9053 |Test Loss: 0.8189|lr = 0.00681\n",
      "Epoch:  232|steps:   60|Train Avg Loss: 0.8826 |Test Loss: 0.8151|lr = 0.00681\n",
      "Epoch:  233|steps:   30|Train Avg Loss: 0.9180 |Test Loss: 0.8217|lr = 0.00668\n",
      "Epoch:  233|steps:   60|Train Avg Loss: 0.8750 |Test Loss: 0.8128|lr = 0.00668\n",
      "Epoch:  234|steps:   30|Train Avg Loss: 0.8903 |Test Loss: 0.8152|lr = 0.00668\n",
      "Epoch:  234|steps:   60|Train Avg Loss: 0.8843 |Test Loss: 0.8152|lr = 0.00668\n",
      "Epoch:  235|steps:   30|Train Avg Loss: 0.8825 |Test Loss: 0.8122|lr = 0.00668\n",
      "Epoch:  235|steps:   60|Train Avg Loss: 0.9110 |Test Loss: 0.8173|lr = 0.00668\n",
      "Epoch:  236|steps:   30|Train Avg Loss: 0.9035 |Test Loss: 0.8149|lr = 0.00668\n",
      "Epoch:  236|steps:   60|Train Avg Loss: 0.8763 |Test Loss: 0.8092|lr = 0.00668\n",
      "Epoch:  237|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8159|lr = 0.00668\n",
      "Epoch:  237|steps:   60|Train Avg Loss: 0.8925 |Test Loss: 0.8234|lr = 0.00668\n",
      "Epoch:  238|steps:   30|Train Avg Loss: 0.8902 |Test Loss: 0.8179|lr = 0.00668\n",
      "Epoch:  238|steps:   60|Train Avg Loss: 0.8937 |Test Loss: 0.8132|lr = 0.00668\n",
      "Epoch:  239|steps:   30|Train Avg Loss: 0.8992 |Test Loss: 0.8154|lr = 0.00668\n",
      "Epoch:  239|steps:   60|Train Avg Loss: 0.8889 |Test Loss: 0.8154|lr = 0.00668\n",
      "Epoch:  240|steps:   30|Train Avg Loss: 0.9006 |Test Loss: 0.8151|lr = 0.00668\n",
      "Epoch:  240|steps:   60|Train Avg Loss: 0.8737 |Test Loss: 0.8095|lr = 0.00668\n",
      "Epoch:  241|steps:   30|Train Avg Loss: 0.8953 |Test Loss: 0.8050|lr = 0.00668\n",
      "Epoch:  241|steps:   60|Train Avg Loss: 0.9095 |Test Loss: 0.8231|lr = 0.00668\n",
      "Epoch:  242|steps:   30|Train Avg Loss: 0.8861 |Test Loss: 0.8126|lr = 0.00668\n",
      "Epoch:  242|steps:   60|Train Avg Loss: 0.9028 |Test Loss: 0.8185|lr = 0.00668\n",
      "Epoch:  243|steps:   30|Train Avg Loss: 0.8958 |Test Loss: 0.8162|lr = 0.00668\n",
      "Epoch:  243|steps:   60|Train Avg Loss: 0.9033 |Test Loss: 0.8200|lr = 0.00668\n",
      "Epoch:  244|steps:   30|Train Avg Loss: 0.8940 |Test Loss: 0.8141|lr = 0.00654\n",
      "Epoch:  244|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8139|lr = 0.00654\n",
      "Epoch:  245|steps:   30|Train Avg Loss: 0.8887 |Test Loss: 0.8175|lr = 0.00654\n",
      "Epoch:  245|steps:   60|Train Avg Loss: 0.8979 |Test Loss: 0.8151|lr = 0.00654\n",
      "Epoch:  246|steps:   30|Train Avg Loss: 0.8788 |Test Loss: 0.8157|lr = 0.00654\n",
      "Epoch:  246|steps:   60|Train Avg Loss: 0.9170 |Test Loss: 0.8201|lr = 0.00654\n",
      "Epoch:  247|steps:   30|Train Avg Loss: 0.9064 |Test Loss: 0.8170|lr = 0.00654\n",
      "Epoch:  247|steps:   60|Train Avg Loss: 0.8774 |Test Loss: 0.8085|lr = 0.00654\n",
      "Epoch:  248|steps:   30|Train Avg Loss: 0.9013 |Test Loss: 0.8155|lr = 0.00654\n",
      "Epoch:  248|steps:   60|Train Avg Loss: 0.8945 |Test Loss: 0.8152|lr = 0.00654\n",
      "Epoch:  249|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8145|lr = 0.00654\n",
      "Epoch:  249|steps:   60|Train Avg Loss: 0.8841 |Test Loss: 0.8128|lr = 0.00654\n",
      "Epoch:  250|steps:   30|Train Avg Loss: 0.9160 |Test Loss: 0.8188|lr = 0.00654\n",
      "Epoch:  250|steps:   60|Train Avg Loss: 0.8856 |Test Loss: 0.8118|lr = 0.00654\n",
      "Epoch:  251|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8130|lr = 0.00654\n",
      "Epoch:  251|steps:   60|Train Avg Loss: 0.9098 |Test Loss: 0.8151|lr = 0.00654\n",
      "Epoch:  252|steps:   30|Train Avg Loss: 0.9153 |Test Loss: 0.8139|lr = 0.00654\n",
      "Epoch:  252|steps:   60|Train Avg Loss: 0.8915 |Test Loss: 0.8168|lr = 0.00654\n",
      "Epoch:  253|steps:   30|Train Avg Loss: 0.9121 |Test Loss: 0.8168|lr = 0.00654\n",
      "Epoch:  253|steps:   60|Train Avg Loss: 0.8863 |Test Loss: 0.8138|lr = 0.00654\n",
      "Epoch:  254|steps:   30|Train Avg Loss: 0.9008 |Test Loss: 0.8136|lr = 0.00654\n",
      "Epoch:  254|steps:   60|Train Avg Loss: 0.8826 |Test Loss: 0.8042|lr = 0.00654\n",
      "Epoch:  255|steps:   30|Train Avg Loss: 0.8922 |Test Loss: 0.8163|lr = 0.00641\n",
      "Epoch:  255|steps:   60|Train Avg Loss: 0.8972 |Test Loss: 0.8132|lr = 0.00641\n",
      "Epoch:  256|steps:   30|Train Avg Loss: 0.8753 |Test Loss: 0.8119|lr = 0.00641\n",
      "Epoch:  256|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8196|lr = 0.00641\n",
      "Epoch:  257|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8141|lr = 0.00641\n",
      "Epoch:  257|steps:   60|Train Avg Loss: 0.8975 |Test Loss: 0.8155|lr = 0.00641\n",
      "Epoch:  258|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8128|lr = 0.00641\n",
      "Epoch:  258|steps:   60|Train Avg Loss: 0.9032 |Test Loss: 0.8187|lr = 0.00641\n",
      "Epoch:  259|steps:   30|Train Avg Loss: 0.9006 |Test Loss: 0.8188|lr = 0.00641\n",
      "Epoch:  259|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8128|lr = 0.00641\n",
      "Epoch:  260|steps:   30|Train Avg Loss: 0.9031 |Test Loss: 0.8185|lr = 0.00641\n",
      "Epoch:  260|steps:   60|Train Avg Loss: 0.8840 |Test Loss: 0.8191|lr = 0.00641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  261|steps:   30|Train Avg Loss: 0.9137 |Test Loss: 0.8235|lr = 0.00641\n",
      "Epoch:  261|steps:   60|Train Avg Loss: 0.8863 |Test Loss: 0.8137|lr = 0.00641\n",
      "Epoch:  262|steps:   30|Train Avg Loss: 0.8839 |Test Loss: 0.8153|lr = 0.00641\n",
      "Epoch:  262|steps:   60|Train Avg Loss: 0.8892 |Test Loss: 0.8129|lr = 0.00641\n",
      "Epoch:  263|steps:   30|Train Avg Loss: 0.8814 |Test Loss: 0.8161|lr = 0.00641\n",
      "Epoch:  263|steps:   60|Train Avg Loss: 0.9068 |Test Loss: 0.8184|lr = 0.00641\n",
      "Epoch:  264|steps:   30|Train Avg Loss: 0.9025 |Test Loss: 0.8156|lr = 0.00641\n",
      "Epoch:  264|steps:   60|Train Avg Loss: 0.8962 |Test Loss: 0.8150|lr = 0.00641\n",
      "Epoch:  265|steps:   30|Train Avg Loss: 0.9051 |Test Loss: 0.8185|lr = 0.00641\n",
      "Epoch:  265|steps:   60|Train Avg Loss: 0.8747 |Test Loss: 0.8130|lr = 0.00641\n",
      "Epoch:  266|steps:   30|Train Avg Loss: 0.8893 |Test Loss: 0.8141|lr = 0.00628\n",
      "Epoch:  266|steps:   60|Train Avg Loss: 0.8929 |Test Loss: 0.8231|lr = 0.00628\n",
      "Epoch:  267|steps:   30|Train Avg Loss: 0.8854 |Test Loss: 0.8114|lr = 0.00628\n",
      "Epoch:  267|steps:   60|Train Avg Loss: 0.8887 |Test Loss: 0.8109|lr = 0.00628\n",
      "Epoch:  268|steps:   30|Train Avg Loss: 0.9138 |Test Loss: 0.8208|lr = 0.00628\n",
      "Epoch:  268|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8124|lr = 0.00628\n",
      "Epoch:  269|steps:   30|Train Avg Loss: 0.9088 |Test Loss: 0.8213|lr = 0.00628\n",
      "Epoch:  269|steps:   60|Train Avg Loss: 0.8800 |Test Loss: 0.8171|lr = 0.00628\n",
      "Epoch:  270|steps:   30|Train Avg Loss: 0.9024 |Test Loss: 0.8195|lr = 0.00628\n",
      "Epoch:  270|steps:   60|Train Avg Loss: 0.8836 |Test Loss: 0.8107|lr = 0.00628\n",
      "Epoch:  271|steps:   30|Train Avg Loss: 0.9277 |Test Loss: 0.8255|lr = 0.00628\n",
      "Epoch:  271|steps:   60|Train Avg Loss: 0.8698 |Test Loss: 0.8109|lr = 0.00628\n",
      "Epoch:  272|steps:   30|Train Avg Loss: 0.9079 |Test Loss: 0.8151|lr = 0.00628\n",
      "Epoch:  272|steps:   60|Train Avg Loss: 0.8809 |Test Loss: 0.8128|lr = 0.00628\n",
      "Epoch:  273|steps:   30|Train Avg Loss: 0.8764 |Test Loss: 0.8088|lr = 0.00628\n",
      "Epoch:  273|steps:   60|Train Avg Loss: 0.8928 |Test Loss: 0.8139|lr = 0.00628\n",
      "Epoch:  274|steps:   30|Train Avg Loss: 0.9150 |Test Loss: 0.8178|lr = 0.00628\n",
      "Epoch:  274|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8122|lr = 0.00628\n",
      "Epoch:  275|steps:   30|Train Avg Loss: 0.9153 |Test Loss: 0.8182|lr = 0.00628\n",
      "Epoch:  275|steps:   60|Train Avg Loss: 0.8802 |Test Loss: 0.8139|lr = 0.00628\n",
      "Epoch:  276|steps:   30|Train Avg Loss: 0.9128 |Test Loss: 0.8216|lr = 0.00628\n",
      "Epoch:  276|steps:   60|Train Avg Loss: 0.8785 |Test Loss: 0.8094|lr = 0.00628\n",
      "Epoch:  277|steps:   30|Train Avg Loss: 0.8967 |Test Loss: 0.8196|lr = 0.00616\n",
      "Epoch:  277|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8128|lr = 0.00616\n",
      "Epoch:  278|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8171|lr = 0.00616\n",
      "Epoch:  278|steps:   60|Train Avg Loss: 0.8994 |Test Loss: 0.8172|lr = 0.00616\n",
      "Epoch:  279|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8211|lr = 0.00616\n",
      "Epoch:  279|steps:   60|Train Avg Loss: 0.8968 |Test Loss: 0.8161|lr = 0.00616\n",
      "Epoch:  280|steps:   30|Train Avg Loss: 0.9192 |Test Loss: 0.8195|lr = 0.00616\n",
      "Epoch:  280|steps:   60|Train Avg Loss: 0.8834 |Test Loss: 0.8147|lr = 0.00616\n",
      "Epoch:  281|steps:   30|Train Avg Loss: 0.9054 |Test Loss: 0.8135|lr = 0.00616\n",
      "Epoch:  281|steps:   60|Train Avg Loss: 0.8726 |Test Loss: 0.8105|lr = 0.00616\n",
      "Epoch:  282|steps:   30|Train Avg Loss: 0.9060 |Test Loss: 0.8178|lr = 0.00616\n",
      "Epoch:  282|steps:   60|Train Avg Loss: 0.8782 |Test Loss: 0.8104|lr = 0.00616\n",
      "Epoch:  283|steps:   30|Train Avg Loss: 0.8933 |Test Loss: 0.8152|lr = 0.00616\n",
      "Epoch:  283|steps:   60|Train Avg Loss: 0.8956 |Test Loss: 0.8153|lr = 0.00616\n",
      "Epoch:  284|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8177|lr = 0.00616\n",
      "Epoch:  284|steps:   60|Train Avg Loss: 0.9031 |Test Loss: 0.8204|lr = 0.00616\n",
      "Epoch:  285|steps:   30|Train Avg Loss: 0.9055 |Test Loss: 0.8190|lr = 0.00616\n",
      "Epoch:  285|steps:   60|Train Avg Loss: 0.8703 |Test Loss: 0.8077|lr = 0.00616\n",
      "Epoch:  286|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8144|lr = 0.00616\n",
      "Epoch:  286|steps:   60|Train Avg Loss: 0.8820 |Test Loss: 0.8103|lr = 0.00616\n",
      "Epoch:  287|steps:   30|Train Avg Loss: 0.8920 |Test Loss: 0.8176|lr = 0.00616\n",
      "Epoch:  287|steps:   60|Train Avg Loss: 0.9117 |Test Loss: 0.8156|lr = 0.00616\n",
      "Epoch:  288|steps:   30|Train Avg Loss: 0.9154 |Test Loss: 0.8232|lr = 0.00603\n",
      "Epoch:  288|steps:   60|Train Avg Loss: 0.8784 |Test Loss: 0.8092|lr = 0.00603\n",
      "Epoch:  289|steps:   30|Train Avg Loss: 0.8897 |Test Loss: 0.8106|lr = 0.00603\n",
      "Epoch:  289|steps:   60|Train Avg Loss: 0.8973 |Test Loss: 0.8146|lr = 0.00603\n",
      "Epoch:  290|steps:   30|Train Avg Loss: 0.8588 |Test Loss: 0.8128|lr = 0.00603\n",
      "Epoch:  290|steps:   60|Train Avg Loss: 0.9253 |Test Loss: 0.8375|lr = 0.00603\n",
      "Epoch:  291|steps:   30|Train Avg Loss: 0.8844 |Test Loss: 0.8141|lr = 0.00603\n",
      "Epoch:  291|steps:   60|Train Avg Loss: 0.9040 |Test Loss: 0.8147|lr = 0.00603\n",
      "Epoch:  292|steps:   30|Train Avg Loss: 0.8623 |Test Loss: 0.8109|lr = 0.00603\n",
      "Epoch:  292|steps:   60|Train Avg Loss: 0.9190 |Test Loss: 0.8214|lr = 0.00603\n",
      "Epoch:  293|steps:   30|Train Avg Loss: 0.8975 |Test Loss: 0.8209|lr = 0.00603\n",
      "Epoch:  293|steps:   60|Train Avg Loss: 0.8861 |Test Loss: 0.8178|lr = 0.00603\n",
      "Epoch:  294|steps:   30|Train Avg Loss: 0.9030 |Test Loss: 0.8174|lr = 0.00603\n",
      "Epoch:  294|steps:   60|Train Avg Loss: 0.8898 |Test Loss: 0.8163|lr = 0.00603\n",
      "Epoch:  295|steps:   30|Train Avg Loss: 0.8805 |Test Loss: 0.8145|lr = 0.00603\n",
      "Epoch:  295|steps:   60|Train Avg Loss: 0.9066 |Test Loss: 0.8164|lr = 0.00603\n",
      "Epoch:  296|steps:   30|Train Avg Loss: 0.8887 |Test Loss: 0.8162|lr = 0.00603\n",
      "Epoch:  296|steps:   60|Train Avg Loss: 0.8978 |Test Loss: 0.8146|lr = 0.00603\n",
      "Epoch:  297|steps:   30|Train Avg Loss: 0.8992 |Test Loss: 0.8149|lr = 0.00603\n",
      "Epoch:  297|steps:   60|Train Avg Loss: 0.8907 |Test Loss: 0.8175|lr = 0.00603\n",
      "Epoch:  298|steps:   30|Train Avg Loss: 0.8997 |Test Loss: 0.8188|lr = 0.00603\n",
      "Epoch:  298|steps:   60|Train Avg Loss: 0.8905 |Test Loss: 0.8149|lr = 0.00603\n",
      "Epoch:  299|steps:   30|Train Avg Loss: 0.9110 |Test Loss: 0.8217|lr = 0.00591\n",
      "Epoch:  299|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8146|lr = 0.00591\n",
      "Epoch:  300|steps:   30|Train Avg Loss: 0.9111 |Test Loss: 0.8181|lr = 0.00591\n",
      "Epoch:  300|steps:   60|Train Avg Loss: 0.8805 |Test Loss: 0.8144|lr = 0.00591\n",
      "Epoch:  301|steps:   30|Train Avg Loss: 0.9091 |Test Loss: 0.8245|lr = 0.00591\n",
      "Epoch:  301|steps:   60|Train Avg Loss: 0.8771 |Test Loss: 0.8122|lr = 0.00591\n",
      "Epoch:  302|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8146|lr = 0.00591\n",
      "Epoch:  302|steps:   60|Train Avg Loss: 0.9049 |Test Loss: 0.8216|lr = 0.00591\n",
      "Epoch:  303|steps:   30|Train Avg Loss: 0.8990 |Test Loss: 0.8174|lr = 0.00591\n",
      "Epoch:  303|steps:   60|Train Avg Loss: 0.8994 |Test Loss: 0.8154|lr = 0.00591\n",
      "Epoch:  304|steps:   30|Train Avg Loss: 0.8964 |Test Loss: 0.8219|lr = 0.00591\n",
      "Epoch:  304|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8142|lr = 0.00591\n",
      "Epoch:  305|steps:   30|Train Avg Loss: 0.9015 |Test Loss: 0.8186|lr = 0.00591\n",
      "Epoch:  305|steps:   60|Train Avg Loss: 0.8909 |Test Loss: 0.8111|lr = 0.00591\n",
      "Epoch:  306|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8172|lr = 0.00591\n",
      "Epoch:  306|steps:   60|Train Avg Loss: 0.8915 |Test Loss: 0.8208|lr = 0.00591\n",
      "Epoch:  307|steps:   30|Train Avg Loss: 0.8910 |Test Loss: 0.8155|lr = 0.00591\n",
      "Epoch:  307|steps:   60|Train Avg Loss: 0.8936 |Test Loss: 0.8221|lr = 0.00591\n",
      "Epoch:  308|steps:   30|Train Avg Loss: 0.8926 |Test Loss: 0.8172|lr = 0.00591\n",
      "Epoch:  308|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8272|lr = 0.00591\n",
      "Epoch:  309|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8145|lr = 0.00591\n",
      "Epoch:  309|steps:   60|Train Avg Loss: 0.8952 |Test Loss: 0.8203|lr = 0.00591\n",
      "Epoch:  310|steps:   30|Train Avg Loss: 0.8881 |Test Loss: 0.8124|lr = 0.00580\n",
      "Epoch:  310|steps:   60|Train Avg Loss: 0.9058 |Test Loss: 0.8202|lr = 0.00580\n",
      "Epoch:  311|steps:   30|Train Avg Loss: 0.8809 |Test Loss: 0.8149|lr = 0.00580\n",
      "Epoch:  311|steps:   60|Train Avg Loss: 0.9047 |Test Loss: 0.8148|lr = 0.00580\n",
      "Epoch:  312|steps:   30|Train Avg Loss: 0.8917 |Test Loss: 0.8149|lr = 0.00580\n",
      "Epoch:  312|steps:   60|Train Avg Loss: 0.8917 |Test Loss: 0.8192|lr = 0.00580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  313|steps:   30|Train Avg Loss: 0.8852 |Test Loss: 0.8150|lr = 0.00580\n",
      "Epoch:  313|steps:   60|Train Avg Loss: 0.8908 |Test Loss: 0.8148|lr = 0.00580\n",
      "Epoch:  314|steps:   30|Train Avg Loss: 0.8988 |Test Loss: 0.8171|lr = 0.00580\n",
      "Epoch:  314|steps:   60|Train Avg Loss: 0.8996 |Test Loss: 0.8205|lr = 0.00580\n",
      "Epoch:  315|steps:   30|Train Avg Loss: 0.9038 |Test Loss: 0.8143|lr = 0.00580\n",
      "Epoch:  315|steps:   60|Train Avg Loss: 0.8809 |Test Loss: 0.8101|lr = 0.00580\n",
      "Epoch:  316|steps:   30|Train Avg Loss: 0.8714 |Test Loss: 0.8140|lr = 0.00580\n",
      "Epoch:  316|steps:   60|Train Avg Loss: 0.9059 |Test Loss: 0.8230|lr = 0.00580\n",
      "Epoch:  317|steps:   30|Train Avg Loss: 0.8947 |Test Loss: 0.8139|lr = 0.00580\n",
      "Epoch:  317|steps:   60|Train Avg Loss: 0.8908 |Test Loss: 0.8091|lr = 0.00580\n",
      "Epoch:  318|steps:   30|Train Avg Loss: 0.8979 |Test Loss: 0.8187|lr = 0.00580\n",
      "Epoch:  318|steps:   60|Train Avg Loss: 0.8864 |Test Loss: 0.8143|lr = 0.00580\n",
      "Epoch:  319|steps:   30|Train Avg Loss: 0.9239 |Test Loss: 0.8211|lr = 0.00580\n",
      "Epoch:  319|steps:   60|Train Avg Loss: 0.8751 |Test Loss: 0.8083|lr = 0.00580\n",
      "Epoch:  320|steps:   30|Train Avg Loss: 0.8951 |Test Loss: 0.8171|lr = 0.00580\n",
      "Epoch:  320|steps:   60|Train Avg Loss: 0.8970 |Test Loss: 0.8137|lr = 0.00580\n",
      "Epoch:  321|steps:   30|Train Avg Loss: 0.8969 |Test Loss: 0.8245|lr = 0.00568\n",
      "Epoch:  321|steps:   60|Train Avg Loss: 0.8924 |Test Loss: 0.8202|lr = 0.00568\n",
      "Epoch:  322|steps:   30|Train Avg Loss: 0.9006 |Test Loss: 0.8137|lr = 0.00568\n",
      "Epoch:  322|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8183|lr = 0.00568\n",
      "Epoch:  323|steps:   30|Train Avg Loss: 0.8826 |Test Loss: 0.8134|lr = 0.00568\n",
      "Epoch:  323|steps:   60|Train Avg Loss: 0.8922 |Test Loss: 0.8147|lr = 0.00568\n",
      "Epoch:  324|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8142|lr = 0.00568\n",
      "Epoch:  324|steps:   60|Train Avg Loss: 0.9061 |Test Loss: 0.8192|lr = 0.00568\n",
      "Epoch:  325|steps:   30|Train Avg Loss: 0.9056 |Test Loss: 0.8247|lr = 0.00568\n",
      "Epoch:  325|steps:   60|Train Avg Loss: 0.8611 |Test Loss: 0.8054|lr = 0.00568\n",
      "Epoch:  326|steps:   30|Train Avg Loss: 0.9067 |Test Loss: 0.8196|lr = 0.00568\n",
      "Epoch:  326|steps:   60|Train Avg Loss: 0.8920 |Test Loss: 0.8170|lr = 0.00568\n",
      "Epoch:  327|steps:   30|Train Avg Loss: 0.8974 |Test Loss: 0.8153|lr = 0.00568\n",
      "Epoch:  327|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8162|lr = 0.00568\n",
      "Epoch:  328|steps:   30|Train Avg Loss: 0.8704 |Test Loss: 0.8111|lr = 0.00568\n",
      "Epoch:  328|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8135|lr = 0.00568\n",
      "Epoch:  329|steps:   30|Train Avg Loss: 0.9000 |Test Loss: 0.8211|lr = 0.00568\n",
      "Epoch:  329|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8168|lr = 0.00568\n",
      "Epoch:  330|steps:   30|Train Avg Loss: 0.9088 |Test Loss: 0.8190|lr = 0.00568\n",
      "Epoch:  330|steps:   60|Train Avg Loss: 0.8717 |Test Loss: 0.8138|lr = 0.00568\n",
      "Epoch:  331|steps:   30|Train Avg Loss: 0.8762 |Test Loss: 0.8151|lr = 0.00568\n",
      "Epoch:  331|steps:   60|Train Avg Loss: 0.9023 |Test Loss: 0.8170|lr = 0.00568\n",
      "Epoch:  332|steps:   30|Train Avg Loss: 0.8896 |Test Loss: 0.8174|lr = 0.00557\n",
      "Epoch:  332|steps:   60|Train Avg Loss: 0.9028 |Test Loss: 0.8194|lr = 0.00557\n",
      "Epoch:  333|steps:   30|Train Avg Loss: 0.8960 |Test Loss: 0.8172|lr = 0.00557\n",
      "Epoch:  333|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8141|lr = 0.00557\n",
      "Epoch:  334|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8163|lr = 0.00557\n",
      "Epoch:  334|steps:   60|Train Avg Loss: 0.8940 |Test Loss: 0.8169|lr = 0.00557\n",
      "Epoch:  335|steps:   30|Train Avg Loss: 0.8993 |Test Loss: 0.8187|lr = 0.00557\n",
      "Epoch:  335|steps:   60|Train Avg Loss: 0.8906 |Test Loss: 0.8154|lr = 0.00557\n",
      "Epoch:  336|steps:   30|Train Avg Loss: 0.8803 |Test Loss: 0.8138|lr = 0.00557\n",
      "Epoch:  336|steps:   60|Train Avg Loss: 0.9105 |Test Loss: 0.8166|lr = 0.00557\n",
      "Epoch:  337|steps:   30|Train Avg Loss: 0.8701 |Test Loss: 0.8101|lr = 0.00557\n",
      "Epoch:  337|steps:   60|Train Avg Loss: 0.9120 |Test Loss: 0.8297|lr = 0.00557\n",
      "Epoch:  338|steps:   30|Train Avg Loss: 0.8710 |Test Loss: 0.8146|lr = 0.00557\n",
      "Epoch:  338|steps:   60|Train Avg Loss: 0.9190 |Test Loss: 0.8204|lr = 0.00557\n",
      "Epoch:  339|steps:   30|Train Avg Loss: 0.8802 |Test Loss: 0.8154|lr = 0.00557\n",
      "Epoch:  339|steps:   60|Train Avg Loss: 0.9028 |Test Loss: 0.8177|lr = 0.00557\n",
      "Epoch:  340|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8153|lr = 0.00557\n",
      "Epoch:  340|steps:   60|Train Avg Loss: 0.9113 |Test Loss: 0.8173|lr = 0.00557\n",
      "Epoch:  341|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8183|lr = 0.00557\n",
      "Epoch:  341|steps:   60|Train Avg Loss: 0.8826 |Test Loss: 0.8179|lr = 0.00557\n",
      "Epoch:  342|steps:   30|Train Avg Loss: 0.8984 |Test Loss: 0.8175|lr = 0.00557\n",
      "Epoch:  342|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8162|lr = 0.00557\n",
      "Epoch:  343|steps:   30|Train Avg Loss: 0.8996 |Test Loss: 0.8211|lr = 0.00545\n",
      "Epoch:  343|steps:   60|Train Avg Loss: 0.8782 |Test Loss: 0.8160|lr = 0.00545\n",
      "Epoch:  344|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8178|lr = 0.00545\n",
      "Epoch:  344|steps:   60|Train Avg Loss: 0.9022 |Test Loss: 0.8333|lr = 0.00545\n",
      "Epoch:  345|steps:   30|Train Avg Loss: 0.8916 |Test Loss: 0.8159|lr = 0.00545\n",
      "Epoch:  345|steps:   60|Train Avg Loss: 0.8913 |Test Loss: 0.8147|lr = 0.00545\n",
      "Epoch:  346|steps:   30|Train Avg Loss: 0.9123 |Test Loss: 0.8248|lr = 0.00545\n",
      "Epoch:  346|steps:   60|Train Avg Loss: 0.8861 |Test Loss: 0.8149|lr = 0.00545\n",
      "Epoch:  347|steps:   30|Train Avg Loss: 0.8907 |Test Loss: 0.8147|lr = 0.00545\n",
      "Epoch:  347|steps:   60|Train Avg Loss: 0.8921 |Test Loss: 0.8163|lr = 0.00545\n",
      "Epoch:  348|steps:   30|Train Avg Loss: 0.8910 |Test Loss: 0.8148|lr = 0.00545\n",
      "Epoch:  348|steps:   60|Train Avg Loss: 0.8938 |Test Loss: 0.8172|lr = 0.00545\n",
      "Epoch:  349|steps:   30|Train Avg Loss: 0.8737 |Test Loss: 0.8115|lr = 0.00545\n",
      "Epoch:  349|steps:   60|Train Avg Loss: 0.9127 |Test Loss: 0.8206|lr = 0.00545\n",
      "Epoch:  350|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8118|lr = 0.00545\n",
      "Epoch:  350|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8094|lr = 0.00545\n",
      "Epoch:  351|steps:   30|Train Avg Loss: 0.9089 |Test Loss: 0.8214|lr = 0.00545\n",
      "Epoch:  351|steps:   60|Train Avg Loss: 0.8711 |Test Loss: 0.8135|lr = 0.00545\n",
      "Epoch:  352|steps:   30|Train Avg Loss: 0.8865 |Test Loss: 0.8153|lr = 0.00545\n",
      "Epoch:  352|steps:   60|Train Avg Loss: 0.9008 |Test Loss: 0.8200|lr = 0.00545\n",
      "Epoch:  353|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8125|lr = 0.00545\n",
      "Epoch:  353|steps:   60|Train Avg Loss: 0.8982 |Test Loss: 0.8181|lr = 0.00545\n",
      "Epoch:  354|steps:   30|Train Avg Loss: 0.8967 |Test Loss: 0.8201|lr = 0.00535\n",
      "Epoch:  354|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8143|lr = 0.00535\n",
      "Epoch:  355|steps:   30|Train Avg Loss: 0.8858 |Test Loss: 0.8154|lr = 0.00535\n",
      "Epoch:  355|steps:   60|Train Avg Loss: 0.9055 |Test Loss: 0.8190|lr = 0.00535\n",
      "Epoch:  356|steps:   30|Train Avg Loss: 0.8968 |Test Loss: 0.8201|lr = 0.00535\n",
      "Epoch:  356|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8149|lr = 0.00535\n",
      "Epoch:  357|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8142|lr = 0.00535\n",
      "Epoch:  357|steps:   60|Train Avg Loss: 0.8795 |Test Loss: 0.8133|lr = 0.00535\n",
      "Epoch:  358|steps:   30|Train Avg Loss: 0.8840 |Test Loss: 0.8172|lr = 0.00535\n",
      "Epoch:  358|steps:   60|Train Avg Loss: 0.9179 |Test Loss: 0.8201|lr = 0.00535\n",
      "Epoch:  359|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8201|lr = 0.00535\n",
      "Epoch:  359|steps:   60|Train Avg Loss: 0.9009 |Test Loss: 0.8179|lr = 0.00535\n",
      "Epoch:  360|steps:   30|Train Avg Loss: 0.8777 |Test Loss: 0.8128|lr = 0.00535\n",
      "Epoch:  360|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8177|lr = 0.00535\n",
      "Epoch:  361|steps:   30|Train Avg Loss: 0.9013 |Test Loss: 0.8174|lr = 0.00535\n",
      "Epoch:  361|steps:   60|Train Avg Loss: 0.8881 |Test Loss: 0.8145|lr = 0.00535\n",
      "Epoch:  362|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8135|lr = 0.00535\n",
      "Epoch:  362|steps:   60|Train Avg Loss: 0.8934 |Test Loss: 0.8155|lr = 0.00535\n",
      "Epoch:  363|steps:   30|Train Avg Loss: 0.8813 |Test Loss: 0.8175|lr = 0.00535\n",
      "Epoch:  363|steps:   60|Train Avg Loss: 0.9083 |Test Loss: 0.8181|lr = 0.00535\n",
      "Epoch:  364|steps:   30|Train Avg Loss: 0.9012 |Test Loss: 0.8175|lr = 0.00535\n",
      "Epoch:  364|steps:   60|Train Avg Loss: 0.8772 |Test Loss: 0.8163|lr = 0.00535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  365|steps:   30|Train Avg Loss: 0.8842 |Test Loss: 0.8128|lr = 0.00524\n",
      "Epoch:  365|steps:   60|Train Avg Loss: 0.9006 |Test Loss: 0.8180|lr = 0.00524\n",
      "Epoch:  366|steps:   30|Train Avg Loss: 0.8934 |Test Loss: 0.8139|lr = 0.00524\n",
      "Epoch:  366|steps:   60|Train Avg Loss: 0.8915 |Test Loss: 0.8174|lr = 0.00524\n",
      "Epoch:  367|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8152|lr = 0.00524\n",
      "Epoch:  367|steps:   60|Train Avg Loss: 0.8984 |Test Loss: 0.8138|lr = 0.00524\n",
      "Epoch:  368|steps:   30|Train Avg Loss: 0.9001 |Test Loss: 0.8185|lr = 0.00524\n",
      "Epoch:  368|steps:   60|Train Avg Loss: 0.8865 |Test Loss: 0.8138|lr = 0.00524\n",
      "Epoch:  369|steps:   30|Train Avg Loss: 0.9120 |Test Loss: 0.8222|lr = 0.00524\n",
      "Epoch:  369|steps:   60|Train Avg Loss: 0.8949 |Test Loss: 0.8178|lr = 0.00524\n",
      "Epoch:  370|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8133|lr = 0.00524\n",
      "Epoch:  370|steps:   60|Train Avg Loss: 0.8993 |Test Loss: 0.8131|lr = 0.00524\n",
      "Epoch:  371|steps:   30|Train Avg Loss: 0.9084 |Test Loss: 0.8178|lr = 0.00524\n",
      "Epoch:  371|steps:   60|Train Avg Loss: 0.8737 |Test Loss: 0.8109|lr = 0.00524\n",
      "Epoch:  372|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8168|lr = 0.00524\n",
      "Epoch:  372|steps:   60|Train Avg Loss: 0.8799 |Test Loss: 0.8102|lr = 0.00524\n",
      "Epoch:  373|steps:   30|Train Avg Loss: 0.8866 |Test Loss: 0.8045|lr = 0.00524\n",
      "Epoch:  373|steps:   60|Train Avg Loss: 0.8947 |Test Loss: 0.8139|lr = 0.00524\n",
      "Epoch:  374|steps:   30|Train Avg Loss: 0.8890 |Test Loss: 0.8138|lr = 0.00524\n",
      "Epoch:  374|steps:   60|Train Avg Loss: 0.9071 |Test Loss: 0.8276|lr = 0.00524\n",
      "Epoch:  375|steps:   30|Train Avg Loss: 0.8894 |Test Loss: 0.8128|lr = 0.00524\n",
      "Epoch:  375|steps:   60|Train Avg Loss: 0.8975 |Test Loss: 0.8173|lr = 0.00524\n",
      "Epoch:  376|steps:   30|Train Avg Loss: 0.8833 |Test Loss: 0.8124|lr = 0.00513\n",
      "Epoch:  376|steps:   60|Train Avg Loss: 0.8950 |Test Loss: 0.8154|lr = 0.00513\n",
      "Epoch:  377|steps:   30|Train Avg Loss: 0.8755 |Test Loss: 0.8091|lr = 0.00513\n",
      "Epoch:  377|steps:   60|Train Avg Loss: 0.9065 |Test Loss: 0.8178|lr = 0.00513\n",
      "Epoch:  378|steps:   30|Train Avg Loss: 0.9046 |Test Loss: 0.8195|lr = 0.00513\n",
      "Epoch:  378|steps:   60|Train Avg Loss: 0.8696 |Test Loss: 0.8154|lr = 0.00513\n",
      "Epoch:  379|steps:   30|Train Avg Loss: 0.8918 |Test Loss: 0.8204|lr = 0.00513\n",
      "Epoch:  379|steps:   60|Train Avg Loss: 0.8914 |Test Loss: 0.8159|lr = 0.00513\n",
      "Epoch:  380|steps:   30|Train Avg Loss: 0.9041 |Test Loss: 0.8170|lr = 0.00513\n",
      "Epoch:  380|steps:   60|Train Avg Loss: 0.9102 |Test Loss: 0.8217|lr = 0.00513\n",
      "Epoch:  381|steps:   30|Train Avg Loss: 0.9102 |Test Loss: 0.8217|lr = 0.00513\n",
      "Epoch:  381|steps:   60|Train Avg Loss: 0.8881 |Test Loss: 0.8160|lr = 0.00513\n",
      "Epoch:  382|steps:   30|Train Avg Loss: 0.8917 |Test Loss: 0.8112|lr = 0.00513\n",
      "Epoch:  382|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8085|lr = 0.00513\n",
      "Epoch:  383|steps:   30|Train Avg Loss: 0.8820 |Test Loss: 0.8131|lr = 0.00513\n",
      "Epoch:  383|steps:   60|Train Avg Loss: 0.8947 |Test Loss: 0.8136|lr = 0.00513\n",
      "Epoch:  384|steps:   30|Train Avg Loss: 0.8568 |Test Loss: 0.8108|lr = 0.00513\n",
      "Epoch:  384|steps:   60|Train Avg Loss: 0.9165 |Test Loss: 0.8184|lr = 0.00513\n",
      "Epoch:  385|steps:   30|Train Avg Loss: 0.8897 |Test Loss: 0.8130|lr = 0.00513\n",
      "Epoch:  385|steps:   60|Train Avg Loss: 0.8945 |Test Loss: 0.8168|lr = 0.00513\n",
      "Epoch:  386|steps:   30|Train Avg Loss: 0.8876 |Test Loss: 0.8157|lr = 0.00513\n",
      "Epoch:  386|steps:   60|Train Avg Loss: 0.9024 |Test Loss: 0.8158|lr = 0.00513\n",
      "Epoch:  387|steps:   30|Train Avg Loss: 0.9016 |Test Loss: 0.8180|lr = 0.00513\n",
      "Epoch:  387|steps:   60|Train Avg Loss: 0.8743 |Test Loss: 0.8096|lr = 0.00513\n",
      "Epoch:  388|steps:   30|Train Avg Loss: 0.8968 |Test Loss: 0.8170|lr = 0.00513\n",
      "Epoch:  388|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8137|lr = 0.00513\n",
      "Epoch:  389|steps:   30|Train Avg Loss: 0.8905 |Test Loss: 0.8165|lr = 0.00513\n",
      "Epoch:  389|steps:   60|Train Avg Loss: 0.8987 |Test Loss: 0.8170|lr = 0.00513\n",
      "Epoch:  390|steps:   30|Train Avg Loss: 0.8868 |Test Loss: 0.8133|lr = 0.00513\n",
      "Epoch:  390|steps:   60|Train Avg Loss: 0.8985 |Test Loss: 0.8116|lr = 0.00513\n",
      "Epoch:  391|steps:   30|Train Avg Loss: 0.8977 |Test Loss: 0.8138|lr = 0.00513\n",
      "Epoch:  391|steps:   60|Train Avg Loss: 0.8925 |Test Loss: 0.8143|lr = 0.00513\n",
      "Epoch:  392|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8134|lr = 0.00503\n",
      "Epoch:  392|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8191|lr = 0.00503\n",
      "Epoch:  393|steps:   30|Train Avg Loss: 0.8962 |Test Loss: 0.8175|lr = 0.00503\n",
      "Epoch:  393|steps:   60|Train Avg Loss: 0.8968 |Test Loss: 0.8168|lr = 0.00503\n",
      "Epoch:  394|steps:   30|Train Avg Loss: 0.8724 |Test Loss: 0.8081|lr = 0.00503\n",
      "Epoch:  394|steps:   60|Train Avg Loss: 0.9077 |Test Loss: 0.8207|lr = 0.00503\n",
      "Epoch:  395|steps:   30|Train Avg Loss: 0.9065 |Test Loss: 0.8185|lr = 0.00503\n",
      "Epoch:  395|steps:   60|Train Avg Loss: 0.8714 |Test Loss: 0.8148|lr = 0.00503\n",
      "Epoch:  396|steps:   30|Train Avg Loss: 0.8851 |Test Loss: 0.8131|lr = 0.00503\n",
      "Epoch:  396|steps:   60|Train Avg Loss: 0.8888 |Test Loss: 0.8155|lr = 0.00503\n",
      "Epoch:  397|steps:   30|Train Avg Loss: 0.8729 |Test Loss: 0.8121|lr = 0.00503\n",
      "Epoch:  397|steps:   60|Train Avg Loss: 0.9141 |Test Loss: 0.8165|lr = 0.00503\n",
      "Epoch:  398|steps:   30|Train Avg Loss: 0.9132 |Test Loss: 0.8210|lr = 0.00503\n",
      "Epoch:  398|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8173|lr = 0.00503\n",
      "Epoch:  399|steps:   30|Train Avg Loss: 0.8975 |Test Loss: 0.8180|lr = 0.00503\n",
      "Epoch:  399|steps:   60|Train Avg Loss: 0.8996 |Test Loss: 0.8218|lr = 0.00503\n",
      "Epoch:  400|steps:   30|Train Avg Loss: 0.9139 |Test Loss: 0.8264|lr = 0.00503\n",
      "Epoch:  400|steps:   60|Train Avg Loss: 0.8753 |Test Loss: 0.8175|lr = 0.00503\n",
      "Epoch:  401|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8113|lr = 0.00503\n",
      "Epoch:  401|steps:   60|Train Avg Loss: 0.9059 |Test Loss: 0.8164|lr = 0.00503\n",
      "Epoch:  402|steps:   30|Train Avg Loss: 0.9121 |Test Loss: 0.8196|lr = 0.00503\n",
      "Epoch:  402|steps:   60|Train Avg Loss: 0.8850 |Test Loss: 0.8195|lr = 0.00503\n",
      "Epoch:  403|steps:   30|Train Avg Loss: 0.8991 |Test Loss: 0.8213|lr = 0.00493\n",
      "Epoch:  403|steps:   60|Train Avg Loss: 0.8934 |Test Loss: 0.8171|lr = 0.00493\n",
      "Epoch:  404|steps:   30|Train Avg Loss: 0.8880 |Test Loss: 0.8128|lr = 0.00493\n",
      "Epoch:  404|steps:   60|Train Avg Loss: 0.9069 |Test Loss: 0.8163|lr = 0.00493\n",
      "Epoch:  405|steps:   30|Train Avg Loss: 0.8976 |Test Loss: 0.8200|lr = 0.00493\n",
      "Epoch:  405|steps:   60|Train Avg Loss: 0.8985 |Test Loss: 0.8152|lr = 0.00493\n",
      "Epoch:  406|steps:   30|Train Avg Loss: 0.8933 |Test Loss: 0.8133|lr = 0.00493\n",
      "Epoch:  406|steps:   60|Train Avg Loss: 0.8926 |Test Loss: 0.8188|lr = 0.00493\n",
      "Epoch:  407|steps:   30|Train Avg Loss: 0.8877 |Test Loss: 0.8092|lr = 0.00493\n",
      "Epoch:  407|steps:   60|Train Avg Loss: 0.9068 |Test Loss: 0.8199|lr = 0.00493\n",
      "Epoch:  408|steps:   30|Train Avg Loss: 0.8874 |Test Loss: 0.8132|lr = 0.00493\n",
      "Epoch:  408|steps:   60|Train Avg Loss: 0.9012 |Test Loss: 0.8179|lr = 0.00493\n",
      "Epoch:  409|steps:   30|Train Avg Loss: 0.9152 |Test Loss: 0.8207|lr = 0.00493\n",
      "Epoch:  409|steps:   60|Train Avg Loss: 0.8749 |Test Loss: 0.8119|lr = 0.00493\n",
      "Epoch:  410|steps:   30|Train Avg Loss: 0.9032 |Test Loss: 0.8220|lr = 0.00493\n",
      "Epoch:  410|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8119|lr = 0.00493\n",
      "Epoch:  411|steps:   30|Train Avg Loss: 0.9007 |Test Loss: 0.8230|lr = 0.00493\n",
      "Epoch:  411|steps:   60|Train Avg Loss: 0.8995 |Test Loss: 0.8163|lr = 0.00493\n",
      "Epoch:  412|steps:   30|Train Avg Loss: 0.8905 |Test Loss: 0.8137|lr = 0.00493\n",
      "Epoch:  412|steps:   60|Train Avg Loss: 0.8980 |Test Loss: 0.8173|lr = 0.00493\n",
      "Epoch:  413|steps:   30|Train Avg Loss: 0.8920 |Test Loss: 0.8150|lr = 0.00493\n",
      "Epoch:  413|steps:   60|Train Avg Loss: 0.8890 |Test Loss: 0.8187|lr = 0.00493\n",
      "Epoch:  414|steps:   30|Train Avg Loss: 0.9130 |Test Loss: 0.8217|lr = 0.00493\n",
      "Epoch:  414|steps:   60|Train Avg Loss: 0.8789 |Test Loss: 0.8169|lr = 0.00493\n",
      "Epoch:  415|steps:   30|Train Avg Loss: 0.8975 |Test Loss: 0.8154|lr = 0.00493\n",
      "Epoch:  415|steps:   60|Train Avg Loss: 0.8848 |Test Loss: 0.8138|lr = 0.00493\n",
      "Epoch:  416|steps:   30|Train Avg Loss: 0.8855 |Test Loss: 0.8156|lr = 0.00493\n",
      "Epoch:  416|steps:   60|Train Avg Loss: 0.8839 |Test Loss: 0.8147|lr = 0.00493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  417|steps:   30|Train Avg Loss: 0.8894 |Test Loss: 0.8166|lr = 0.00493\n",
      "Epoch:  417|steps:   60|Train Avg Loss: 0.8984 |Test Loss: 0.8196|lr = 0.00493\n",
      "Epoch:  418|steps:   30|Train Avg Loss: 0.9039 |Test Loss: 0.8181|lr = 0.00493\n",
      "Epoch:  418|steps:   60|Train Avg Loss: 0.8761 |Test Loss: 0.8183|lr = 0.00493\n",
      "Epoch:  419|steps:   30|Train Avg Loss: 0.8757 |Test Loss: 0.8100|lr = 0.00493\n",
      "Epoch:  419|steps:   60|Train Avg Loss: 0.9149 |Test Loss: 0.8213|lr = 0.00493\n",
      "Epoch:  420|steps:   30|Train Avg Loss: 0.8979 |Test Loss: 0.8194|lr = 0.00493\n",
      "Epoch:  420|steps:   60|Train Avg Loss: 0.8741 |Test Loss: 0.8103|lr = 0.00493\n",
      "Epoch:  421|steps:   30|Train Avg Loss: 0.9043 |Test Loss: 0.8183|lr = 0.00493\n",
      "Epoch:  421|steps:   60|Train Avg Loss: 0.8961 |Test Loss: 0.8206|lr = 0.00493\n",
      "Epoch:  422|steps:   30|Train Avg Loss: 0.9011 |Test Loss: 0.8161|lr = 0.00493\n",
      "Epoch:  422|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8204|lr = 0.00493\n",
      "Epoch:  423|steps:   30|Train Avg Loss: 0.9009 |Test Loss: 0.8175|lr = 0.00483\n",
      "Epoch:  423|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8182|lr = 0.00483\n",
      "Epoch:  424|steps:   30|Train Avg Loss: 0.8791 |Test Loss: 0.8159|lr = 0.00483\n",
      "Epoch:  424|steps:   60|Train Avg Loss: 0.9043 |Test Loss: 0.8216|lr = 0.00483\n",
      "Epoch:  425|steps:   30|Train Avg Loss: 0.9028 |Test Loss: 0.8127|lr = 0.00483\n",
      "Epoch:  425|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8144|lr = 0.00483\n",
      "Epoch:  426|steps:   30|Train Avg Loss: 0.9059 |Test Loss: 0.8169|lr = 0.00483\n",
      "Epoch:  426|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8140|lr = 0.00483\n",
      "Epoch:  427|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8163|lr = 0.00483\n",
      "Epoch:  427|steps:   60|Train Avg Loss: 0.8903 |Test Loss: 0.8164|lr = 0.00483\n",
      "Epoch:  428|steps:   30|Train Avg Loss: 0.8791 |Test Loss: 0.8148|lr = 0.00483\n",
      "Epoch:  428|steps:   60|Train Avg Loss: 0.9247 |Test Loss: 0.8232|lr = 0.00483\n",
      "Epoch:  429|steps:   30|Train Avg Loss: 0.8973 |Test Loss: 0.8143|lr = 0.00483\n",
      "Epoch:  429|steps:   60|Train Avg Loss: 0.8819 |Test Loss: 0.8172|lr = 0.00483\n",
      "Epoch:  430|steps:   30|Train Avg Loss: 0.8968 |Test Loss: 0.8173|lr = 0.00483\n",
      "Epoch:  430|steps:   60|Train Avg Loss: 0.8931 |Test Loss: 0.8155|lr = 0.00483\n",
      "Epoch:  431|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8165|lr = 0.00483\n",
      "Epoch:  431|steps:   60|Train Avg Loss: 0.8938 |Test Loss: 0.8188|lr = 0.00483\n",
      "Epoch:  432|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8156|lr = 0.00483\n",
      "Epoch:  432|steps:   60|Train Avg Loss: 0.8950 |Test Loss: 0.8131|lr = 0.00483\n",
      "Epoch:  433|steps:   30|Train Avg Loss: 0.9009 |Test Loss: 0.8216|lr = 0.00483\n",
      "Epoch:  433|steps:   60|Train Avg Loss: 0.8798 |Test Loss: 0.8114|lr = 0.00483\n",
      "Epoch:  434|steps:   30|Train Avg Loss: 0.8868 |Test Loss: 0.8132|lr = 0.00474\n",
      "Epoch:  434|steps:   60|Train Avg Loss: 0.8928 |Test Loss: 0.8151|lr = 0.00474\n",
      "Epoch:  435|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8147|lr = 0.00474\n",
      "Epoch:  435|steps:   60|Train Avg Loss: 0.8960 |Test Loss: 0.8183|lr = 0.00474\n",
      "Epoch:  436|steps:   30|Train Avg Loss: 0.9044 |Test Loss: 0.8199|lr = 0.00474\n",
      "Epoch:  436|steps:   60|Train Avg Loss: 0.8924 |Test Loss: 0.8149|lr = 0.00474\n",
      "Epoch:  437|steps:   30|Train Avg Loss: 0.8790 |Test Loss: 0.8142|lr = 0.00474\n",
      "Epoch:  437|steps:   60|Train Avg Loss: 0.8929 |Test Loss: 0.8186|lr = 0.00474\n",
      "Epoch:  438|steps:   30|Train Avg Loss: 0.8616 |Test Loss: 0.8104|lr = 0.00474\n",
      "Epoch:  438|steps:   60|Train Avg Loss: 0.9283 |Test Loss: 0.8235|lr = 0.00474\n",
      "Epoch:  439|steps:   30|Train Avg Loss: 0.8843 |Test Loss: 0.8143|lr = 0.00474\n",
      "Epoch:  439|steps:   60|Train Avg Loss: 0.8955 |Test Loss: 0.8220|lr = 0.00474\n",
      "Epoch:  440|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8155|lr = 0.00474\n",
      "Epoch:  440|steps:   60|Train Avg Loss: 0.9250 |Test Loss: 0.8190|lr = 0.00474\n",
      "Epoch:  441|steps:   30|Train Avg Loss: 0.8909 |Test Loss: 0.8133|lr = 0.00474\n",
      "Epoch:  441|steps:   60|Train Avg Loss: 0.8880 |Test Loss: 0.8225|lr = 0.00474\n",
      "Epoch:  442|steps:   30|Train Avg Loss: 0.8994 |Test Loss: 0.8190|lr = 0.00474\n",
      "Epoch:  442|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8106|lr = 0.00474\n",
      "Epoch:  443|steps:   30|Train Avg Loss: 0.8774 |Test Loss: 0.8145|lr = 0.00474\n",
      "Epoch:  443|steps:   60|Train Avg Loss: 0.9054 |Test Loss: 0.8298|lr = 0.00474\n",
      "Epoch:  444|steps:   30|Train Avg Loss: 0.8858 |Test Loss: 0.8242|lr = 0.00474\n",
      "Epoch:  444|steps:   60|Train Avg Loss: 0.8963 |Test Loss: 0.8157|lr = 0.00474\n",
      "Epoch:  445|steps:   30|Train Avg Loss: 0.8817 |Test Loss: 0.8156|lr = 0.00464\n",
      "Epoch:  445|steps:   60|Train Avg Loss: 0.9037 |Test Loss: 0.8176|lr = 0.00464\n",
      "Epoch:  446|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8175|lr = 0.00464\n",
      "Epoch:  446|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8161|lr = 0.00464\n",
      "Epoch:  447|steps:   30|Train Avg Loss: 0.8980 |Test Loss: 0.8173|lr = 0.00464\n",
      "Epoch:  447|steps:   60|Train Avg Loss: 0.8788 |Test Loss: 0.8109|lr = 0.00464\n",
      "Epoch:  448|steps:   30|Train Avg Loss: 0.8858 |Test Loss: 0.8146|lr = 0.00464\n",
      "Epoch:  448|steps:   60|Train Avg Loss: 0.9065 |Test Loss: 0.8173|lr = 0.00464\n",
      "Epoch:  449|steps:   30|Train Avg Loss: 0.8738 |Test Loss: 0.8154|lr = 0.00464\n",
      "Epoch:  449|steps:   60|Train Avg Loss: 0.8996 |Test Loss: 0.8158|lr = 0.00464\n",
      "Epoch:  450|steps:   30|Train Avg Loss: 0.8944 |Test Loss: 0.8183|lr = 0.00464\n",
      "Epoch:  450|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8188|lr = 0.00464\n",
      "Epoch:  451|steps:   30|Train Avg Loss: 0.9075 |Test Loss: 0.8210|lr = 0.00464\n",
      "Epoch:  451|steps:   60|Train Avg Loss: 0.8816 |Test Loss: 0.8145|lr = 0.00464\n",
      "Epoch:  452|steps:   30|Train Avg Loss: 0.8848 |Test Loss: 0.8149|lr = 0.00464\n",
      "Epoch:  452|steps:   60|Train Avg Loss: 0.9093 |Test Loss: 0.8194|lr = 0.00464\n",
      "Epoch:  453|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8156|lr = 0.00464\n",
      "Epoch:  453|steps:   60|Train Avg Loss: 0.8983 |Test Loss: 0.8188|lr = 0.00464\n",
      "Epoch:  454|steps:   30|Train Avg Loss: 0.9139 |Test Loss: 0.8245|lr = 0.00464\n",
      "Epoch:  454|steps:   60|Train Avg Loss: 0.8706 |Test Loss: 0.8148|lr = 0.00464\n",
      "Epoch:  455|steps:   30|Train Avg Loss: 0.8913 |Test Loss: 0.8146|lr = 0.00464\n",
      "Epoch:  455|steps:   60|Train Avg Loss: 0.9024 |Test Loss: 0.8215|lr = 0.00464\n",
      "Epoch:  456|steps:   30|Train Avg Loss: 0.9053 |Test Loss: 0.8191|lr = 0.00455\n",
      "Epoch:  456|steps:   60|Train Avg Loss: 0.8786 |Test Loss: 0.8143|lr = 0.00455\n",
      "Epoch:  457|steps:   30|Train Avg Loss: 0.9009 |Test Loss: 0.8175|lr = 0.00455\n",
      "Epoch:  457|steps:   60|Train Avg Loss: 0.8777 |Test Loss: 0.8130|lr = 0.00455\n",
      "Epoch:  458|steps:   30|Train Avg Loss: 0.9074 |Test Loss: 0.8214|lr = 0.00455\n",
      "Epoch:  458|steps:   60|Train Avg Loss: 0.8834 |Test Loss: 0.8206|lr = 0.00455\n",
      "Epoch:  459|steps:   30|Train Avg Loss: 0.9045 |Test Loss: 0.8178|lr = 0.00455\n",
      "Epoch:  459|steps:   60|Train Avg Loss: 0.8702 |Test Loss: 0.8103|lr = 0.00455\n",
      "Epoch:  460|steps:   30|Train Avg Loss: 0.8807 |Test Loss: 0.8166|lr = 0.00455\n",
      "Epoch:  460|steps:   60|Train Avg Loss: 0.9033 |Test Loss: 0.8194|lr = 0.00455\n",
      "Epoch:  461|steps:   30|Train Avg Loss: 0.8975 |Test Loss: 0.8172|lr = 0.00455\n",
      "Epoch:  461|steps:   60|Train Avg Loss: 0.8921 |Test Loss: 0.8169|lr = 0.00455\n",
      "Epoch:  462|steps:   30|Train Avg Loss: 0.8929 |Test Loss: 0.8189|lr = 0.00455\n",
      "Epoch:  462|steps:   60|Train Avg Loss: 0.8983 |Test Loss: 0.8179|lr = 0.00455\n",
      "Epoch:  463|steps:   30|Train Avg Loss: 0.8823 |Test Loss: 0.8140|lr = 0.00455\n",
      "Epoch:  463|steps:   60|Train Avg Loss: 0.9107 |Test Loss: 0.8218|lr = 0.00455\n",
      "Epoch:  464|steps:   30|Train Avg Loss: 0.9027 |Test Loss: 0.8193|lr = 0.00455\n",
      "Epoch:  464|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8165|lr = 0.00455\n",
      "Epoch:  465|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8162|lr = 0.00455\n",
      "Epoch:  465|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8188|lr = 0.00455\n",
      "Epoch:  466|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8174|lr = 0.00455\n",
      "Epoch:  466|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8176|lr = 0.00455\n",
      "Epoch:  467|steps:   30|Train Avg Loss: 0.8868 |Test Loss: 0.8140|lr = 0.00446\n",
      "Epoch:  467|steps:   60|Train Avg Loss: 0.8993 |Test Loss: 0.8194|lr = 0.00446\n",
      "Epoch:  468|steps:   30|Train Avg Loss: 0.8791 |Test Loss: 0.8164|lr = 0.00446\n",
      "Epoch:  468|steps:   60|Train Avg Loss: 0.8937 |Test Loss: 0.8142|lr = 0.00446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  469|steps:   30|Train Avg Loss: 0.8994 |Test Loss: 0.8208|lr = 0.00446\n",
      "Epoch:  469|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8166|lr = 0.00446\n",
      "Epoch:  470|steps:   30|Train Avg Loss: 0.8969 |Test Loss: 0.8149|lr = 0.00446\n",
      "Epoch:  470|steps:   60|Train Avg Loss: 0.8852 |Test Loss: 0.8168|lr = 0.00446\n",
      "Epoch:  471|steps:   30|Train Avg Loss: 0.9096 |Test Loss: 0.8195|lr = 0.00446\n",
      "Epoch:  471|steps:   60|Train Avg Loss: 0.8703 |Test Loss: 0.8141|lr = 0.00446\n",
      "Epoch:  472|steps:   30|Train Avg Loss: 0.8790 |Test Loss: 0.8213|lr = 0.00446\n",
      "Epoch:  472|steps:   60|Train Avg Loss: 0.9127 |Test Loss: 0.8209|lr = 0.00446\n",
      "Epoch:  473|steps:   30|Train Avg Loss: 0.8897 |Test Loss: 0.8172|lr = 0.00446\n",
      "Epoch:  473|steps:   60|Train Avg Loss: 0.8923 |Test Loss: 0.8197|lr = 0.00446\n",
      "Epoch:  474|steps:   30|Train Avg Loss: 0.8965 |Test Loss: 0.8177|lr = 0.00446\n",
      "Epoch:  474|steps:   60|Train Avg Loss: 0.8835 |Test Loss: 0.8136|lr = 0.00446\n",
      "Epoch:  475|steps:   30|Train Avg Loss: 0.8829 |Test Loss: 0.8171|lr = 0.00446\n",
      "Epoch:  475|steps:   60|Train Avg Loss: 0.8876 |Test Loss: 0.8155|lr = 0.00446\n",
      "Epoch:  476|steps:   30|Train Avg Loss: 0.9066 |Test Loss: 0.8176|lr = 0.00446\n",
      "Epoch:  476|steps:   60|Train Avg Loss: 0.8845 |Test Loss: 0.8199|lr = 0.00446\n",
      "Epoch:  477|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8133|lr = 0.00446\n",
      "Epoch:  477|steps:   60|Train Avg Loss: 0.9074 |Test Loss: 0.8191|lr = 0.00446\n",
      "Epoch:  478|steps:   30|Train Avg Loss: 0.8846 |Test Loss: 0.8126|lr = 0.00437\n",
      "Epoch:  478|steps:   60|Train Avg Loss: 0.9105 |Test Loss: 0.8137|lr = 0.00437\n",
      "Epoch:  479|steps:   30|Train Avg Loss: 0.8923 |Test Loss: 0.8169|lr = 0.00437\n",
      "Epoch:  479|steps:   60|Train Avg Loss: 0.8975 |Test Loss: 0.8157|lr = 0.00437\n",
      "Epoch:  480|steps:   30|Train Avg Loss: 0.8949 |Test Loss: 0.8153|lr = 0.00437\n",
      "Epoch:  480|steps:   60|Train Avg Loss: 0.8917 |Test Loss: 0.8181|lr = 0.00437\n",
      "Epoch:  481|steps:   30|Train Avg Loss: 0.9042 |Test Loss: 0.8211|lr = 0.00437\n",
      "Epoch:  481|steps:   60|Train Avg Loss: 0.8790 |Test Loss: 0.8144|lr = 0.00437\n",
      "Epoch:  482|steps:   30|Train Avg Loss: 0.9256 |Test Loss: 0.8272|lr = 0.00437\n",
      "Epoch:  482|steps:   60|Train Avg Loss: 0.8679 |Test Loss: 0.8395|lr = 0.00437\n",
      "Epoch:  483|steps:   30|Train Avg Loss: 0.8816 |Test Loss: 0.8115|lr = 0.00437\n",
      "Epoch:  483|steps:   60|Train Avg Loss: 0.9098 |Test Loss: 0.8263|lr = 0.00437\n",
      "Epoch:  484|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8172|lr = 0.00437\n",
      "Epoch:  484|steps:   60|Train Avg Loss: 0.9128 |Test Loss: 0.8176|lr = 0.00437\n",
      "Epoch:  485|steps:   30|Train Avg Loss: 0.8963 |Test Loss: 0.8223|lr = 0.00437\n",
      "Epoch:  485|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8172|lr = 0.00437\n",
      "Epoch:  486|steps:   30|Train Avg Loss: 0.8919 |Test Loss: 0.8172|lr = 0.00437\n",
      "Epoch:  486|steps:   60|Train Avg Loss: 0.8976 |Test Loss: 0.8191|lr = 0.00437\n",
      "Epoch:  487|steps:   30|Train Avg Loss: 0.9107 |Test Loss: 0.8213|lr = 0.00437\n",
      "Epoch:  487|steps:   60|Train Avg Loss: 0.8860 |Test Loss: 0.8154|lr = 0.00437\n",
      "Epoch:  488|steps:   30|Train Avg Loss: 0.9135 |Test Loss: 0.8186|lr = 0.00437\n",
      "Epoch:  488|steps:   60|Train Avg Loss: 0.8723 |Test Loss: 0.8128|lr = 0.00437\n",
      "Epoch:  489|steps:   30|Train Avg Loss: 0.9124 |Test Loss: 0.8257|lr = 0.00428\n",
      "Epoch:  489|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8109|lr = 0.00428\n",
      "Epoch:  490|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8130|lr = 0.00428\n",
      "Epoch:  490|steps:   60|Train Avg Loss: 0.8885 |Test Loss: 0.8157|lr = 0.00428\n",
      "Epoch:  491|steps:   30|Train Avg Loss: 0.8988 |Test Loss: 0.8158|lr = 0.00428\n",
      "Epoch:  491|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8162|lr = 0.00428\n",
      "Epoch:  492|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8125|lr = 0.00428\n",
      "Epoch:  492|steps:   60|Train Avg Loss: 0.9096 |Test Loss: 0.8179|lr = 0.00428\n",
      "Epoch:  493|steps:   30|Train Avg Loss: 0.8993 |Test Loss: 0.8188|lr = 0.00428\n",
      "Epoch:  493|steps:   60|Train Avg Loss: 0.8910 |Test Loss: 0.8113|lr = 0.00428\n",
      "Epoch:  494|steps:   30|Train Avg Loss: 0.8918 |Test Loss: 0.8144|lr = 0.00428\n",
      "Epoch:  494|steps:   60|Train Avg Loss: 0.9007 |Test Loss: 0.8203|lr = 0.00428\n",
      "Epoch:  495|steps:   30|Train Avg Loss: 0.8835 |Test Loss: 0.8116|lr = 0.00428\n",
      "Epoch:  495|steps:   60|Train Avg Loss: 0.9060 |Test Loss: 0.8155|lr = 0.00428\n",
      "Epoch:  496|steps:   30|Train Avg Loss: 0.8767 |Test Loss: 0.8132|lr = 0.00428\n",
      "Epoch:  496|steps:   60|Train Avg Loss: 0.9118 |Test Loss: 0.8256|lr = 0.00428\n",
      "Epoch:  497|steps:   30|Train Avg Loss: 0.8907 |Test Loss: 0.8203|lr = 0.00428\n",
      "Epoch:  497|steps:   60|Train Avg Loss: 0.8884 |Test Loss: 0.8135|lr = 0.00428\n",
      "Epoch:  498|steps:   30|Train Avg Loss: 0.9120 |Test Loss: 0.8221|lr = 0.00428\n",
      "Epoch:  498|steps:   60|Train Avg Loss: 0.8813 |Test Loss: 0.8130|lr = 0.00428\n",
      "Epoch:  499|steps:   30|Train Avg Loss: 0.8645 |Test Loss: 0.8126|lr = 0.00428\n",
      "Epoch:  499|steps:   60|Train Avg Loss: 0.9282 |Test Loss: 0.8225|lr = 0.00428\n",
      "Epoch:  500|steps:   30|Train Avg Loss: 0.9042 |Test Loss: 0.8218|lr = 0.00419\n",
      "Epoch:  500|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8135|lr = 0.00419\n",
      "Epoch:  501|steps:   30|Train Avg Loss: 0.9135 |Test Loss: 0.8209|lr = 0.00419\n",
      "Epoch:  501|steps:   60|Train Avg Loss: 0.8796 |Test Loss: 0.8156|lr = 0.00419\n",
      "Epoch:  502|steps:   30|Train Avg Loss: 0.8745 |Test Loss: 0.8125|lr = 0.00419\n",
      "Epoch:  502|steps:   60|Train Avg Loss: 0.8906 |Test Loss: 0.8140|lr = 0.00419\n",
      "Epoch:  503|steps:   30|Train Avg Loss: 0.9000 |Test Loss: 0.8217|lr = 0.00419\n",
      "Epoch:  503|steps:   60|Train Avg Loss: 0.8853 |Test Loss: 0.8134|lr = 0.00419\n",
      "Epoch:  504|steps:   30|Train Avg Loss: 0.8890 |Test Loss: 0.8157|lr = 0.00419\n",
      "Epoch:  504|steps:   60|Train Avg Loss: 0.8896 |Test Loss: 0.8164|lr = 0.00419\n",
      "Epoch:  505|steps:   30|Train Avg Loss: 0.8937 |Test Loss: 0.8181|lr = 0.00419\n",
      "Epoch:  505|steps:   60|Train Avg Loss: 0.8935 |Test Loss: 0.8157|lr = 0.00419\n",
      "Epoch:  506|steps:   30|Train Avg Loss: 0.8877 |Test Loss: 0.8148|lr = 0.00419\n",
      "Epoch:  506|steps:   60|Train Avg Loss: 0.8917 |Test Loss: 0.8135|lr = 0.00419\n",
      "Epoch:  507|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8183|lr = 0.00419\n",
      "Epoch:  507|steps:   60|Train Avg Loss: 0.8805 |Test Loss: 0.8083|lr = 0.00419\n",
      "Epoch:  508|steps:   30|Train Avg Loss: 0.8986 |Test Loss: 0.8213|lr = 0.00419\n",
      "Epoch:  508|steps:   60|Train Avg Loss: 0.8872 |Test Loss: 0.8168|lr = 0.00419\n",
      "Epoch:  509|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8168|lr = 0.00419\n",
      "Epoch:  509|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8161|lr = 0.00419\n",
      "Epoch:  510|steps:   30|Train Avg Loss: 0.8959 |Test Loss: 0.8167|lr = 0.00419\n",
      "Epoch:  510|steps:   60|Train Avg Loss: 0.9017 |Test Loss: 0.8174|lr = 0.00419\n",
      "Epoch:  511|steps:   30|Train Avg Loss: 0.8937 |Test Loss: 0.8186|lr = 0.00411\n",
      "Epoch:  511|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8143|lr = 0.00411\n",
      "Epoch:  512|steps:   30|Train Avg Loss: 0.9017 |Test Loss: 0.8235|lr = 0.00411\n",
      "Epoch:  512|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8158|lr = 0.00411\n",
      "Epoch:  513|steps:   30|Train Avg Loss: 0.8843 |Test Loss: 0.8178|lr = 0.00411\n",
      "Epoch:  513|steps:   60|Train Avg Loss: 0.8987 |Test Loss: 0.8179|lr = 0.00411\n",
      "Epoch:  514|steps:   30|Train Avg Loss: 0.8909 |Test Loss: 0.8190|lr = 0.00411\n",
      "Epoch:  514|steps:   60|Train Avg Loss: 0.9027 |Test Loss: 0.8123|lr = 0.00411\n",
      "Epoch:  515|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8132|lr = 0.00411\n",
      "Epoch:  515|steps:   60|Train Avg Loss: 0.9004 |Test Loss: 0.8146|lr = 0.00411\n",
      "Epoch:  516|steps:   30|Train Avg Loss: 0.8759 |Test Loss: 0.8139|lr = 0.00411\n",
      "Epoch:  516|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8142|lr = 0.00411\n",
      "Epoch:  517|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8182|lr = 0.00411\n",
      "Epoch:  517|steps:   60|Train Avg Loss: 0.8870 |Test Loss: 0.8151|lr = 0.00411\n",
      "Epoch:  518|steps:   30|Train Avg Loss: 0.8831 |Test Loss: 0.8155|lr = 0.00411\n",
      "Epoch:  518|steps:   60|Train Avg Loss: 0.9037 |Test Loss: 0.8186|lr = 0.00411\n",
      "Epoch:  519|steps:   30|Train Avg Loss: 0.8953 |Test Loss: 0.8163|lr = 0.00411\n",
      "Epoch:  519|steps:   60|Train Avg Loss: 0.8872 |Test Loss: 0.8156|lr = 0.00411\n",
      "Epoch:  520|steps:   30|Train Avg Loss: 0.8835 |Test Loss: 0.8158|lr = 0.00411\n",
      "Epoch:  520|steps:   60|Train Avg Loss: 0.9038 |Test Loss: 0.8199|lr = 0.00411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  521|steps:   30|Train Avg Loss: 0.9039 |Test Loss: 0.8217|lr = 0.00411\n",
      "Epoch:  521|steps:   60|Train Avg Loss: 0.8831 |Test Loss: 0.8148|lr = 0.00411\n",
      "Epoch:  522|steps:   30|Train Avg Loss: 0.8913 |Test Loss: 0.8145|lr = 0.00403\n",
      "Epoch:  522|steps:   60|Train Avg Loss: 0.8968 |Test Loss: 0.8196|lr = 0.00403\n",
      "Epoch:  523|steps:   30|Train Avg Loss: 0.8864 |Test Loss: 0.8152|lr = 0.00403\n",
      "Epoch:  523|steps:   60|Train Avg Loss: 0.9004 |Test Loss: 0.8236|lr = 0.00403\n",
      "Epoch:  524|steps:   30|Train Avg Loss: 0.8906 |Test Loss: 0.8144|lr = 0.00403\n",
      "Epoch:  524|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8195|lr = 0.00403\n",
      "Epoch:  525|steps:   30|Train Avg Loss: 0.8970 |Test Loss: 0.8185|lr = 0.00403\n",
      "Epoch:  525|steps:   60|Train Avg Loss: 0.8903 |Test Loss: 0.8162|lr = 0.00403\n",
      "Epoch:  526|steps:   30|Train Avg Loss: 0.8855 |Test Loss: 0.8173|lr = 0.00403\n",
      "Epoch:  526|steps:   60|Train Avg Loss: 0.8948 |Test Loss: 0.8183|lr = 0.00403\n",
      "Epoch:  527|steps:   30|Train Avg Loss: 0.8965 |Test Loss: 0.8172|lr = 0.00403\n",
      "Epoch:  527|steps:   60|Train Avg Loss: 0.8994 |Test Loss: 0.8231|lr = 0.00403\n",
      "Epoch:  528|steps:   30|Train Avg Loss: 0.8935 |Test Loss: 0.8184|lr = 0.00403\n",
      "Epoch:  528|steps:   60|Train Avg Loss: 0.8979 |Test Loss: 0.8216|lr = 0.00403\n",
      "Epoch:  529|steps:   30|Train Avg Loss: 0.9118 |Test Loss: 0.8253|lr = 0.00403\n",
      "Epoch:  529|steps:   60|Train Avg Loss: 0.8801 |Test Loss: 0.8235|lr = 0.00403\n",
      "Epoch:  530|steps:   30|Train Avg Loss: 0.8669 |Test Loss: 0.8117|lr = 0.00403\n",
      "Epoch:  530|steps:   60|Train Avg Loss: 0.9174 |Test Loss: 0.8232|lr = 0.00403\n",
      "Epoch:  531|steps:   30|Train Avg Loss: 0.9232 |Test Loss: 0.8294|lr = 0.00403\n",
      "Epoch:  531|steps:   60|Train Avg Loss: 0.8603 |Test Loss: 0.8171|lr = 0.00403\n",
      "Epoch:  532|steps:   30|Train Avg Loss: 0.8731 |Test Loss: 0.8151|lr = 0.00403\n",
      "Epoch:  532|steps:   60|Train Avg Loss: 0.8989 |Test Loss: 0.8187|lr = 0.00403\n",
      "Epoch:  533|steps:   30|Train Avg Loss: 0.9124 |Test Loss: 0.8200|lr = 0.00395\n",
      "Epoch:  533|steps:   60|Train Avg Loss: 0.8861 |Test Loss: 0.8125|lr = 0.00395\n",
      "Epoch:  534|steps:   30|Train Avg Loss: 0.9019 |Test Loss: 0.8149|lr = 0.00395\n",
      "Epoch:  534|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 0.8199|lr = 0.00395\n",
      "Epoch:  535|steps:   30|Train Avg Loss: 0.8857 |Test Loss: 0.8157|lr = 0.00395\n",
      "Epoch:  535|steps:   60|Train Avg Loss: 0.8997 |Test Loss: 0.8182|lr = 0.00395\n",
      "Epoch:  536|steps:   30|Train Avg Loss: 0.8902 |Test Loss: 0.8148|lr = 0.00395\n",
      "Epoch:  536|steps:   60|Train Avg Loss: 0.9029 |Test Loss: 0.8243|lr = 0.00395\n",
      "Epoch:  537|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8164|lr = 0.00395\n",
      "Epoch:  537|steps:   60|Train Avg Loss: 0.8898 |Test Loss: 0.8158|lr = 0.00395\n",
      "Epoch:  538|steps:   30|Train Avg Loss: 0.8953 |Test Loss: 0.8202|lr = 0.00395\n",
      "Epoch:  538|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8140|lr = 0.00395\n",
      "Epoch:  539|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8147|lr = 0.00395\n",
      "Epoch:  539|steps:   60|Train Avg Loss: 0.8972 |Test Loss: 0.8181|lr = 0.00395\n",
      "Epoch:  540|steps:   30|Train Avg Loss: 0.8927 |Test Loss: 0.8169|lr = 0.00395\n",
      "Epoch:  540|steps:   60|Train Avg Loss: 0.8809 |Test Loss: 0.8153|lr = 0.00395\n",
      "Epoch:  541|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8078|lr = 0.00395\n",
      "Epoch:  541|steps:   60|Train Avg Loss: 0.8992 |Test Loss: 0.8163|lr = 0.00395\n",
      "Epoch:  542|steps:   30|Train Avg Loss: 0.9057 |Test Loss: 0.8209|lr = 0.00395\n",
      "Epoch:  542|steps:   60|Train Avg Loss: 0.8846 |Test Loss: 0.8179|lr = 0.00395\n",
      "Epoch:  543|steps:   30|Train Avg Loss: 0.8973 |Test Loss: 0.8170|lr = 0.00395\n",
      "Epoch:  543|steps:   60|Train Avg Loss: 0.8790 |Test Loss: 0.8154|lr = 0.00395\n",
      "Epoch:  544|steps:   30|Train Avg Loss: 0.8976 |Test Loss: 0.8204|lr = 0.00387\n",
      "Epoch:  544|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8163|lr = 0.00387\n",
      "Epoch:  545|steps:   30|Train Avg Loss: 0.9008 |Test Loss: 0.8208|lr = 0.00387\n",
      "Epoch:  545|steps:   60|Train Avg Loss: 0.8962 |Test Loss: 0.8158|lr = 0.00387\n",
      "Epoch:  546|steps:   30|Train Avg Loss: 0.8784 |Test Loss: 0.8122|lr = 0.00387\n",
      "Epoch:  546|steps:   60|Train Avg Loss: 0.8947 |Test Loss: 0.8260|lr = 0.00387\n",
      "Epoch:  547|steps:   30|Train Avg Loss: 0.8912 |Test Loss: 0.8168|lr = 0.00387\n",
      "Epoch:  547|steps:   60|Train Avg Loss: 0.8884 |Test Loss: 0.8174|lr = 0.00387\n",
      "Epoch:  548|steps:   30|Train Avg Loss: 0.9055 |Test Loss: 0.8211|lr = 0.00387\n",
      "Epoch:  548|steps:   60|Train Avg Loss: 0.8870 |Test Loss: 0.8158|lr = 0.00387\n",
      "Epoch:  549|steps:   30|Train Avg Loss: 0.8889 |Test Loss: 0.8148|lr = 0.00387\n",
      "Epoch:  549|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8196|lr = 0.00387\n",
      "Epoch:  550|steps:   30|Train Avg Loss: 0.8848 |Test Loss: 0.8126|lr = 0.00387\n",
      "Epoch:  550|steps:   60|Train Avg Loss: 0.8885 |Test Loss: 0.8180|lr = 0.00387\n",
      "Epoch:  551|steps:   30|Train Avg Loss: 0.8728 |Test Loss: 0.8154|lr = 0.00387\n",
      "Epoch:  551|steps:   60|Train Avg Loss: 0.8991 |Test Loss: 0.8183|lr = 0.00387\n",
      "Epoch:  552|steps:   30|Train Avg Loss: 0.8807 |Test Loss: 0.8147|lr = 0.00387\n",
      "Epoch:  552|steps:   60|Train Avg Loss: 0.9054 |Test Loss: 0.8207|lr = 0.00387\n",
      "Epoch:  553|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8206|lr = 0.00387\n",
      "Epoch:  553|steps:   60|Train Avg Loss: 0.9007 |Test Loss: 0.8155|lr = 0.00387\n",
      "Epoch:  554|steps:   30|Train Avg Loss: 0.8697 |Test Loss: 0.8132|lr = 0.00387\n",
      "Epoch:  554|steps:   60|Train Avg Loss: 0.9009 |Test Loss: 0.8179|lr = 0.00387\n",
      "Epoch:  555|steps:   30|Train Avg Loss: 0.8956 |Test Loss: 0.8202|lr = 0.00379\n",
      "Epoch:  555|steps:   60|Train Avg Loss: 0.8887 |Test Loss: 0.8096|lr = 0.00379\n",
      "Epoch:  556|steps:   30|Train Avg Loss: 0.8707 |Test Loss: 0.8140|lr = 0.00379\n",
      "Epoch:  556|steps:   60|Train Avg Loss: 0.9074 |Test Loss: 0.8187|lr = 0.00379\n",
      "Epoch:  557|steps:   30|Train Avg Loss: 0.8909 |Test Loss: 0.8206|lr = 0.00379\n",
      "Epoch:  557|steps:   60|Train Avg Loss: 0.8951 |Test Loss: 0.8172|lr = 0.00379\n",
      "Epoch:  558|steps:   30|Train Avg Loss: 0.9059 |Test Loss: 0.8147|lr = 0.00379\n",
      "Epoch:  558|steps:   60|Train Avg Loss: 0.8920 |Test Loss: 0.8198|lr = 0.00379\n",
      "Epoch:  559|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8157|lr = 0.00379\n",
      "Epoch:  559|steps:   60|Train Avg Loss: 0.9005 |Test Loss: 0.8204|lr = 0.00379\n",
      "Epoch:  560|steps:   30|Train Avg Loss: 0.9005 |Test Loss: 0.8168|lr = 0.00379\n",
      "Epoch:  560|steps:   60|Train Avg Loss: 0.8863 |Test Loss: 0.8134|lr = 0.00379\n",
      "Epoch:  561|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8146|lr = 0.00379\n",
      "Epoch:  561|steps:   60|Train Avg Loss: 0.8995 |Test Loss: 0.8157|lr = 0.00379\n",
      "Epoch:  562|steps:   30|Train Avg Loss: 0.8899 |Test Loss: 0.8199|lr = 0.00379\n",
      "Epoch:  562|steps:   60|Train Avg Loss: 0.8976 |Test Loss: 0.8171|lr = 0.00379\n",
      "Epoch:  563|steps:   30|Train Avg Loss: 0.8862 |Test Loss: 0.8177|lr = 0.00379\n",
      "Epoch:  563|steps:   60|Train Avg Loss: 0.8945 |Test Loss: 0.8194|lr = 0.00379\n",
      "Epoch:  564|steps:   30|Train Avg Loss: 0.8989 |Test Loss: 0.8170|lr = 0.00379\n",
      "Epoch:  564|steps:   60|Train Avg Loss: 0.8952 |Test Loss: 0.8179|lr = 0.00379\n",
      "Epoch:  565|steps:   30|Train Avg Loss: 0.8721 |Test Loss: 0.8196|lr = 0.00379\n",
      "Epoch:  565|steps:   60|Train Avg Loss: 0.8990 |Test Loss: 0.8219|lr = 0.00379\n",
      "Epoch:  566|steps:   30|Train Avg Loss: 0.8831 |Test Loss: 0.8177|lr = 0.00372\n",
      "Epoch:  566|steps:   60|Train Avg Loss: 0.9055 |Test Loss: 0.8220|lr = 0.00372\n",
      "Epoch:  567|steps:   30|Train Avg Loss: 0.8821 |Test Loss: 0.8137|lr = 0.00372\n",
      "Epoch:  567|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8215|lr = 0.00372\n",
      "Epoch:  568|steps:   30|Train Avg Loss: 0.8727 |Test Loss: 0.8154|lr = 0.00372\n",
      "Epoch:  568|steps:   60|Train Avg Loss: 0.9059 |Test Loss: 0.8154|lr = 0.00372\n",
      "Epoch:  569|steps:   30|Train Avg Loss: 0.8838 |Test Loss: 0.8155|lr = 0.00372\n",
      "Epoch:  569|steps:   60|Train Avg Loss: 0.8896 |Test Loss: 0.8184|lr = 0.00372\n",
      "Epoch:  570|steps:   30|Train Avg Loss: 0.8933 |Test Loss: 0.8166|lr = 0.00372\n",
      "Epoch:  570|steps:   60|Train Avg Loss: 0.8903 |Test Loss: 0.8149|lr = 0.00372\n",
      "Epoch:  571|steps:   30|Train Avg Loss: 0.8919 |Test Loss: 0.8183|lr = 0.00372\n",
      "Epoch:  571|steps:   60|Train Avg Loss: 0.8993 |Test Loss: 0.8184|lr = 0.00372\n",
      "Epoch:  572|steps:   30|Train Avg Loss: 0.8904 |Test Loss: 0.8162|lr = 0.00372\n",
      "Epoch:  572|steps:   60|Train Avg Loss: 0.9054 |Test Loss: 0.8195|lr = 0.00372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  573|steps:   30|Train Avg Loss: 0.8956 |Test Loss: 0.8122|lr = 0.00372\n",
      "Epoch:  573|steps:   60|Train Avg Loss: 0.9056 |Test Loss: 0.8188|lr = 0.00372\n",
      "Epoch:  574|steps:   30|Train Avg Loss: 0.8993 |Test Loss: 0.8210|lr = 0.00372\n",
      "Epoch:  574|steps:   60|Train Avg Loss: 0.8819 |Test Loss: 0.8177|lr = 0.00372\n",
      "Epoch:  575|steps:   30|Train Avg Loss: 0.8644 |Test Loss: 0.8106|lr = 0.00372\n",
      "Epoch:  575|steps:   60|Train Avg Loss: 0.9107 |Test Loss: 0.8210|lr = 0.00372\n",
      "Epoch:  576|steps:   30|Train Avg Loss: 0.8726 |Test Loss: 0.8132|lr = 0.00372\n",
      "Epoch:  576|steps:   60|Train Avg Loss: 0.9137 |Test Loss: 0.8259|lr = 0.00372\n",
      "Epoch:  577|steps:   30|Train Avg Loss: 0.9027 |Test Loss: 0.8217|lr = 0.00364\n",
      "Epoch:  577|steps:   60|Train Avg Loss: 0.8847 |Test Loss: 0.8100|lr = 0.00364\n",
      "Epoch:  578|steps:   30|Train Avg Loss: 0.8845 |Test Loss: 0.8157|lr = 0.00364\n",
      "Epoch:  578|steps:   60|Train Avg Loss: 0.9010 |Test Loss: 0.8200|lr = 0.00364\n",
      "Epoch:  579|steps:   30|Train Avg Loss: 0.8706 |Test Loss: 0.8082|lr = 0.00364\n",
      "Epoch:  579|steps:   60|Train Avg Loss: 0.9021 |Test Loss: 0.8187|lr = 0.00364\n",
      "Epoch:  580|steps:   30|Train Avg Loss: 0.8927 |Test Loss: 0.8147|lr = 0.00364\n",
      "Epoch:  580|steps:   60|Train Avg Loss: 0.9064 |Test Loss: 0.8217|lr = 0.00364\n",
      "Epoch:  581|steps:   30|Train Avg Loss: 0.8929 |Test Loss: 0.8181|lr = 0.00364\n",
      "Epoch:  581|steps:   60|Train Avg Loss: 0.9016 |Test Loss: 0.8203|lr = 0.00364\n",
      "Epoch:  582|steps:   30|Train Avg Loss: 0.8977 |Test Loss: 0.8162|lr = 0.00364\n",
      "Epoch:  582|steps:   60|Train Avg Loss: 0.8809 |Test Loss: 0.8163|lr = 0.00364\n",
      "Epoch:  583|steps:   30|Train Avg Loss: 0.9013 |Test Loss: 0.8210|lr = 0.00364\n",
      "Epoch:  583|steps:   60|Train Avg Loss: 0.8863 |Test Loss: 0.8140|lr = 0.00364\n",
      "Epoch:  584|steps:   30|Train Avg Loss: 0.8953 |Test Loss: 0.8193|lr = 0.00364\n",
      "Epoch:  584|steps:   60|Train Avg Loss: 0.9054 |Test Loss: 0.8237|lr = 0.00364\n",
      "Epoch:  585|steps:   30|Train Avg Loss: 0.8833 |Test Loss: 0.8176|lr = 0.00364\n",
      "Epoch:  585|steps:   60|Train Avg Loss: 0.8900 |Test Loss: 0.8151|lr = 0.00364\n",
      "Epoch:  586|steps:   30|Train Avg Loss: 0.8925 |Test Loss: 0.8178|lr = 0.00364\n",
      "Epoch:  586|steps:   60|Train Avg Loss: 0.8818 |Test Loss: 0.8145|lr = 0.00364\n",
      "Epoch:  587|steps:   30|Train Avg Loss: 0.8904 |Test Loss: 0.8179|lr = 0.00364\n",
      "Epoch:  587|steps:   60|Train Avg Loss: 0.8935 |Test Loss: 0.8154|lr = 0.00364\n",
      "Epoch:  588|steps:   30|Train Avg Loss: 0.9014 |Test Loss: 0.8321|lr = 0.00357\n",
      "Epoch:  588|steps:   60|Train Avg Loss: 0.8880 |Test Loss: 0.8150|lr = 0.00357\n",
      "Epoch:  589|steps:   30|Train Avg Loss: 0.8947 |Test Loss: 0.8169|lr = 0.00357\n",
      "Epoch:  589|steps:   60|Train Avg Loss: 0.8916 |Test Loss: 0.8169|lr = 0.00357\n",
      "Epoch:  590|steps:   30|Train Avg Loss: 0.8890 |Test Loss: 0.8210|lr = 0.00357\n",
      "Epoch:  590|steps:   60|Train Avg Loss: 0.8972 |Test Loss: 0.8184|lr = 0.00357\n",
      "Epoch:  591|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8124|lr = 0.00357\n",
      "Epoch:  591|steps:   60|Train Avg Loss: 0.9050 |Test Loss: 0.8213|lr = 0.00357\n",
      "Epoch:  592|steps:   30|Train Avg Loss: 0.8865 |Test Loss: 0.8166|lr = 0.00357\n",
      "Epoch:  592|steps:   60|Train Avg Loss: 0.9047 |Test Loss: 0.8185|lr = 0.00357\n",
      "Epoch:  593|steps:   30|Train Avg Loss: 0.8867 |Test Loss: 0.8151|lr = 0.00357\n",
      "Epoch:  593|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8161|lr = 0.00357\n",
      "Epoch:  594|steps:   30|Train Avg Loss: 0.8832 |Test Loss: 0.8199|lr = 0.00357\n",
      "Epoch:  594|steps:   60|Train Avg Loss: 0.8969 |Test Loss: 0.8125|lr = 0.00357\n",
      "Epoch:  595|steps:   30|Train Avg Loss: 0.9322 |Test Loss: 0.8275|lr = 0.00357\n",
      "Epoch:  595|steps:   60|Train Avg Loss: 0.8662 |Test Loss: 0.8153|lr = 0.00357\n",
      "Epoch:  596|steps:   30|Train Avg Loss: 0.8980 |Test Loss: 0.8261|lr = 0.00357\n",
      "Epoch:  596|steps:   60|Train Avg Loss: 0.8780 |Test Loss: 0.8152|lr = 0.00357\n",
      "Epoch:  597|steps:   30|Train Avg Loss: 0.9004 |Test Loss: 0.8177|lr = 0.00357\n",
      "Epoch:  597|steps:   60|Train Avg Loss: 0.8900 |Test Loss: 0.8185|lr = 0.00357\n",
      "Epoch:  598|steps:   30|Train Avg Loss: 0.8833 |Test Loss: 0.8173|lr = 0.00357\n",
      "Epoch:  598|steps:   60|Train Avg Loss: 0.8855 |Test Loss: 0.8178|lr = 0.00357\n",
      "Epoch:  599|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8181|lr = 0.00350\n",
      "Epoch:  599|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8162|lr = 0.00350\n",
      "Epoch:  600|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8176|lr = 0.00350\n",
      "Epoch:  600|steps:   60|Train Avg Loss: 0.9019 |Test Loss: 0.8184|lr = 0.00350\n",
      "Epoch:  601|steps:   30|Train Avg Loss: 0.9058 |Test Loss: 0.8177|lr = 0.00350\n",
      "Epoch:  601|steps:   60|Train Avg Loss: 0.8783 |Test Loss: 0.8151|lr = 0.00350\n",
      "Epoch:  602|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8160|lr = 0.00350\n",
      "Epoch:  602|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8161|lr = 0.00350\n",
      "Epoch:  603|steps:   30|Train Avg Loss: 0.9087 |Test Loss: 0.8220|lr = 0.00350\n",
      "Epoch:  603|steps:   60|Train Avg Loss: 0.8716 |Test Loss: 0.8154|lr = 0.00350\n",
      "Epoch:  604|steps:   30|Train Avg Loss: 0.9130 |Test Loss: 0.8206|lr = 0.00350\n",
      "Epoch:  604|steps:   60|Train Avg Loss: 0.8759 |Test Loss: 0.8134|lr = 0.00350\n",
      "Epoch:  605|steps:   30|Train Avg Loss: 0.8822 |Test Loss: 0.8167|lr = 0.00350\n",
      "Epoch:  605|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8176|lr = 0.00350\n",
      "Epoch:  606|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8155|lr = 0.00350\n",
      "Epoch:  606|steps:   60|Train Avg Loss: 0.8871 |Test Loss: 0.8158|lr = 0.00350\n",
      "Epoch:  607|steps:   30|Train Avg Loss: 0.9076 |Test Loss: 0.8242|lr = 0.00350\n",
      "Epoch:  607|steps:   60|Train Avg Loss: 0.8844 |Test Loss: 0.8177|lr = 0.00350\n",
      "Epoch:  608|steps:   30|Train Avg Loss: 0.8920 |Test Loss: 0.8172|lr = 0.00350\n",
      "Epoch:  608|steps:   60|Train Avg Loss: 0.8916 |Test Loss: 0.8173|lr = 0.00350\n",
      "Epoch:  609|steps:   30|Train Avg Loss: 0.8799 |Test Loss: 0.8169|lr = 0.00350\n",
      "Epoch:  609|steps:   60|Train Avg Loss: 0.9025 |Test Loss: 0.8182|lr = 0.00350\n",
      "Epoch:  610|steps:   30|Train Avg Loss: 0.9025 |Test Loss: 0.8215|lr = 0.00343\n",
      "Epoch:  610|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8197|lr = 0.00343\n",
      "Epoch:  611|steps:   30|Train Avg Loss: 0.8953 |Test Loss: 0.8170|lr = 0.00343\n",
      "Epoch:  611|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8161|lr = 0.00343\n",
      "Epoch:  612|steps:   30|Train Avg Loss: 0.8927 |Test Loss: 0.8194|lr = 0.00343\n",
      "Epoch:  612|steps:   60|Train Avg Loss: 0.9037 |Test Loss: 0.8182|lr = 0.00343\n",
      "Epoch:  613|steps:   30|Train Avg Loss: 0.8814 |Test Loss: 0.8154|lr = 0.00343\n",
      "Epoch:  613|steps:   60|Train Avg Loss: 0.8924 |Test Loss: 0.8139|lr = 0.00343\n",
      "Epoch:  614|steps:   30|Train Avg Loss: 0.8948 |Test Loss: 0.8191|lr = 0.00343\n",
      "Epoch:  614|steps:   60|Train Avg Loss: 0.8860 |Test Loss: 0.8184|lr = 0.00343\n",
      "Epoch:  615|steps:   30|Train Avg Loss: 0.8920 |Test Loss: 0.8184|lr = 0.00343\n",
      "Epoch:  615|steps:   60|Train Avg Loss: 0.8940 |Test Loss: 0.8196|lr = 0.00343\n",
      "Epoch:  616|steps:   30|Train Avg Loss: 0.8874 |Test Loss: 0.8147|lr = 0.00343\n",
      "Epoch:  616|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8183|lr = 0.00343\n",
      "Epoch:  617|steps:   30|Train Avg Loss: 0.9015 |Test Loss: 0.8205|lr = 0.00343\n",
      "Epoch:  617|steps:   60|Train Avg Loss: 0.8825 |Test Loss: 0.8151|lr = 0.00343\n",
      "Epoch:  618|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8152|lr = 0.00343\n",
      "Epoch:  618|steps:   60|Train Avg Loss: 0.8924 |Test Loss: 0.8195|lr = 0.00343\n",
      "Epoch:  619|steps:   30|Train Avg Loss: 0.9066 |Test Loss: 0.8198|lr = 0.00343\n",
      "Epoch:  619|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8152|lr = 0.00343\n",
      "Epoch:  620|steps:   30|Train Avg Loss: 0.8990 |Test Loss: 0.8167|lr = 0.00343\n",
      "Epoch:  620|steps:   60|Train Avg Loss: 0.8938 |Test Loss: 0.8216|lr = 0.00343\n",
      "Epoch:  621|steps:   30|Train Avg Loss: 0.8852 |Test Loss: 0.8222|lr = 0.00336\n",
      "Epoch:  621|steps:   60|Train Avg Loss: 0.8786 |Test Loss: 0.8124|lr = 0.00336\n",
      "Epoch:  622|steps:   30|Train Avg Loss: 0.8848 |Test Loss: 0.8125|lr = 0.00336\n",
      "Epoch:  622|steps:   60|Train Avg Loss: 0.9041 |Test Loss: 0.8172|lr = 0.00336\n",
      "Epoch:  623|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8147|lr = 0.00336\n",
      "Epoch:  623|steps:   60|Train Avg Loss: 0.8810 |Test Loss: 0.8149|lr = 0.00336\n",
      "Epoch:  624|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8137|lr = 0.00336\n",
      "Epoch:  624|steps:   60|Train Avg Loss: 0.9027 |Test Loss: 0.8165|lr = 0.00336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  625|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8157|lr = 0.00336\n",
      "Epoch:  625|steps:   60|Train Avg Loss: 0.9027 |Test Loss: 0.8248|lr = 0.00336\n",
      "Epoch:  626|steps:   30|Train Avg Loss: 0.8967 |Test Loss: 0.8239|lr = 0.00336\n",
      "Epoch:  626|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8112|lr = 0.00336\n",
      "Epoch:  627|steps:   30|Train Avg Loss: 0.9008 |Test Loss: 0.8220|lr = 0.00336\n",
      "Epoch:  627|steps:   60|Train Avg Loss: 0.8782 |Test Loss: 0.8179|lr = 0.00336\n",
      "Epoch:  628|steps:   30|Train Avg Loss: 0.8838 |Test Loss: 0.8125|lr = 0.00336\n",
      "Epoch:  628|steps:   60|Train Avg Loss: 0.8943 |Test Loss: 0.8177|lr = 0.00336\n",
      "Epoch:  629|steps:   30|Train Avg Loss: 0.8908 |Test Loss: 0.8176|lr = 0.00336\n",
      "Epoch:  629|steps:   60|Train Avg Loss: 0.8969 |Test Loss: 0.8179|lr = 0.00336\n",
      "Epoch:  630|steps:   30|Train Avg Loss: 0.9024 |Test Loss: 0.8192|lr = 0.00336\n",
      "Epoch:  630|steps:   60|Train Avg Loss: 0.8778 |Test Loss: 0.8170|lr = 0.00336\n",
      "Epoch:  631|steps:   30|Train Avg Loss: 0.8725 |Test Loss: 0.8127|lr = 0.00336\n",
      "Epoch:  631|steps:   60|Train Avg Loss: 0.9018 |Test Loss: 0.8183|lr = 0.00336\n",
      "Epoch:  632|steps:   30|Train Avg Loss: 0.8812 |Test Loss: 0.8146|lr = 0.00329\n",
      "Epoch:  632|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8150|lr = 0.00329\n",
      "Epoch:  633|steps:   30|Train Avg Loss: 0.8877 |Test Loss: 0.8164|lr = 0.00329\n",
      "Epoch:  633|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8168|lr = 0.00329\n",
      "Epoch:  634|steps:   30|Train Avg Loss: 0.8569 |Test Loss: 0.8126|lr = 0.00329\n",
      "Epoch:  634|steps:   60|Train Avg Loss: 0.9016 |Test Loss: 0.8169|lr = 0.00329\n",
      "Epoch:  635|steps:   30|Train Avg Loss: 0.9098 |Test Loss: 0.8244|lr = 0.00329\n",
      "Epoch:  635|steps:   60|Train Avg Loss: 0.8825 |Test Loss: 0.8135|lr = 0.00329\n",
      "Epoch:  636|steps:   30|Train Avg Loss: 0.8894 |Test Loss: 0.8156|lr = 0.00329\n",
      "Epoch:  636|steps:   60|Train Avg Loss: 0.8820 |Test Loss: 0.8137|lr = 0.00329\n",
      "Epoch:  637|steps:   30|Train Avg Loss: 0.8696 |Test Loss: 0.8142|lr = 0.00329\n",
      "Epoch:  637|steps:   60|Train Avg Loss: 0.9157 |Test Loss: 0.8228|lr = 0.00329\n",
      "Epoch:  638|steps:   30|Train Avg Loss: 0.8926 |Test Loss: 0.8171|lr = 0.00329\n",
      "Epoch:  638|steps:   60|Train Avg Loss: 0.8751 |Test Loss: 0.8132|lr = 0.00329\n",
      "Epoch:  639|steps:   30|Train Avg Loss: 0.9043 |Test Loss: 0.8168|lr = 0.00329\n",
      "Epoch:  639|steps:   60|Train Avg Loss: 0.8802 |Test Loss: 0.8187|lr = 0.00329\n",
      "Epoch:  640|steps:   30|Train Avg Loss: 0.8976 |Test Loss: 0.8170|lr = 0.00329\n",
      "Epoch:  640|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8153|lr = 0.00329\n",
      "Epoch:  641|steps:   30|Train Avg Loss: 0.8810 |Test Loss: 0.8203|lr = 0.00329\n",
      "Epoch:  641|steps:   60|Train Avg Loss: 0.9065 |Test Loss: 0.8171|lr = 0.00329\n",
      "Epoch:  642|steps:   30|Train Avg Loss: 0.9005 |Test Loss: 0.8188|lr = 0.00329\n",
      "Epoch:  642|steps:   60|Train Avg Loss: 0.8884 |Test Loss: 0.8152|lr = 0.00329\n",
      "Epoch:  643|steps:   30|Train Avg Loss: 0.9131 |Test Loss: 0.8228|lr = 0.00323\n",
      "Epoch:  643|steps:   60|Train Avg Loss: 0.8668 |Test Loss: 0.8121|lr = 0.00323\n",
      "Epoch:  644|steps:   30|Train Avg Loss: 0.8794 |Test Loss: 0.8113|lr = 0.00323\n",
      "Epoch:  644|steps:   60|Train Avg Loss: 0.9027 |Test Loss: 0.8140|lr = 0.00323\n",
      "Epoch:  645|steps:   30|Train Avg Loss: 0.8934 |Test Loss: 0.8177|lr = 0.00323\n",
      "Epoch:  645|steps:   60|Train Avg Loss: 0.8941 |Test Loss: 0.8194|lr = 0.00323\n",
      "Epoch:  646|steps:   30|Train Avg Loss: 0.8818 |Test Loss: 0.8154|lr = 0.00323\n",
      "Epoch:  646|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8142|lr = 0.00323\n",
      "Epoch:  647|steps:   30|Train Avg Loss: 0.8709 |Test Loss: 0.8174|lr = 0.00323\n",
      "Epoch:  647|steps:   60|Train Avg Loss: 0.9042 |Test Loss: 0.8175|lr = 0.00323\n",
      "Epoch:  648|steps:   30|Train Avg Loss: 0.9077 |Test Loss: 0.8202|lr = 0.00323\n",
      "Epoch:  648|steps:   60|Train Avg Loss: 0.8842 |Test Loss: 0.8171|lr = 0.00323\n",
      "Epoch:  649|steps:   30|Train Avg Loss: 0.8768 |Test Loss: 0.8152|lr = 0.00323\n",
      "Epoch:  649|steps:   60|Train Avg Loss: 0.8981 |Test Loss: 0.8170|lr = 0.00323\n",
      "Epoch:  650|steps:   30|Train Avg Loss: 0.9082 |Test Loss: 0.8192|lr = 0.00323\n",
      "Epoch:  650|steps:   60|Train Avg Loss: 0.8665 |Test Loss: 0.8158|lr = 0.00323\n",
      "Epoch:  651|steps:   30|Train Avg Loss: 0.8932 |Test Loss: 0.8176|lr = 0.00323\n",
      "Epoch:  651|steps:   60|Train Avg Loss: 0.9037 |Test Loss: 0.8204|lr = 0.00323\n",
      "Epoch:  652|steps:   30|Train Avg Loss: 0.8905 |Test Loss: 0.8184|lr = 0.00323\n",
      "Epoch:  652|steps:   60|Train Avg Loss: 0.8980 |Test Loss: 0.8201|lr = 0.00323\n",
      "Epoch:  653|steps:   30|Train Avg Loss: 0.9007 |Test Loss: 0.8210|lr = 0.00323\n",
      "Epoch:  653|steps:   60|Train Avg Loss: 0.8760 |Test Loss: 0.8157|lr = 0.00323\n",
      "Epoch:  654|steps:   30|Train Avg Loss: 0.8988 |Test Loss: 0.8168|lr = 0.00316\n",
      "Epoch:  654|steps:   60|Train Avg Loss: 0.8830 |Test Loss: 0.8163|lr = 0.00316\n",
      "Epoch:  655|steps:   30|Train Avg Loss: 0.8884 |Test Loss: 0.8157|lr = 0.00316\n",
      "Epoch:  655|steps:   60|Train Avg Loss: 0.8941 |Test Loss: 0.8154|lr = 0.00316\n",
      "Epoch:  656|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8165|lr = 0.00316\n",
      "Epoch:  656|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8154|lr = 0.00316\n",
      "Epoch:  657|steps:   30|Train Avg Loss: 0.8923 |Test Loss: 0.8175|lr = 0.00316\n",
      "Epoch:  657|steps:   60|Train Avg Loss: 0.8943 |Test Loss: 0.8171|lr = 0.00316\n",
      "Epoch:  658|steps:   30|Train Avg Loss: 0.8997 |Test Loss: 0.8179|lr = 0.00316\n",
      "Epoch:  658|steps:   60|Train Avg Loss: 0.8871 |Test Loss: 0.8172|lr = 0.00316\n",
      "Epoch:  659|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8181|lr = 0.00316\n",
      "Epoch:  659|steps:   60|Train Avg Loss: 0.9081 |Test Loss: 0.8212|lr = 0.00316\n",
      "Epoch:  660|steps:   30|Train Avg Loss: 0.8993 |Test Loss: 0.8205|lr = 0.00316\n",
      "Epoch:  660|steps:   60|Train Avg Loss: 0.8971 |Test Loss: 0.8154|lr = 0.00316\n",
      "Epoch:  661|steps:   30|Train Avg Loss: 0.8915 |Test Loss: 0.8169|lr = 0.00316\n",
      "Epoch:  661|steps:   60|Train Avg Loss: 0.8805 |Test Loss: 0.8106|lr = 0.00316\n",
      "Epoch:  662|steps:   30|Train Avg Loss: 0.9189 |Test Loss: 0.8233|lr = 0.00316\n",
      "Epoch:  662|steps:   60|Train Avg Loss: 0.8718 |Test Loss: 0.8154|lr = 0.00316\n",
      "Epoch:  663|steps:   30|Train Avg Loss: 0.8834 |Test Loss: 0.8169|lr = 0.00316\n",
      "Epoch:  663|steps:   60|Train Avg Loss: 0.8960 |Test Loss: 0.8163|lr = 0.00316\n",
      "Epoch:  664|steps:   30|Train Avg Loss: 0.9016 |Test Loss: 0.8194|lr = 0.00316\n",
      "Epoch:  664|steps:   60|Train Avg Loss: 0.8840 |Test Loss: 0.8196|lr = 0.00316\n",
      "Epoch:  665|steps:   30|Train Avg Loss: 0.8827 |Test Loss: 0.8158|lr = 0.00310\n",
      "Epoch:  665|steps:   60|Train Avg Loss: 0.8958 |Test Loss: 0.8158|lr = 0.00310\n",
      "Epoch:  666|steps:   30|Train Avg Loss: 0.8918 |Test Loss: 0.8161|lr = 0.00310\n",
      "Epoch:  666|steps:   60|Train Avg Loss: 0.8852 |Test Loss: 0.8151|lr = 0.00310\n",
      "Epoch:  667|steps:   30|Train Avg Loss: 0.8905 |Test Loss: 0.8175|lr = 0.00310\n",
      "Epoch:  667|steps:   60|Train Avg Loss: 0.8936 |Test Loss: 0.8163|lr = 0.00310\n",
      "Epoch:  668|steps:   30|Train Avg Loss: 0.9088 |Test Loss: 0.8203|lr = 0.00310\n",
      "Epoch:  668|steps:   60|Train Avg Loss: 0.8881 |Test Loss: 0.8169|lr = 0.00310\n",
      "Epoch:  669|steps:   30|Train Avg Loss: 0.8787 |Test Loss: 0.8139|lr = 0.00310\n",
      "Epoch:  669|steps:   60|Train Avg Loss: 0.9066 |Test Loss: 0.8221|lr = 0.00310\n",
      "Epoch:  670|steps:   30|Train Avg Loss: 0.8977 |Test Loss: 0.8155|lr = 0.00310\n",
      "Epoch:  670|steps:   60|Train Avg Loss: 0.8884 |Test Loss: 0.8177|lr = 0.00310\n",
      "Epoch:  671|steps:   30|Train Avg Loss: 0.8939 |Test Loss: 0.8166|lr = 0.00310\n",
      "Epoch:  671|steps:   60|Train Avg Loss: 0.8839 |Test Loss: 0.8210|lr = 0.00310\n",
      "Epoch:  672|steps:   30|Train Avg Loss: 0.8814 |Test Loss: 0.8146|lr = 0.00310\n",
      "Epoch:  672|steps:   60|Train Avg Loss: 0.9005 |Test Loss: 0.8178|lr = 0.00310\n",
      "Epoch:  673|steps:   30|Train Avg Loss: 0.8794 |Test Loss: 0.8158|lr = 0.00310\n",
      "Epoch:  673|steps:   60|Train Avg Loss: 0.9013 |Test Loss: 0.8178|lr = 0.00310\n",
      "Epoch:  674|steps:   30|Train Avg Loss: 0.8821 |Test Loss: 0.8151|lr = 0.00310\n",
      "Epoch:  674|steps:   60|Train Avg Loss: 0.9022 |Test Loss: 0.8206|lr = 0.00310\n",
      "Epoch:  675|steps:   30|Train Avg Loss: 0.8924 |Test Loss: 0.8177|lr = 0.00310\n",
      "Epoch:  675|steps:   60|Train Avg Loss: 0.8788 |Test Loss: 0.8154|lr = 0.00310\n",
      "Epoch:  676|steps:   30|Train Avg Loss: 0.8858 |Test Loss: 0.8146|lr = 0.00304\n",
      "Epoch:  676|steps:   60|Train Avg Loss: 0.8983 |Test Loss: 0.8172|lr = 0.00304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  677|steps:   30|Train Avg Loss: 0.9035 |Test Loss: 0.8201|lr = 0.00304\n",
      "Epoch:  677|steps:   60|Train Avg Loss: 0.8690 |Test Loss: 0.8149|lr = 0.00304\n",
      "Epoch:  678|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8173|lr = 0.00304\n",
      "Epoch:  678|steps:   60|Train Avg Loss: 0.8759 |Test Loss: 0.8157|lr = 0.00304\n",
      "Epoch:  679|steps:   30|Train Avg Loss: 0.9022 |Test Loss: 0.8161|lr = 0.00304\n",
      "Epoch:  679|steps:   60|Train Avg Loss: 0.8853 |Test Loss: 0.8150|lr = 0.00304\n",
      "Epoch:  680|steps:   30|Train Avg Loss: 0.9017 |Test Loss: 0.8172|lr = 0.00304\n",
      "Epoch:  680|steps:   60|Train Avg Loss: 0.8859 |Test Loss: 0.8156|lr = 0.00304\n",
      "Epoch:  681|steps:   30|Train Avg Loss: 0.8772 |Test Loss: 0.8168|lr = 0.00304\n",
      "Epoch:  681|steps:   60|Train Avg Loss: 0.8992 |Test Loss: 0.8132|lr = 0.00304\n",
      "Epoch:  682|steps:   30|Train Avg Loss: 0.8932 |Test Loss: 0.8216|lr = 0.00304\n",
      "Epoch:  682|steps:   60|Train Avg Loss: 0.9068 |Test Loss: 0.8195|lr = 0.00304\n",
      "Epoch:  683|steps:   30|Train Avg Loss: 0.8854 |Test Loss: 0.8161|lr = 0.00304\n",
      "Epoch:  683|steps:   60|Train Avg Loss: 0.9052 |Test Loss: 0.8186|lr = 0.00304\n",
      "Epoch:  684|steps:   30|Train Avg Loss: 0.8733 |Test Loss: 0.8145|lr = 0.00304\n",
      "Epoch:  684|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8187|lr = 0.00304\n",
      "Epoch:  685|steps:   30|Train Avg Loss: 0.9023 |Test Loss: 0.8181|lr = 0.00304\n",
      "Epoch:  685|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8193|lr = 0.00304\n",
      "Epoch:  686|steps:   30|Train Avg Loss: 0.8956 |Test Loss: 0.8171|lr = 0.00304\n",
      "Epoch:  686|steps:   60|Train Avg Loss: 0.8808 |Test Loss: 0.8143|lr = 0.00304\n",
      "Epoch:  687|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8200|lr = 0.00298\n",
      "Epoch:  687|steps:   60|Train Avg Loss: 0.8976 |Test Loss: 0.8178|lr = 0.00298\n",
      "Epoch:  688|steps:   30|Train Avg Loss: 0.9174 |Test Loss: 0.8229|lr = 0.00298\n",
      "Epoch:  688|steps:   60|Train Avg Loss: 0.8649 |Test Loss: 0.8111|lr = 0.00298\n",
      "Epoch:  689|steps:   30|Train Avg Loss: 0.8965 |Test Loss: 0.8162|lr = 0.00298\n",
      "Epoch:  689|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8211|lr = 0.00298\n",
      "Epoch:  690|steps:   30|Train Avg Loss: 0.9083 |Test Loss: 0.8206|lr = 0.00298\n",
      "Epoch:  690|steps:   60|Train Avg Loss: 0.8752 |Test Loss: 0.8138|lr = 0.00298\n",
      "Epoch:  691|steps:   30|Train Avg Loss: 0.8886 |Test Loss: 0.8160|lr = 0.00298\n",
      "Epoch:  691|steps:   60|Train Avg Loss: 0.8935 |Test Loss: 0.8233|lr = 0.00298\n",
      "Epoch:  692|steps:   30|Train Avg Loss: 0.8984 |Test Loss: 0.8192|lr = 0.00298\n",
      "Epoch:  692|steps:   60|Train Avg Loss: 0.8827 |Test Loss: 0.8143|lr = 0.00298\n",
      "Epoch:  693|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8156|lr = 0.00298\n",
      "Epoch:  693|steps:   60|Train Avg Loss: 0.8923 |Test Loss: 0.8164|lr = 0.00298\n",
      "Epoch:  694|steps:   30|Train Avg Loss: 0.8998 |Test Loss: 0.8200|lr = 0.00298\n",
      "Epoch:  694|steps:   60|Train Avg Loss: 0.8701 |Test Loss: 0.8113|lr = 0.00298\n",
      "Epoch:  695|steps:   30|Train Avg Loss: 0.8812 |Test Loss: 0.8142|lr = 0.00298\n",
      "Epoch:  695|steps:   60|Train Avg Loss: 0.9019 |Test Loss: 0.8173|lr = 0.00298\n",
      "Epoch:  696|steps:   30|Train Avg Loss: 0.8934 |Test Loss: 0.8160|lr = 0.00298\n",
      "Epoch:  696|steps:   60|Train Avg Loss: 0.8876 |Test Loss: 0.8133|lr = 0.00298\n",
      "Epoch:  697|steps:   30|Train Avg Loss: 0.8665 |Test Loss: 0.8130|lr = 0.00298\n",
      "Epoch:  697|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8165|lr = 0.00298\n",
      "Epoch:  698|steps:   30|Train Avg Loss: 0.8710 |Test Loss: 0.8121|lr = 0.00292\n",
      "Epoch:  698|steps:   60|Train Avg Loss: 0.9057 |Test Loss: 0.8182|lr = 0.00292\n",
      "Epoch:  699|steps:   30|Train Avg Loss: 0.8741 |Test Loss: 0.8133|lr = 0.00292\n",
      "Epoch:  699|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8171|lr = 0.00292\n",
      "Epoch:  700|steps:   30|Train Avg Loss: 0.8927 |Test Loss: 0.8161|lr = 0.00292\n",
      "Epoch:  700|steps:   60|Train Avg Loss: 0.8972 |Test Loss: 0.8114|lr = 0.00292\n",
      "Epoch:  701|steps:   30|Train Avg Loss: 0.8779 |Test Loss: 0.8129|lr = 0.00292\n",
      "Epoch:  701|steps:   60|Train Avg Loss: 0.9162 |Test Loss: 0.8256|lr = 0.00292\n",
      "Epoch:  702|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8195|lr = 0.00292\n",
      "Epoch:  702|steps:   60|Train Avg Loss: 0.8807 |Test Loss: 0.8164|lr = 0.00292\n",
      "Epoch:  703|steps:   30|Train Avg Loss: 0.9107 |Test Loss: 0.8212|lr = 0.00292\n",
      "Epoch:  703|steps:   60|Train Avg Loss: 0.8732 |Test Loss: 0.8125|lr = 0.00292\n",
      "Epoch:  704|steps:   30|Train Avg Loss: 0.9014 |Test Loss: 0.8180|lr = 0.00292\n",
      "Epoch:  704|steps:   60|Train Avg Loss: 0.8812 |Test Loss: 0.8149|lr = 0.00292\n",
      "Epoch:  705|steps:   30|Train Avg Loss: 0.8824 |Test Loss: 0.8143|lr = 0.00292\n",
      "Epoch:  705|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8171|lr = 0.00292\n",
      "Epoch:  706|steps:   30|Train Avg Loss: 0.9059 |Test Loss: 0.8198|lr = 0.00292\n",
      "Epoch:  706|steps:   60|Train Avg Loss: 0.8741 |Test Loss: 0.8157|lr = 0.00292\n",
      "Epoch:  707|steps:   30|Train Avg Loss: 0.8869 |Test Loss: 0.8156|lr = 0.00292\n",
      "Epoch:  707|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8163|lr = 0.00292\n",
      "Epoch:  708|steps:   30|Train Avg Loss: 0.8958 |Test Loss: 0.8214|lr = 0.00292\n",
      "Epoch:  708|steps:   60|Train Avg Loss: 0.8840 |Test Loss: 0.8121|lr = 0.00292\n",
      "Epoch:  709|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8185|lr = 0.00286\n",
      "Epoch:  709|steps:   60|Train Avg Loss: 0.9009 |Test Loss: 0.8197|lr = 0.00286\n",
      "Epoch:  710|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.8143|lr = 0.00286\n",
      "Epoch:  710|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8140|lr = 0.00286\n",
      "Epoch:  711|steps:   30|Train Avg Loss: 0.8999 |Test Loss: 0.8181|lr = 0.00286\n",
      "Epoch:  711|steps:   60|Train Avg Loss: 0.8727 |Test Loss: 0.8134|lr = 0.00286\n",
      "Epoch:  712|steps:   30|Train Avg Loss: 0.8990 |Test Loss: 0.8169|lr = 0.00286\n",
      "Epoch:  712|steps:   60|Train Avg Loss: 0.8722 |Test Loss: 0.8140|lr = 0.00286\n",
      "Epoch:  713|steps:   30|Train Avg Loss: 0.8673 |Test Loss: 0.8135|lr = 0.00286\n",
      "Epoch:  713|steps:   60|Train Avg Loss: 0.9148 |Test Loss: 0.8208|lr = 0.00286\n",
      "Epoch:  714|steps:   30|Train Avg Loss: 0.8822 |Test Loss: 0.8167|lr = 0.00286\n",
      "Epoch:  714|steps:   60|Train Avg Loss: 0.9083 |Test Loss: 0.8204|lr = 0.00286\n",
      "Epoch:  715|steps:   30|Train Avg Loss: 0.8843 |Test Loss: 0.8147|lr = 0.00286\n",
      "Epoch:  715|steps:   60|Train Avg Loss: 0.8950 |Test Loss: 0.8190|lr = 0.00286\n",
      "Epoch:  716|steps:   30|Train Avg Loss: 0.8996 |Test Loss: 0.8176|lr = 0.00286\n",
      "Epoch:  716|steps:   60|Train Avg Loss: 0.8822 |Test Loss: 0.8143|lr = 0.00286\n",
      "Epoch:  717|steps:   30|Train Avg Loss: 0.8769 |Test Loss: 0.8143|lr = 0.00286\n",
      "Epoch:  717|steps:   60|Train Avg Loss: 0.8963 |Test Loss: 0.8158|lr = 0.00286\n",
      "Epoch:  718|steps:   30|Train Avg Loss: 0.8826 |Test Loss: 0.8154|lr = 0.00286\n",
      "Epoch:  718|steps:   60|Train Avg Loss: 0.9028 |Test Loss: 0.8228|lr = 0.00286\n",
      "Epoch:  719|steps:   30|Train Avg Loss: 0.9014 |Test Loss: 0.8165|lr = 0.00286\n",
      "Epoch:  719|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8160|lr = 0.00286\n",
      "Epoch:  720|steps:   30|Train Avg Loss: 0.8824 |Test Loss: 0.8130|lr = 0.00280\n",
      "Epoch:  720|steps:   60|Train Avg Loss: 0.8990 |Test Loss: 0.8171|lr = 0.00280\n",
      "Epoch:  721|steps:   30|Train Avg Loss: 0.8739 |Test Loss: 0.8195|lr = 0.00280\n",
      "Epoch:  721|steps:   60|Train Avg Loss: 0.8971 |Test Loss: 0.8107|lr = 0.00280\n",
      "Epoch:  722|steps:   30|Train Avg Loss: 0.8868 |Test Loss: 0.8181|lr = 0.00280\n",
      "Epoch:  722|steps:   60|Train Avg Loss: 0.8841 |Test Loss: 0.8173|lr = 0.00280\n",
      "Epoch:  723|steps:   30|Train Avg Loss: 0.8991 |Test Loss: 0.8178|lr = 0.00280\n",
      "Epoch:  723|steps:   60|Train Avg Loss: 0.8732 |Test Loss: 0.8162|lr = 0.00280\n",
      "Epoch:  724|steps:   30|Train Avg Loss: 0.9134 |Test Loss: 0.8192|lr = 0.00280\n",
      "Epoch:  724|steps:   60|Train Avg Loss: 0.8750 |Test Loss: 0.8144|lr = 0.00280\n",
      "Epoch:  725|steps:   30|Train Avg Loss: 0.8549 |Test Loss: 0.8114|lr = 0.00280\n",
      "Epoch:  725|steps:   60|Train Avg Loss: 0.9137 |Test Loss: 0.8197|lr = 0.00280\n",
      "Epoch:  726|steps:   30|Train Avg Loss: 0.8848 |Test Loss: 0.8164|lr = 0.00280\n",
      "Epoch:  726|steps:   60|Train Avg Loss: 0.8966 |Test Loss: 0.8187|lr = 0.00280\n",
      "Epoch:  727|steps:   30|Train Avg Loss: 0.8991 |Test Loss: 0.8187|lr = 0.00280\n",
      "Epoch:  727|steps:   60|Train Avg Loss: 0.8908 |Test Loss: 0.8170|lr = 0.00280\n",
      "Epoch:  728|steps:   30|Train Avg Loss: 0.8869 |Test Loss: 0.8142|lr = 0.00280\n",
      "Epoch:  728|steps:   60|Train Avg Loss: 0.8870 |Test Loss: 0.8135|lr = 0.00280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  729|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8171|lr = 0.00280\n",
      "Epoch:  729|steps:   60|Train Avg Loss: 0.8903 |Test Loss: 0.8187|lr = 0.00280\n",
      "Epoch:  730|steps:   30|Train Avg Loss: 0.8973 |Test Loss: 0.8184|lr = 0.00280\n",
      "Epoch:  730|steps:   60|Train Avg Loss: 0.8808 |Test Loss: 0.8144|lr = 0.00280\n",
      "Epoch:  731|steps:   30|Train Avg Loss: 0.8918 |Test Loss: 0.8191|lr = 0.00274\n",
      "Epoch:  731|steps:   60|Train Avg Loss: 0.8906 |Test Loss: 0.8159|lr = 0.00274\n",
      "Epoch:  732|steps:   30|Train Avg Loss: 0.9125 |Test Loss: 0.8212|lr = 0.00274\n",
      "Epoch:  732|steps:   60|Train Avg Loss: 0.8806 |Test Loss: 0.8133|lr = 0.00274\n",
      "Epoch:  733|steps:   30|Train Avg Loss: 0.9026 |Test Loss: 0.8175|lr = 0.00274\n",
      "Epoch:  733|steps:   60|Train Avg Loss: 0.8871 |Test Loss: 0.8158|lr = 0.00274\n",
      "Epoch:  734|steps:   30|Train Avg Loss: 0.9071 |Test Loss: 0.8196|lr = 0.00274\n",
      "Epoch:  734|steps:   60|Train Avg Loss: 0.8913 |Test Loss: 0.8162|lr = 0.00274\n",
      "Epoch:  735|steps:   30|Train Avg Loss: 0.9062 |Test Loss: 0.8174|lr = 0.00274\n",
      "Epoch:  735|steps:   60|Train Avg Loss: 0.8715 |Test Loss: 0.8144|lr = 0.00274\n",
      "Epoch:  736|steps:   30|Train Avg Loss: 0.9060 |Test Loss: 0.8192|lr = 0.00274\n",
      "Epoch:  736|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8151|lr = 0.00274\n",
      "Epoch:  737|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.8160|lr = 0.00274\n",
      "Epoch:  737|steps:   60|Train Avg Loss: 0.8997 |Test Loss: 0.8165|lr = 0.00274\n",
      "Epoch:  738|steps:   30|Train Avg Loss: 0.8803 |Test Loss: 0.8097|lr = 0.00274\n",
      "Epoch:  738|steps:   60|Train Avg Loss: 0.8957 |Test Loss: 0.8188|lr = 0.00274\n",
      "Epoch:  739|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8186|lr = 0.00274\n",
      "Epoch:  739|steps:   60|Train Avg Loss: 0.8853 |Test Loss: 0.8173|lr = 0.00274\n",
      "Epoch:  740|steps:   30|Train Avg Loss: 0.8919 |Test Loss: 0.8187|lr = 0.00274\n",
      "Epoch:  740|steps:   60|Train Avg Loss: 0.8975 |Test Loss: 0.8159|lr = 0.00274\n",
      "Epoch:  741|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8139|lr = 0.00274\n",
      "Epoch:  741|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8169|lr = 0.00274\n",
      "Epoch:  742|steps:   30|Train Avg Loss: 0.8820 |Test Loss: 0.8132|lr = 0.00269\n",
      "Epoch:  742|steps:   60|Train Avg Loss: 0.9074 |Test Loss: 0.8178|lr = 0.00269\n",
      "Epoch:  743|steps:   30|Train Avg Loss: 0.8997 |Test Loss: 0.8194|lr = 0.00269\n",
      "Epoch:  743|steps:   60|Train Avg Loss: 0.8745 |Test Loss: 0.8117|lr = 0.00269\n",
      "Epoch:  744|steps:   30|Train Avg Loss: 0.8866 |Test Loss: 0.8163|lr = 0.00269\n",
      "Epoch:  744|steps:   60|Train Avg Loss: 0.9027 |Test Loss: 0.8169|lr = 0.00269\n",
      "Epoch:  745|steps:   30|Train Avg Loss: 0.8952 |Test Loss: 0.8194|lr = 0.00269\n",
      "Epoch:  745|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8182|lr = 0.00269\n",
      "Epoch:  746|steps:   30|Train Avg Loss: 0.9119 |Test Loss: 0.8182|lr = 0.00269\n",
      "Epoch:  746|steps:   60|Train Avg Loss: 0.8768 |Test Loss: 0.8161|lr = 0.00269\n",
      "Epoch:  747|steps:   30|Train Avg Loss: 0.8777 |Test Loss: 0.8173|lr = 0.00269\n",
      "Epoch:  747|steps:   60|Train Avg Loss: 0.9022 |Test Loss: 0.8169|lr = 0.00269\n",
      "Epoch:  748|steps:   30|Train Avg Loss: 0.8880 |Test Loss: 0.8166|lr = 0.00269\n",
      "Epoch:  748|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8145|lr = 0.00269\n",
      "Epoch:  749|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8139|lr = 0.00269\n",
      "Epoch:  749|steps:   60|Train Avg Loss: 0.8912 |Test Loss: 0.8165|lr = 0.00269\n",
      "Epoch:  750|steps:   30|Train Avg Loss: 0.9038 |Test Loss: 0.8190|lr = 0.00269\n",
      "Epoch:  750|steps:   60|Train Avg Loss: 0.8799 |Test Loss: 0.8155|lr = 0.00269\n",
      "Epoch:  751|steps:   30|Train Avg Loss: 0.8841 |Test Loss: 0.8150|lr = 0.00269\n",
      "Epoch:  751|steps:   60|Train Avg Loss: 0.8892 |Test Loss: 0.8175|lr = 0.00269\n",
      "Epoch:  752|steps:   30|Train Avg Loss: 0.9078 |Test Loss: 0.8198|lr = 0.00269\n",
      "Epoch:  752|steps:   60|Train Avg Loss: 0.8742 |Test Loss: 0.8122|lr = 0.00269\n",
      "Epoch:  753|steps:   30|Train Avg Loss: 0.8717 |Test Loss: 0.8130|lr = 0.00264\n",
      "Epoch:  753|steps:   60|Train Avg Loss: 0.9159 |Test Loss: 0.8264|lr = 0.00264\n",
      "Epoch:  754|steps:   30|Train Avg Loss: 0.8806 |Test Loss: 0.8170|lr = 0.00264\n",
      "Epoch:  754|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8136|lr = 0.00264\n",
      "Epoch:  755|steps:   30|Train Avg Loss: 0.9010 |Test Loss: 0.8141|lr = 0.00264\n",
      "Epoch:  755|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8171|lr = 0.00264\n",
      "Epoch:  756|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8127|lr = 0.00264\n",
      "Epoch:  756|steps:   60|Train Avg Loss: 0.8783 |Test Loss: 0.8131|lr = 0.00264\n",
      "Epoch:  757|steps:   30|Train Avg Loss: 0.8822 |Test Loss: 0.8181|lr = 0.00264\n",
      "Epoch:  757|steps:   60|Train Avg Loss: 0.9014 |Test Loss: 0.8193|lr = 0.00264\n",
      "Epoch:  758|steps:   30|Train Avg Loss: 0.9049 |Test Loss: 0.8193|lr = 0.00264\n",
      "Epoch:  758|steps:   60|Train Avg Loss: 0.8714 |Test Loss: 0.8122|lr = 0.00264\n",
      "Epoch:  759|steps:   30|Train Avg Loss: 0.9070 |Test Loss: 0.8179|lr = 0.00264\n",
      "Epoch:  759|steps:   60|Train Avg Loss: 0.8804 |Test Loss: 0.8164|lr = 0.00264\n",
      "Epoch:  760|steps:   30|Train Avg Loss: 0.9076 |Test Loss: 0.8160|lr = 0.00264\n",
      "Epoch:  760|steps:   60|Train Avg Loss: 0.8837 |Test Loss: 0.8166|lr = 0.00264\n",
      "Epoch:  761|steps:   30|Train Avg Loss: 0.8626 |Test Loss: 0.8063|lr = 0.00264\n",
      "Epoch:  761|steps:   60|Train Avg Loss: 0.9141 |Test Loss: 0.8199|lr = 0.00264\n",
      "Epoch:  762|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8142|lr = 0.00264\n",
      "Epoch:  762|steps:   60|Train Avg Loss: 0.8849 |Test Loss: 0.8165|lr = 0.00264\n",
      "Epoch:  763|steps:   30|Train Avg Loss: 0.8724 |Test Loss: 0.8144|lr = 0.00264\n",
      "Epoch:  763|steps:   60|Train Avg Loss: 0.8907 |Test Loss: 0.8127|lr = 0.00264\n",
      "Epoch:  764|steps:   30|Train Avg Loss: 0.8929 |Test Loss: 0.8166|lr = 0.00258\n",
      "Epoch:  764|steps:   60|Train Avg Loss: 0.8837 |Test Loss: 0.8137|lr = 0.00258\n",
      "Epoch:  765|steps:   30|Train Avg Loss: 0.8741 |Test Loss: 0.8127|lr = 0.00258\n",
      "Epoch:  765|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8191|lr = 0.00258\n",
      "Epoch:  766|steps:   30|Train Avg Loss: 0.8979 |Test Loss: 0.8167|lr = 0.00258\n",
      "Epoch:  766|steps:   60|Train Avg Loss: 0.8963 |Test Loss: 0.8181|lr = 0.00258\n",
      "Epoch:  767|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8132|lr = 0.00258\n",
      "Epoch:  767|steps:   60|Train Avg Loss: 0.8919 |Test Loss: 0.8130|lr = 0.00258\n",
      "Epoch:  768|steps:   30|Train Avg Loss: 0.8837 |Test Loss: 0.8147|lr = 0.00258\n",
      "Epoch:  768|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8173|lr = 0.00258\n",
      "Epoch:  769|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8147|lr = 0.00258\n",
      "Epoch:  769|steps:   60|Train Avg Loss: 0.8922 |Test Loss: 0.8147|lr = 0.00258\n",
      "Epoch:  770|steps:   30|Train Avg Loss: 0.8884 |Test Loss: 0.8132|lr = 0.00258\n",
      "Epoch:  770|steps:   60|Train Avg Loss: 0.8917 |Test Loss: 0.8166|lr = 0.00258\n",
      "Epoch:  771|steps:   30|Train Avg Loss: 0.8841 |Test Loss: 0.8154|lr = 0.00258\n",
      "Epoch:  771|steps:   60|Train Avg Loss: 0.8826 |Test Loss: 0.8125|lr = 0.00258\n",
      "Epoch:  772|steps:   30|Train Avg Loss: 0.9081 |Test Loss: 0.8189|lr = 0.00258\n",
      "Epoch:  772|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8144|lr = 0.00258\n",
      "Epoch:  773|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8181|lr = 0.00258\n",
      "Epoch:  773|steps:   60|Train Avg Loss: 0.8840 |Test Loss: 0.8144|lr = 0.00258\n",
      "Epoch:  774|steps:   30|Train Avg Loss: 0.8843 |Test Loss: 0.8151|lr = 0.00258\n",
      "Epoch:  774|steps:   60|Train Avg Loss: 0.9042 |Test Loss: 0.8184|lr = 0.00258\n",
      "Epoch:  775|steps:   30|Train Avg Loss: 0.8996 |Test Loss: 0.8194|lr = 0.00253\n",
      "Epoch:  775|steps:   60|Train Avg Loss: 0.8877 |Test Loss: 0.8136|lr = 0.00253\n",
      "Epoch:  776|steps:   30|Train Avg Loss: 0.8896 |Test Loss: 0.8158|lr = 0.00253\n",
      "Epoch:  776|steps:   60|Train Avg Loss: 0.8865 |Test Loss: 0.8180|lr = 0.00253\n",
      "Epoch:  777|steps:   30|Train Avg Loss: 0.8924 |Test Loss: 0.8136|lr = 0.00253\n",
      "Epoch:  777|steps:   60|Train Avg Loss: 0.8941 |Test Loss: 0.8154|lr = 0.00253\n",
      "Epoch:  778|steps:   30|Train Avg Loss: 0.8944 |Test Loss: 0.8151|lr = 0.00253\n",
      "Epoch:  778|steps:   60|Train Avg Loss: 0.9029 |Test Loss: 0.8182|lr = 0.00253\n",
      "Epoch:  779|steps:   30|Train Avg Loss: 0.8749 |Test Loss: 0.8100|lr = 0.00253\n",
      "Epoch:  779|steps:   60|Train Avg Loss: 0.9055 |Test Loss: 0.8216|lr = 0.00253\n",
      "Epoch:  780|steps:   30|Train Avg Loss: 0.8939 |Test Loss: 0.8168|lr = 0.00253\n",
      "Epoch:  780|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8109|lr = 0.00253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  781|steps:   30|Train Avg Loss: 0.8722 |Test Loss: 0.8132|lr = 0.00253\n",
      "Epoch:  781|steps:   60|Train Avg Loss: 0.9070 |Test Loss: 0.8278|lr = 0.00253\n",
      "Epoch:  782|steps:   30|Train Avg Loss: 0.8677 |Test Loss: 0.8092|lr = 0.00253\n",
      "Epoch:  782|steps:   60|Train Avg Loss: 0.9097 |Test Loss: 0.8153|lr = 0.00253\n",
      "Epoch:  783|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8203|lr = 0.00253\n",
      "Epoch:  783|steps:   60|Train Avg Loss: 0.8845 |Test Loss: 0.8149|lr = 0.00253\n",
      "Epoch:  784|steps:   30|Train Avg Loss: 0.8944 |Test Loss: 0.8167|lr = 0.00253\n",
      "Epoch:  784|steps:   60|Train Avg Loss: 0.8726 |Test Loss: 0.8160|lr = 0.00253\n",
      "Epoch:  785|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8141|lr = 0.00253\n",
      "Epoch:  785|steps:   60|Train Avg Loss: 0.9020 |Test Loss: 0.8172|lr = 0.00253\n",
      "Epoch:  786|steps:   30|Train Avg Loss: 0.8935 |Test Loss: 0.8233|lr = 0.00248\n",
      "Epoch:  786|steps:   60|Train Avg Loss: 0.8991 |Test Loss: 0.8124|lr = 0.00248\n",
      "Epoch:  787|steps:   30|Train Avg Loss: 0.8997 |Test Loss: 0.8125|lr = 0.00248\n",
      "Epoch:  787|steps:   60|Train Avg Loss: 0.8734 |Test Loss: 0.8087|lr = 0.00248\n",
      "Epoch:  788|steps:   30|Train Avg Loss: 0.8904 |Test Loss: 0.8154|lr = 0.00248\n",
      "Epoch:  788|steps:   60|Train Avg Loss: 0.8993 |Test Loss: 0.8163|lr = 0.00248\n",
      "Epoch:  789|steps:   30|Train Avg Loss: 0.8892 |Test Loss: 0.8147|lr = 0.00248\n",
      "Epoch:  789|steps:   60|Train Avg Loss: 0.8776 |Test Loss: 0.8129|lr = 0.00248\n",
      "Epoch:  790|steps:   30|Train Avg Loss: 0.8876 |Test Loss: 0.8151|lr = 0.00248\n",
      "Epoch:  790|steps:   60|Train Avg Loss: 0.8937 |Test Loss: 0.8152|lr = 0.00248\n",
      "Epoch:  791|steps:   30|Train Avg Loss: 0.8761 |Test Loss: 0.8118|lr = 0.00248\n",
      "Epoch:  791|steps:   60|Train Avg Loss: 0.8976 |Test Loss: 0.8149|lr = 0.00248\n",
      "Epoch:  792|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8146|lr = 0.00248\n",
      "Epoch:  792|steps:   60|Train Avg Loss: 0.8861 |Test Loss: 0.8137|lr = 0.00248\n",
      "Epoch:  793|steps:   30|Train Avg Loss: 0.8874 |Test Loss: 0.8156|lr = 0.00248\n",
      "Epoch:  793|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8143|lr = 0.00248\n",
      "Epoch:  794|steps:   30|Train Avg Loss: 0.9064 |Test Loss: 0.8189|lr = 0.00248\n",
      "Epoch:  794|steps:   60|Train Avg Loss: 0.8732 |Test Loss: 0.8138|lr = 0.00248\n",
      "Epoch:  795|steps:   30|Train Avg Loss: 0.8814 |Test Loss: 0.8141|lr = 0.00248\n",
      "Epoch:  795|steps:   60|Train Avg Loss: 0.8854 |Test Loss: 0.8129|lr = 0.00248\n",
      "Epoch:  796|steps:   30|Train Avg Loss: 0.8913 |Test Loss: 0.8142|lr = 0.00248\n",
      "Epoch:  796|steps:   60|Train Avg Loss: 0.8843 |Test Loss: 0.8151|lr = 0.00248\n",
      "Epoch:  797|steps:   30|Train Avg Loss: 0.8731 |Test Loss: 0.8136|lr = 0.00243\n",
      "Epoch:  797|steps:   60|Train Avg Loss: 0.9079 |Test Loss: 0.8162|lr = 0.00243\n",
      "Epoch:  798|steps:   30|Train Avg Loss: 0.8836 |Test Loss: 0.8163|lr = 0.00243\n",
      "Epoch:  798|steps:   60|Train Avg Loss: 0.8878 |Test Loss: 0.8127|lr = 0.00243\n",
      "Epoch:  799|steps:   30|Train Avg Loss: 0.9022 |Test Loss: 0.8142|lr = 0.00243\n",
      "Epoch:  799|steps:   60|Train Avg Loss: 0.8776 |Test Loss: 0.8147|lr = 0.00243\n",
      "Epoch:  800|steps:   30|Train Avg Loss: 0.8999 |Test Loss: 0.8164|lr = 0.00243\n",
      "Epoch:  800|steps:   60|Train Avg Loss: 0.8778 |Test Loss: 0.8123|lr = 0.00243\n",
      "Epoch:  801|steps:   30|Train Avg Loss: 0.9126 |Test Loss: 0.8175|lr = 0.00243\n",
      "Epoch:  801|steps:   60|Train Avg Loss: 0.8670 |Test Loss: 0.8128|lr = 0.00243\n",
      "Epoch:  802|steps:   30|Train Avg Loss: 0.8853 |Test Loss: 0.8143|lr = 0.00243\n",
      "Epoch:  802|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8125|lr = 0.00243\n",
      "Epoch:  803|steps:   30|Train Avg Loss: 0.8769 |Test Loss: 0.8118|lr = 0.00243\n",
      "Epoch:  803|steps:   60|Train Avg Loss: 0.9093 |Test Loss: 0.8164|lr = 0.00243\n",
      "Epoch:  804|steps:   30|Train Avg Loss: 0.9028 |Test Loss: 0.8170|lr = 0.00243\n",
      "Epoch:  804|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8149|lr = 0.00243\n",
      "Epoch:  805|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8124|lr = 0.00243\n",
      "Epoch:  805|steps:   60|Train Avg Loss: 0.8983 |Test Loss: 0.8143|lr = 0.00243\n",
      "Epoch:  806|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8157|lr = 0.00243\n",
      "Epoch:  806|steps:   60|Train Avg Loss: 0.8893 |Test Loss: 0.8127|lr = 0.00243\n",
      "Epoch:  807|steps:   30|Train Avg Loss: 0.9027 |Test Loss: 0.8174|lr = 0.00243\n",
      "Epoch:  807|steps:   60|Train Avg Loss: 0.8835 |Test Loss: 0.8114|lr = 0.00243\n",
      "Epoch:  808|steps:   30|Train Avg Loss: 0.8862 |Test Loss: 0.8122|lr = 0.00238\n",
      "Epoch:  808|steps:   60|Train Avg Loss: 0.8793 |Test Loss: 0.8131|lr = 0.00238\n",
      "Epoch:  809|steps:   30|Train Avg Loss: 0.8828 |Test Loss: 0.8143|lr = 0.00238\n",
      "Epoch:  809|steps:   60|Train Avg Loss: 0.8790 |Test Loss: 0.8125|lr = 0.00238\n",
      "Epoch:  810|steps:   30|Train Avg Loss: 0.8695 |Test Loss: 0.8072|lr = 0.00238\n",
      "Epoch:  810|steps:   60|Train Avg Loss: 0.9058 |Test Loss: 0.8166|lr = 0.00238\n",
      "Epoch:  811|steps:   30|Train Avg Loss: 0.8617 |Test Loss: 0.8091|lr = 0.00238\n",
      "Epoch:  811|steps:   60|Train Avg Loss: 0.9181 |Test Loss: 0.8239|lr = 0.00238\n",
      "Epoch:  812|steps:   30|Train Avg Loss: 0.9101 |Test Loss: 0.8189|lr = 0.00238\n",
      "Epoch:  812|steps:   60|Train Avg Loss: 0.8756 |Test Loss: 0.8130|lr = 0.00238\n",
      "Epoch:  813|steps:   30|Train Avg Loss: 0.8840 |Test Loss: 0.8139|lr = 0.00238\n",
      "Epoch:  813|steps:   60|Train Avg Loss: 0.8960 |Test Loss: 0.8162|lr = 0.00238\n",
      "Epoch:  814|steps:   30|Train Avg Loss: 0.8675 |Test Loss: 0.8129|lr = 0.00238\n",
      "Epoch:  814|steps:   60|Train Avg Loss: 0.9009 |Test Loss: 0.8140|lr = 0.00238\n",
      "Epoch:  815|steps:   30|Train Avg Loss: 0.9070 |Test Loss: 0.8182|lr = 0.00238\n",
      "Epoch:  815|steps:   60|Train Avg Loss: 0.8824 |Test Loss: 0.8145|lr = 0.00238\n",
      "Epoch:  816|steps:   30|Train Avg Loss: 0.9205 |Test Loss: 0.8180|lr = 0.00238\n",
      "Epoch:  816|steps:   60|Train Avg Loss: 0.8644 |Test Loss: 0.8098|lr = 0.00238\n",
      "Epoch:  817|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8148|lr = 0.00238\n",
      "Epoch:  817|steps:   60|Train Avg Loss: 0.8783 |Test Loss: 0.8195|lr = 0.00238\n",
      "Epoch:  818|steps:   30|Train Avg Loss: 0.9016 |Test Loss: 0.8195|lr = 0.00238\n",
      "Epoch:  818|steps:   60|Train Avg Loss: 0.8877 |Test Loss: 0.8166|lr = 0.00238\n",
      "Epoch:  819|steps:   30|Train Avg Loss: 0.9065 |Test Loss: 0.8157|lr = 0.00233\n",
      "Epoch:  819|steps:   60|Train Avg Loss: 0.8846 |Test Loss: 0.8118|lr = 0.00233\n",
      "Epoch:  820|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8144|lr = 0.00233\n",
      "Epoch:  820|steps:   60|Train Avg Loss: 0.8875 |Test Loss: 0.8137|lr = 0.00233\n",
      "Epoch:  821|steps:   30|Train Avg Loss: 0.9094 |Test Loss: 0.8157|lr = 0.00233\n",
      "Epoch:  821|steps:   60|Train Avg Loss: 0.8758 |Test Loss: 0.8135|lr = 0.00233\n",
      "Epoch:  822|steps:   30|Train Avg Loss: 0.8659 |Test Loss: 0.8123|lr = 0.00233\n",
      "Epoch:  822|steps:   60|Train Avg Loss: 0.8980 |Test Loss: 0.8134|lr = 0.00233\n",
      "Epoch:  823|steps:   30|Train Avg Loss: 0.8542 |Test Loss: 0.8075|lr = 0.00233\n",
      "Epoch:  823|steps:   60|Train Avg Loss: 0.9136 |Test Loss: 0.8164|lr = 0.00233\n",
      "Epoch:  824|steps:   30|Train Avg Loss: 0.8990 |Test Loss: 0.8167|lr = 0.00233\n",
      "Epoch:  824|steps:   60|Train Avg Loss: 0.8859 |Test Loss: 0.8137|lr = 0.00233\n",
      "Epoch:  825|steps:   30|Train Avg Loss: 0.8983 |Test Loss: 0.8163|lr = 0.00233\n",
      "Epoch:  825|steps:   60|Train Avg Loss: 0.8925 |Test Loss: 0.8133|lr = 0.00233\n",
      "Epoch:  826|steps:   30|Train Avg Loss: 0.8763 |Test Loss: 0.8110|lr = 0.00233\n",
      "Epoch:  826|steps:   60|Train Avg Loss: 0.9011 |Test Loss: 0.8177|lr = 0.00233\n",
      "Epoch:  827|steps:   30|Train Avg Loss: 0.8986 |Test Loss: 0.8139|lr = 0.00233\n",
      "Epoch:  827|steps:   60|Train Avg Loss: 0.8773 |Test Loss: 0.8142|lr = 0.00233\n",
      "Epoch:  828|steps:   30|Train Avg Loss: 0.8981 |Test Loss: 0.8159|lr = 0.00233\n",
      "Epoch:  828|steps:   60|Train Avg Loss: 0.8821 |Test Loss: 0.8125|lr = 0.00233\n",
      "Epoch:  829|steps:   30|Train Avg Loss: 0.8810 |Test Loss: 0.8117|lr = 0.00233\n",
      "Epoch:  829|steps:   60|Train Avg Loss: 0.9043 |Test Loss: 0.8171|lr = 0.00233\n",
      "Epoch:  830|steps:   30|Train Avg Loss: 0.8812 |Test Loss: 0.8129|lr = 0.00229\n",
      "Epoch:  830|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8126|lr = 0.00229\n",
      "Epoch:  831|steps:   30|Train Avg Loss: 0.9083 |Test Loss: 0.8174|lr = 0.00229\n",
      "Epoch:  831|steps:   60|Train Avg Loss: 0.8711 |Test Loss: 0.8131|lr = 0.00229\n",
      "Epoch:  832|steps:   30|Train Avg Loss: 0.8994 |Test Loss: 0.8138|lr = 0.00229\n",
      "Epoch:  832|steps:   60|Train Avg Loss: 0.8822 |Test Loss: 0.8136|lr = 0.00229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  833|steps:   30|Train Avg Loss: 0.8838 |Test Loss: 0.8130|lr = 0.00229\n",
      "Epoch:  833|steps:   60|Train Avg Loss: 0.8771 |Test Loss: 0.8112|lr = 0.00229\n",
      "Epoch:  834|steps:   30|Train Avg Loss: 0.9174 |Test Loss: 0.8236|lr = 0.00229\n",
      "Epoch:  834|steps:   60|Train Avg Loss: 0.8679 |Test Loss: 0.8107|lr = 0.00229\n",
      "Epoch:  835|steps:   30|Train Avg Loss: 0.8930 |Test Loss: 0.8137|lr = 0.00229\n",
      "Epoch:  835|steps:   60|Train Avg Loss: 0.8786 |Test Loss: 0.8110|lr = 0.00229\n",
      "Epoch:  836|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8133|lr = 0.00229\n",
      "Epoch:  836|steps:   60|Train Avg Loss: 0.8823 |Test Loss: 0.8132|lr = 0.00229\n",
      "Epoch:  837|steps:   30|Train Avg Loss: 0.9009 |Test Loss: 0.8167|lr = 0.00229\n",
      "Epoch:  837|steps:   60|Train Avg Loss: 0.8768 |Test Loss: 0.8129|lr = 0.00229\n",
      "Epoch:  838|steps:   30|Train Avg Loss: 0.8910 |Test Loss: 0.8145|lr = 0.00229\n",
      "Epoch:  838|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8148|lr = 0.00229\n",
      "Epoch:  839|steps:   30|Train Avg Loss: 0.8886 |Test Loss: 0.8121|lr = 0.00229\n",
      "Epoch:  839|steps:   60|Train Avg Loss: 0.8885 |Test Loss: 0.8140|lr = 0.00229\n",
      "Epoch:  840|steps:   30|Train Avg Loss: 0.9090 |Test Loss: 0.8191|lr = 0.00229\n",
      "Epoch:  840|steps:   60|Train Avg Loss: 0.8753 |Test Loss: 0.8125|lr = 0.00229\n",
      "Epoch:  841|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8139|lr = 0.00224\n",
      "Epoch:  841|steps:   60|Train Avg Loss: 0.8909 |Test Loss: 0.8130|lr = 0.00224\n",
      "Epoch:  842|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8130|lr = 0.00224\n",
      "Epoch:  842|steps:   60|Train Avg Loss: 0.9017 |Test Loss: 0.8151|lr = 0.00224\n",
      "Epoch:  843|steps:   30|Train Avg Loss: 0.9084 |Test Loss: 0.8177|lr = 0.00224\n",
      "Epoch:  843|steps:   60|Train Avg Loss: 0.8864 |Test Loss: 0.8141|lr = 0.00224\n",
      "Epoch:  844|steps:   30|Train Avg Loss: 0.8809 |Test Loss: 0.8112|lr = 0.00224\n",
      "Epoch:  844|steps:   60|Train Avg Loss: 0.8971 |Test Loss: 0.8133|lr = 0.00224\n",
      "Epoch:  845|steps:   30|Train Avg Loss: 0.8899 |Test Loss: 0.8142|lr = 0.00224\n",
      "Epoch:  845|steps:   60|Train Avg Loss: 0.8741 |Test Loss: 0.8097|lr = 0.00224\n",
      "Epoch:  846|steps:   30|Train Avg Loss: 0.8896 |Test Loss: 0.8130|lr = 0.00224\n",
      "Epoch:  846|steps:   60|Train Avg Loss: 0.8948 |Test Loss: 0.8115|lr = 0.00224\n",
      "Epoch:  847|steps:   30|Train Avg Loss: 0.9074 |Test Loss: 0.8173|lr = 0.00224\n",
      "Epoch:  847|steps:   60|Train Avg Loss: 0.8741 |Test Loss: 0.8096|lr = 0.00224\n",
      "Epoch:  848|steps:   30|Train Avg Loss: 0.8816 |Test Loss: 0.8123|lr = 0.00224\n",
      "Epoch:  848|steps:   60|Train Avg Loss: 0.8903 |Test Loss: 0.8105|lr = 0.00224\n",
      "Epoch:  849|steps:   30|Train Avg Loss: 0.8779 |Test Loss: 0.8133|lr = 0.00224\n",
      "Epoch:  849|steps:   60|Train Avg Loss: 0.8998 |Test Loss: 0.8156|lr = 0.00224\n",
      "Epoch:  850|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8123|lr = 0.00224\n",
      "Epoch:  850|steps:   60|Train Avg Loss: 0.8896 |Test Loss: 0.8162|lr = 0.00224\n",
      "Epoch:  851|steps:   30|Train Avg Loss: 0.9033 |Test Loss: 0.8179|lr = 0.00224\n",
      "Epoch:  851|steps:   60|Train Avg Loss: 0.8747 |Test Loss: 0.8111|lr = 0.00224\n",
      "Epoch:  852|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8120|lr = 0.00220\n",
      "Epoch:  852|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8108|lr = 0.00220\n",
      "Epoch:  853|steps:   30|Train Avg Loss: 0.8787 |Test Loss: 0.8113|lr = 0.00220\n",
      "Epoch:  853|steps:   60|Train Avg Loss: 0.8955 |Test Loss: 0.8156|lr = 0.00220\n",
      "Epoch:  854|steps:   30|Train Avg Loss: 0.8895 |Test Loss: 0.8153|lr = 0.00220\n",
      "Epoch:  854|steps:   60|Train Avg Loss: 0.8979 |Test Loss: 0.8135|lr = 0.00220\n",
      "Epoch:  855|steps:   30|Train Avg Loss: 0.8972 |Test Loss: 0.8150|lr = 0.00220\n",
      "Epoch:  855|steps:   60|Train Avg Loss: 0.8852 |Test Loss: 0.8098|lr = 0.00220\n",
      "Epoch:  856|steps:   30|Train Avg Loss: 0.8801 |Test Loss: 0.8171|lr = 0.00220\n",
      "Epoch:  856|steps:   60|Train Avg Loss: 0.8963 |Test Loss: 0.8093|lr = 0.00220\n",
      "Epoch:  857|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8150|lr = 0.00220\n",
      "Epoch:  857|steps:   60|Train Avg Loss: 0.8837 |Test Loss: 0.8148|lr = 0.00220\n",
      "Epoch:  858|steps:   30|Train Avg Loss: 0.8969 |Test Loss: 0.8160|lr = 0.00220\n",
      "Epoch:  858|steps:   60|Train Avg Loss: 0.8844 |Test Loss: 0.8124|lr = 0.00220\n",
      "Epoch:  859|steps:   30|Train Avg Loss: 0.8814 |Test Loss: 0.8119|lr = 0.00220\n",
      "Epoch:  859|steps:   60|Train Avg Loss: 0.8762 |Test Loss: 0.8115|lr = 0.00220\n",
      "Epoch:  860|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8171|lr = 0.00220\n",
      "Epoch:  860|steps:   60|Train Avg Loss: 0.8819 |Test Loss: 0.8134|lr = 0.00220\n",
      "Epoch:  861|steps:   30|Train Avg Loss: 0.9007 |Test Loss: 0.8146|lr = 0.00220\n",
      "Epoch:  861|steps:   60|Train Avg Loss: 0.8741 |Test Loss: 0.8091|lr = 0.00220\n",
      "Epoch:  862|steps:   30|Train Avg Loss: 0.8784 |Test Loss: 0.8142|lr = 0.00220\n",
      "Epoch:  862|steps:   60|Train Avg Loss: 0.9079 |Test Loss: 0.8181|lr = 0.00220\n",
      "Epoch:  863|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8139|lr = 0.00215\n",
      "Epoch:  863|steps:   60|Train Avg Loss: 0.8931 |Test Loss: 0.8144|lr = 0.00215\n",
      "Epoch:  864|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8134|lr = 0.00215\n",
      "Epoch:  864|steps:   60|Train Avg Loss: 0.8939 |Test Loss: 0.8116|lr = 0.00215\n",
      "Epoch:  865|steps:   30|Train Avg Loss: 0.9055 |Test Loss: 0.8179|lr = 0.00215\n",
      "Epoch:  865|steps:   60|Train Avg Loss: 0.8774 |Test Loss: 0.8130|lr = 0.00215\n",
      "Epoch:  866|steps:   30|Train Avg Loss: 0.8805 |Test Loss: 0.8118|lr = 0.00215\n",
      "Epoch:  866|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8112|lr = 0.00215\n",
      "Epoch:  867|steps:   30|Train Avg Loss: 0.8676 |Test Loss: 0.8075|lr = 0.00215\n",
      "Epoch:  867|steps:   60|Train Avg Loss: 0.8956 |Test Loss: 0.8173|lr = 0.00215\n",
      "Epoch:  868|steps:   30|Train Avg Loss: 0.8845 |Test Loss: 0.8101|lr = 0.00215\n",
      "Epoch:  868|steps:   60|Train Avg Loss: 0.9021 |Test Loss: 0.8175|lr = 0.00215\n",
      "Epoch:  869|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8145|lr = 0.00215\n",
      "Epoch:  869|steps:   60|Train Avg Loss: 0.8997 |Test Loss: 0.8151|lr = 0.00215\n",
      "Epoch:  870|steps:   30|Train Avg Loss: 0.8817 |Test Loss: 0.8124|lr = 0.00215\n",
      "Epoch:  870|steps:   60|Train Avg Loss: 0.9019 |Test Loss: 0.8158|lr = 0.00215\n",
      "Epoch:  871|steps:   30|Train Avg Loss: 0.8943 |Test Loss: 0.8140|lr = 0.00215\n",
      "Epoch:  871|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8147|lr = 0.00215\n",
      "Epoch:  872|steps:   30|Train Avg Loss: 0.8699 |Test Loss: 0.8084|lr = 0.00215\n",
      "Epoch:  872|steps:   60|Train Avg Loss: 0.9053 |Test Loss: 0.8138|lr = 0.00215\n",
      "Epoch:  873|steps:   30|Train Avg Loss: 0.8823 |Test Loss: 0.8115|lr = 0.00215\n",
      "Epoch:  873|steps:   60|Train Avg Loss: 0.8959 |Test Loss: 0.8130|lr = 0.00215\n",
      "Epoch:  874|steps:   30|Train Avg Loss: 0.8783 |Test Loss: 0.8147|lr = 0.00211\n",
      "Epoch:  874|steps:   60|Train Avg Loss: 0.8960 |Test Loss: 0.8122|lr = 0.00211\n",
      "Epoch:  875|steps:   30|Train Avg Loss: 0.8704 |Test Loss: 0.8110|lr = 0.00211\n",
      "Epoch:  875|steps:   60|Train Avg Loss: 0.8870 |Test Loss: 0.8101|lr = 0.00211\n",
      "Epoch:  876|steps:   30|Train Avg Loss: 0.8826 |Test Loss: 0.8149|lr = 0.00211\n",
      "Epoch:  876|steps:   60|Train Avg Loss: 0.8896 |Test Loss: 0.8127|lr = 0.00211\n",
      "Epoch:  877|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8142|lr = 0.00211\n",
      "Epoch:  877|steps:   60|Train Avg Loss: 0.8916 |Test Loss: 0.8110|lr = 0.00211\n",
      "Epoch:  878|steps:   30|Train Avg Loss: 0.8967 |Test Loss: 0.8152|lr = 0.00211\n",
      "Epoch:  878|steps:   60|Train Avg Loss: 0.8757 |Test Loss: 0.8110|lr = 0.00211\n",
      "Epoch:  879|steps:   30|Train Avg Loss: 0.8873 |Test Loss: 0.8150|lr = 0.00211\n",
      "Epoch:  879|steps:   60|Train Avg Loss: 0.8803 |Test Loss: 0.8114|lr = 0.00211\n",
      "Epoch:  880|steps:   30|Train Avg Loss: 0.8915 |Test Loss: 0.8136|lr = 0.00211\n",
      "Epoch:  880|steps:   60|Train Avg Loss: 0.8852 |Test Loss: 0.8095|lr = 0.00211\n",
      "Epoch:  881|steps:   30|Train Avg Loss: 0.8961 |Test Loss: 0.8143|lr = 0.00211\n",
      "Epoch:  881|steps:   60|Train Avg Loss: 0.8823 |Test Loss: 0.8151|lr = 0.00211\n",
      "Epoch:  882|steps:   30|Train Avg Loss: 0.8882 |Test Loss: 0.8131|lr = 0.00211\n",
      "Epoch:  882|steps:   60|Train Avg Loss: 0.8885 |Test Loss: 0.8130|lr = 0.00211\n",
      "Epoch:  883|steps:   30|Train Avg Loss: 0.9200 |Test Loss: 0.8201|lr = 0.00211\n",
      "Epoch:  883|steps:   60|Train Avg Loss: 0.8653 |Test Loss: 0.8119|lr = 0.00211\n",
      "Epoch:  884|steps:   30|Train Avg Loss: 0.8916 |Test Loss: 0.8156|lr = 0.00211\n",
      "Epoch:  884|steps:   60|Train Avg Loss: 0.8893 |Test Loss: 0.8130|lr = 0.00211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  885|steps:   30|Train Avg Loss: 0.9060 |Test Loss: 0.8172|lr = 0.00207\n",
      "Epoch:  885|steps:   60|Train Avg Loss: 0.8716 |Test Loss: 0.8095|lr = 0.00207\n",
      "Epoch:  886|steps:   30|Train Avg Loss: 0.8844 |Test Loss: 0.8102|lr = 0.00207\n",
      "Epoch:  886|steps:   60|Train Avg Loss: 0.8967 |Test Loss: 0.8135|lr = 0.00207\n",
      "Epoch:  887|steps:   30|Train Avg Loss: 0.8926 |Test Loss: 0.8178|lr = 0.00207\n",
      "Epoch:  887|steps:   60|Train Avg Loss: 0.8772 |Test Loss: 0.8098|lr = 0.00207\n",
      "Epoch:  888|steps:   30|Train Avg Loss: 0.9021 |Test Loss: 0.8164|lr = 0.00207\n",
      "Epoch:  888|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8122|lr = 0.00207\n",
      "Epoch:  889|steps:   30|Train Avg Loss: 0.8887 |Test Loss: 0.8113|lr = 0.00207\n",
      "Epoch:  889|steps:   60|Train Avg Loss: 0.8782 |Test Loss: 0.8115|lr = 0.00207\n",
      "Epoch:  890|steps:   30|Train Avg Loss: 0.8775 |Test Loss: 0.8111|lr = 0.00207\n",
      "Epoch:  890|steps:   60|Train Avg Loss: 0.8991 |Test Loss: 0.8129|lr = 0.00207\n",
      "Epoch:  891|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8149|lr = 0.00207\n",
      "Epoch:  891|steps:   60|Train Avg Loss: 0.8800 |Test Loss: 0.8148|lr = 0.00207\n",
      "Epoch:  892|steps:   30|Train Avg Loss: 0.8921 |Test Loss: 0.8133|lr = 0.00207\n",
      "Epoch:  892|steps:   60|Train Avg Loss: 0.8876 |Test Loss: 0.8129|lr = 0.00207\n",
      "Epoch:  893|steps:   30|Train Avg Loss: 0.8914 |Test Loss: 0.8135|lr = 0.00207\n",
      "Epoch:  893|steps:   60|Train Avg Loss: 0.8974 |Test Loss: 0.8172|lr = 0.00207\n",
      "Epoch:  894|steps:   30|Train Avg Loss: 0.8999 |Test Loss: 0.8143|lr = 0.00207\n",
      "Epoch:  894|steps:   60|Train Avg Loss: 0.8806 |Test Loss: 0.8101|lr = 0.00207\n",
      "Epoch:  895|steps:   30|Train Avg Loss: 0.8626 |Test Loss: 0.8099|lr = 0.00207\n",
      "Epoch:  895|steps:   60|Train Avg Loss: 0.9149 |Test Loss: 0.8190|lr = 0.00207\n",
      "Epoch:  896|steps:   30|Train Avg Loss: 0.8932 |Test Loss: 0.8148|lr = 0.00203\n",
      "Epoch:  896|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8117|lr = 0.00203\n",
      "Epoch:  897|steps:   30|Train Avg Loss: 0.8964 |Test Loss: 0.8149|lr = 0.00203\n",
      "Epoch:  897|steps:   60|Train Avg Loss: 0.8809 |Test Loss: 0.8122|lr = 0.00203\n",
      "Epoch:  898|steps:   30|Train Avg Loss: 0.8862 |Test Loss: 0.8140|lr = 0.00203\n",
      "Epoch:  898|steps:   60|Train Avg Loss: 0.8948 |Test Loss: 0.8130|lr = 0.00203\n",
      "Epoch:  899|steps:   30|Train Avg Loss: 0.8916 |Test Loss: 0.8128|lr = 0.00203\n",
      "Epoch:  899|steps:   60|Train Avg Loss: 0.8823 |Test Loss: 0.8116|lr = 0.00203\n",
      "Epoch:  900|steps:   30|Train Avg Loss: 0.9100 |Test Loss: 0.8182|lr = 0.00203\n",
      "Epoch:  900|steps:   60|Train Avg Loss: 0.8718 |Test Loss: 0.8131|lr = 0.00203\n",
      "Epoch:  901|steps:   30|Train Avg Loss: 0.8650 |Test Loss: 0.8081|lr = 0.00203\n",
      "Epoch:  901|steps:   60|Train Avg Loss: 0.8993 |Test Loss: 0.8138|lr = 0.00203\n",
      "Epoch:  902|steps:   30|Train Avg Loss: 0.8813 |Test Loss: 0.8148|lr = 0.00203\n",
      "Epoch:  902|steps:   60|Train Avg Loss: 0.8934 |Test Loss: 0.8136|lr = 0.00203\n",
      "Epoch:  903|steps:   30|Train Avg Loss: 0.8916 |Test Loss: 0.8127|lr = 0.00203\n",
      "Epoch:  903|steps:   60|Train Avg Loss: 0.8831 |Test Loss: 0.8126|lr = 0.00203\n",
      "Epoch:  904|steps:   30|Train Avg Loss: 0.8980 |Test Loss: 0.8121|lr = 0.00203\n",
      "Epoch:  904|steps:   60|Train Avg Loss: 0.8825 |Test Loss: 0.8167|lr = 0.00203\n",
      "Epoch:  905|steps:   30|Train Avg Loss: 0.8993 |Test Loss: 0.8156|lr = 0.00203\n",
      "Epoch:  905|steps:   60|Train Avg Loss: 0.8719 |Test Loss: 0.8112|lr = 0.00203\n",
      "Epoch:  906|steps:   30|Train Avg Loss: 0.9025 |Test Loss: 0.8180|lr = 0.00203\n",
      "Epoch:  906|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8116|lr = 0.00203\n",
      "Epoch:  907|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8141|lr = 0.00199\n",
      "Epoch:  907|steps:   60|Train Avg Loss: 0.8726 |Test Loss: 0.8091|lr = 0.00199\n",
      "Epoch:  908|steps:   30|Train Avg Loss: 0.8856 |Test Loss: 0.8198|lr = 0.00199\n",
      "Epoch:  908|steps:   60|Train Avg Loss: 0.9019 |Test Loss: 0.8120|lr = 0.00199\n",
      "Epoch:  909|steps:   30|Train Avg Loss: 0.9052 |Test Loss: 0.8171|lr = 0.00199\n",
      "Epoch:  909|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8111|lr = 0.00199\n",
      "Epoch:  910|steps:   30|Train Avg Loss: 0.8806 |Test Loss: 0.8095|lr = 0.00199\n",
      "Epoch:  910|steps:   60|Train Avg Loss: 0.8877 |Test Loss: 0.8145|lr = 0.00199\n",
      "Epoch:  911|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8142|lr = 0.00199\n",
      "Epoch:  911|steps:   60|Train Avg Loss: 0.8738 |Test Loss: 0.8100|lr = 0.00199\n",
      "Epoch:  912|steps:   30|Train Avg Loss: 0.8969 |Test Loss: 0.8112|lr = 0.00199\n",
      "Epoch:  912|steps:   60|Train Avg Loss: 0.8729 |Test Loss: 0.8125|lr = 0.00199\n",
      "Epoch:  913|steps:   30|Train Avg Loss: 0.8807 |Test Loss: 0.8147|lr = 0.00199\n",
      "Epoch:  913|steps:   60|Train Avg Loss: 0.8989 |Test Loss: 0.8144|lr = 0.00199\n",
      "Epoch:  914|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8158|lr = 0.00199\n",
      "Epoch:  914|steps:   60|Train Avg Loss: 0.8912 |Test Loss: 0.8121|lr = 0.00199\n",
      "Epoch:  915|steps:   30|Train Avg Loss: 0.8793 |Test Loss: 0.8116|lr = 0.00199\n",
      "Epoch:  915|steps:   60|Train Avg Loss: 0.9044 |Test Loss: 0.8146|lr = 0.00199\n",
      "Epoch:  916|steps:   30|Train Avg Loss: 0.8956 |Test Loss: 0.8109|lr = 0.00199\n",
      "Epoch:  916|steps:   60|Train Avg Loss: 0.8731 |Test Loss: 0.8122|lr = 0.00199\n",
      "Epoch:  917|steps:   30|Train Avg Loss: 0.8849 |Test Loss: 0.8138|lr = 0.00199\n",
      "Epoch:  917|steps:   60|Train Avg Loss: 0.8821 |Test Loss: 0.8114|lr = 0.00199\n",
      "Epoch:  918|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8139|lr = 0.00195\n",
      "Epoch:  918|steps:   60|Train Avg Loss: 0.8847 |Test Loss: 0.8106|lr = 0.00195\n",
      "Epoch:  919|steps:   30|Train Avg Loss: 0.8699 |Test Loss: 0.8098|lr = 0.00195\n",
      "Epoch:  919|steps:   60|Train Avg Loss: 0.8998 |Test Loss: 0.8183|lr = 0.00195\n",
      "Epoch:  920|steps:   30|Train Avg Loss: 0.9007 |Test Loss: 0.8159|lr = 0.00195\n",
      "Epoch:  920|steps:   60|Train Avg Loss: 0.8703 |Test Loss: 0.8100|lr = 0.00195\n",
      "Epoch:  921|steps:   30|Train Avg Loss: 0.8773 |Test Loss: 0.8148|lr = 0.00195\n",
      "Epoch:  921|steps:   60|Train Avg Loss: 0.8943 |Test Loss: 0.8115|lr = 0.00195\n",
      "Epoch:  922|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8151|lr = 0.00195\n",
      "Epoch:  922|steps:   60|Train Avg Loss: 0.8923 |Test Loss: 0.8128|lr = 0.00195\n",
      "Epoch:  923|steps:   30|Train Avg Loss: 0.8698 |Test Loss: 0.8093|lr = 0.00195\n",
      "Epoch:  923|steps:   60|Train Avg Loss: 0.9134 |Test Loss: 0.8157|lr = 0.00195\n",
      "Epoch:  924|steps:   30|Train Avg Loss: 0.8828 |Test Loss: 0.8103|lr = 0.00195\n",
      "Epoch:  924|steps:   60|Train Avg Loss: 0.8882 |Test Loss: 0.8084|lr = 0.00195\n",
      "Epoch:  925|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8132|lr = 0.00195\n",
      "Epoch:  925|steps:   60|Train Avg Loss: 0.8897 |Test Loss: 0.8113|lr = 0.00195\n",
      "Epoch:  926|steps:   30|Train Avg Loss: 0.8764 |Test Loss: 0.8075|lr = 0.00195\n",
      "Epoch:  926|steps:   60|Train Avg Loss: 0.9081 |Test Loss: 0.8239|lr = 0.00195\n",
      "Epoch:  927|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8136|lr = 0.00195\n",
      "Epoch:  927|steps:   60|Train Avg Loss: 0.8780 |Test Loss: 0.8104|lr = 0.00195\n",
      "Epoch:  928|steps:   30|Train Avg Loss: 0.9007 |Test Loss: 0.8177|lr = 0.00195\n",
      "Epoch:  928|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8124|lr = 0.00195\n",
      "Epoch:  929|steps:   30|Train Avg Loss: 0.8854 |Test Loss: 0.8117|lr = 0.00191\n",
      "Epoch:  929|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8121|lr = 0.00191\n",
      "Epoch:  930|steps:   30|Train Avg Loss: 0.8848 |Test Loss: 0.8104|lr = 0.00191\n",
      "Epoch:  930|steps:   60|Train Avg Loss: 0.8827 |Test Loss: 0.8113|lr = 0.00191\n",
      "Epoch:  931|steps:   30|Train Avg Loss: 0.8943 |Test Loss: 0.8151|lr = 0.00191\n",
      "Epoch:  931|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8100|lr = 0.00191\n",
      "Epoch:  932|steps:   30|Train Avg Loss: 0.8760 |Test Loss: 0.8115|lr = 0.00191\n",
      "Epoch:  932|steps:   60|Train Avg Loss: 0.8909 |Test Loss: 0.8138|lr = 0.00191\n",
      "Epoch:  933|steps:   30|Train Avg Loss: 0.8807 |Test Loss: 0.8099|lr = 0.00191\n",
      "Epoch:  933|steps:   60|Train Avg Loss: 0.9022 |Test Loss: 0.8204|lr = 0.00191\n",
      "Epoch:  934|steps:   30|Train Avg Loss: 0.8935 |Test Loss: 0.8137|lr = 0.00191\n",
      "Epoch:  934|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8121|lr = 0.00191\n",
      "Epoch:  935|steps:   30|Train Avg Loss: 0.8671 |Test Loss: 0.8087|lr = 0.00191\n",
      "Epoch:  935|steps:   60|Train Avg Loss: 0.9062 |Test Loss: 0.8163|lr = 0.00191\n",
      "Epoch:  936|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8148|lr = 0.00191\n",
      "Epoch:  936|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8110|lr = 0.00191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  937|steps:   30|Train Avg Loss: 0.8830 |Test Loss: 0.8125|lr = 0.00191\n",
      "Epoch:  937|steps:   60|Train Avg Loss: 0.9054 |Test Loss: 0.8163|lr = 0.00191\n",
      "Epoch:  938|steps:   30|Train Avg Loss: 0.9006 |Test Loss: 0.8141|lr = 0.00191\n",
      "Epoch:  938|steps:   60|Train Avg Loss: 0.8801 |Test Loss: 0.8111|lr = 0.00191\n",
      "Epoch:  939|steps:   30|Train Avg Loss: 0.8783 |Test Loss: 0.8128|lr = 0.00191\n",
      "Epoch:  939|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8121|lr = 0.00191\n",
      "Epoch:  940|steps:   30|Train Avg Loss: 0.8803 |Test Loss: 0.8091|lr = 0.00187\n",
      "Epoch:  940|steps:   60|Train Avg Loss: 0.8942 |Test Loss: 0.8114|lr = 0.00187\n",
      "Epoch:  941|steps:   30|Train Avg Loss: 0.8642 |Test Loss: 0.8087|lr = 0.00187\n",
      "Epoch:  941|steps:   60|Train Avg Loss: 0.9140 |Test Loss: 0.8152|lr = 0.00187\n",
      "Epoch:  942|steps:   30|Train Avg Loss: 0.8733 |Test Loss: 0.8053|lr = 0.00187\n",
      "Epoch:  942|steps:   60|Train Avg Loss: 0.8891 |Test Loss: 0.8118|lr = 0.00187\n",
      "Epoch:  943|steps:   30|Train Avg Loss: 0.8804 |Test Loss: 0.8144|lr = 0.00187\n",
      "Epoch:  943|steps:   60|Train Avg Loss: 0.8914 |Test Loss: 0.8142|lr = 0.00187\n",
      "Epoch:  944|steps:   30|Train Avg Loss: 0.8784 |Test Loss: 0.8125|lr = 0.00187\n",
      "Epoch:  944|steps:   60|Train Avg Loss: 0.9039 |Test Loss: 0.8145|lr = 0.00187\n",
      "Epoch:  945|steps:   30|Train Avg Loss: 0.8921 |Test Loss: 0.8126|lr = 0.00187\n",
      "Epoch:  945|steps:   60|Train Avg Loss: 0.8860 |Test Loss: 0.8119|lr = 0.00187\n",
      "Epoch:  946|steps:   30|Train Avg Loss: 0.8956 |Test Loss: 0.8166|lr = 0.00187\n",
      "Epoch:  946|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8128|lr = 0.00187\n",
      "Epoch:  947|steps:   30|Train Avg Loss: 0.8970 |Test Loss: 0.8133|lr = 0.00187\n",
      "Epoch:  947|steps:   60|Train Avg Loss: 0.8789 |Test Loss: 0.8129|lr = 0.00187\n",
      "Epoch:  948|steps:   30|Train Avg Loss: 0.8904 |Test Loss: 0.8126|lr = 0.00187\n",
      "Epoch:  948|steps:   60|Train Avg Loss: 0.8999 |Test Loss: 0.8135|lr = 0.00187\n",
      "Epoch:  949|steps:   30|Train Avg Loss: 0.8985 |Test Loss: 0.8144|lr = 0.00187\n",
      "Epoch:  949|steps:   60|Train Avg Loss: 0.8852 |Test Loss: 0.8140|lr = 0.00187\n",
      "Epoch:  950|steps:   30|Train Avg Loss: 0.8844 |Test Loss: 0.8123|lr = 0.00187\n",
      "Epoch:  950|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8115|lr = 0.00187\n",
      "Epoch:  951|steps:   30|Train Avg Loss: 0.8790 |Test Loss: 0.8122|lr = 0.00183\n",
      "Epoch:  951|steps:   60|Train Avg Loss: 0.8935 |Test Loss: 0.8136|lr = 0.00183\n",
      "Epoch:  952|steps:   30|Train Avg Loss: 0.8835 |Test Loss: 0.8125|lr = 0.00183\n",
      "Epoch:  952|steps:   60|Train Avg Loss: 0.8940 |Test Loss: 0.8143|lr = 0.00183\n",
      "Epoch:  953|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.8163|lr = 0.00183\n",
      "Epoch:  953|steps:   60|Train Avg Loss: 0.8831 |Test Loss: 0.8145|lr = 0.00183\n",
      "Epoch:  954|steps:   30|Train Avg Loss: 0.8823 |Test Loss: 0.8135|lr = 0.00183\n",
      "Epoch:  954|steps:   60|Train Avg Loss: 0.8783 |Test Loss: 0.8113|lr = 0.00183\n",
      "Epoch:  955|steps:   30|Train Avg Loss: 0.8765 |Test Loss: 0.8086|lr = 0.00183\n",
      "Epoch:  955|steps:   60|Train Avg Loss: 0.8998 |Test Loss: 0.8128|lr = 0.00183\n",
      "Epoch:  956|steps:   30|Train Avg Loss: 0.8873 |Test Loss: 0.8116|lr = 0.00183\n",
      "Epoch:  956|steps:   60|Train Avg Loss: 0.8839 |Test Loss: 0.8144|lr = 0.00183\n",
      "Epoch:  957|steps:   30|Train Avg Loss: 0.8738 |Test Loss: 0.8098|lr = 0.00183\n",
      "Epoch:  957|steps:   60|Train Avg Loss: 0.9107 |Test Loss: 0.8134|lr = 0.00183\n",
      "Epoch:  958|steps:   30|Train Avg Loss: 0.8994 |Test Loss: 0.8149|lr = 0.00183\n",
      "Epoch:  958|steps:   60|Train Avg Loss: 0.8825 |Test Loss: 0.8132|lr = 0.00183\n",
      "Epoch:  959|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8125|lr = 0.00183\n",
      "Epoch:  959|steps:   60|Train Avg Loss: 0.8971 |Test Loss: 0.8133|lr = 0.00183\n",
      "Epoch:  960|steps:   30|Train Avg Loss: 0.8910 |Test Loss: 0.8111|lr = 0.00183\n",
      "Epoch:  960|steps:   60|Train Avg Loss: 0.8855 |Test Loss: 0.8104|lr = 0.00183\n",
      "Epoch:  961|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8118|lr = 0.00183\n",
      "Epoch:  961|steps:   60|Train Avg Loss: 0.8789 |Test Loss: 0.8125|lr = 0.00183\n",
      "Epoch:  962|steps:   30|Train Avg Loss: 0.8908 |Test Loss: 0.8135|lr = 0.00180\n",
      "Epoch:  962|steps:   60|Train Avg Loss: 0.8782 |Test Loss: 0.8104|lr = 0.00180\n",
      "Epoch:  963|steps:   30|Train Avg Loss: 0.8940 |Test Loss: 0.8161|lr = 0.00180\n",
      "Epoch:  963|steps:   60|Train Avg Loss: 0.8875 |Test Loss: 0.8128|lr = 0.00180\n",
      "Epoch:  964|steps:   30|Train Avg Loss: 0.8896 |Test Loss: 0.8105|lr = 0.00180\n",
      "Epoch:  964|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8111|lr = 0.00180\n",
      "Epoch:  965|steps:   30|Train Avg Loss: 0.8992 |Test Loss: 0.8109|lr = 0.00180\n",
      "Epoch:  965|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8154|lr = 0.00180\n",
      "Epoch:  966|steps:   30|Train Avg Loss: 0.8829 |Test Loss: 0.8103|lr = 0.00180\n",
      "Epoch:  966|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8150|lr = 0.00180\n",
      "Epoch:  967|steps:   30|Train Avg Loss: 0.9023 |Test Loss: 0.8154|lr = 0.00180\n",
      "Epoch:  967|steps:   60|Train Avg Loss: 0.8725 |Test Loss: 0.8120|lr = 0.00180\n",
      "Epoch:  968|steps:   30|Train Avg Loss: 0.8727 |Test Loss: 0.8090|lr = 0.00180\n",
      "Epoch:  968|steps:   60|Train Avg Loss: 0.9004 |Test Loss: 0.8153|lr = 0.00180\n",
      "Epoch:  969|steps:   30|Train Avg Loss: 0.8772 |Test Loss: 0.8113|lr = 0.00180\n",
      "Epoch:  969|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8143|lr = 0.00180\n",
      "Epoch:  970|steps:   30|Train Avg Loss: 0.8985 |Test Loss: 0.8167|lr = 0.00180\n",
      "Epoch:  970|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8145|lr = 0.00180\n",
      "Epoch:  971|steps:   30|Train Avg Loss: 0.8791 |Test Loss: 0.8085|lr = 0.00180\n",
      "Epoch:  971|steps:   60|Train Avg Loss: 0.8915 |Test Loss: 0.8142|lr = 0.00180\n",
      "Epoch:  972|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8142|lr = 0.00180\n",
      "Epoch:  972|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 0.8133|lr = 0.00180\n",
      "Epoch:  973|steps:   30|Train Avg Loss: 0.8940 |Test Loss: 0.8092|lr = 0.00176\n",
      "Epoch:  973|steps:   60|Train Avg Loss: 0.8875 |Test Loss: 0.8151|lr = 0.00176\n",
      "Epoch:  974|steps:   30|Train Avg Loss: 0.8951 |Test Loss: 0.8158|lr = 0.00176\n",
      "Epoch:  974|steps:   60|Train Avg Loss: 0.8845 |Test Loss: 0.8116|lr = 0.00176\n",
      "Epoch:  975|steps:   30|Train Avg Loss: 0.8719 |Test Loss: 0.8095|lr = 0.00176\n",
      "Epoch:  975|steps:   60|Train Avg Loss: 0.8945 |Test Loss: 0.8116|lr = 0.00176\n",
      "Epoch:  976|steps:   30|Train Avg Loss: 0.8817 |Test Loss: 0.8109|lr = 0.00176\n",
      "Epoch:  976|steps:   60|Train Avg Loss: 0.8824 |Test Loss: 0.8116|lr = 0.00176\n",
      "Epoch:  977|steps:   30|Train Avg Loss: 0.8840 |Test Loss: 0.8129|lr = 0.00176\n",
      "Epoch:  977|steps:   60|Train Avg Loss: 0.8807 |Test Loss: 0.8127|lr = 0.00176\n",
      "Epoch:  978|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8141|lr = 0.00176\n",
      "Epoch:  978|steps:   60|Train Avg Loss: 0.8878 |Test Loss: 0.8132|lr = 0.00176\n",
      "Epoch:  979|steps:   30|Train Avg Loss: 0.8921 |Test Loss: 0.8140|lr = 0.00176\n",
      "Epoch:  979|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8136|lr = 0.00176\n",
      "Epoch:  980|steps:   30|Train Avg Loss: 0.8841 |Test Loss: 0.8153|lr = 0.00176\n",
      "Epoch:  980|steps:   60|Train Avg Loss: 0.8849 |Test Loss: 0.8110|lr = 0.00176\n",
      "Epoch:  981|steps:   30|Train Avg Loss: 0.8774 |Test Loss: 0.8104|lr = 0.00176\n",
      "Epoch:  981|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 0.8102|lr = 0.00176\n",
      "Epoch:  982|steps:   30|Train Avg Loss: 0.8956 |Test Loss: 0.8192|lr = 0.00176\n",
      "Epoch:  982|steps:   60|Train Avg Loss: 0.8787 |Test Loss: 0.8098|lr = 0.00176\n",
      "Epoch:  983|steps:   30|Train Avg Loss: 0.8792 |Test Loss: 0.8089|lr = 0.00176\n",
      "Epoch:  983|steps:   60|Train Avg Loss: 0.8876 |Test Loss: 0.8129|lr = 0.00176\n",
      "Epoch:  984|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8157|lr = 0.00172\n",
      "Epoch:  984|steps:   60|Train Avg Loss: 0.8705 |Test Loss: 0.8090|lr = 0.00172\n",
      "Epoch:  985|steps:   30|Train Avg Loss: 0.8939 |Test Loss: 0.8138|lr = 0.00172\n",
      "Epoch:  985|steps:   60|Train Avg Loss: 0.8865 |Test Loss: 0.8132|lr = 0.00172\n",
      "Epoch:  986|steps:   30|Train Avg Loss: 0.8812 |Test Loss: 0.8090|lr = 0.00172\n",
      "Epoch:  986|steps:   60|Train Avg Loss: 0.8877 |Test Loss: 0.8117|lr = 0.00172\n",
      "Epoch:  987|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8150|lr = 0.00172\n",
      "Epoch:  987|steps:   60|Train Avg Loss: 0.8837 |Test Loss: 0.8123|lr = 0.00172\n",
      "Epoch:  988|steps:   30|Train Avg Loss: 0.8755 |Test Loss: 0.8107|lr = 0.00172\n",
      "Epoch:  988|steps:   60|Train Avg Loss: 0.8945 |Test Loss: 0.8111|lr = 0.00172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  989|steps:   30|Train Avg Loss: 0.8629 |Test Loss: 0.8100|lr = 0.00172\n",
      "Epoch:  989|steps:   60|Train Avg Loss: 0.9052 |Test Loss: 0.8133|lr = 0.00172\n",
      "Epoch:  990|steps:   30|Train Avg Loss: 0.8709 |Test Loss: 0.8136|lr = 0.00172\n",
      "Epoch:  990|steps:   60|Train Avg Loss: 0.8959 |Test Loss: 0.8117|lr = 0.00172\n",
      "Epoch:  991|steps:   30|Train Avg Loss: 0.9016 |Test Loss: 0.8145|lr = 0.00172\n",
      "Epoch:  991|steps:   60|Train Avg Loss: 0.8810 |Test Loss: 0.8137|lr = 0.00172\n",
      "Epoch:  992|steps:   30|Train Avg Loss: 0.8955 |Test Loss: 0.8136|lr = 0.00172\n",
      "Epoch:  992|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8127|lr = 0.00172\n",
      "Epoch:  993|steps:   30|Train Avg Loss: 0.9089 |Test Loss: 0.8163|lr = 0.00172\n",
      "Epoch:  993|steps:   60|Train Avg Loss: 0.8709 |Test Loss: 0.8089|lr = 0.00172\n",
      "Epoch:  994|steps:   30|Train Avg Loss: 0.8983 |Test Loss: 0.8116|lr = 0.00172\n",
      "Epoch:  994|steps:   60|Train Avg Loss: 0.8955 |Test Loss: 0.8147|lr = 0.00172\n",
      "Epoch:  995|steps:   30|Train Avg Loss: 0.8805 |Test Loss: 0.8129|lr = 0.00169\n",
      "Epoch:  995|steps:   60|Train Avg Loss: 0.8734 |Test Loss: 0.8079|lr = 0.00169\n",
      "Epoch:  996|steps:   30|Train Avg Loss: 0.8895 |Test Loss: 0.8137|lr = 0.00169\n",
      "Epoch:  996|steps:   60|Train Avg Loss: 0.8956 |Test Loss: 0.8143|lr = 0.00169\n",
      "Epoch:  997|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8125|lr = 0.00169\n",
      "Epoch:  997|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8140|lr = 0.00169\n",
      "Epoch:  998|steps:   30|Train Avg Loss: 0.8976 |Test Loss: 0.8128|lr = 0.00169\n",
      "Epoch:  998|steps:   60|Train Avg Loss: 0.8766 |Test Loss: 0.8108|lr = 0.00169\n",
      "Epoch:  999|steps:   30|Train Avg Loss: 0.8763 |Test Loss: 0.8108|lr = 0.00169\n",
      "Epoch:  999|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8148|lr = 0.00169\n",
      "Epoch: 1000|steps:   30|Train Avg Loss: 0.9013 |Test Loss: 0.8144|lr = 0.00169\n",
      "Epoch: 1000|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8139|lr = 0.00169\n",
      "Epoch: 1001|steps:   30|Train Avg Loss: 0.8973 |Test Loss: 0.8107|lr = 0.00169\n",
      "Epoch: 1001|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8130|lr = 0.00169\n",
      "Epoch: 1002|steps:   30|Train Avg Loss: 0.8895 |Test Loss: 0.8112|lr = 0.00169\n",
      "Epoch: 1002|steps:   60|Train Avg Loss: 0.8661 |Test Loss: 0.8100|lr = 0.00169\n",
      "Epoch: 1003|steps:   30|Train Avg Loss: 0.8738 |Test Loss: 0.8108|lr = 0.00169\n",
      "Epoch: 1003|steps:   60|Train Avg Loss: 0.8878 |Test Loss: 0.8137|lr = 0.00169\n",
      "Epoch: 1004|steps:   30|Train Avg Loss: 0.8828 |Test Loss: 0.8159|lr = 0.00169\n",
      "Epoch: 1004|steps:   60|Train Avg Loss: 0.8962 |Test Loss: 0.8125|lr = 0.00169\n",
      "Epoch: 1005|steps:   30|Train Avg Loss: 0.8926 |Test Loss: 0.8133|lr = 0.00169\n",
      "Epoch: 1005|steps:   60|Train Avg Loss: 0.8870 |Test Loss: 0.8125|lr = 0.00169\n",
      "Epoch: 1006|steps:   30|Train Avg Loss: 0.9014 |Test Loss: 0.8126|lr = 0.00166\n",
      "Epoch: 1006|steps:   60|Train Avg Loss: 0.8685 |Test Loss: 0.8123|lr = 0.00166\n",
      "Epoch: 1007|steps:   30|Train Avg Loss: 0.9011 |Test Loss: 0.8183|lr = 0.00166\n",
      "Epoch: 1007|steps:   60|Train Avg Loss: 0.8750 |Test Loss: 0.8102|lr = 0.00166\n",
      "Epoch: 1008|steps:   30|Train Avg Loss: 0.8882 |Test Loss: 0.8139|lr = 0.00166\n",
      "Epoch: 1008|steps:   60|Train Avg Loss: 0.8797 |Test Loss: 0.8115|lr = 0.00166\n",
      "Epoch: 1009|steps:   30|Train Avg Loss: 0.8880 |Test Loss: 0.8135|lr = 0.00166\n",
      "Epoch: 1009|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8140|lr = 0.00166\n",
      "Epoch: 1010|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8124|lr = 0.00166\n",
      "Epoch: 1010|steps:   60|Train Avg Loss: 0.8830 |Test Loss: 0.8117|lr = 0.00166\n",
      "Epoch: 1011|steps:   30|Train Avg Loss: 0.8950 |Test Loss: 0.8131|lr = 0.00166\n",
      "Epoch: 1011|steps:   60|Train Avg Loss: 0.8800 |Test Loss: 0.8111|lr = 0.00166\n",
      "Epoch: 1012|steps:   30|Train Avg Loss: 0.8858 |Test Loss: 0.8122|lr = 0.00166\n",
      "Epoch: 1012|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8136|lr = 0.00166\n",
      "Epoch: 1013|steps:   30|Train Avg Loss: 0.8931 |Test Loss: 0.8119|lr = 0.00166\n",
      "Epoch: 1013|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8131|lr = 0.00166\n",
      "Epoch: 1014|steps:   30|Train Avg Loss: 0.8848 |Test Loss: 0.8118|lr = 0.00166\n",
      "Epoch: 1014|steps:   60|Train Avg Loss: 0.8960 |Test Loss: 0.8129|lr = 0.00166\n",
      "Epoch: 1015|steps:   30|Train Avg Loss: 0.8687 |Test Loss: 0.8096|lr = 0.00166\n",
      "Epoch: 1015|steps:   60|Train Avg Loss: 0.9015 |Test Loss: 0.8135|lr = 0.00166\n",
      "Epoch: 1016|steps:   30|Train Avg Loss: 0.8965 |Test Loss: 0.8137|lr = 0.00166\n",
      "Epoch: 1016|steps:   60|Train Avg Loss: 0.8801 |Test Loss: 0.8144|lr = 0.00166\n",
      "Epoch: 1017|steps:   30|Train Avg Loss: 0.8907 |Test Loss: 0.8104|lr = 0.00162\n",
      "Epoch: 1017|steps:   60|Train Avg Loss: 0.8848 |Test Loss: 0.8120|lr = 0.00162\n",
      "Epoch: 1018|steps:   30|Train Avg Loss: 0.9040 |Test Loss: 0.8170|lr = 0.00162\n",
      "Epoch: 1018|steps:   60|Train Avg Loss: 0.8826 |Test Loss: 0.8109|lr = 0.00162\n",
      "Epoch: 1019|steps:   30|Train Avg Loss: 0.8952 |Test Loss: 0.8116|lr = 0.00162\n",
      "Epoch: 1019|steps:   60|Train Avg Loss: 0.8708 |Test Loss: 0.8110|lr = 0.00162\n",
      "Epoch: 1020|steps:   30|Train Avg Loss: 0.9201 |Test Loss: 0.8221|lr = 0.00162\n",
      "Epoch: 1020|steps:   60|Train Avg Loss: 0.8669 |Test Loss: 0.8100|lr = 0.00162\n",
      "Epoch: 1021|steps:   30|Train Avg Loss: 0.8795 |Test Loss: 0.8119|lr = 0.00162\n",
      "Epoch: 1021|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8123|lr = 0.00162\n",
      "Epoch: 1022|steps:   30|Train Avg Loss: 0.8791 |Test Loss: 0.8108|lr = 0.00162\n",
      "Epoch: 1022|steps:   60|Train Avg Loss: 0.8910 |Test Loss: 0.8106|lr = 0.00162\n",
      "Epoch: 1023|steps:   30|Train Avg Loss: 0.8876 |Test Loss: 0.8113|lr = 0.00162\n",
      "Epoch: 1023|steps:   60|Train Avg Loss: 0.8942 |Test Loss: 0.8142|lr = 0.00162\n",
      "Epoch: 1024|steps:   30|Train Avg Loss: 0.9049 |Test Loss: 0.8154|lr = 0.00162\n",
      "Epoch: 1024|steps:   60|Train Avg Loss: 0.8830 |Test Loss: 0.8113|lr = 0.00162\n",
      "Epoch: 1025|steps:   30|Train Avg Loss: 0.8734 |Test Loss: 0.8090|lr = 0.00162\n",
      "Epoch: 1025|steps:   60|Train Avg Loss: 0.8989 |Test Loss: 0.8142|lr = 0.00162\n",
      "Epoch: 1026|steps:   30|Train Avg Loss: 0.8804 |Test Loss: 0.8126|lr = 0.00162\n",
      "Epoch: 1026|steps:   60|Train Avg Loss: 0.8863 |Test Loss: 0.8129|lr = 0.00162\n",
      "Epoch: 1027|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8116|lr = 0.00162\n",
      "Epoch: 1027|steps:   60|Train Avg Loss: 0.8950 |Test Loss: 0.8142|lr = 0.00162\n",
      "Epoch: 1028|steps:   30|Train Avg Loss: 0.9010 |Test Loss: 0.8164|lr = 0.00159\n",
      "Epoch: 1028|steps:   60|Train Avg Loss: 0.8770 |Test Loss: 0.8108|lr = 0.00159\n",
      "Epoch: 1029|steps:   30|Train Avg Loss: 0.8948 |Test Loss: 0.8158|lr = 0.00159\n",
      "Epoch: 1029|steps:   60|Train Avg Loss: 0.8898 |Test Loss: 0.8143|lr = 0.00159\n",
      "Epoch: 1030|steps:   30|Train Avg Loss: 0.8729 |Test Loss: 0.8103|lr = 0.00159\n",
      "Epoch: 1030|steps:   60|Train Avg Loss: 0.9038 |Test Loss: 0.8136|lr = 0.00159\n",
      "Epoch: 1031|steps:   30|Train Avg Loss: 0.8937 |Test Loss: 0.8141|lr = 0.00159\n",
      "Epoch: 1031|steps:   60|Train Avg Loss: 0.8769 |Test Loss: 0.8089|lr = 0.00159\n",
      "Epoch: 1032|steps:   30|Train Avg Loss: 0.8790 |Test Loss: 0.8107|lr = 0.00159\n",
      "Epoch: 1032|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 0.8123|lr = 0.00159\n",
      "Epoch: 1033|steps:   30|Train Avg Loss: 0.8874 |Test Loss: 0.8123|lr = 0.00159\n",
      "Epoch: 1033|steps:   60|Train Avg Loss: 0.8800 |Test Loss: 0.8117|lr = 0.00159\n",
      "Epoch: 1034|steps:   30|Train Avg Loss: 0.8785 |Test Loss: 0.8143|lr = 0.00159\n",
      "Epoch: 1034|steps:   60|Train Avg Loss: 0.8916 |Test Loss: 0.8120|lr = 0.00159\n",
      "Epoch: 1035|steps:   30|Train Avg Loss: 0.8792 |Test Loss: 0.8096|lr = 0.00159\n",
      "Epoch: 1035|steps:   60|Train Avg Loss: 0.9042 |Test Loss: 0.8153|lr = 0.00159\n",
      "Epoch: 1036|steps:   30|Train Avg Loss: 0.9065 |Test Loss: 0.8155|lr = 0.00159\n",
      "Epoch: 1036|steps:   60|Train Avg Loss: 0.8692 |Test Loss: 0.8109|lr = 0.00159\n",
      "Epoch: 1037|steps:   30|Train Avg Loss: 0.8887 |Test Loss: 0.8118|lr = 0.00159\n",
      "Epoch: 1037|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8135|lr = 0.00159\n",
      "Epoch: 1038|steps:   30|Train Avg Loss: 0.8857 |Test Loss: 0.8100|lr = 0.00159\n",
      "Epoch: 1038|steps:   60|Train Avg Loss: 0.8772 |Test Loss: 0.8103|lr = 0.00159\n",
      "Epoch: 1039|steps:   30|Train Avg Loss: 0.8829 |Test Loss: 0.8129|lr = 0.00156\n",
      "Epoch: 1039|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8151|lr = 0.00156\n",
      "Epoch: 1040|steps:   30|Train Avg Loss: 0.8892 |Test Loss: 0.8134|lr = 0.00156\n",
      "Epoch: 1040|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8114|lr = 0.00156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1041|steps:   30|Train Avg Loss: 0.8620 |Test Loss: 0.8102|lr = 0.00156\n",
      "Epoch: 1041|steps:   60|Train Avg Loss: 0.9090 |Test Loss: 0.8163|lr = 0.00156\n",
      "Epoch: 1042|steps:   30|Train Avg Loss: 0.9104 |Test Loss: 0.8165|lr = 0.00156\n",
      "Epoch: 1042|steps:   60|Train Avg Loss: 0.8763 |Test Loss: 0.8130|lr = 0.00156\n",
      "Epoch: 1043|steps:   30|Train Avg Loss: 0.8986 |Test Loss: 0.8128|lr = 0.00156\n",
      "Epoch: 1043|steps:   60|Train Avg Loss: 0.8689 |Test Loss: 0.8146|lr = 0.00156\n",
      "Epoch: 1044|steps:   30|Train Avg Loss: 0.8996 |Test Loss: 0.8143|lr = 0.00156\n",
      "Epoch: 1044|steps:   60|Train Avg Loss: 0.8780 |Test Loss: 0.8128|lr = 0.00156\n",
      "Epoch: 1045|steps:   30|Train Avg Loss: 0.8812 |Test Loss: 0.8110|lr = 0.00156\n",
      "Epoch: 1045|steps:   60|Train Avg Loss: 0.8909 |Test Loss: 0.8122|lr = 0.00156\n",
      "Epoch: 1046|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8136|lr = 0.00156\n",
      "Epoch: 1046|steps:   60|Train Avg Loss: 0.8978 |Test Loss: 0.8115|lr = 0.00156\n",
      "Epoch: 1047|steps:   30|Train Avg Loss: 0.8897 |Test Loss: 0.8120|lr = 0.00156\n",
      "Epoch: 1047|steps:   60|Train Avg Loss: 0.8831 |Test Loss: 0.8130|lr = 0.00156\n",
      "Epoch: 1048|steps:   30|Train Avg Loss: 0.8726 |Test Loss: 0.8095|lr = 0.00156\n",
      "Epoch: 1048|steps:   60|Train Avg Loss: 0.8935 |Test Loss: 0.8112|lr = 0.00156\n",
      "Epoch: 1049|steps:   30|Train Avg Loss: 0.8666 |Test Loss: 0.8115|lr = 0.00156\n",
      "Epoch: 1049|steps:   60|Train Avg Loss: 0.9083 |Test Loss: 0.8131|lr = 0.00156\n",
      "Epoch: 1050|steps:   30|Train Avg Loss: 0.8949 |Test Loss: 0.8120|lr = 0.00153\n",
      "Epoch: 1050|steps:   60|Train Avg Loss: 0.8723 |Test Loss: 0.8105|lr = 0.00153\n",
      "Epoch: 1051|steps:   30|Train Avg Loss: 0.8786 |Test Loss: 0.8131|lr = 0.00153\n",
      "Epoch: 1051|steps:   60|Train Avg Loss: 0.8917 |Test Loss: 0.8153|lr = 0.00153\n",
      "Epoch: 1052|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8110|lr = 0.00153\n",
      "Epoch: 1052|steps:   60|Train Avg Loss: 0.9035 |Test Loss: 0.8146|lr = 0.00153\n",
      "Epoch: 1053|steps:   30|Train Avg Loss: 0.9029 |Test Loss: 0.8136|lr = 0.00153\n",
      "Epoch: 1053|steps:   60|Train Avg Loss: 0.8719 |Test Loss: 0.8116|lr = 0.00153\n",
      "Epoch: 1054|steps:   30|Train Avg Loss: 0.8836 |Test Loss: 0.8121|lr = 0.00153\n",
      "Epoch: 1054|steps:   60|Train Avg Loss: 0.8866 |Test Loss: 0.8121|lr = 0.00153\n",
      "Epoch: 1055|steps:   30|Train Avg Loss: 0.8865 |Test Loss: 0.8112|lr = 0.00153\n",
      "Epoch: 1055|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8130|lr = 0.00153\n",
      "Epoch: 1056|steps:   30|Train Avg Loss: 0.8878 |Test Loss: 0.8122|lr = 0.00153\n",
      "Epoch: 1056|steps:   60|Train Avg Loss: 0.8818 |Test Loss: 0.8114|lr = 0.00153\n",
      "Epoch: 1057|steps:   30|Train Avg Loss: 0.8580 |Test Loss: 0.8098|lr = 0.00153\n",
      "Epoch: 1057|steps:   60|Train Avg Loss: 0.9033 |Test Loss: 0.8120|lr = 0.00153\n",
      "Epoch: 1058|steps:   30|Train Avg Loss: 0.8738 |Test Loss: 0.8111|lr = 0.00153\n",
      "Epoch: 1058|steps:   60|Train Avg Loss: 0.8998 |Test Loss: 0.8161|lr = 0.00153\n",
      "Epoch: 1059|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8141|lr = 0.00153\n",
      "Epoch: 1059|steps:   60|Train Avg Loss: 0.8784 |Test Loss: 0.8123|lr = 0.00153\n",
      "Epoch: 1060|steps:   30|Train Avg Loss: 0.8962 |Test Loss: 0.8137|lr = 0.00153\n",
      "Epoch: 1060|steps:   60|Train Avg Loss: 0.8759 |Test Loss: 0.8095|lr = 0.00153\n",
      "Epoch: 1061|steps:   30|Train Avg Loss: 0.8799 |Test Loss: 0.8148|lr = 0.00150\n",
      "Epoch: 1061|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8116|lr = 0.00150\n",
      "Epoch: 1062|steps:   30|Train Avg Loss: 0.8795 |Test Loss: 0.8100|lr = 0.00150\n",
      "Epoch: 1062|steps:   60|Train Avg Loss: 0.8929 |Test Loss: 0.8136|lr = 0.00150\n",
      "Epoch: 1063|steps:   30|Train Avg Loss: 0.8961 |Test Loss: 0.8143|lr = 0.00150\n",
      "Epoch: 1063|steps:   60|Train Avg Loss: 0.8762 |Test Loss: 0.8108|lr = 0.00150\n",
      "Epoch: 1064|steps:   30|Train Avg Loss: 0.9055 |Test Loss: 0.8135|lr = 0.00150\n",
      "Epoch: 1064|steps:   60|Train Avg Loss: 0.8604 |Test Loss: 0.8136|lr = 0.00150\n",
      "Epoch: 1065|steps:   30|Train Avg Loss: 0.8715 |Test Loss: 0.8098|lr = 0.00150\n",
      "Epoch: 1065|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8127|lr = 0.00150\n",
      "Epoch: 1066|steps:   30|Train Avg Loss: 0.8721 |Test Loss: 0.8090|lr = 0.00150\n",
      "Epoch: 1066|steps:   60|Train Avg Loss: 0.8928 |Test Loss: 0.8112|lr = 0.00150\n",
      "Epoch: 1067|steps:   30|Train Avg Loss: 0.8908 |Test Loss: 0.8147|lr = 0.00150\n",
      "Epoch: 1067|steps:   60|Train Avg Loss: 0.8794 |Test Loss: 0.8133|lr = 0.00150\n",
      "Epoch: 1068|steps:   30|Train Avg Loss: 0.8783 |Test Loss: 0.8119|lr = 0.00150\n",
      "Epoch: 1068|steps:   60|Train Avg Loss: 0.8916 |Test Loss: 0.8119|lr = 0.00150\n",
      "Epoch: 1069|steps:   30|Train Avg Loss: 0.8896 |Test Loss: 0.8153|lr = 0.00150\n",
      "Epoch: 1069|steps:   60|Train Avg Loss: 0.8721 |Test Loss: 0.8094|lr = 0.00150\n",
      "Epoch: 1070|steps:   30|Train Avg Loss: 0.8959 |Test Loss: 0.8168|lr = 0.00150\n",
      "Epoch: 1070|steps:   60|Train Avg Loss: 0.8871 |Test Loss: 0.8156|lr = 0.00150\n",
      "Epoch: 1071|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8105|lr = 0.00150\n",
      "Epoch: 1071|steps:   60|Train Avg Loss: 0.8787 |Test Loss: 0.8116|lr = 0.00150\n",
      "Epoch: 1072|steps:   30|Train Avg Loss: 0.8601 |Test Loss: 0.8082|lr = 0.00147\n",
      "Epoch: 1072|steps:   60|Train Avg Loss: 0.9152 |Test Loss: 0.8118|lr = 0.00147\n",
      "Epoch: 1073|steps:   30|Train Avg Loss: 0.8886 |Test Loss: 0.8152|lr = 0.00147\n",
      "Epoch: 1073|steps:   60|Train Avg Loss: 0.8866 |Test Loss: 0.8117|lr = 0.00147\n",
      "Epoch: 1074|steps:   30|Train Avg Loss: 0.8996 |Test Loss: 0.8143|lr = 0.00147\n",
      "Epoch: 1074|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8140|lr = 0.00147\n",
      "Epoch: 1075|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8114|lr = 0.00147\n",
      "Epoch: 1075|steps:   60|Train Avg Loss: 0.8682 |Test Loss: 0.8102|lr = 0.00147\n",
      "Epoch: 1076|steps:   30|Train Avg Loss: 0.8803 |Test Loss: 0.8125|lr = 0.00147\n",
      "Epoch: 1076|steps:   60|Train Avg Loss: 0.8900 |Test Loss: 0.8147|lr = 0.00147\n",
      "Epoch: 1077|steps:   30|Train Avg Loss: 0.8840 |Test Loss: 0.8134|lr = 0.00147\n",
      "Epoch: 1077|steps:   60|Train Avg Loss: 0.8884 |Test Loss: 0.8151|lr = 0.00147\n",
      "Epoch: 1078|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8134|lr = 0.00147\n",
      "Epoch: 1078|steps:   60|Train Avg Loss: 0.8696 |Test Loss: 0.8127|lr = 0.00147\n",
      "Epoch: 1079|steps:   30|Train Avg Loss: 0.8857 |Test Loss: 0.8116|lr = 0.00147\n",
      "Epoch: 1079|steps:   60|Train Avg Loss: 0.8810 |Test Loss: 0.8120|lr = 0.00147\n",
      "Epoch: 1080|steps:   30|Train Avg Loss: 0.9017 |Test Loss: 0.8138|lr = 0.00147\n",
      "Epoch: 1080|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8125|lr = 0.00147\n",
      "Epoch: 1081|steps:   30|Train Avg Loss: 0.8837 |Test Loss: 0.8131|lr = 0.00147\n",
      "Epoch: 1081|steps:   60|Train Avg Loss: 0.8805 |Test Loss: 0.8107|lr = 0.00147\n",
      "Epoch: 1082|steps:   30|Train Avg Loss: 0.9005 |Test Loss: 0.8155|lr = 0.00147\n",
      "Epoch: 1082|steps:   60|Train Avg Loss: 0.8820 |Test Loss: 0.8128|lr = 0.00147\n",
      "Epoch: 1083|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8090|lr = 0.00144\n",
      "Epoch: 1083|steps:   60|Train Avg Loss: 0.8875 |Test Loss: 0.8127|lr = 0.00144\n",
      "Epoch: 1084|steps:   30|Train Avg Loss: 0.8853 |Test Loss: 0.8141|lr = 0.00144\n",
      "Epoch: 1084|steps:   60|Train Avg Loss: 0.8768 |Test Loss: 0.8112|lr = 0.00144\n",
      "Epoch: 1085|steps:   30|Train Avg Loss: 0.8989 |Test Loss: 0.8159|lr = 0.00144\n",
      "Epoch: 1085|steps:   60|Train Avg Loss: 0.8790 |Test Loss: 0.8097|lr = 0.00144\n",
      "Epoch: 1086|steps:   30|Train Avg Loss: 0.8596 |Test Loss: 0.8116|lr = 0.00144\n",
      "Epoch: 1086|steps:   60|Train Avg Loss: 0.9133 |Test Loss: 0.8169|lr = 0.00144\n",
      "Epoch: 1087|steps:   30|Train Avg Loss: 0.8740 |Test Loss: 0.8121|lr = 0.00144\n",
      "Epoch: 1087|steps:   60|Train Avg Loss: 0.9027 |Test Loss: 0.8152|lr = 0.00144\n",
      "Epoch: 1088|steps:   30|Train Avg Loss: 0.8903 |Test Loss: 0.8108|lr = 0.00144\n",
      "Epoch: 1088|steps:   60|Train Avg Loss: 0.8983 |Test Loss: 0.8127|lr = 0.00144\n",
      "Epoch: 1089|steps:   30|Train Avg Loss: 0.9089 |Test Loss: 0.8155|lr = 0.00144\n",
      "Epoch: 1089|steps:   60|Train Avg Loss: 0.8713 |Test Loss: 0.8144|lr = 0.00144\n",
      "Epoch: 1090|steps:   30|Train Avg Loss: 0.8976 |Test Loss: 0.8152|lr = 0.00144\n",
      "Epoch: 1090|steps:   60|Train Avg Loss: 0.8669 |Test Loss: 0.8102|lr = 0.00144\n",
      "Epoch: 1091|steps:   30|Train Avg Loss: 0.8695 |Test Loss: 0.8095|lr = 0.00144\n",
      "Epoch: 1091|steps:   60|Train Avg Loss: 0.9044 |Test Loss: 0.8163|lr = 0.00144\n",
      "Epoch: 1092|steps:   30|Train Avg Loss: 0.8705 |Test Loss: 0.8106|lr = 0.00144\n",
      "Epoch: 1092|steps:   60|Train Avg Loss: 0.9047 |Test Loss: 0.8145|lr = 0.00144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1093|steps:   30|Train Avg Loss: 0.8796 |Test Loss: 0.8125|lr = 0.00144\n",
      "Epoch: 1093|steps:   60|Train Avg Loss: 0.8937 |Test Loss: 0.8116|lr = 0.00144\n",
      "Epoch: 1094|steps:   30|Train Avg Loss: 0.8708 |Test Loss: 0.8115|lr = 0.00141\n",
      "Epoch: 1094|steps:   60|Train Avg Loss: 0.8989 |Test Loss: 0.8124|lr = 0.00141\n",
      "Epoch: 1095|steps:   30|Train Avg Loss: 0.8806 |Test Loss: 0.8134|lr = 0.00141\n",
      "Epoch: 1095|steps:   60|Train Avg Loss: 0.8878 |Test Loss: 0.8130|lr = 0.00141\n",
      "Epoch: 1096|steps:   30|Train Avg Loss: 0.8793 |Test Loss: 0.8132|lr = 0.00141\n",
      "Epoch: 1096|steps:   60|Train Avg Loss: 0.8771 |Test Loss: 0.8105|lr = 0.00141\n",
      "Epoch: 1097|steps:   30|Train Avg Loss: 0.8804 |Test Loss: 0.8107|lr = 0.00141\n",
      "Epoch: 1097|steps:   60|Train Avg Loss: 0.8965 |Test Loss: 0.8136|lr = 0.00141\n",
      "Epoch: 1098|steps:   30|Train Avg Loss: 0.8868 |Test Loss: 0.8115|lr = 0.00141\n",
      "Epoch: 1098|steps:   60|Train Avg Loss: 0.8842 |Test Loss: 0.8127|lr = 0.00141\n",
      "Epoch: 1099|steps:   30|Train Avg Loss: 0.8893 |Test Loss: 0.8134|lr = 0.00141\n",
      "Epoch: 1099|steps:   60|Train Avg Loss: 0.8853 |Test Loss: 0.8137|lr = 0.00141\n",
      "Epoch: 1100|steps:   30|Train Avg Loss: 0.8885 |Test Loss: 0.8147|lr = 0.00141\n",
      "Epoch: 1100|steps:   60|Train Avg Loss: 0.8820 |Test Loss: 0.8125|lr = 0.00141\n",
      "Epoch: 1101|steps:   30|Train Avg Loss: 0.9045 |Test Loss: 0.8140|lr = 0.00141\n",
      "Epoch: 1101|steps:   60|Train Avg Loss: 0.8700 |Test Loss: 0.8129|lr = 0.00141\n",
      "Epoch: 1102|steps:   30|Train Avg Loss: 0.9051 |Test Loss: 0.8146|lr = 0.00141\n",
      "Epoch: 1102|steps:   60|Train Avg Loss: 0.8710 |Test Loss: 0.8109|lr = 0.00141\n",
      "Epoch: 1103|steps:   30|Train Avg Loss: 0.8715 |Test Loss: 0.8101|lr = 0.00141\n",
      "Epoch: 1103|steps:   60|Train Avg Loss: 0.8854 |Test Loss: 0.8101|lr = 0.00141\n",
      "Epoch: 1104|steps:   30|Train Avg Loss: 0.8703 |Test Loss: 0.8104|lr = 0.00141\n",
      "Epoch: 1104|steps:   60|Train Avg Loss: 0.8975 |Test Loss: 0.8133|lr = 0.00141\n",
      "Epoch: 1105|steps:   30|Train Avg Loss: 0.8999 |Test Loss: 0.8172|lr = 0.00138\n",
      "Epoch: 1105|steps:   60|Train Avg Loss: 0.8803 |Test Loss: 0.8090|lr = 0.00138\n",
      "Epoch: 1106|steps:   30|Train Avg Loss: 0.8867 |Test Loss: 0.8113|lr = 0.00138\n",
      "Epoch: 1106|steps:   60|Train Avg Loss: 0.8739 |Test Loss: 0.8128|lr = 0.00138\n",
      "Epoch: 1107|steps:   30|Train Avg Loss: 0.8990 |Test Loss: 0.8175|lr = 0.00138\n",
      "Epoch: 1107|steps:   60|Train Avg Loss: 0.8688 |Test Loss: 0.8115|lr = 0.00138\n",
      "Epoch: 1108|steps:   30|Train Avg Loss: 0.8849 |Test Loss: 0.8128|lr = 0.00138\n",
      "Epoch: 1108|steps:   60|Train Avg Loss: 0.8907 |Test Loss: 0.8115|lr = 0.00138\n",
      "Epoch: 1109|steps:   30|Train Avg Loss: 0.8732 |Test Loss: 0.8109|lr = 0.00138\n",
      "Epoch: 1109|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8125|lr = 0.00138\n",
      "Epoch: 1110|steps:   30|Train Avg Loss: 0.8825 |Test Loss: 0.8135|lr = 0.00138\n",
      "Epoch: 1110|steps:   60|Train Avg Loss: 0.8757 |Test Loss: 0.8095|lr = 0.00138\n",
      "Epoch: 1111|steps:   30|Train Avg Loss: 0.8951 |Test Loss: 0.8169|lr = 0.00138\n",
      "Epoch: 1111|steps:   60|Train Avg Loss: 0.8699 |Test Loss: 0.8112|lr = 0.00138\n",
      "Epoch: 1112|steps:   30|Train Avg Loss: 0.9228 |Test Loss: 0.8183|lr = 0.00138\n",
      "Epoch: 1112|steps:   60|Train Avg Loss: 0.8669 |Test Loss: 0.8133|lr = 0.00138\n",
      "Epoch: 1113|steps:   30|Train Avg Loss: 0.8856 |Test Loss: 0.8145|lr = 0.00138\n",
      "Epoch: 1113|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8090|lr = 0.00138\n",
      "Epoch: 1114|steps:   30|Train Avg Loss: 0.8815 |Test Loss: 0.8130|lr = 0.00138\n",
      "Epoch: 1114|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8120|lr = 0.00138\n",
      "Epoch: 1115|steps:   30|Train Avg Loss: 0.8878 |Test Loss: 0.8142|lr = 0.00138\n",
      "Epoch: 1115|steps:   60|Train Avg Loss: 0.8865 |Test Loss: 0.8138|lr = 0.00138\n",
      "Epoch: 1116|steps:   30|Train Avg Loss: 0.8685 |Test Loss: 0.8099|lr = 0.00135\n",
      "Epoch: 1116|steps:   60|Train Avg Loss: 0.9015 |Test Loss: 0.8150|lr = 0.00135\n",
      "Epoch: 1117|steps:   30|Train Avg Loss: 0.8839 |Test Loss: 0.8098|lr = 0.00135\n",
      "Epoch: 1117|steps:   60|Train Avg Loss: 0.8974 |Test Loss: 0.8141|lr = 0.00135\n",
      "Epoch: 1118|steps:   30|Train Avg Loss: 0.8984 |Test Loss: 0.8198|lr = 0.00135\n",
      "Epoch: 1118|steps:   60|Train Avg Loss: 0.8825 |Test Loss: 0.8137|lr = 0.00135\n",
      "Epoch: 1119|steps:   30|Train Avg Loss: 0.8952 |Test Loss: 0.8132|lr = 0.00135\n",
      "Epoch: 1119|steps:   60|Train Avg Loss: 0.8792 |Test Loss: 0.8124|lr = 0.00135\n",
      "Epoch: 1120|steps:   30|Train Avg Loss: 0.8815 |Test Loss: 0.8114|lr = 0.00135\n",
      "Epoch: 1120|steps:   60|Train Avg Loss: 0.8990 |Test Loss: 0.8142|lr = 0.00135\n",
      "Epoch: 1121|steps:   30|Train Avg Loss: 0.8864 |Test Loss: 0.8117|lr = 0.00135\n",
      "Epoch: 1121|steps:   60|Train Avg Loss: 0.8905 |Test Loss: 0.8139|lr = 0.00135\n",
      "Epoch: 1122|steps:   30|Train Avg Loss: 0.8614 |Test Loss: 0.8096|lr = 0.00135\n",
      "Epoch: 1122|steps:   60|Train Avg Loss: 0.9075 |Test Loss: 0.8135|lr = 0.00135\n",
      "Epoch: 1123|steps:   30|Train Avg Loss: 0.8778 |Test Loss: 0.8102|lr = 0.00135\n",
      "Epoch: 1123|steps:   60|Train Avg Loss: 0.8946 |Test Loss: 0.8144|lr = 0.00135\n",
      "Epoch: 1124|steps:   30|Train Avg Loss: 0.8801 |Test Loss: 0.8121|lr = 0.00135\n",
      "Epoch: 1124|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8129|lr = 0.00135\n",
      "Epoch: 1125|steps:   30|Train Avg Loss: 0.8802 |Test Loss: 0.8129|lr = 0.00135\n",
      "Epoch: 1125|steps:   60|Train Avg Loss: 0.8909 |Test Loss: 0.8115|lr = 0.00135\n",
      "Epoch: 1126|steps:   30|Train Avg Loss: 0.8826 |Test Loss: 0.8111|lr = 0.00135\n",
      "Epoch: 1126|steps:   60|Train Avg Loss: 0.8807 |Test Loss: 0.8121|lr = 0.00135\n",
      "Epoch: 1127|steps:   30|Train Avg Loss: 0.8902 |Test Loss: 0.8145|lr = 0.00133\n",
      "Epoch: 1127|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8128|lr = 0.00133\n",
      "Epoch: 1128|steps:   30|Train Avg Loss: 0.8805 |Test Loss: 0.8120|lr = 0.00133\n",
      "Epoch: 1128|steps:   60|Train Avg Loss: 0.8924 |Test Loss: 0.8131|lr = 0.00133\n",
      "Epoch: 1129|steps:   30|Train Avg Loss: 0.8643 |Test Loss: 0.8092|lr = 0.00133\n",
      "Epoch: 1129|steps:   60|Train Avg Loss: 0.8981 |Test Loss: 0.8134|lr = 0.00133\n",
      "Epoch: 1130|steps:   30|Train Avg Loss: 0.8896 |Test Loss: 0.8115|lr = 0.00133\n",
      "Epoch: 1130|steps:   60|Train Avg Loss: 0.8842 |Test Loss: 0.8151|lr = 0.00133\n",
      "Epoch: 1131|steps:   30|Train Avg Loss: 0.8888 |Test Loss: 0.8132|lr = 0.00133\n",
      "Epoch: 1131|steps:   60|Train Avg Loss: 0.8833 |Test Loss: 0.8137|lr = 0.00133\n",
      "Epoch: 1132|steps:   30|Train Avg Loss: 0.8762 |Test Loss: 0.8128|lr = 0.00133\n",
      "Epoch: 1132|steps:   60|Train Avg Loss: 0.8949 |Test Loss: 0.8128|lr = 0.00133\n",
      "Epoch: 1133|steps:   30|Train Avg Loss: 0.8885 |Test Loss: 0.8103|lr = 0.00133\n",
      "Epoch: 1133|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8125|lr = 0.00133\n",
      "Epoch: 1134|steps:   30|Train Avg Loss: 0.8922 |Test Loss: 0.8132|lr = 0.00133\n",
      "Epoch: 1134|steps:   60|Train Avg Loss: 0.8888 |Test Loss: 0.8149|lr = 0.00133\n",
      "Epoch: 1135|steps:   30|Train Avg Loss: 0.8675 |Test Loss: 0.8091|lr = 0.00133\n",
      "Epoch: 1135|steps:   60|Train Avg Loss: 0.9070 |Test Loss: 0.8139|lr = 0.00133\n",
      "Epoch: 1136|steps:   30|Train Avg Loss: 0.8689 |Test Loss: 0.8099|lr = 0.00133\n",
      "Epoch: 1136|steps:   60|Train Avg Loss: 0.8934 |Test Loss: 0.8118|lr = 0.00133\n",
      "Epoch: 1137|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8117|lr = 0.00133\n",
      "Epoch: 1137|steps:   60|Train Avg Loss: 0.9036 |Test Loss: 0.8165|lr = 0.00133\n",
      "Epoch: 1138|steps:   30|Train Avg Loss: 0.8862 |Test Loss: 0.8105|lr = 0.00130\n",
      "Epoch: 1138|steps:   60|Train Avg Loss: 0.8913 |Test Loss: 0.8159|lr = 0.00130\n",
      "Epoch: 1139|steps:   30|Train Avg Loss: 0.8886 |Test Loss: 0.8134|lr = 0.00130\n",
      "Epoch: 1139|steps:   60|Train Avg Loss: 0.8929 |Test Loss: 0.8131|lr = 0.00130\n",
      "Epoch: 1140|steps:   30|Train Avg Loss: 0.8618 |Test Loss: 0.8098|lr = 0.00130\n",
      "Epoch: 1140|steps:   60|Train Avg Loss: 0.9061 |Test Loss: 0.8120|lr = 0.00130\n",
      "Epoch: 1141|steps:   30|Train Avg Loss: 0.8867 |Test Loss: 0.8124|lr = 0.00130\n",
      "Epoch: 1141|steps:   60|Train Avg Loss: 0.8788 |Test Loss: 0.8113|lr = 0.00130\n",
      "Epoch: 1142|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8124|lr = 0.00130\n",
      "Epoch: 1142|steps:   60|Train Avg Loss: 0.8671 |Test Loss: 0.8119|lr = 0.00130\n",
      "Epoch: 1143|steps:   30|Train Avg Loss: 0.8804 |Test Loss: 0.8131|lr = 0.00130\n",
      "Epoch: 1143|steps:   60|Train Avg Loss: 0.8846 |Test Loss: 0.8109|lr = 0.00130\n",
      "Epoch: 1144|steps:   30|Train Avg Loss: 0.9009 |Test Loss: 0.8178|lr = 0.00130\n",
      "Epoch: 1144|steps:   60|Train Avg Loss: 0.8770 |Test Loss: 0.8126|lr = 0.00130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1145|steps:   30|Train Avg Loss: 0.8993 |Test Loss: 0.8162|lr = 0.00130\n",
      "Epoch: 1145|steps:   60|Train Avg Loss: 0.8861 |Test Loss: 0.8139|lr = 0.00130\n",
      "Epoch: 1146|steps:   30|Train Avg Loss: 0.8997 |Test Loss: 0.8132|lr = 0.00130\n",
      "Epoch: 1146|steps:   60|Train Avg Loss: 0.8636 |Test Loss: 0.8108|lr = 0.00130\n",
      "Epoch: 1147|steps:   30|Train Avg Loss: 0.9033 |Test Loss: 0.8165|lr = 0.00130\n",
      "Epoch: 1147|steps:   60|Train Avg Loss: 0.8605 |Test Loss: 0.8095|lr = 0.00130\n",
      "Epoch: 1148|steps:   30|Train Avg Loss: 0.8817 |Test Loss: 0.8117|lr = 0.00130\n",
      "Epoch: 1148|steps:   60|Train Avg Loss: 0.8900 |Test Loss: 0.8126|lr = 0.00130\n",
      "Epoch: 1149|steps:   30|Train Avg Loss: 0.8851 |Test Loss: 0.8116|lr = 0.00127\n",
      "Epoch: 1149|steps:   60|Train Avg Loss: 0.8963 |Test Loss: 0.8150|lr = 0.00127\n",
      "Epoch: 1150|steps:   30|Train Avg Loss: 0.8992 |Test Loss: 0.8166|lr = 0.00127\n",
      "Epoch: 1150|steps:   60|Train Avg Loss: 0.8703 |Test Loss: 0.8116|lr = 0.00127\n",
      "Epoch: 1151|steps:   30|Train Avg Loss: 0.8893 |Test Loss: 0.8128|lr = 0.00127\n",
      "Epoch: 1151|steps:   60|Train Avg Loss: 0.8892 |Test Loss: 0.8138|lr = 0.00127\n",
      "Epoch: 1152|steps:   30|Train Avg Loss: 0.8673 |Test Loss: 0.8092|lr = 0.00127\n",
      "Epoch: 1152|steps:   60|Train Avg Loss: 0.8888 |Test Loss: 0.8115|lr = 0.00127\n",
      "Epoch: 1153|steps:   30|Train Avg Loss: 0.8894 |Test Loss: 0.8135|lr = 0.00127\n",
      "Epoch: 1153|steps:   60|Train Avg Loss: 0.8759 |Test Loss: 0.8133|lr = 0.00127\n",
      "Epoch: 1154|steps:   30|Train Avg Loss: 0.8981 |Test Loss: 0.8151|lr = 0.00127\n",
      "Epoch: 1154|steps:   60|Train Avg Loss: 0.8689 |Test Loss: 0.8115|lr = 0.00127\n",
      "Epoch: 1155|steps:   30|Train Avg Loss: 0.8697 |Test Loss: 0.8109|lr = 0.00127\n",
      "Epoch: 1155|steps:   60|Train Avg Loss: 0.9205 |Test Loss: 0.8151|lr = 0.00127\n",
      "Epoch: 1156|steps:   30|Train Avg Loss: 0.8857 |Test Loss: 0.8115|lr = 0.00127\n",
      "Epoch: 1156|steps:   60|Train Avg Loss: 0.8925 |Test Loss: 0.8119|lr = 0.00127\n",
      "Epoch: 1157|steps:   30|Train Avg Loss: 0.8835 |Test Loss: 0.8132|lr = 0.00127\n",
      "Epoch: 1157|steps:   60|Train Avg Loss: 0.8829 |Test Loss: 0.8133|lr = 0.00127\n",
      "Epoch: 1158|steps:   30|Train Avg Loss: 0.8643 |Test Loss: 0.8121|lr = 0.00127\n",
      "Epoch: 1158|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8112|lr = 0.00127\n",
      "Epoch: 1159|steps:   30|Train Avg Loss: 0.8837 |Test Loss: 0.8134|lr = 0.00127\n",
      "Epoch: 1159|steps:   60|Train Avg Loss: 0.9038 |Test Loss: 0.8132|lr = 0.00127\n",
      "Epoch: 1160|steps:   30|Train Avg Loss: 0.8749 |Test Loss: 0.8121|lr = 0.00125\n",
      "Epoch: 1160|steps:   60|Train Avg Loss: 0.8963 |Test Loss: 0.8150|lr = 0.00125\n",
      "Epoch: 1161|steps:   30|Train Avg Loss: 0.8896 |Test Loss: 0.8129|lr = 0.00125\n",
      "Epoch: 1161|steps:   60|Train Avg Loss: 0.8816 |Test Loss: 0.8122|lr = 0.00125\n",
      "Epoch: 1162|steps:   30|Train Avg Loss: 0.8923 |Test Loss: 0.8128|lr = 0.00125\n",
      "Epoch: 1162|steps:   60|Train Avg Loss: 0.8800 |Test Loss: 0.8127|lr = 0.00125\n",
      "Epoch: 1163|steps:   30|Train Avg Loss: 0.9001 |Test Loss: 0.8167|lr = 0.00125\n",
      "Epoch: 1163|steps:   60|Train Avg Loss: 0.8593 |Test Loss: 0.8108|lr = 0.00125\n",
      "Epoch: 1164|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8111|lr = 0.00125\n",
      "Epoch: 1164|steps:   60|Train Avg Loss: 0.8824 |Test Loss: 0.8120|lr = 0.00125\n",
      "Epoch: 1165|steps:   30|Train Avg Loss: 0.8808 |Test Loss: 0.8116|lr = 0.00125\n",
      "Epoch: 1165|steps:   60|Train Avg Loss: 0.8847 |Test Loss: 0.8119|lr = 0.00125\n",
      "Epoch: 1166|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8142|lr = 0.00125\n",
      "Epoch: 1166|steps:   60|Train Avg Loss: 0.8786 |Test Loss: 0.8116|lr = 0.00125\n",
      "Epoch: 1167|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8165|lr = 0.00125\n",
      "Epoch: 1167|steps:   60|Train Avg Loss: 0.8774 |Test Loss: 0.8123|lr = 0.00125\n",
      "Epoch: 1168|steps:   30|Train Avg Loss: 0.9008 |Test Loss: 0.8140|lr = 0.00125\n",
      "Epoch: 1168|steps:   60|Train Avg Loss: 0.8830 |Test Loss: 0.8129|lr = 0.00125\n",
      "Epoch: 1169|steps:   30|Train Avg Loss: 0.8783 |Test Loss: 0.8123|lr = 0.00125\n",
      "Epoch: 1169|steps:   60|Train Avg Loss: 0.8911 |Test Loss: 0.8138|lr = 0.00125\n",
      "Epoch: 1170|steps:   30|Train Avg Loss: 0.8594 |Test Loss: 0.8094|lr = 0.00125\n",
      "Epoch: 1170|steps:   60|Train Avg Loss: 0.9181 |Test Loss: 0.8149|lr = 0.00125\n",
      "Epoch: 1171|steps:   30|Train Avg Loss: 0.8694 |Test Loss: 0.8110|lr = 0.00122\n",
      "Epoch: 1171|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8110|lr = 0.00122\n",
      "Epoch: 1172|steps:   30|Train Avg Loss: 0.8725 |Test Loss: 0.8121|lr = 0.00122\n",
      "Epoch: 1172|steps:   60|Train Avg Loss: 0.9095 |Test Loss: 0.8159|lr = 0.00122\n",
      "Epoch: 1173|steps:   30|Train Avg Loss: 0.8905 |Test Loss: 0.8136|lr = 0.00122\n",
      "Epoch: 1173|steps:   60|Train Avg Loss: 0.8816 |Test Loss: 0.8129|lr = 0.00122\n",
      "Epoch: 1174|steps:   30|Train Avg Loss: 0.8807 |Test Loss: 0.8102|lr = 0.00122\n",
      "Epoch: 1174|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8139|lr = 0.00122\n",
      "Epoch: 1175|steps:   30|Train Avg Loss: 0.8899 |Test Loss: 0.8142|lr = 0.00122\n",
      "Epoch: 1175|steps:   60|Train Avg Loss: 0.8695 |Test Loss: 0.8105|lr = 0.00122\n",
      "Epoch: 1176|steps:   30|Train Avg Loss: 0.8629 |Test Loss: 0.8092|lr = 0.00122\n",
      "Epoch: 1176|steps:   60|Train Avg Loss: 0.8954 |Test Loss: 0.8131|lr = 0.00122\n",
      "Epoch: 1177|steps:   30|Train Avg Loss: 0.9029 |Test Loss: 0.8165|lr = 0.00122\n",
      "Epoch: 1177|steps:   60|Train Avg Loss: 0.8736 |Test Loss: 0.8121|lr = 0.00122\n",
      "Epoch: 1178|steps:   30|Train Avg Loss: 0.8589 |Test Loss: 0.8090|lr = 0.00122\n",
      "Epoch: 1178|steps:   60|Train Avg Loss: 0.9010 |Test Loss: 0.8131|lr = 0.00122\n",
      "Epoch: 1179|steps:   30|Train Avg Loss: 0.8653 |Test Loss: 0.8122|lr = 0.00122\n",
      "Epoch: 1179|steps:   60|Train Avg Loss: 0.9046 |Test Loss: 0.8136|lr = 0.00122\n",
      "Epoch: 1180|steps:   30|Train Avg Loss: 0.8765 |Test Loss: 0.8131|lr = 0.00122\n",
      "Epoch: 1180|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8125|lr = 0.00122\n",
      "Epoch: 1181|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8129|lr = 0.00122\n",
      "Epoch: 1181|steps:   60|Train Avg Loss: 0.8824 |Test Loss: 0.8129|lr = 0.00122\n",
      "Epoch: 1182|steps:   30|Train Avg Loss: 0.8704 |Test Loss: 0.8110|lr = 0.00120\n",
      "Epoch: 1182|steps:   60|Train Avg Loss: 0.8913 |Test Loss: 0.8112|lr = 0.00120\n",
      "Epoch: 1183|steps:   30|Train Avg Loss: 0.8866 |Test Loss: 0.8141|lr = 0.00120\n",
      "Epoch: 1183|steps:   60|Train Avg Loss: 0.8934 |Test Loss: 0.8140|lr = 0.00120\n",
      "Epoch: 1184|steps:   30|Train Avg Loss: 0.8760 |Test Loss: 0.8122|lr = 0.00120\n",
      "Epoch: 1184|steps:   60|Train Avg Loss: 0.8967 |Test Loss: 0.8137|lr = 0.00120\n",
      "Epoch: 1185|steps:   30|Train Avg Loss: 0.8940 |Test Loss: 0.8130|lr = 0.00120\n",
      "Epoch: 1185|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8127|lr = 0.00120\n",
      "Epoch: 1186|steps:   30|Train Avg Loss: 0.8816 |Test Loss: 0.8104|lr = 0.00120\n",
      "Epoch: 1186|steps:   60|Train Avg Loss: 0.8921 |Test Loss: 0.8132|lr = 0.00120\n",
      "Epoch: 1187|steps:   30|Train Avg Loss: 0.8932 |Test Loss: 0.8139|lr = 0.00120\n",
      "Epoch: 1187|steps:   60|Train Avg Loss: 0.8939 |Test Loss: 0.8152|lr = 0.00120\n",
      "Epoch: 1188|steps:   30|Train Avg Loss: 0.8857 |Test Loss: 0.8117|lr = 0.00120\n",
      "Epoch: 1188|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8115|lr = 0.00120\n",
      "Epoch: 1189|steps:   30|Train Avg Loss: 0.8844 |Test Loss: 0.8119|lr = 0.00120\n",
      "Epoch: 1189|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8135|lr = 0.00120\n",
      "Epoch: 1190|steps:   30|Train Avg Loss: 0.8914 |Test Loss: 0.8121|lr = 0.00120\n",
      "Epoch: 1190|steps:   60|Train Avg Loss: 0.8885 |Test Loss: 0.8145|lr = 0.00120\n",
      "Epoch: 1191|steps:   30|Train Avg Loss: 0.8905 |Test Loss: 0.8121|lr = 0.00120\n",
      "Epoch: 1191|steps:   60|Train Avg Loss: 0.8728 |Test Loss: 0.8104|lr = 0.00120\n",
      "Epoch: 1192|steps:   30|Train Avg Loss: 0.8976 |Test Loss: 0.8146|lr = 0.00120\n",
      "Epoch: 1192|steps:   60|Train Avg Loss: 0.8745 |Test Loss: 0.8152|lr = 0.00120\n",
      "Epoch: 1193|steps:   30|Train Avg Loss: 0.8858 |Test Loss: 0.8146|lr = 0.00117\n",
      "Epoch: 1193|steps:   60|Train Avg Loss: 0.8961 |Test Loss: 0.8136|lr = 0.00117\n",
      "Epoch: 1194|steps:   30|Train Avg Loss: 0.8854 |Test Loss: 0.8156|lr = 0.00117\n",
      "Epoch: 1194|steps:   60|Train Avg Loss: 0.8888 |Test Loss: 0.8124|lr = 0.00117\n",
      "Epoch: 1195|steps:   30|Train Avg Loss: 0.8804 |Test Loss: 0.8142|lr = 0.00117\n",
      "Epoch: 1195|steps:   60|Train Avg Loss: 0.8866 |Test Loss: 0.8121|lr = 0.00117\n",
      "Epoch: 1196|steps:   30|Train Avg Loss: 0.9035 |Test Loss: 0.8172|lr = 0.00117\n",
      "Epoch: 1196|steps:   60|Train Avg Loss: 0.8784 |Test Loss: 0.8137|lr = 0.00117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1197|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8125|lr = 0.00117\n",
      "Epoch: 1197|steps:   60|Train Avg Loss: 0.8850 |Test Loss: 0.8123|lr = 0.00117\n",
      "Epoch: 1198|steps:   30|Train Avg Loss: 0.8789 |Test Loss: 0.8135|lr = 0.00117\n",
      "Epoch: 1198|steps:   60|Train Avg Loss: 0.9022 |Test Loss: 0.8140|lr = 0.00117\n",
      "Epoch: 1199|steps:   30|Train Avg Loss: 0.8822 |Test Loss: 0.8114|lr = 0.00117\n",
      "Epoch: 1199|steps:   60|Train Avg Loss: 0.8978 |Test Loss: 0.8133|lr = 0.00117\n",
      "Epoch: 1200|steps:   30|Train Avg Loss: 0.9102 |Test Loss: 0.8138|lr = 0.00117\n",
      "Epoch: 1200|steps:   60|Train Avg Loss: 0.8626 |Test Loss: 0.8134|lr = 0.00117\n",
      "Epoch: 1201|steps:   30|Train Avg Loss: 0.9121 |Test Loss: 0.8183|lr = 0.00117\n",
      "Epoch: 1201|steps:   60|Train Avg Loss: 0.8611 |Test Loss: 0.8082|lr = 0.00117\n",
      "Epoch: 1202|steps:   30|Train Avg Loss: 0.9072 |Test Loss: 0.8130|lr = 0.00117\n",
      "Epoch: 1202|steps:   60|Train Avg Loss: 0.8647 |Test Loss: 0.8096|lr = 0.00117\n",
      "Epoch: 1203|steps:   30|Train Avg Loss: 0.8826 |Test Loss: 0.8121|lr = 0.00117\n",
      "Epoch: 1203|steps:   60|Train Avg Loss: 0.8912 |Test Loss: 0.8120|lr = 0.00117\n",
      "Epoch: 1204|steps:   30|Train Avg Loss: 0.8775 |Test Loss: 0.8106|lr = 0.00115\n",
      "Epoch: 1204|steps:   60|Train Avg Loss: 0.8827 |Test Loss: 0.8120|lr = 0.00115\n",
      "Epoch: 1205|steps:   30|Train Avg Loss: 0.8761 |Test Loss: 0.8127|lr = 0.00115\n",
      "Epoch: 1205|steps:   60|Train Avg Loss: 0.8992 |Test Loss: 0.8164|lr = 0.00115\n",
      "Epoch: 1206|steps:   30|Train Avg Loss: 0.8602 |Test Loss: 0.8103|lr = 0.00115\n",
      "Epoch: 1206|steps:   60|Train Avg Loss: 0.9050 |Test Loss: 0.8117|lr = 0.00115\n",
      "Epoch: 1207|steps:   30|Train Avg Loss: 0.8757 |Test Loss: 0.8139|lr = 0.00115\n",
      "Epoch: 1207|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8131|lr = 0.00115\n",
      "Epoch: 1208|steps:   30|Train Avg Loss: 0.8659 |Test Loss: 0.8126|lr = 0.00115\n",
      "Epoch: 1208|steps:   60|Train Avg Loss: 0.8951 |Test Loss: 0.8128|lr = 0.00115\n",
      "Epoch: 1209|steps:   30|Train Avg Loss: 0.8877 |Test Loss: 0.8147|lr = 0.00115\n",
      "Epoch: 1209|steps:   60|Train Avg Loss: 0.8695 |Test Loss: 0.8134|lr = 0.00115\n",
      "Epoch: 1210|steps:   30|Train Avg Loss: 0.8782 |Test Loss: 0.8124|lr = 0.00115\n",
      "Epoch: 1210|steps:   60|Train Avg Loss: 0.8729 |Test Loss: 0.8107|lr = 0.00115\n",
      "Epoch: 1211|steps:   30|Train Avg Loss: 0.9056 |Test Loss: 0.8161|lr = 0.00115\n",
      "Epoch: 1211|steps:   60|Train Avg Loss: 0.8676 |Test Loss: 0.8130|lr = 0.00115\n",
      "Epoch: 1212|steps:   30|Train Avg Loss: 0.9034 |Test Loss: 0.8159|lr = 0.00115\n",
      "Epoch: 1212|steps:   60|Train Avg Loss: 0.8798 |Test Loss: 0.8125|lr = 0.00115\n",
      "Epoch: 1213|steps:   30|Train Avg Loss: 0.8695 |Test Loss: 0.8110|lr = 0.00115\n",
      "Epoch: 1213|steps:   60|Train Avg Loss: 0.9008 |Test Loss: 0.8116|lr = 0.00115\n",
      "Epoch: 1214|steps:   30|Train Avg Loss: 0.8987 |Test Loss: 0.8158|lr = 0.00115\n",
      "Epoch: 1214|steps:   60|Train Avg Loss: 0.8839 |Test Loss: 0.8140|lr = 0.00115\n",
      "Epoch: 1215|steps:   30|Train Avg Loss: 0.8627 |Test Loss: 0.8097|lr = 0.00113\n",
      "Epoch: 1215|steps:   60|Train Avg Loss: 0.9156 |Test Loss: 0.8204|lr = 0.00113\n",
      "Epoch: 1216|steps:   30|Train Avg Loss: 0.8956 |Test Loss: 0.8161|lr = 0.00113\n",
      "Epoch: 1216|steps:   60|Train Avg Loss: 0.8804 |Test Loss: 0.8153|lr = 0.00113\n",
      "Epoch: 1217|steps:   30|Train Avg Loss: 0.8715 |Test Loss: 0.8118|lr = 0.00113\n",
      "Epoch: 1217|steps:   60|Train Avg Loss: 0.9039 |Test Loss: 0.8128|lr = 0.00113\n",
      "Epoch: 1218|steps:   30|Train Avg Loss: 0.8972 |Test Loss: 0.8148|lr = 0.00113\n",
      "Epoch: 1218|steps:   60|Train Avg Loss: 0.8653 |Test Loss: 0.8119|lr = 0.00113\n",
      "Epoch: 1219|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8141|lr = 0.00113\n",
      "Epoch: 1219|steps:   60|Train Avg Loss: 0.8792 |Test Loss: 0.8124|lr = 0.00113\n",
      "Epoch: 1220|steps:   30|Train Avg Loss: 0.8846 |Test Loss: 0.8139|lr = 0.00113\n",
      "Epoch: 1220|steps:   60|Train Avg Loss: 0.8897 |Test Loss: 0.8129|lr = 0.00113\n",
      "Epoch: 1221|steps:   30|Train Avg Loss: 0.8947 |Test Loss: 0.8129|lr = 0.00113\n",
      "Epoch: 1221|steps:   60|Train Avg Loss: 0.8860 |Test Loss: 0.8138|lr = 0.00113\n",
      "Epoch: 1222|steps:   30|Train Avg Loss: 0.8806 |Test Loss: 0.8134|lr = 0.00113\n",
      "Epoch: 1222|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8130|lr = 0.00113\n",
      "Epoch: 1223|steps:   30|Train Avg Loss: 0.9013 |Test Loss: 0.8161|lr = 0.00113\n",
      "Epoch: 1223|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8130|lr = 0.00113\n",
      "Epoch: 1224|steps:   30|Train Avg Loss: 0.8911 |Test Loss: 0.8157|lr = 0.00113\n",
      "Epoch: 1224|steps:   60|Train Avg Loss: 0.8904 |Test Loss: 0.8135|lr = 0.00113\n",
      "Epoch: 1225|steps:   30|Train Avg Loss: 0.8931 |Test Loss: 0.8143|lr = 0.00113\n",
      "Epoch: 1225|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8153|lr = 0.00113\n",
      "Epoch: 1226|steps:   30|Train Avg Loss: 0.8635 |Test Loss: 0.8062|lr = 0.00111\n",
      "Epoch: 1226|steps:   60|Train Avg Loss: 0.9063 |Test Loss: 0.8155|lr = 0.00111\n",
      "Epoch: 1227|steps:   30|Train Avg Loss: 0.8904 |Test Loss: 0.8135|lr = 0.00111\n",
      "Epoch: 1227|steps:   60|Train Avg Loss: 0.8807 |Test Loss: 0.8138|lr = 0.00111\n",
      "Epoch: 1228|steps:   30|Train Avg Loss: 0.8891 |Test Loss: 0.8139|lr = 0.00111\n",
      "Epoch: 1228|steps:   60|Train Avg Loss: 0.8803 |Test Loss: 0.8133|lr = 0.00111\n",
      "Epoch: 1229|steps:   30|Train Avg Loss: 0.8713 |Test Loss: 0.8117|lr = 0.00111\n",
      "Epoch: 1229|steps:   60|Train Avg Loss: 0.9021 |Test Loss: 0.8135|lr = 0.00111\n",
      "Epoch: 1230|steps:   30|Train Avg Loss: 0.8922 |Test Loss: 0.8133|lr = 0.00111\n",
      "Epoch: 1230|steps:   60|Train Avg Loss: 0.8772 |Test Loss: 0.8111|lr = 0.00111\n",
      "Epoch: 1231|steps:   30|Train Avg Loss: 0.8833 |Test Loss: 0.8145|lr = 0.00111\n",
      "Epoch: 1231|steps:   60|Train Avg Loss: 0.8980 |Test Loss: 0.8137|lr = 0.00111\n",
      "Epoch: 1232|steps:   30|Train Avg Loss: 0.8836 |Test Loss: 0.8119|lr = 0.00111\n",
      "Epoch: 1232|steps:   60|Train Avg Loss: 0.8880 |Test Loss: 0.8125|lr = 0.00111\n",
      "Epoch: 1233|steps:   30|Train Avg Loss: 0.8867 |Test Loss: 0.8140|lr = 0.00111\n",
      "Epoch: 1233|steps:   60|Train Avg Loss: 0.8759 |Test Loss: 0.8117|lr = 0.00111\n",
      "Epoch: 1234|steps:   30|Train Avg Loss: 0.8839 |Test Loss: 0.8130|lr = 0.00111\n",
      "Epoch: 1234|steps:   60|Train Avg Loss: 0.8889 |Test Loss: 0.8143|lr = 0.00111\n",
      "Epoch: 1235|steps:   30|Train Avg Loss: 0.9005 |Test Loss: 0.8134|lr = 0.00111\n",
      "Epoch: 1235|steps:   60|Train Avg Loss: 0.8619 |Test Loss: 0.8136|lr = 0.00111\n",
      "Epoch: 1236|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8085|lr = 0.00111\n",
      "Epoch: 1236|steps:   60|Train Avg Loss: 0.8959 |Test Loss: 0.8159|lr = 0.00111\n",
      "Epoch: 1237|steps:   30|Train Avg Loss: 0.8732 |Test Loss: 0.8134|lr = 0.00108\n",
      "Epoch: 1237|steps:   60|Train Avg Loss: 0.9004 |Test Loss: 0.8139|lr = 0.00108\n",
      "Epoch: 1238|steps:   30|Train Avg Loss: 0.8813 |Test Loss: 0.8139|lr = 0.00108\n",
      "Epoch: 1238|steps:   60|Train Avg Loss: 0.8891 |Test Loss: 0.8114|lr = 0.00108\n",
      "Epoch: 1239|steps:   30|Train Avg Loss: 0.8720 |Test Loss: 0.8129|lr = 0.00108\n",
      "Epoch: 1239|steps:   60|Train Avg Loss: 0.8934 |Test Loss: 0.8125|lr = 0.00108\n",
      "Epoch: 1240|steps:   30|Train Avg Loss: 0.8989 |Test Loss: 0.8137|lr = 0.00108\n",
      "Epoch: 1240|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8137|lr = 0.00108\n",
      "Epoch: 1241|steps:   30|Train Avg Loss: 0.8780 |Test Loss: 0.8122|lr = 0.00108\n",
      "Epoch: 1241|steps:   60|Train Avg Loss: 0.8952 |Test Loss: 0.8124|lr = 0.00108\n",
      "Epoch: 1242|steps:   30|Train Avg Loss: 0.8906 |Test Loss: 0.8151|lr = 0.00108\n",
      "Epoch: 1242|steps:   60|Train Avg Loss: 0.8761 |Test Loss: 0.8130|lr = 0.00108\n",
      "Epoch: 1243|steps:   30|Train Avg Loss: 0.9239 |Test Loss: 0.8180|lr = 0.00108\n",
      "Epoch: 1243|steps:   60|Train Avg Loss: 0.8499 |Test Loss: 0.8113|lr = 0.00108\n",
      "Epoch: 1244|steps:   30|Train Avg Loss: 0.8924 |Test Loss: 0.8121|lr = 0.00108\n",
      "Epoch: 1244|steps:   60|Train Avg Loss: 0.8872 |Test Loss: 0.8136|lr = 0.00108\n",
      "Epoch: 1245|steps:   30|Train Avg Loss: 0.8888 |Test Loss: 0.8139|lr = 0.00108\n",
      "Epoch: 1245|steps:   60|Train Avg Loss: 0.8785 |Test Loss: 0.8101|lr = 0.00108\n",
      "Epoch: 1246|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8135|lr = 0.00108\n",
      "Epoch: 1246|steps:   60|Train Avg Loss: 0.9026 |Test Loss: 0.8135|lr = 0.00108\n",
      "Epoch: 1247|steps:   30|Train Avg Loss: 0.8579 |Test Loss: 0.8098|lr = 0.00108\n",
      "Epoch: 1247|steps:   60|Train Avg Loss: 0.9187 |Test Loss: 0.8142|lr = 0.00108\n",
      "Epoch: 1248|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8145|lr = 0.00106\n",
      "Epoch: 1248|steps:   60|Train Avg Loss: 0.8749 |Test Loss: 0.8115|lr = 0.00106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1249|steps:   30|Train Avg Loss: 0.8977 |Test Loss: 0.8136|lr = 0.00106\n",
      "Epoch: 1249|steps:   60|Train Avg Loss: 0.8768 |Test Loss: 0.8120|lr = 0.00106\n",
      "Epoch: 1250|steps:   30|Train Avg Loss: 0.8815 |Test Loss: 0.8135|lr = 0.00106\n",
      "Epoch: 1250|steps:   60|Train Avg Loss: 0.9035 |Test Loss: 0.8147|lr = 0.00106\n",
      "Epoch: 1251|steps:   30|Train Avg Loss: 0.8772 |Test Loss: 0.8119|lr = 0.00106\n",
      "Epoch: 1251|steps:   60|Train Avg Loss: 0.8983 |Test Loss: 0.8157|lr = 0.00106\n",
      "Epoch: 1252|steps:   30|Train Avg Loss: 0.8921 |Test Loss: 0.8159|lr = 0.00106\n",
      "Epoch: 1252|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8121|lr = 0.00106\n",
      "Epoch: 1253|steps:   30|Train Avg Loss: 0.8972 |Test Loss: 0.8135|lr = 0.00106\n",
      "Epoch: 1253|steps:   60|Train Avg Loss: 0.8706 |Test Loss: 0.8111|lr = 0.00106\n",
      "Epoch: 1254|steps:   30|Train Avg Loss: 0.8925 |Test Loss: 0.8148|lr = 0.00106\n",
      "Epoch: 1254|steps:   60|Train Avg Loss: 0.8757 |Test Loss: 0.8131|lr = 0.00106\n",
      "Epoch: 1255|steps:   30|Train Avg Loss: 0.9112 |Test Loss: 0.8151|lr = 0.00106\n",
      "Epoch: 1255|steps:   60|Train Avg Loss: 0.8556 |Test Loss: 0.8110|lr = 0.00106\n",
      "Epoch: 1256|steps:   30|Train Avg Loss: 0.9065 |Test Loss: 0.8157|lr = 0.00106\n",
      "Epoch: 1256|steps:   60|Train Avg Loss: 0.8630 |Test Loss: 0.8132|lr = 0.00106\n",
      "Epoch: 1257|steps:   30|Train Avg Loss: 0.8934 |Test Loss: 0.8121|lr = 0.00106\n",
      "Epoch: 1257|steps:   60|Train Avg Loss: 0.8765 |Test Loss: 0.8134|lr = 0.00106\n",
      "Epoch: 1258|steps:   30|Train Avg Loss: 0.8832 |Test Loss: 0.8125|lr = 0.00106\n",
      "Epoch: 1258|steps:   60|Train Avg Loss: 0.8726 |Test Loss: 0.8118|lr = 0.00106\n",
      "Epoch: 1259|steps:   30|Train Avg Loss: 0.8984 |Test Loss: 0.8154|lr = 0.00104\n",
      "Epoch: 1259|steps:   60|Train Avg Loss: 0.8854 |Test Loss: 0.8133|lr = 0.00104\n",
      "Epoch: 1260|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8138|lr = 0.00104\n",
      "Epoch: 1260|steps:   60|Train Avg Loss: 0.8625 |Test Loss: 0.8109|lr = 0.00104\n",
      "Epoch: 1261|steps:   30|Train Avg Loss: 0.8731 |Test Loss: 0.8122|lr = 0.00104\n",
      "Epoch: 1261|steps:   60|Train Avg Loss: 0.8834 |Test Loss: 0.8133|lr = 0.00104\n",
      "Epoch: 1262|steps:   30|Train Avg Loss: 0.8828 |Test Loss: 0.8126|lr = 0.00104\n",
      "Epoch: 1262|steps:   60|Train Avg Loss: 0.9024 |Test Loss: 0.8138|lr = 0.00104\n",
      "Epoch: 1263|steps:   30|Train Avg Loss: 0.8872 |Test Loss: 0.8157|lr = 0.00104\n",
      "Epoch: 1263|steps:   60|Train Avg Loss: 0.8814 |Test Loss: 0.8139|lr = 0.00104\n",
      "Epoch: 1264|steps:   30|Train Avg Loss: 0.8671 |Test Loss: 0.8092|lr = 0.00104\n",
      "Epoch: 1264|steps:   60|Train Avg Loss: 0.8995 |Test Loss: 0.8156|lr = 0.00104\n",
      "Epoch: 1265|steps:   30|Train Avg Loss: 0.9104 |Test Loss: 0.8180|lr = 0.00104\n",
      "Epoch: 1265|steps:   60|Train Avg Loss: 0.8637 |Test Loss: 0.8104|lr = 0.00104\n",
      "Epoch: 1266|steps:   30|Train Avg Loss: 0.8779 |Test Loss: 0.8123|lr = 0.00104\n",
      "Epoch: 1266|steps:   60|Train Avg Loss: 0.8957 |Test Loss: 0.8122|lr = 0.00104\n",
      "Epoch: 1267|steps:   30|Train Avg Loss: 0.9145 |Test Loss: 0.8148|lr = 0.00104\n",
      "Epoch: 1267|steps:   60|Train Avg Loss: 0.8576 |Test Loss: 0.8114|lr = 0.00104\n",
      "Epoch: 1268|steps:   30|Train Avg Loss: 0.8738 |Test Loss: 0.8116|lr = 0.00104\n",
      "Epoch: 1268|steps:   60|Train Avg Loss: 0.8956 |Test Loss: 0.8127|lr = 0.00104\n",
      "Epoch: 1269|steps:   30|Train Avg Loss: 0.8982 |Test Loss: 0.8153|lr = 0.00104\n",
      "Epoch: 1269|steps:   60|Train Avg Loss: 0.8800 |Test Loss: 0.8138|lr = 0.00104\n",
      "Epoch: 1270|steps:   30|Train Avg Loss: 0.8802 |Test Loss: 0.8108|lr = 0.00102\n",
      "Epoch: 1270|steps:   60|Train Avg Loss: 0.8859 |Test Loss: 0.8120|lr = 0.00102\n",
      "Epoch: 1271|steps:   30|Train Avg Loss: 0.8600 |Test Loss: 0.8112|lr = 0.00102\n",
      "Epoch: 1271|steps:   60|Train Avg Loss: 0.8966 |Test Loss: 0.8122|lr = 0.00102\n",
      "Epoch: 1272|steps:   30|Train Avg Loss: 0.8941 |Test Loss: 0.8164|lr = 0.00102\n",
      "Epoch: 1272|steps:   60|Train Avg Loss: 0.8772 |Test Loss: 0.8133|lr = 0.00102\n",
      "Epoch: 1273|steps:   30|Train Avg Loss: 0.9020 |Test Loss: 0.8159|lr = 0.00102\n",
      "Epoch: 1273|steps:   60|Train Avg Loss: 0.8679 |Test Loss: 0.8140|lr = 0.00102\n",
      "Epoch: 1274|steps:   30|Train Avg Loss: 0.8793 |Test Loss: 0.8124|lr = 0.00102\n",
      "Epoch: 1274|steps:   60|Train Avg Loss: 0.8913 |Test Loss: 0.8136|lr = 0.00102\n",
      "Epoch: 1275|steps:   30|Train Avg Loss: 0.8789 |Test Loss: 0.8135|lr = 0.00102\n",
      "Epoch: 1275|steps:   60|Train Avg Loss: 0.9065 |Test Loss: 0.8142|lr = 0.00102\n",
      "Epoch: 1276|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8107|lr = 0.00102\n",
      "Epoch: 1276|steps:   60|Train Avg Loss: 0.8881 |Test Loss: 0.8113|lr = 0.00102\n",
      "Epoch: 1277|steps:   30|Train Avg Loss: 0.8930 |Test Loss: 0.8140|lr = 0.00102\n",
      "Epoch: 1277|steps:   60|Train Avg Loss: 0.8690 |Test Loss: 0.8123|lr = 0.00102\n",
      "Epoch: 1278|steps:   30|Train Avg Loss: 0.8921 |Test Loss: 0.8157|lr = 0.00102\n",
      "Epoch: 1278|steps:   60|Train Avg Loss: 0.8838 |Test Loss: 0.8144|lr = 0.00102\n",
      "Epoch: 1279|steps:   30|Train Avg Loss: 0.9002 |Test Loss: 0.8132|lr = 0.00102\n",
      "Epoch: 1279|steps:   60|Train Avg Loss: 0.8826 |Test Loss: 0.8143|lr = 0.00102\n",
      "Epoch: 1280|steps:   30|Train Avg Loss: 0.8973 |Test Loss: 0.8135|lr = 0.00102\n",
      "Epoch: 1280|steps:   60|Train Avg Loss: 0.8743 |Test Loss: 0.8124|lr = 0.00102\n",
      "Epoch: 1281|steps:   30|Train Avg Loss: 0.8828 |Test Loss: 0.8131|lr = 0.00100\n",
      "Epoch: 1281|steps:   60|Train Avg Loss: 0.8821 |Test Loss: 0.8136|lr = 0.00100\n",
      "Epoch: 1282|steps:   30|Train Avg Loss: 0.8883 |Test Loss: 0.8133|lr = 0.00100\n",
      "Epoch: 1282|steps:   60|Train Avg Loss: 0.8907 |Test Loss: 0.8145|lr = 0.00100\n",
      "Epoch: 1283|steps:   30|Train Avg Loss: 0.9053 |Test Loss: 0.8140|lr = 0.00100\n",
      "Epoch: 1283|steps:   60|Train Avg Loss: 0.8640 |Test Loss: 0.8128|lr = 0.00100\n",
      "Epoch: 1284|steps:   30|Train Avg Loss: 0.8615 |Test Loss: 0.8104|lr = 0.00100\n",
      "Epoch: 1284|steps:   60|Train Avg Loss: 0.9145 |Test Loss: 0.8154|lr = 0.00100\n",
      "Epoch: 1285|steps:   30|Train Avg Loss: 0.8811 |Test Loss: 0.8145|lr = 0.00100\n",
      "Epoch: 1285|steps:   60|Train Avg Loss: 0.8872 |Test Loss: 0.8121|lr = 0.00100\n",
      "Epoch: 1286|steps:   30|Train Avg Loss: 0.9042 |Test Loss: 0.8161|lr = 0.00100\n",
      "Epoch: 1286|steps:   60|Train Avg Loss: 0.8705 |Test Loss: 0.8140|lr = 0.00100\n",
      "Epoch: 1287|steps:   30|Train Avg Loss: 0.8904 |Test Loss: 0.8160|lr = 0.00100\n",
      "Epoch: 1287|steps:   60|Train Avg Loss: 0.8796 |Test Loss: 0.8122|lr = 0.00100\n",
      "Epoch: 1288|steps:   30|Train Avg Loss: 0.8822 |Test Loss: 0.8129|lr = 0.00100\n",
      "Epoch: 1288|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8140|lr = 0.00100\n",
      "Epoch: 1289|steps:   30|Train Avg Loss: 0.8888 |Test Loss: 0.8146|lr = 0.00100\n",
      "Epoch: 1289|steps:   60|Train Avg Loss: 0.8772 |Test Loss: 0.8131|lr = 0.00100\n",
      "Epoch: 1290|steps:   30|Train Avg Loss: 0.8836 |Test Loss: 0.8144|lr = 0.00100\n",
      "Epoch: 1290|steps:   60|Train Avg Loss: 0.8833 |Test Loss: 0.8137|lr = 0.00100\n",
      "Epoch: 1291|steps:   30|Train Avg Loss: 0.8873 |Test Loss: 0.8124|lr = 0.00100\n",
      "Epoch: 1291|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8143|lr = 0.00100\n",
      "Epoch: 1292|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8139|lr = 0.00098\n",
      "Epoch: 1292|steps:   60|Train Avg Loss: 0.8533 |Test Loss: 0.8099|lr = 0.00098\n",
      "Epoch: 1293|steps:   30|Train Avg Loss: 0.8629 |Test Loss: 0.8118|lr = 0.00098\n",
      "Epoch: 1293|steps:   60|Train Avg Loss: 0.9076 |Test Loss: 0.8151|lr = 0.00098\n",
      "Epoch: 1294|steps:   30|Train Avg Loss: 0.8831 |Test Loss: 0.8148|lr = 0.00098\n",
      "Epoch: 1294|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8137|lr = 0.00098\n",
      "Epoch: 1295|steps:   30|Train Avg Loss: 0.8912 |Test Loss: 0.8139|lr = 0.00098\n",
      "Epoch: 1295|steps:   60|Train Avg Loss: 0.8698 |Test Loss: 0.8122|lr = 0.00098\n",
      "Epoch: 1296|steps:   30|Train Avg Loss: 0.8790 |Test Loss: 0.8128|lr = 0.00098\n",
      "Epoch: 1296|steps:   60|Train Avg Loss: 0.8946 |Test Loss: 0.8134|lr = 0.00098\n",
      "Epoch: 1297|steps:   30|Train Avg Loss: 0.8805 |Test Loss: 0.8125|lr = 0.00098\n",
      "Epoch: 1297|steps:   60|Train Avg Loss: 0.8849 |Test Loss: 0.8133|lr = 0.00098\n",
      "Epoch: 1298|steps:   30|Train Avg Loss: 0.8625 |Test Loss: 0.8133|lr = 0.00098\n",
      "Epoch: 1298|steps:   60|Train Avg Loss: 0.9086 |Test Loss: 0.8138|lr = 0.00098\n",
      "Epoch: 1299|steps:   30|Train Avg Loss: 0.8785 |Test Loss: 0.8141|lr = 0.00098\n",
      "Epoch: 1299|steps:   60|Train Avg Loss: 0.8952 |Test Loss: 0.8151|lr = 0.00098\n",
      "Epoch: 1300|steps:   30|Train Avg Loss: 0.8857 |Test Loss: 0.8134|lr = 0.00098\n",
      "Epoch: 1300|steps:   60|Train Avg Loss: 0.8835 |Test Loss: 0.8148|lr = 0.00098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1301|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8134|lr = 0.00098\n",
      "Epoch: 1301|steps:   60|Train Avg Loss: 0.8908 |Test Loss: 0.8127|lr = 0.00098\n",
      "Epoch: 1302|steps:   30|Train Avg Loss: 0.8735 |Test Loss: 0.8116|lr = 0.00098\n",
      "Epoch: 1302|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8132|lr = 0.00098\n",
      "Epoch: 1303|steps:   30|Train Avg Loss: 0.8902 |Test Loss: 0.8152|lr = 0.00096\n",
      "Epoch: 1303|steps:   60|Train Avg Loss: 0.8837 |Test Loss: 0.8137|lr = 0.00096\n",
      "Epoch: 1304|steps:   30|Train Avg Loss: 0.9021 |Test Loss: 0.8161|lr = 0.00096\n",
      "Epoch: 1304|steps:   60|Train Avg Loss: 0.8658 |Test Loss: 0.8125|lr = 0.00096\n",
      "Epoch: 1305|steps:   30|Train Avg Loss: 0.8642 |Test Loss: 0.8112|lr = 0.00096\n",
      "Epoch: 1305|steps:   60|Train Avg Loss: 0.8978 |Test Loss: 0.8138|lr = 0.00096\n",
      "Epoch: 1306|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8122|lr = 0.00096\n",
      "Epoch: 1306|steps:   60|Train Avg Loss: 0.8912 |Test Loss: 0.8154|lr = 0.00096\n",
      "Epoch: 1307|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8131|lr = 0.00096\n",
      "Epoch: 1307|steps:   60|Train Avg Loss: 0.8801 |Test Loss: 0.8156|lr = 0.00096\n",
      "Epoch: 1308|steps:   30|Train Avg Loss: 0.8806 |Test Loss: 0.8130|lr = 0.00096\n",
      "Epoch: 1308|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8129|lr = 0.00096\n",
      "Epoch: 1309|steps:   30|Train Avg Loss: 0.8847 |Test Loss: 0.8137|lr = 0.00096\n",
      "Epoch: 1309|steps:   60|Train Avg Loss: 0.8871 |Test Loss: 0.8136|lr = 0.00096\n",
      "Epoch: 1310|steps:   30|Train Avg Loss: 0.8674 |Test Loss: 0.8118|lr = 0.00096\n",
      "Epoch: 1310|steps:   60|Train Avg Loss: 0.8976 |Test Loss: 0.8130|lr = 0.00096\n",
      "Epoch: 1311|steps:   30|Train Avg Loss: 0.8835 |Test Loss: 0.8152|lr = 0.00096\n",
      "Epoch: 1311|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8150|lr = 0.00096\n",
      "Epoch: 1312|steps:   30|Train Avg Loss: 0.8703 |Test Loss: 0.8119|lr = 0.00096\n",
      "Epoch: 1312|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8163|lr = 0.00096\n",
      "Epoch: 1313|steps:   30|Train Avg Loss: 0.9077 |Test Loss: 0.8167|lr = 0.00096\n",
      "Epoch: 1313|steps:   60|Train Avg Loss: 0.8689 |Test Loss: 0.8137|lr = 0.00096\n",
      "Epoch: 1314|steps:   30|Train Avg Loss: 0.8951 |Test Loss: 0.8136|lr = 0.00094\n",
      "Epoch: 1314|steps:   60|Train Avg Loss: 0.8753 |Test Loss: 0.8127|lr = 0.00094\n",
      "Epoch: 1315|steps:   30|Train Avg Loss: 0.8526 |Test Loss: 0.8106|lr = 0.00094\n",
      "Epoch: 1315|steps:   60|Train Avg Loss: 0.9181 |Test Loss: 0.8143|lr = 0.00094\n",
      "Epoch: 1316|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8132|lr = 0.00094\n",
      "Epoch: 1316|steps:   60|Train Avg Loss: 0.8806 |Test Loss: 0.8132|lr = 0.00094\n",
      "Epoch: 1317|steps:   30|Train Avg Loss: 0.8933 |Test Loss: 0.8147|lr = 0.00094\n",
      "Epoch: 1317|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8142|lr = 0.00094\n",
      "Epoch: 1318|steps:   30|Train Avg Loss: 0.8806 |Test Loss: 0.8120|lr = 0.00094\n",
      "Epoch: 1318|steps:   60|Train Avg Loss: 0.8959 |Test Loss: 0.8142|lr = 0.00094\n",
      "Epoch: 1319|steps:   30|Train Avg Loss: 0.8587 |Test Loss: 0.8112|lr = 0.00094\n",
      "Epoch: 1319|steps:   60|Train Avg Loss: 0.9007 |Test Loss: 0.8121|lr = 0.00094\n",
      "Epoch: 1320|steps:   30|Train Avg Loss: 0.8756 |Test Loss: 0.8155|lr = 0.00094\n",
      "Epoch: 1320|steps:   60|Train Avg Loss: 0.8957 |Test Loss: 0.8148|lr = 0.00094\n",
      "Epoch: 1321|steps:   30|Train Avg Loss: 0.8809 |Test Loss: 0.8137|lr = 0.00094\n",
      "Epoch: 1321|steps:   60|Train Avg Loss: 0.8820 |Test Loss: 0.8122|lr = 0.00094\n",
      "Epoch: 1322|steps:   30|Train Avg Loss: 0.8685 |Test Loss: 0.8135|lr = 0.00094\n",
      "Epoch: 1322|steps:   60|Train Avg Loss: 0.9167 |Test Loss: 0.8168|lr = 0.00094\n",
      "Epoch: 1323|steps:   30|Train Avg Loss: 0.8831 |Test Loss: 0.8144|lr = 0.00094\n",
      "Epoch: 1323|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8154|lr = 0.00094\n",
      "Epoch: 1324|steps:   30|Train Avg Loss: 0.8712 |Test Loss: 0.8111|lr = 0.00094\n",
      "Epoch: 1324|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8140|lr = 0.00094\n",
      "Epoch: 1325|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8157|lr = 0.00092\n",
      "Epoch: 1325|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8158|lr = 0.00092\n",
      "Epoch: 1326|steps:   30|Train Avg Loss: 0.8918 |Test Loss: 0.8136|lr = 0.00092\n",
      "Epoch: 1326|steps:   60|Train Avg Loss: 0.8959 |Test Loss: 0.8162|lr = 0.00092\n",
      "Epoch: 1327|steps:   30|Train Avg Loss: 0.8690 |Test Loss: 0.8110|lr = 0.00092\n",
      "Epoch: 1327|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8126|lr = 0.00092\n",
      "Epoch: 1328|steps:   30|Train Avg Loss: 0.9062 |Test Loss: 0.8164|lr = 0.00092\n",
      "Epoch: 1328|steps:   60|Train Avg Loss: 0.8728 |Test Loss: 0.8122|lr = 0.00092\n",
      "Epoch: 1329|steps:   30|Train Avg Loss: 0.8966 |Test Loss: 0.8153|lr = 0.00092\n",
      "Epoch: 1329|steps:   60|Train Avg Loss: 0.8683 |Test Loss: 0.8133|lr = 0.00092\n",
      "Epoch: 1330|steps:   30|Train Avg Loss: 0.8657 |Test Loss: 0.8122|lr = 0.00092\n",
      "Epoch: 1330|steps:   60|Train Avg Loss: 0.9149 |Test Loss: 0.8151|lr = 0.00092\n",
      "Epoch: 1331|steps:   30|Train Avg Loss: 0.9004 |Test Loss: 0.8157|lr = 0.00092\n",
      "Epoch: 1331|steps:   60|Train Avg Loss: 0.8586 |Test Loss: 0.8127|lr = 0.00092\n",
      "Epoch: 1332|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8134|lr = 0.00092\n",
      "Epoch: 1332|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8139|lr = 0.00092\n",
      "Epoch: 1333|steps:   30|Train Avg Loss: 0.8879 |Test Loss: 0.8138|lr = 0.00092\n",
      "Epoch: 1333|steps:   60|Train Avg Loss: 0.8818 |Test Loss: 0.8137|lr = 0.00092\n",
      "Epoch: 1334|steps:   30|Train Avg Loss: 0.8822 |Test Loss: 0.8135|lr = 0.00092\n",
      "Epoch: 1334|steps:   60|Train Avg Loss: 0.8966 |Test Loss: 0.8133|lr = 0.00092\n",
      "Epoch: 1335|steps:   30|Train Avg Loss: 0.8954 |Test Loss: 0.8133|lr = 0.00092\n",
      "Epoch: 1335|steps:   60|Train Avg Loss: 0.8776 |Test Loss: 0.8137|lr = 0.00092\n",
      "Epoch: 1336|steps:   30|Train Avg Loss: 0.8895 |Test Loss: 0.8138|lr = 0.00090\n",
      "Epoch: 1336|steps:   60|Train Avg Loss: 0.8855 |Test Loss: 0.8163|lr = 0.00090\n",
      "Epoch: 1337|steps:   30|Train Avg Loss: 0.8862 |Test Loss: 0.8128|lr = 0.00090\n",
      "Epoch: 1337|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8136|lr = 0.00090\n",
      "Epoch: 1338|steps:   30|Train Avg Loss: 0.8884 |Test Loss: 0.8131|lr = 0.00090\n",
      "Epoch: 1338|steps:   60|Train Avg Loss: 0.8848 |Test Loss: 0.8158|lr = 0.00090\n",
      "Epoch: 1339|steps:   30|Train Avg Loss: 0.8802 |Test Loss: 0.8127|lr = 0.00090\n",
      "Epoch: 1339|steps:   60|Train Avg Loss: 0.8918 |Test Loss: 0.8143|lr = 0.00090\n",
      "Epoch: 1340|steps:   30|Train Avg Loss: 0.8769 |Test Loss: 0.8144|lr = 0.00090\n",
      "Epoch: 1340|steps:   60|Train Avg Loss: 0.8834 |Test Loss: 0.8133|lr = 0.00090\n",
      "Epoch: 1341|steps:   30|Train Avg Loss: 0.8840 |Test Loss: 0.8143|lr = 0.00090\n",
      "Epoch: 1341|steps:   60|Train Avg Loss: 0.8746 |Test Loss: 0.8134|lr = 0.00090\n",
      "Epoch: 1342|steps:   30|Train Avg Loss: 0.8869 |Test Loss: 0.8134|lr = 0.00090\n",
      "Epoch: 1342|steps:   60|Train Avg Loss: 0.8816 |Test Loss: 0.8146|lr = 0.00090\n",
      "Epoch: 1343|steps:   30|Train Avg Loss: 0.8886 |Test Loss: 0.8142|lr = 0.00090\n",
      "Epoch: 1343|steps:   60|Train Avg Loss: 0.8845 |Test Loss: 0.8143|lr = 0.00090\n",
      "Epoch: 1344|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8136|lr = 0.00090\n",
      "Epoch: 1344|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8139|lr = 0.00090\n",
      "Epoch: 1345|steps:   30|Train Avg Loss: 0.9093 |Test Loss: 0.8160|lr = 0.00090\n",
      "Epoch: 1345|steps:   60|Train Avg Loss: 0.8720 |Test Loss: 0.8143|lr = 0.00090\n",
      "Epoch: 1346|steps:   30|Train Avg Loss: 0.8881 |Test Loss: 0.8139|lr = 0.00090\n",
      "Epoch: 1346|steps:   60|Train Avg Loss: 0.8816 |Test Loss: 0.8127|lr = 0.00090\n",
      "Epoch: 1347|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8127|lr = 0.00089\n",
      "Epoch: 1347|steps:   60|Train Avg Loss: 0.8981 |Test Loss: 0.8149|lr = 0.00089\n",
      "Epoch: 1348|steps:   30|Train Avg Loss: 0.8908 |Test Loss: 0.8139|lr = 0.00089\n",
      "Epoch: 1348|steps:   60|Train Avg Loss: 0.8887 |Test Loss: 0.8143|lr = 0.00089\n",
      "Epoch: 1349|steps:   30|Train Avg Loss: 0.8968 |Test Loss: 0.8153|lr = 0.00089\n",
      "Epoch: 1349|steps:   60|Train Avg Loss: 0.8687 |Test Loss: 0.8124|lr = 0.00089\n",
      "Epoch: 1350|steps:   30|Train Avg Loss: 0.8829 |Test Loss: 0.8143|lr = 0.00089\n",
      "Epoch: 1350|steps:   60|Train Avg Loss: 0.8874 |Test Loss: 0.8154|lr = 0.00089\n",
      "Epoch: 1351|steps:   30|Train Avg Loss: 0.8730 |Test Loss: 0.8124|lr = 0.00089\n",
      "Epoch: 1351|steps:   60|Train Avg Loss: 0.8935 |Test Loss: 0.8144|lr = 0.00089\n",
      "Epoch: 1352|steps:   30|Train Avg Loss: 0.8837 |Test Loss: 0.8151|lr = 0.00089\n",
      "Epoch: 1352|steps:   60|Train Avg Loss: 0.8936 |Test Loss: 0.8145|lr = 0.00089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1353|steps:   30|Train Avg Loss: 0.9002 |Test Loss: 0.8165|lr = 0.00089\n",
      "Epoch: 1353|steps:   60|Train Avg Loss: 0.8731 |Test Loss: 0.8141|lr = 0.00089\n",
      "Epoch: 1354|steps:   30|Train Avg Loss: 0.8860 |Test Loss: 0.8134|lr = 0.00089\n",
      "Epoch: 1354|steps:   60|Train Avg Loss: 0.8735 |Test Loss: 0.8134|lr = 0.00089\n",
      "Epoch: 1355|steps:   30|Train Avg Loss: 0.8811 |Test Loss: 0.8123|lr = 0.00089\n",
      "Epoch: 1355|steps:   60|Train Avg Loss: 0.8969 |Test Loss: 0.8132|lr = 0.00089\n",
      "Epoch: 1356|steps:   30|Train Avg Loss: 0.8842 |Test Loss: 0.8130|lr = 0.00089\n",
      "Epoch: 1356|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8166|lr = 0.00089\n",
      "Epoch: 1357|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8159|lr = 0.00089\n",
      "Epoch: 1357|steps:   60|Train Avg Loss: 0.9023 |Test Loss: 0.8154|lr = 0.00089\n",
      "Epoch: 1358|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8137|lr = 0.00087\n",
      "Epoch: 1358|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8132|lr = 0.00087\n",
      "Epoch: 1359|steps:   30|Train Avg Loss: 0.8940 |Test Loss: 0.8158|lr = 0.00087\n",
      "Epoch: 1359|steps:   60|Train Avg Loss: 0.8790 |Test Loss: 0.8134|lr = 0.00087\n",
      "Epoch: 1360|steps:   30|Train Avg Loss: 0.8465 |Test Loss: 0.8104|lr = 0.00087\n",
      "Epoch: 1360|steps:   60|Train Avg Loss: 0.9067 |Test Loss: 0.8120|lr = 0.00087\n",
      "Epoch: 1361|steps:   30|Train Avg Loss: 0.8828 |Test Loss: 0.8142|lr = 0.00087\n",
      "Epoch: 1361|steps:   60|Train Avg Loss: 0.8938 |Test Loss: 0.8144|lr = 0.00087\n",
      "Epoch: 1362|steps:   30|Train Avg Loss: 0.8754 |Test Loss: 0.8122|lr = 0.00087\n",
      "Epoch: 1362|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8143|lr = 0.00087\n",
      "Epoch: 1363|steps:   30|Train Avg Loss: 0.8811 |Test Loss: 0.8131|lr = 0.00087\n",
      "Epoch: 1363|steps:   60|Train Avg Loss: 0.8731 |Test Loss: 0.8122|lr = 0.00087\n",
      "Epoch: 1364|steps:   30|Train Avg Loss: 0.8689 |Test Loss: 0.8142|lr = 0.00087\n",
      "Epoch: 1364|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8150|lr = 0.00087\n",
      "Epoch: 1365|steps:   30|Train Avg Loss: 0.8952 |Test Loss: 0.8155|lr = 0.00087\n",
      "Epoch: 1365|steps:   60|Train Avg Loss: 0.8616 |Test Loss: 0.8129|lr = 0.00087\n",
      "Epoch: 1366|steps:   30|Train Avg Loss: 0.8673 |Test Loss: 0.8114|lr = 0.00087\n",
      "Epoch: 1366|steps:   60|Train Avg Loss: 0.8979 |Test Loss: 0.8137|lr = 0.00087\n",
      "Epoch: 1367|steps:   30|Train Avg Loss: 0.9031 |Test Loss: 0.8204|lr = 0.00087\n",
      "Epoch: 1367|steps:   60|Train Avg Loss: 0.8835 |Test Loss: 0.8160|lr = 0.00087\n",
      "Epoch: 1368|steps:   30|Train Avg Loss: 0.8922 |Test Loss: 0.8121|lr = 0.00087\n",
      "Epoch: 1368|steps:   60|Train Avg Loss: 0.8817 |Test Loss: 0.8143|lr = 0.00087\n",
      "Epoch: 1369|steps:   30|Train Avg Loss: 0.8755 |Test Loss: 0.8122|lr = 0.00085\n",
      "Epoch: 1369|steps:   60|Train Avg Loss: 0.8759 |Test Loss: 0.8121|lr = 0.00085\n",
      "Epoch: 1370|steps:   30|Train Avg Loss: 0.8868 |Test Loss: 0.8136|lr = 0.00085\n",
      "Epoch: 1370|steps:   60|Train Avg Loss: 0.8880 |Test Loss: 0.8162|lr = 0.00085\n",
      "Epoch: 1371|steps:   30|Train Avg Loss: 0.8992 |Test Loss: 0.8182|lr = 0.00085\n",
      "Epoch: 1371|steps:   60|Train Avg Loss: 0.8698 |Test Loss: 0.8130|lr = 0.00085\n",
      "Epoch: 1372|steps:   30|Train Avg Loss: 0.8849 |Test Loss: 0.8130|lr = 0.00085\n",
      "Epoch: 1372|steps:   60|Train Avg Loss: 0.8729 |Test Loss: 0.8126|lr = 0.00085\n",
      "Epoch: 1373|steps:   30|Train Avg Loss: 0.8746 |Test Loss: 0.8133|lr = 0.00085\n",
      "Epoch: 1373|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 0.8136|lr = 0.00085\n",
      "Epoch: 1374|steps:   30|Train Avg Loss: 0.8666 |Test Loss: 0.8130|lr = 0.00085\n",
      "Epoch: 1374|steps:   60|Train Avg Loss: 0.9010 |Test Loss: 0.8150|lr = 0.00085\n",
      "Epoch: 1375|steps:   30|Train Avg Loss: 0.8733 |Test Loss: 0.8140|lr = 0.00085\n",
      "Epoch: 1375|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8134|lr = 0.00085\n",
      "Epoch: 1376|steps:   30|Train Avg Loss: 0.8952 |Test Loss: 0.8172|lr = 0.00085\n",
      "Epoch: 1376|steps:   60|Train Avg Loss: 0.8776 |Test Loss: 0.8140|lr = 0.00085\n",
      "Epoch: 1377|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8131|lr = 0.00085\n",
      "Epoch: 1377|steps:   60|Train Avg Loss: 0.8816 |Test Loss: 0.8141|lr = 0.00085\n",
      "Epoch: 1378|steps:   30|Train Avg Loss: 0.8670 |Test Loss: 0.8125|lr = 0.00085\n",
      "Epoch: 1378|steps:   60|Train Avg Loss: 0.8915 |Test Loss: 0.8132|lr = 0.00085\n",
      "Epoch: 1379|steps:   30|Train Avg Loss: 0.8916 |Test Loss: 0.8150|lr = 0.00085\n",
      "Epoch: 1379|steps:   60|Train Avg Loss: 0.8829 |Test Loss: 0.8144|lr = 0.00085\n",
      "Epoch: 1380|steps:   30|Train Avg Loss: 0.8829 |Test Loss: 0.8127|lr = 0.00083\n",
      "Epoch: 1380|steps:   60|Train Avg Loss: 0.8863 |Test Loss: 0.8135|lr = 0.00083\n",
      "Epoch: 1381|steps:   30|Train Avg Loss: 0.8743 |Test Loss: 0.8138|lr = 0.00083\n",
      "Epoch: 1381|steps:   60|Train Avg Loss: 0.9058 |Test Loss: 0.8154|lr = 0.00083\n",
      "Epoch: 1382|steps:   30|Train Avg Loss: 0.8841 |Test Loss: 0.8125|lr = 0.00083\n",
      "Epoch: 1382|steps:   60|Train Avg Loss: 0.8913 |Test Loss: 0.8145|lr = 0.00083\n",
      "Epoch: 1383|steps:   30|Train Avg Loss: 0.8765 |Test Loss: 0.8130|lr = 0.00083\n",
      "Epoch: 1383|steps:   60|Train Avg Loss: 0.8978 |Test Loss: 0.8150|lr = 0.00083\n",
      "Epoch: 1384|steps:   30|Train Avg Loss: 0.8768 |Test Loss: 0.8140|lr = 0.00083\n",
      "Epoch: 1384|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8131|lr = 0.00083\n",
      "Epoch: 1385|steps:   30|Train Avg Loss: 0.8872 |Test Loss: 0.8150|lr = 0.00083\n",
      "Epoch: 1385|steps:   60|Train Avg Loss: 0.8766 |Test Loss: 0.8132|lr = 0.00083\n",
      "Epoch: 1386|steps:   30|Train Avg Loss: 0.9052 |Test Loss: 0.8167|lr = 0.00083\n",
      "Epoch: 1386|steps:   60|Train Avg Loss: 0.8643 |Test Loss: 0.8142|lr = 0.00083\n",
      "Epoch: 1387|steps:   30|Train Avg Loss: 0.8919 |Test Loss: 0.8129|lr = 0.00083\n",
      "Epoch: 1387|steps:   60|Train Avg Loss: 0.8855 |Test Loss: 0.8141|lr = 0.00083\n",
      "Epoch: 1388|steps:   30|Train Avg Loss: 0.8990 |Test Loss: 0.8147|lr = 0.00083\n",
      "Epoch: 1388|steps:   60|Train Avg Loss: 0.8858 |Test Loss: 0.8145|lr = 0.00083\n",
      "Epoch: 1389|steps:   30|Train Avg Loss: 0.8746 |Test Loss: 0.8123|lr = 0.00083\n",
      "Epoch: 1389|steps:   60|Train Avg Loss: 0.8977 |Test Loss: 0.8140|lr = 0.00083\n",
      "Epoch: 1390|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8152|lr = 0.00083\n",
      "Epoch: 1390|steps:   60|Train Avg Loss: 0.8730 |Test Loss: 0.8122|lr = 0.00083\n",
      "Epoch: 1391|steps:   30|Train Avg Loss: 0.8884 |Test Loss: 0.8143|lr = 0.00082\n",
      "Epoch: 1391|steps:   60|Train Avg Loss: 0.8909 |Test Loss: 0.8152|lr = 0.00082\n",
      "Epoch: 1392|steps:   30|Train Avg Loss: 0.8712 |Test Loss: 0.8145|lr = 0.00082\n",
      "Epoch: 1392|steps:   60|Train Avg Loss: 0.8997 |Test Loss: 0.8152|lr = 0.00082\n",
      "Epoch: 1393|steps:   30|Train Avg Loss: 0.9035 |Test Loss: 0.8154|lr = 0.00082\n",
      "Epoch: 1393|steps:   60|Train Avg Loss: 0.8751 |Test Loss: 0.8137|lr = 0.00082\n",
      "Epoch: 1394|steps:   30|Train Avg Loss: 0.8698 |Test Loss: 0.8132|lr = 0.00082\n",
      "Epoch: 1394|steps:   60|Train Avg Loss: 0.8986 |Test Loss: 0.8143|lr = 0.00082\n",
      "Epoch: 1395|steps:   30|Train Avg Loss: 0.8771 |Test Loss: 0.8144|lr = 0.00082\n",
      "Epoch: 1395|steps:   60|Train Avg Loss: 0.8817 |Test Loss: 0.8132|lr = 0.00082\n",
      "Epoch: 1396|steps:   30|Train Avg Loss: 0.8931 |Test Loss: 0.8143|lr = 0.00082\n",
      "Epoch: 1396|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8136|lr = 0.00082\n",
      "Epoch: 1397|steps:   30|Train Avg Loss: 0.8827 |Test Loss: 0.8142|lr = 0.00082\n",
      "Epoch: 1397|steps:   60|Train Avg Loss: 0.8948 |Test Loss: 0.8144|lr = 0.00082\n",
      "Epoch: 1398|steps:   30|Train Avg Loss: 0.8912 |Test Loss: 0.8153|lr = 0.00082\n",
      "Epoch: 1398|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8145|lr = 0.00082\n",
      "Epoch: 1399|steps:   30|Train Avg Loss: 0.8773 |Test Loss: 0.8141|lr = 0.00082\n",
      "Epoch: 1399|steps:   60|Train Avg Loss: 0.8954 |Test Loss: 0.8145|lr = 0.00082\n",
      "Epoch: 1400|steps:   30|Train Avg Loss: 0.8678 |Test Loss: 0.8108|lr = 0.00082\n",
      "Epoch: 1400|steps:   60|Train Avg Loss: 0.9002 |Test Loss: 0.8171|lr = 0.00082\n",
      "Epoch: 1401|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8174|lr = 0.00082\n",
      "Epoch: 1401|steps:   60|Train Avg Loss: 0.8892 |Test Loss: 0.8152|lr = 0.00082\n",
      "Epoch: 1402|steps:   30|Train Avg Loss: 0.8846 |Test Loss: 0.8138|lr = 0.00080\n",
      "Epoch: 1402|steps:   60|Train Avg Loss: 0.8932 |Test Loss: 0.8145|lr = 0.00080\n",
      "Epoch: 1403|steps:   30|Train Avg Loss: 0.8843 |Test Loss: 0.8143|lr = 0.00080\n",
      "Epoch: 1403|steps:   60|Train Avg Loss: 0.9024 |Test Loss: 0.8160|lr = 0.00080\n",
      "Epoch: 1404|steps:   30|Train Avg Loss: 0.9002 |Test Loss: 0.8145|lr = 0.00080\n",
      "Epoch: 1404|steps:   60|Train Avg Loss: 0.8673 |Test Loss: 0.8136|lr = 0.00080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1405|steps:   30|Train Avg Loss: 0.8825 |Test Loss: 0.8135|lr = 0.00080\n",
      "Epoch: 1405|steps:   60|Train Avg Loss: 0.9010 |Test Loss: 0.8152|lr = 0.00080\n",
      "Epoch: 1406|steps:   30|Train Avg Loss: 0.8707 |Test Loss: 0.8134|lr = 0.00080\n",
      "Epoch: 1406|steps:   60|Train Avg Loss: 0.8921 |Test Loss: 0.8139|lr = 0.00080\n",
      "Epoch: 1407|steps:   30|Train Avg Loss: 0.8947 |Test Loss: 0.8159|lr = 0.00080\n",
      "Epoch: 1407|steps:   60|Train Avg Loss: 0.8763 |Test Loss: 0.8151|lr = 0.00080\n",
      "Epoch: 1408|steps:   30|Train Avg Loss: 0.8867 |Test Loss: 0.8154|lr = 0.00080\n",
      "Epoch: 1408|steps:   60|Train Avg Loss: 0.8751 |Test Loss: 0.8134|lr = 0.00080\n",
      "Epoch: 1409|steps:   30|Train Avg Loss: 0.8905 |Test Loss: 0.8149|lr = 0.00080\n",
      "Epoch: 1409|steps:   60|Train Avg Loss: 0.8805 |Test Loss: 0.8145|lr = 0.00080\n",
      "Epoch: 1410|steps:   30|Train Avg Loss: 0.8665 |Test Loss: 0.8140|lr = 0.00080\n",
      "Epoch: 1410|steps:   60|Train Avg Loss: 0.8891 |Test Loss: 0.8128|lr = 0.00080\n",
      "Epoch: 1411|steps:   30|Train Avg Loss: 0.8881 |Test Loss: 0.8170|lr = 0.00080\n",
      "Epoch: 1411|steps:   60|Train Avg Loss: 0.8649 |Test Loss: 0.8119|lr = 0.00080\n",
      "Epoch: 1412|steps:   30|Train Avg Loss: 0.8779 |Test Loss: 0.8133|lr = 0.00080\n",
      "Epoch: 1412|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8141|lr = 0.00080\n",
      "Epoch: 1413|steps:   30|Train Avg Loss: 0.8783 |Test Loss: 0.8129|lr = 0.00078\n",
      "Epoch: 1413|steps:   60|Train Avg Loss: 0.8966 |Test Loss: 0.8149|lr = 0.00078\n",
      "Epoch: 1414|steps:   30|Train Avg Loss: 0.8967 |Test Loss: 0.8157|lr = 0.00078\n",
      "Epoch: 1414|steps:   60|Train Avg Loss: 0.8604 |Test Loss: 0.8126|lr = 0.00078\n",
      "Epoch: 1415|steps:   30|Train Avg Loss: 0.9100 |Test Loss: 0.8157|lr = 0.00078\n",
      "Epoch: 1415|steps:   60|Train Avg Loss: 0.8556 |Test Loss: 0.8126|lr = 0.00078\n",
      "Epoch: 1416|steps:   30|Train Avg Loss: 0.8786 |Test Loss: 0.8159|lr = 0.00078\n",
      "Epoch: 1416|steps:   60|Train Avg Loss: 0.8846 |Test Loss: 0.8146|lr = 0.00078\n",
      "Epoch: 1417|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8135|lr = 0.00078\n",
      "Epoch: 1417|steps:   60|Train Avg Loss: 0.8654 |Test Loss: 0.8095|lr = 0.00078\n",
      "Epoch: 1418|steps:   30|Train Avg Loss: 0.8787 |Test Loss: 0.8138|lr = 0.00078\n",
      "Epoch: 1418|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8151|lr = 0.00078\n",
      "Epoch: 1419|steps:   30|Train Avg Loss: 0.8830 |Test Loss: 0.8150|lr = 0.00078\n",
      "Epoch: 1419|steps:   60|Train Avg Loss: 0.8817 |Test Loss: 0.8134|lr = 0.00078\n",
      "Epoch: 1420|steps:   30|Train Avg Loss: 0.8919 |Test Loss: 0.8157|lr = 0.00078\n",
      "Epoch: 1420|steps:   60|Train Avg Loss: 0.8932 |Test Loss: 0.8161|lr = 0.00078\n",
      "Epoch: 1421|steps:   30|Train Avg Loss: 0.8916 |Test Loss: 0.8142|lr = 0.00078\n",
      "Epoch: 1421|steps:   60|Train Avg Loss: 0.8834 |Test Loss: 0.8143|lr = 0.00078\n",
      "Epoch: 1422|steps:   30|Train Avg Loss: 0.8969 |Test Loss: 0.8144|lr = 0.00078\n",
      "Epoch: 1422|steps:   60|Train Avg Loss: 0.8632 |Test Loss: 0.8142|lr = 0.00078\n",
      "Epoch: 1423|steps:   30|Train Avg Loss: 0.8852 |Test Loss: 0.8145|lr = 0.00078\n",
      "Epoch: 1423|steps:   60|Train Avg Loss: 0.8796 |Test Loss: 0.8135|lr = 0.00078\n",
      "Epoch: 1424|steps:   30|Train Avg Loss: 0.8787 |Test Loss: 0.8141|lr = 0.00077\n",
      "Epoch: 1424|steps:   60|Train Avg Loss: 0.9024 |Test Loss: 0.8158|lr = 0.00077\n",
      "Epoch: 1425|steps:   30|Train Avg Loss: 0.8813 |Test Loss: 0.8143|lr = 0.00077\n",
      "Epoch: 1425|steps:   60|Train Avg Loss: 0.8981 |Test Loss: 0.8157|lr = 0.00077\n",
      "Epoch: 1426|steps:   30|Train Avg Loss: 0.8955 |Test Loss: 0.8146|lr = 0.00077\n",
      "Epoch: 1426|steps:   60|Train Avg Loss: 0.8782 |Test Loss: 0.8153|lr = 0.00077\n",
      "Epoch: 1427|steps:   30|Train Avg Loss: 0.8719 |Test Loss: 0.8129|lr = 0.00077\n",
      "Epoch: 1427|steps:   60|Train Avg Loss: 0.8894 |Test Loss: 0.8137|lr = 0.00077\n",
      "Epoch: 1428|steps:   30|Train Avg Loss: 0.8897 |Test Loss: 0.8150|lr = 0.00077\n",
      "Epoch: 1428|steps:   60|Train Avg Loss: 0.8703 |Test Loss: 0.8140|lr = 0.00077\n",
      "Epoch: 1429|steps:   30|Train Avg Loss: 0.8821 |Test Loss: 0.8140|lr = 0.00077\n",
      "Epoch: 1429|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8148|lr = 0.00077\n",
      "Epoch: 1430|steps:   30|Train Avg Loss: 0.8964 |Test Loss: 0.8156|lr = 0.00077\n",
      "Epoch: 1430|steps:   60|Train Avg Loss: 0.8692 |Test Loss: 0.8142|lr = 0.00077\n",
      "Epoch: 1431|steps:   30|Train Avg Loss: 0.8900 |Test Loss: 0.8164|lr = 0.00077\n",
      "Epoch: 1431|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8150|lr = 0.00077\n",
      "Epoch: 1432|steps:   30|Train Avg Loss: 0.8985 |Test Loss: 0.8154|lr = 0.00077\n",
      "Epoch: 1432|steps:   60|Train Avg Loss: 0.8767 |Test Loss: 0.8136|lr = 0.00077\n",
      "Epoch: 1433|steps:   30|Train Avg Loss: 0.8688 |Test Loss: 0.8120|lr = 0.00077\n",
      "Epoch: 1433|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8134|lr = 0.00077\n",
      "Epoch: 1434|steps:   30|Train Avg Loss: 0.8703 |Test Loss: 0.8138|lr = 0.00077\n",
      "Epoch: 1434|steps:   60|Train Avg Loss: 0.8796 |Test Loss: 0.8132|lr = 0.00077\n",
      "Epoch: 1435|steps:   30|Train Avg Loss: 0.8613 |Test Loss: 0.8123|lr = 0.00075\n",
      "Epoch: 1435|steps:   60|Train Avg Loss: 0.9084 |Test Loss: 0.8156|lr = 0.00075\n",
      "Epoch: 1436|steps:   30|Train Avg Loss: 0.8856 |Test Loss: 0.8138|lr = 0.00075\n",
      "Epoch: 1436|steps:   60|Train Avg Loss: 0.8814 |Test Loss: 0.8153|lr = 0.00075\n",
      "Epoch: 1437|steps:   30|Train Avg Loss: 0.8734 |Test Loss: 0.8142|lr = 0.00075\n",
      "Epoch: 1437|steps:   60|Train Avg Loss: 0.8951 |Test Loss: 0.8163|lr = 0.00075\n",
      "Epoch: 1438|steps:   30|Train Avg Loss: 0.8757 |Test Loss: 0.8156|lr = 0.00075\n",
      "Epoch: 1438|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8122|lr = 0.00075\n",
      "Epoch: 1439|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8127|lr = 0.00075\n",
      "Epoch: 1439|steps:   60|Train Avg Loss: 0.8697 |Test Loss: 0.8135|lr = 0.00075\n",
      "Epoch: 1440|steps:   30|Train Avg Loss: 0.8628 |Test Loss: 0.8124|lr = 0.00075\n",
      "Epoch: 1440|steps:   60|Train Avg Loss: 0.9029 |Test Loss: 0.8140|lr = 0.00075\n",
      "Epoch: 1441|steps:   30|Train Avg Loss: 0.8936 |Test Loss: 0.8150|lr = 0.00075\n",
      "Epoch: 1441|steps:   60|Train Avg Loss: 0.8731 |Test Loss: 0.8153|lr = 0.00075\n",
      "Epoch: 1442|steps:   30|Train Avg Loss: 0.8929 |Test Loss: 0.8164|lr = 0.00075\n",
      "Epoch: 1442|steps:   60|Train Avg Loss: 0.8841 |Test Loss: 0.8156|lr = 0.00075\n",
      "Epoch: 1443|steps:   30|Train Avg Loss: 0.8951 |Test Loss: 0.8149|lr = 0.00075\n",
      "Epoch: 1443|steps:   60|Train Avg Loss: 0.8787 |Test Loss: 0.8155|lr = 0.00075\n",
      "Epoch: 1444|steps:   30|Train Avg Loss: 0.8938 |Test Loss: 0.8159|lr = 0.00075\n",
      "Epoch: 1444|steps:   60|Train Avg Loss: 0.8728 |Test Loss: 0.8127|lr = 0.00075\n",
      "Epoch: 1445|steps:   30|Train Avg Loss: 0.8891 |Test Loss: 0.8140|lr = 0.00075\n",
      "Epoch: 1445|steps:   60|Train Avg Loss: 0.8653 |Test Loss: 0.8139|lr = 0.00075\n",
      "Epoch: 1446|steps:   30|Train Avg Loss: 0.8904 |Test Loss: 0.8156|lr = 0.00074\n",
      "Epoch: 1446|steps:   60|Train Avg Loss: 0.8758 |Test Loss: 0.8144|lr = 0.00074\n",
      "Epoch: 1447|steps:   30|Train Avg Loss: 0.8456 |Test Loss: 0.8115|lr = 0.00074\n",
      "Epoch: 1447|steps:   60|Train Avg Loss: 0.9127 |Test Loss: 0.8149|lr = 0.00074\n",
      "Epoch: 1448|steps:   30|Train Avg Loss: 0.8659 |Test Loss: 0.8135|lr = 0.00074\n",
      "Epoch: 1448|steps:   60|Train Avg Loss: 0.8962 |Test Loss: 0.8146|lr = 0.00074\n",
      "Epoch: 1449|steps:   30|Train Avg Loss: 0.8937 |Test Loss: 0.8160|lr = 0.00074\n",
      "Epoch: 1449|steps:   60|Train Avg Loss: 0.8938 |Test Loss: 0.8161|lr = 0.00074\n",
      "Epoch: 1450|steps:   30|Train Avg Loss: 0.8731 |Test Loss: 0.8121|lr = 0.00074\n",
      "Epoch: 1450|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8150|lr = 0.00074\n",
      "Epoch: 1451|steps:   30|Train Avg Loss: 0.8765 |Test Loss: 0.8138|lr = 0.00074\n",
      "Epoch: 1451|steps:   60|Train Avg Loss: 0.8919 |Test Loss: 0.8159|lr = 0.00074\n",
      "Epoch: 1452|steps:   30|Train Avg Loss: 0.9063 |Test Loss: 0.8169|lr = 0.00074\n",
      "Epoch: 1452|steps:   60|Train Avg Loss: 0.8607 |Test Loss: 0.8149|lr = 0.00074\n",
      "Epoch: 1453|steps:   30|Train Avg Loss: 0.8789 |Test Loss: 0.8128|lr = 0.00074\n",
      "Epoch: 1453|steps:   60|Train Avg Loss: 0.9017 |Test Loss: 0.8144|lr = 0.00074\n",
      "Epoch: 1454|steps:   30|Train Avg Loss: 0.8702 |Test Loss: 0.8123|lr = 0.00074\n",
      "Epoch: 1454|steps:   60|Train Avg Loss: 0.8892 |Test Loss: 0.8141|lr = 0.00074\n",
      "Epoch: 1455|steps:   30|Train Avg Loss: 0.9055 |Test Loss: 0.8152|lr = 0.00074\n",
      "Epoch: 1455|steps:   60|Train Avg Loss: 0.8683 |Test Loss: 0.8142|lr = 0.00074\n",
      "Epoch: 1456|steps:   30|Train Avg Loss: 0.8741 |Test Loss: 0.8130|lr = 0.00074\n",
      "Epoch: 1456|steps:   60|Train Avg Loss: 0.8887 |Test Loss: 0.8134|lr = 0.00074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1457|steps:   30|Train Avg Loss: 0.8854 |Test Loss: 0.8146|lr = 0.00072\n",
      "Epoch: 1457|steps:   60|Train Avg Loss: 0.8917 |Test Loss: 0.8156|lr = 0.00072\n",
      "Epoch: 1458|steps:   30|Train Avg Loss: 0.8759 |Test Loss: 0.8130|lr = 0.00072\n",
      "Epoch: 1458|steps:   60|Train Avg Loss: 0.8937 |Test Loss: 0.8148|lr = 0.00072\n",
      "Epoch: 1459|steps:   30|Train Avg Loss: 0.8755 |Test Loss: 0.8140|lr = 0.00072\n",
      "Epoch: 1459|steps:   60|Train Avg Loss: 0.8938 |Test Loss: 0.8137|lr = 0.00072\n",
      "Epoch: 1460|steps:   30|Train Avg Loss: 0.8832 |Test Loss: 0.8141|lr = 0.00072\n",
      "Epoch: 1460|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8139|lr = 0.00072\n",
      "Epoch: 1461|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8147|lr = 0.00072\n",
      "Epoch: 1461|steps:   60|Train Avg Loss: 0.8852 |Test Loss: 0.8148|lr = 0.00072\n",
      "Epoch: 1462|steps:   30|Train Avg Loss: 0.9017 |Test Loss: 0.8159|lr = 0.00072\n",
      "Epoch: 1462|steps:   60|Train Avg Loss: 0.8636 |Test Loss: 0.8149|lr = 0.00072\n",
      "Epoch: 1463|steps:   30|Train Avg Loss: 0.8864 |Test Loss: 0.8145|lr = 0.00072\n",
      "Epoch: 1463|steps:   60|Train Avg Loss: 0.8855 |Test Loss: 0.8145|lr = 0.00072\n",
      "Epoch: 1464|steps:   30|Train Avg Loss: 0.9094 |Test Loss: 0.8152|lr = 0.00072\n",
      "Epoch: 1464|steps:   60|Train Avg Loss: 0.8565 |Test Loss: 0.8126|lr = 0.00072\n",
      "Epoch: 1465|steps:   30|Train Avg Loss: 0.9029 |Test Loss: 0.8162|lr = 0.00072\n",
      "Epoch: 1465|steps:   60|Train Avg Loss: 0.8768 |Test Loss: 0.8154|lr = 0.00072\n",
      "Epoch: 1466|steps:   30|Train Avg Loss: 0.8844 |Test Loss: 0.8138|lr = 0.00072\n",
      "Epoch: 1466|steps:   60|Train Avg Loss: 0.8755 |Test Loss: 0.8134|lr = 0.00072\n",
      "Epoch: 1467|steps:   30|Train Avg Loss: 0.8653 |Test Loss: 0.8125|lr = 0.00072\n",
      "Epoch: 1467|steps:   60|Train Avg Loss: 0.9063 |Test Loss: 0.8163|lr = 0.00072\n",
      "Epoch: 1468|steps:   30|Train Avg Loss: 0.9042 |Test Loss: 0.8167|lr = 0.00071\n",
      "Epoch: 1468|steps:   60|Train Avg Loss: 0.8735 |Test Loss: 0.8136|lr = 0.00071\n",
      "Epoch: 1469|steps:   30|Train Avg Loss: 0.8592 |Test Loss: 0.8091|lr = 0.00071\n",
      "Epoch: 1469|steps:   60|Train Avg Loss: 0.9032 |Test Loss: 0.8145|lr = 0.00071\n",
      "Epoch: 1470|steps:   30|Train Avg Loss: 0.8562 |Test Loss: 0.8128|lr = 0.00071\n",
      "Epoch: 1470|steps:   60|Train Avg Loss: 0.9126 |Test Loss: 0.8149|lr = 0.00071\n",
      "Epoch: 1471|steps:   30|Train Avg Loss: 0.8885 |Test Loss: 0.8164|lr = 0.00071\n",
      "Epoch: 1471|steps:   60|Train Avg Loss: 0.8677 |Test Loss: 0.8151|lr = 0.00071\n",
      "Epoch: 1472|steps:   30|Train Avg Loss: 0.8918 |Test Loss: 0.8148|lr = 0.00071\n",
      "Epoch: 1472|steps:   60|Train Avg Loss: 0.8905 |Test Loss: 0.8145|lr = 0.00071\n",
      "Epoch: 1473|steps:   30|Train Avg Loss: 0.8801 |Test Loss: 0.8128|lr = 0.00071\n",
      "Epoch: 1473|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8137|lr = 0.00071\n",
      "Epoch: 1474|steps:   30|Train Avg Loss: 0.8907 |Test Loss: 0.8153|lr = 0.00071\n",
      "Epoch: 1474|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8169|lr = 0.00071\n",
      "Epoch: 1475|steps:   30|Train Avg Loss: 0.8855 |Test Loss: 0.8141|lr = 0.00071\n",
      "Epoch: 1475|steps:   60|Train Avg Loss: 0.8831 |Test Loss: 0.8150|lr = 0.00071\n",
      "Epoch: 1476|steps:   30|Train Avg Loss: 0.8831 |Test Loss: 0.8149|lr = 0.00071\n",
      "Epoch: 1476|steps:   60|Train Avg Loss: 0.8905 |Test Loss: 0.8149|lr = 0.00071\n",
      "Epoch: 1477|steps:   30|Train Avg Loss: 0.8895 |Test Loss: 0.8158|lr = 0.00071\n",
      "Epoch: 1477|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8148|lr = 0.00071\n",
      "Epoch: 1478|steps:   30|Train Avg Loss: 0.8681 |Test Loss: 0.8126|lr = 0.00071\n",
      "Epoch: 1478|steps:   60|Train Avg Loss: 0.9111 |Test Loss: 0.8155|lr = 0.00071\n",
      "Epoch: 1479|steps:   30|Train Avg Loss: 0.8771 |Test Loss: 0.8126|lr = 0.00069\n",
      "Epoch: 1479|steps:   60|Train Avg Loss: 0.8827 |Test Loss: 0.8138|lr = 0.00069\n",
      "Epoch: 1480|steps:   30|Train Avg Loss: 0.8784 |Test Loss: 0.8141|lr = 0.00069\n",
      "Epoch: 1480|steps:   60|Train Avg Loss: 0.8866 |Test Loss: 0.8130|lr = 0.00069\n",
      "Epoch: 1481|steps:   30|Train Avg Loss: 0.9006 |Test Loss: 0.8169|lr = 0.00069\n",
      "Epoch: 1481|steps:   60|Train Avg Loss: 0.8737 |Test Loss: 0.8152|lr = 0.00069\n",
      "Epoch: 1482|steps:   30|Train Avg Loss: 0.9015 |Test Loss: 0.8152|lr = 0.00069\n",
      "Epoch: 1482|steps:   60|Train Avg Loss: 0.8679 |Test Loss: 0.8143|lr = 0.00069\n",
      "Epoch: 1483|steps:   30|Train Avg Loss: 0.9008 |Test Loss: 0.8145|lr = 0.00069\n",
      "Epoch: 1483|steps:   60|Train Avg Loss: 0.8708 |Test Loss: 0.8149|lr = 0.00069\n",
      "Epoch: 1484|steps:   30|Train Avg Loss: 0.8959 |Test Loss: 0.8144|lr = 0.00069\n",
      "Epoch: 1484|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 0.8153|lr = 0.00069\n",
      "Epoch: 1485|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8141|lr = 0.00069\n",
      "Epoch: 1485|steps:   60|Train Avg Loss: 0.8943 |Test Loss: 0.8143|lr = 0.00069\n",
      "Epoch: 1486|steps:   30|Train Avg Loss: 0.8651 |Test Loss: 0.8131|lr = 0.00069\n",
      "Epoch: 1486|steps:   60|Train Avg Loss: 0.9006 |Test Loss: 0.8155|lr = 0.00069\n",
      "Epoch: 1487|steps:   30|Train Avg Loss: 0.8690 |Test Loss: 0.8121|lr = 0.00069\n",
      "Epoch: 1487|steps:   60|Train Avg Loss: 0.8897 |Test Loss: 0.8140|lr = 0.00069\n",
      "Epoch: 1488|steps:   30|Train Avg Loss: 0.8729 |Test Loss: 0.8156|lr = 0.00069\n",
      "Epoch: 1488|steps:   60|Train Avg Loss: 0.8935 |Test Loss: 0.8147|lr = 0.00069\n",
      "Epoch: 1489|steps:   30|Train Avg Loss: 0.8710 |Test Loss: 0.8137|lr = 0.00069\n",
      "Epoch: 1489|steps:   60|Train Avg Loss: 0.8942 |Test Loss: 0.8149|lr = 0.00069\n",
      "Epoch: 1490|steps:   30|Train Avg Loss: 0.8857 |Test Loss: 0.8149|lr = 0.00068\n",
      "Epoch: 1490|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8166|lr = 0.00068\n",
      "Epoch: 1491|steps:   30|Train Avg Loss: 0.8735 |Test Loss: 0.8125|lr = 0.00068\n",
      "Epoch: 1491|steps:   60|Train Avg Loss: 0.8943 |Test Loss: 0.8160|lr = 0.00068\n",
      "Epoch: 1492|steps:   30|Train Avg Loss: 0.8889 |Test Loss: 0.8139|lr = 0.00068\n",
      "Epoch: 1492|steps:   60|Train Avg Loss: 0.8765 |Test Loss: 0.8146|lr = 0.00068\n",
      "Epoch: 1493|steps:   30|Train Avg Loss: 0.8730 |Test Loss: 0.8151|lr = 0.00068\n",
      "Epoch: 1493|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8151|lr = 0.00068\n",
      "Epoch: 1494|steps:   30|Train Avg Loss: 0.8632 |Test Loss: 0.8123|lr = 0.00068\n",
      "Epoch: 1494|steps:   60|Train Avg Loss: 0.9048 |Test Loss: 0.8146|lr = 0.00068\n",
      "Epoch: 1495|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8162|lr = 0.00068\n",
      "Epoch: 1495|steps:   60|Train Avg Loss: 0.8808 |Test Loss: 0.8143|lr = 0.00068\n",
      "Epoch: 1496|steps:   30|Train Avg Loss: 0.8788 |Test Loss: 0.8153|lr = 0.00068\n",
      "Epoch: 1496|steps:   60|Train Avg Loss: 0.8887 |Test Loss: 0.8149|lr = 0.00068\n",
      "Epoch: 1497|steps:   30|Train Avg Loss: 0.8605 |Test Loss: 0.8136|lr = 0.00068\n",
      "Epoch: 1497|steps:   60|Train Avg Loss: 0.8887 |Test Loss: 0.8134|lr = 0.00068\n",
      "Epoch: 1498|steps:   30|Train Avg Loss: 0.8701 |Test Loss: 0.8138|lr = 0.00068\n",
      "Epoch: 1498|steps:   60|Train Avg Loss: 0.9000 |Test Loss: 0.8152|lr = 0.00068\n",
      "Epoch: 1499|steps:   30|Train Avg Loss: 0.8692 |Test Loss: 0.8128|lr = 0.00068\n",
      "Epoch: 1499|steps:   60|Train Avg Loss: 0.9053 |Test Loss: 0.8155|lr = 0.00068\n",
      "Epoch: 1500|steps:   30|Train Avg Loss: 0.8892 |Test Loss: 0.8157|lr = 0.00068\n",
      "Epoch: 1500|steps:   60|Train Avg Loss: 0.8803 |Test Loss: 0.8167|lr = 0.00068\n",
      "Epoch: 1501|steps:   30|Train Avg Loss: 0.8818 |Test Loss: 0.8137|lr = 0.00067\n",
      "Epoch: 1501|steps:   60|Train Avg Loss: 0.8831 |Test Loss: 0.8143|lr = 0.00067\n",
      "Epoch: 1502|steps:   30|Train Avg Loss: 0.8757 |Test Loss: 0.8117|lr = 0.00067\n",
      "Epoch: 1502|steps:   60|Train Avg Loss: 0.8845 |Test Loss: 0.8146|lr = 0.00067\n",
      "Epoch: 1503|steps:   30|Train Avg Loss: 0.8843 |Test Loss: 0.8160|lr = 0.00067\n",
      "Epoch: 1503|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8142|lr = 0.00067\n",
      "Epoch: 1504|steps:   30|Train Avg Loss: 0.8696 |Test Loss: 0.8129|lr = 0.00067\n",
      "Epoch: 1504|steps:   60|Train Avg Loss: 0.8966 |Test Loss: 0.8150|lr = 0.00067\n",
      "Epoch: 1505|steps:   30|Train Avg Loss: 0.8973 |Test Loss: 0.8182|lr = 0.00067\n",
      "Epoch: 1505|steps:   60|Train Avg Loss: 0.8773 |Test Loss: 0.8153|lr = 0.00067\n",
      "Epoch: 1506|steps:   30|Train Avg Loss: 0.9014 |Test Loss: 0.8155|lr = 0.00067\n",
      "Epoch: 1506|steps:   60|Train Avg Loss: 0.8646 |Test Loss: 0.8123|lr = 0.00067\n",
      "Epoch: 1507|steps:   30|Train Avg Loss: 0.8717 |Test Loss: 0.8132|lr = 0.00067\n",
      "Epoch: 1507|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8147|lr = 0.00067\n",
      "Epoch: 1508|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8130|lr = 0.00067\n",
      "Epoch: 1508|steps:   60|Train Avg Loss: 0.8928 |Test Loss: 0.8145|lr = 0.00067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1509|steps:   30|Train Avg Loss: 0.8769 |Test Loss: 0.8136|lr = 0.00067\n",
      "Epoch: 1509|steps:   60|Train Avg Loss: 0.8998 |Test Loss: 0.8148|lr = 0.00067\n",
      "Epoch: 1510|steps:   30|Train Avg Loss: 0.8808 |Test Loss: 0.8157|lr = 0.00067\n",
      "Epoch: 1510|steps:   60|Train Avg Loss: 0.8952 |Test Loss: 0.8159|lr = 0.00067\n",
      "Epoch: 1511|steps:   30|Train Avg Loss: 0.8779 |Test Loss: 0.8132|lr = 0.00067\n",
      "Epoch: 1511|steps:   60|Train Avg Loss: 0.8825 |Test Loss: 0.8158|lr = 0.00067\n",
      "Epoch: 1512|steps:   30|Train Avg Loss: 0.8784 |Test Loss: 0.8150|lr = 0.00065\n",
      "Epoch: 1512|steps:   60|Train Avg Loss: 0.8823 |Test Loss: 0.8144|lr = 0.00065\n",
      "Epoch: 1513|steps:   30|Train Avg Loss: 0.8771 |Test Loss: 0.8144|lr = 0.00065\n",
      "Epoch: 1513|steps:   60|Train Avg Loss: 0.9003 |Test Loss: 0.8167|lr = 0.00065\n",
      "Epoch: 1514|steps:   30|Train Avg Loss: 0.8780 |Test Loss: 0.8146|lr = 0.00065\n",
      "Epoch: 1514|steps:   60|Train Avg Loss: 0.8652 |Test Loss: 0.8118|lr = 0.00065\n",
      "Epoch: 1515|steps:   30|Train Avg Loss: 0.8834 |Test Loss: 0.8162|lr = 0.00065\n",
      "Epoch: 1515|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8152|lr = 0.00065\n",
      "Epoch: 1516|steps:   30|Train Avg Loss: 0.8928 |Test Loss: 0.8160|lr = 0.00065\n",
      "Epoch: 1516|steps:   60|Train Avg Loss: 0.8771 |Test Loss: 0.8143|lr = 0.00065\n",
      "Epoch: 1517|steps:   30|Train Avg Loss: 0.8822 |Test Loss: 0.8156|lr = 0.00065\n",
      "Epoch: 1517|steps:   60|Train Avg Loss: 0.8806 |Test Loss: 0.8147|lr = 0.00065\n",
      "Epoch: 1518|steps:   30|Train Avg Loss: 0.8830 |Test Loss: 0.8149|lr = 0.00065\n",
      "Epoch: 1518|steps:   60|Train Avg Loss: 0.8926 |Test Loss: 0.8161|lr = 0.00065\n",
      "Epoch: 1519|steps:   30|Train Avg Loss: 0.8935 |Test Loss: 0.8162|lr = 0.00065\n",
      "Epoch: 1519|steps:   60|Train Avg Loss: 0.8767 |Test Loss: 0.8141|lr = 0.00065\n",
      "Epoch: 1520|steps:   30|Train Avg Loss: 0.8870 |Test Loss: 0.8147|lr = 0.00065\n",
      "Epoch: 1520|steps:   60|Train Avg Loss: 0.8777 |Test Loss: 0.8149|lr = 0.00065\n",
      "Epoch: 1521|steps:   30|Train Avg Loss: 0.8968 |Test Loss: 0.8175|lr = 0.00065\n",
      "Epoch: 1521|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8146|lr = 0.00065\n",
      "Epoch: 1522|steps:   30|Train Avg Loss: 0.8864 |Test Loss: 0.8147|lr = 0.00065\n",
      "Epoch: 1522|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8167|lr = 0.00065\n",
      "Epoch: 1523|steps:   30|Train Avg Loss: 0.8753 |Test Loss: 0.8135|lr = 0.00064\n",
      "Epoch: 1523|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8145|lr = 0.00064\n",
      "Epoch: 1524|steps:   30|Train Avg Loss: 0.9005 |Test Loss: 0.8199|lr = 0.00064\n",
      "Epoch: 1524|steps:   60|Train Avg Loss: 0.8630 |Test Loss: 0.8156|lr = 0.00064\n",
      "Epoch: 1525|steps:   30|Train Avg Loss: 0.8713 |Test Loss: 0.8131|lr = 0.00064\n",
      "Epoch: 1525|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8127|lr = 0.00064\n",
      "Epoch: 1526|steps:   30|Train Avg Loss: 0.8807 |Test Loss: 0.8157|lr = 0.00064\n",
      "Epoch: 1526|steps:   60|Train Avg Loss: 0.8846 |Test Loss: 0.8155|lr = 0.00064\n",
      "Epoch: 1527|steps:   30|Train Avg Loss: 0.8833 |Test Loss: 0.8171|lr = 0.00064\n",
      "Epoch: 1527|steps:   60|Train Avg Loss: 0.8972 |Test Loss: 0.8171|lr = 0.00064\n",
      "Epoch: 1528|steps:   30|Train Avg Loss: 0.8796 |Test Loss: 0.8148|lr = 0.00064\n",
      "Epoch: 1528|steps:   60|Train Avg Loss: 0.8813 |Test Loss: 0.8157|lr = 0.00064\n",
      "Epoch: 1529|steps:   30|Train Avg Loss: 0.8863 |Test Loss: 0.8158|lr = 0.00064\n",
      "Epoch: 1529|steps:   60|Train Avg Loss: 0.8713 |Test Loss: 0.8130|lr = 0.00064\n",
      "Epoch: 1530|steps:   30|Train Avg Loss: 0.8867 |Test Loss: 0.8156|lr = 0.00064\n",
      "Epoch: 1530|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8141|lr = 0.00064\n",
      "Epoch: 1531|steps:   30|Train Avg Loss: 0.8743 |Test Loss: 0.8132|lr = 0.00064\n",
      "Epoch: 1531|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8158|lr = 0.00064\n",
      "Epoch: 1532|steps:   30|Train Avg Loss: 0.8649 |Test Loss: 0.8148|lr = 0.00064\n",
      "Epoch: 1532|steps:   60|Train Avg Loss: 0.9037 |Test Loss: 0.8158|lr = 0.00064\n",
      "Epoch: 1533|steps:   30|Train Avg Loss: 0.8955 |Test Loss: 0.8179|lr = 0.00064\n",
      "Epoch: 1533|steps:   60|Train Avg Loss: 0.8685 |Test Loss: 0.8165|lr = 0.00064\n",
      "Epoch: 1534|steps:   30|Train Avg Loss: 0.8731 |Test Loss: 0.8148|lr = 0.00063\n",
      "Epoch: 1534|steps:   60|Train Avg Loss: 0.8951 |Test Loss: 0.8158|lr = 0.00063\n",
      "Epoch: 1535|steps:   30|Train Avg Loss: 0.9001 |Test Loss: 0.8172|lr = 0.00063\n",
      "Epoch: 1535|steps:   60|Train Avg Loss: 0.8781 |Test Loss: 0.8172|lr = 0.00063\n",
      "Epoch: 1536|steps:   30|Train Avg Loss: 0.8643 |Test Loss: 0.8124|lr = 0.00063\n",
      "Epoch: 1536|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8143|lr = 0.00063\n",
      "Epoch: 1537|steps:   30|Train Avg Loss: 0.8972 |Test Loss: 0.8194|lr = 0.00063\n",
      "Epoch: 1537|steps:   60|Train Avg Loss: 0.8745 |Test Loss: 0.8170|lr = 0.00063\n",
      "Epoch: 1538|steps:   30|Train Avg Loss: 0.8965 |Test Loss: 0.8170|lr = 0.00063\n",
      "Epoch: 1538|steps:   60|Train Avg Loss: 0.8695 |Test Loss: 0.8144|lr = 0.00063\n",
      "Epoch: 1539|steps:   30|Train Avg Loss: 0.8831 |Test Loss: 0.8176|lr = 0.00063\n",
      "Epoch: 1539|steps:   60|Train Avg Loss: 0.8801 |Test Loss: 0.8140|lr = 0.00063\n",
      "Epoch: 1540|steps:   30|Train Avg Loss: 0.8898 |Test Loss: 0.8152|lr = 0.00063\n",
      "Epoch: 1540|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8168|lr = 0.00063\n",
      "Epoch: 1541|steps:   30|Train Avg Loss: 0.8902 |Test Loss: 0.8159|lr = 0.00063\n",
      "Epoch: 1541|steps:   60|Train Avg Loss: 0.8707 |Test Loss: 0.8149|lr = 0.00063\n",
      "Epoch: 1542|steps:   30|Train Avg Loss: 0.8995 |Test Loss: 0.8170|lr = 0.00063\n",
      "Epoch: 1542|steps:   60|Train Avg Loss: 0.8674 |Test Loss: 0.8153|lr = 0.00063\n",
      "Epoch: 1543|steps:   30|Train Avg Loss: 0.8702 |Test Loss: 0.8151|lr = 0.00063\n",
      "Epoch: 1543|steps:   60|Train Avg Loss: 0.8774 |Test Loss: 0.8142|lr = 0.00063\n",
      "Epoch: 1544|steps:   30|Train Avg Loss: 0.8881 |Test Loss: 0.8161|lr = 0.00063\n",
      "Epoch: 1544|steps:   60|Train Avg Loss: 0.8881 |Test Loss: 0.8181|lr = 0.00063\n",
      "Epoch: 1545|steps:   30|Train Avg Loss: 0.8786 |Test Loss: 0.8133|lr = 0.00063\n",
      "Epoch: 1545|steps:   60|Train Avg Loss: 0.8835 |Test Loss: 0.8162|lr = 0.00063\n",
      "Epoch: 1546|steps:   30|Train Avg Loss: 0.8834 |Test Loss: 0.8172|lr = 0.00063\n",
      "Epoch: 1546|steps:   60|Train Avg Loss: 0.8937 |Test Loss: 0.8164|lr = 0.00063\n",
      "Epoch: 1547|steps:   30|Train Avg Loss: 0.9022 |Test Loss: 0.8172|lr = 0.00063\n",
      "Epoch: 1547|steps:   60|Train Avg Loss: 0.8784 |Test Loss: 0.8168|lr = 0.00063\n",
      "Epoch: 1548|steps:   30|Train Avg Loss: 0.8817 |Test Loss: 0.8145|lr = 0.00063\n",
      "Epoch: 1548|steps:   60|Train Avg Loss: 0.8855 |Test Loss: 0.8168|lr = 0.00063\n",
      "Epoch: 1549|steps:   30|Train Avg Loss: 0.8999 |Test Loss: 0.8192|lr = 0.00062\n",
      "Epoch: 1549|steps:   60|Train Avg Loss: 0.8727 |Test Loss: 0.8167|lr = 0.00062\n",
      "Epoch: 1550|steps:   30|Train Avg Loss: 0.8621 |Test Loss: 0.8110|lr = 0.00062\n",
      "Epoch: 1550|steps:   60|Train Avg Loss: 0.8910 |Test Loss: 0.8150|lr = 0.00062\n",
      "Epoch: 1551|steps:   30|Train Avg Loss: 0.8795 |Test Loss: 0.8175|lr = 0.00062\n",
      "Epoch: 1551|steps:   60|Train Avg Loss: 0.8969 |Test Loss: 0.8174|lr = 0.00062\n",
      "Epoch: 1552|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8151|lr = 0.00062\n",
      "Epoch: 1552|steps:   60|Train Avg Loss: 0.8803 |Test Loss: 0.8164|lr = 0.00062\n",
      "Epoch: 1553|steps:   30|Train Avg Loss: 0.8960 |Test Loss: 0.8147|lr = 0.00062\n",
      "Epoch: 1553|steps:   60|Train Avg Loss: 0.8687 |Test Loss: 0.8166|lr = 0.00062\n",
      "Epoch: 1554|steps:   30|Train Avg Loss: 0.8780 |Test Loss: 0.8164|lr = 0.00062\n",
      "Epoch: 1554|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8151|lr = 0.00062\n",
      "Epoch: 1555|steps:   30|Train Avg Loss: 0.8767 |Test Loss: 0.8166|lr = 0.00062\n",
      "Epoch: 1555|steps:   60|Train Avg Loss: 0.8829 |Test Loss: 0.8159|lr = 0.00062\n",
      "Epoch: 1556|steps:   30|Train Avg Loss: 0.8810 |Test Loss: 0.8166|lr = 0.00062\n",
      "Epoch: 1556|steps:   60|Train Avg Loss: 0.8849 |Test Loss: 0.8165|lr = 0.00062\n",
      "Epoch: 1557|steps:   30|Train Avg Loss: 0.8854 |Test Loss: 0.8166|lr = 0.00062\n",
      "Epoch: 1557|steps:   60|Train Avg Loss: 0.8795 |Test Loss: 0.8168|lr = 0.00062\n",
      "Epoch: 1558|steps:   30|Train Avg Loss: 0.8580 |Test Loss: 0.8137|lr = 0.00062\n",
      "Epoch: 1558|steps:   60|Train Avg Loss: 0.8885 |Test Loss: 0.8147|lr = 0.00062\n",
      "Epoch: 1559|steps:   30|Train Avg Loss: 0.8782 |Test Loss: 0.8189|lr = 0.00062\n",
      "Epoch: 1559|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8164|lr = 0.00062\n",
      "Epoch: 1560|steps:   30|Train Avg Loss: 0.8674 |Test Loss: 0.8145|lr = 0.00060\n",
      "Epoch: 1560|steps:   60|Train Avg Loss: 0.9100 |Test Loss: 0.8193|lr = 0.00060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1561|steps:   30|Train Avg Loss: 0.8723 |Test Loss: 0.8160|lr = 0.00060\n",
      "Epoch: 1561|steps:   60|Train Avg Loss: 0.8810 |Test Loss: 0.8168|lr = 0.00060\n",
      "Epoch: 1562|steps:   30|Train Avg Loss: 0.8792 |Test Loss: 0.8161|lr = 0.00060\n",
      "Epoch: 1562|steps:   60|Train Avg Loss: 0.8791 |Test Loss: 0.8160|lr = 0.00060\n",
      "Epoch: 1563|steps:   30|Train Avg Loss: 0.8736 |Test Loss: 0.8149|lr = 0.00060\n",
      "Epoch: 1563|steps:   60|Train Avg Loss: 0.8843 |Test Loss: 0.8169|lr = 0.00060\n",
      "Epoch: 1564|steps:   30|Train Avg Loss: 0.8554 |Test Loss: 0.8156|lr = 0.00060\n",
      "Epoch: 1564|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8153|lr = 0.00060\n",
      "Epoch: 1565|steps:   30|Train Avg Loss: 0.8765 |Test Loss: 0.8184|lr = 0.00060\n",
      "Epoch: 1565|steps:   60|Train Avg Loss: 0.8931 |Test Loss: 0.8173|lr = 0.00060\n",
      "Epoch: 1566|steps:   30|Train Avg Loss: 0.8705 |Test Loss: 0.8154|lr = 0.00060\n",
      "Epoch: 1566|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8165|lr = 0.00060\n",
      "Epoch: 1567|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8165|lr = 0.00060\n",
      "Epoch: 1567|steps:   60|Train Avg Loss: 0.8992 |Test Loss: 0.8167|lr = 0.00060\n",
      "Epoch: 1568|steps:   30|Train Avg Loss: 0.8773 |Test Loss: 0.8175|lr = 0.00060\n",
      "Epoch: 1568|steps:   60|Train Avg Loss: 0.8794 |Test Loss: 0.8160|lr = 0.00060\n",
      "Epoch: 1569|steps:   30|Train Avg Loss: 0.8703 |Test Loss: 0.8154|lr = 0.00060\n",
      "Epoch: 1569|steps:   60|Train Avg Loss: 0.8988 |Test Loss: 0.8205|lr = 0.00060\n",
      "Epoch: 1570|steps:   30|Train Avg Loss: 0.8779 |Test Loss: 0.8153|lr = 0.00060\n",
      "Epoch: 1570|steps:   60|Train Avg Loss: 0.8774 |Test Loss: 0.8157|lr = 0.00060\n",
      "Epoch: 1571|steps:   30|Train Avg Loss: 0.8770 |Test Loss: 0.8166|lr = 0.00059\n",
      "Epoch: 1571|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8176|lr = 0.00059\n",
      "Epoch: 1572|steps:   30|Train Avg Loss: 0.8637 |Test Loss: 0.8164|lr = 0.00059\n",
      "Epoch: 1572|steps:   60|Train Avg Loss: 0.8922 |Test Loss: 0.8161|lr = 0.00059\n",
      "Epoch: 1573|steps:   30|Train Avg Loss: 0.9073 |Test Loss: 0.8199|lr = 0.00059\n",
      "Epoch: 1573|steps:   60|Train Avg Loss: 0.8647 |Test Loss: 0.8170|lr = 0.00059\n",
      "Epoch: 1574|steps:   30|Train Avg Loss: 0.8816 |Test Loss: 0.8186|lr = 0.00059\n",
      "Epoch: 1574|steps:   60|Train Avg Loss: 0.8685 |Test Loss: 0.8151|lr = 0.00059\n",
      "Epoch: 1575|steps:   30|Train Avg Loss: 0.8595 |Test Loss: 0.8119|lr = 0.00059\n",
      "Epoch: 1575|steps:   60|Train Avg Loss: 0.9067 |Test Loss: 0.8162|lr = 0.00059\n",
      "Epoch: 1576|steps:   30|Train Avg Loss: 0.8838 |Test Loss: 0.8180|lr = 0.00059\n",
      "Epoch: 1576|steps:   60|Train Avg Loss: 0.8808 |Test Loss: 0.8186|lr = 0.00059\n",
      "Epoch: 1577|steps:   30|Train Avg Loss: 0.8726 |Test Loss: 0.8150|lr = 0.00059\n",
      "Epoch: 1577|steps:   60|Train Avg Loss: 0.8878 |Test Loss: 0.8174|lr = 0.00059\n",
      "Epoch: 1578|steps:   30|Train Avg Loss: 0.8656 |Test Loss: 0.8161|lr = 0.00059\n",
      "Epoch: 1578|steps:   60|Train Avg Loss: 0.9012 |Test Loss: 0.8176|lr = 0.00059\n",
      "Epoch: 1579|steps:   30|Train Avg Loss: 0.8739 |Test Loss: 0.8166|lr = 0.00059\n",
      "Epoch: 1579|steps:   60|Train Avg Loss: 0.8886 |Test Loss: 0.8178|lr = 0.00059\n",
      "Epoch: 1580|steps:   30|Train Avg Loss: 0.8685 |Test Loss: 0.8148|lr = 0.00059\n",
      "Epoch: 1580|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8177|lr = 0.00059\n",
      "Epoch: 1581|steps:   30|Train Avg Loss: 0.8984 |Test Loss: 0.8202|lr = 0.00059\n",
      "Epoch: 1581|steps:   60|Train Avg Loss: 0.8635 |Test Loss: 0.8169|lr = 0.00059\n",
      "Epoch: 1582|steps:   30|Train Avg Loss: 0.8864 |Test Loss: 0.8159|lr = 0.00058\n",
      "Epoch: 1582|steps:   60|Train Avg Loss: 0.8805 |Test Loss: 0.8172|lr = 0.00058\n",
      "Epoch: 1583|steps:   30|Train Avg Loss: 0.8810 |Test Loss: 0.8192|lr = 0.00058\n",
      "Epoch: 1583|steps:   60|Train Avg Loss: 0.8907 |Test Loss: 0.8181|lr = 0.00058\n",
      "Epoch: 1584|steps:   30|Train Avg Loss: 0.8770 |Test Loss: 0.8151|lr = 0.00058\n",
      "Epoch: 1584|steps:   60|Train Avg Loss: 0.8811 |Test Loss: 0.8155|lr = 0.00058\n",
      "Epoch: 1585|steps:   30|Train Avg Loss: 0.8692 |Test Loss: 0.8166|lr = 0.00058\n",
      "Epoch: 1585|steps:   60|Train Avg Loss: 0.8873 |Test Loss: 0.8163|lr = 0.00058\n",
      "Epoch: 1586|steps:   30|Train Avg Loss: 0.8767 |Test Loss: 0.8175|lr = 0.00058\n",
      "Epoch: 1586|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8178|lr = 0.00058\n",
      "Epoch: 1587|steps:   30|Train Avg Loss: 0.8942 |Test Loss: 0.8181|lr = 0.00058\n",
      "Epoch: 1587|steps:   60|Train Avg Loss: 0.8682 |Test Loss: 0.8183|lr = 0.00058\n",
      "Epoch: 1588|steps:   30|Train Avg Loss: 0.9035 |Test Loss: 0.8198|lr = 0.00058\n",
      "Epoch: 1588|steps:   60|Train Avg Loss: 0.8675 |Test Loss: 0.8182|lr = 0.00058\n",
      "Epoch: 1589|steps:   30|Train Avg Loss: 0.8716 |Test Loss: 0.8152|lr = 0.00058\n",
      "Epoch: 1589|steps:   60|Train Avg Loss: 0.8910 |Test Loss: 0.8165|lr = 0.00058\n",
      "Epoch: 1590|steps:   30|Train Avg Loss: 0.9008 |Test Loss: 0.8208|lr = 0.00058\n",
      "Epoch: 1590|steps:   60|Train Avg Loss: 0.8675 |Test Loss: 0.8177|lr = 0.00058\n",
      "Epoch: 1591|steps:   30|Train Avg Loss: 0.8584 |Test Loss: 0.8113|lr = 0.00058\n",
      "Epoch: 1591|steps:   60|Train Avg Loss: 0.8925 |Test Loss: 0.8160|lr = 0.00058\n",
      "Epoch: 1592|steps:   30|Train Avg Loss: 0.8785 |Test Loss: 0.8213|lr = 0.00058\n",
      "Epoch: 1592|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8190|lr = 0.00058\n",
      "Epoch: 1593|steps:   30|Train Avg Loss: 0.8696 |Test Loss: 0.8140|lr = 0.00057\n",
      "Epoch: 1593|steps:   60|Train Avg Loss: 0.8868 |Test Loss: 0.8166|lr = 0.00057\n",
      "Epoch: 1594|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8180|lr = 0.00057\n",
      "Epoch: 1594|steps:   60|Train Avg Loss: 0.8914 |Test Loss: 0.8189|lr = 0.00057\n",
      "Epoch: 1595|steps:   30|Train Avg Loss: 0.8635 |Test Loss: 0.8164|lr = 0.00057\n",
      "Epoch: 1595|steps:   60|Train Avg Loss: 0.8843 |Test Loss: 0.8149|lr = 0.00057\n",
      "Epoch: 1596|steps:   30|Train Avg Loss: 0.8754 |Test Loss: 0.8184|lr = 0.00057\n",
      "Epoch: 1596|steps:   60|Train Avg Loss: 0.9039 |Test Loss: 0.8192|lr = 0.00057\n",
      "Epoch: 1597|steps:   30|Train Avg Loss: 0.8971 |Test Loss: 0.8191|lr = 0.00057\n",
      "Epoch: 1597|steps:   60|Train Avg Loss: 0.8761 |Test Loss: 0.8188|lr = 0.00057\n",
      "Epoch: 1598|steps:   30|Train Avg Loss: 0.8869 |Test Loss: 0.8176|lr = 0.00057\n",
      "Epoch: 1598|steps:   60|Train Avg Loss: 0.8817 |Test Loss: 0.8182|lr = 0.00057\n",
      "Epoch: 1599|steps:   30|Train Avg Loss: 0.8829 |Test Loss: 0.8161|lr = 0.00057\n",
      "Epoch: 1599|steps:   60|Train Avg Loss: 0.8778 |Test Loss: 0.8167|lr = 0.00057\n",
      "Epoch: 1600|steps:   30|Train Avg Loss: 0.8724 |Test Loss: 0.8176|lr = 0.00057\n",
      "Epoch: 1600|steps:   60|Train Avg Loss: 0.8843 |Test Loss: 0.8184|lr = 0.00057\n",
      "Epoch: 1601|steps:   30|Train Avg Loss: 0.8945 |Test Loss: 0.8188|lr = 0.00057\n",
      "Epoch: 1601|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8180|lr = 0.00057\n",
      "Epoch: 1602|steps:   30|Train Avg Loss: 0.8846 |Test Loss: 0.8183|lr = 0.00057\n",
      "Epoch: 1602|steps:   60|Train Avg Loss: 0.8703 |Test Loss: 0.8161|lr = 0.00057\n",
      "Epoch: 1603|steps:   30|Train Avg Loss: 0.8792 |Test Loss: 0.8172|lr = 0.00057\n",
      "Epoch: 1603|steps:   60|Train Avg Loss: 0.8838 |Test Loss: 0.8169|lr = 0.00057\n",
      "Epoch: 1604|steps:   30|Train Avg Loss: 0.8714 |Test Loss: 0.8144|lr = 0.00056\n",
      "Epoch: 1604|steps:   60|Train Avg Loss: 0.8872 |Test Loss: 0.8168|lr = 0.00056\n",
      "Epoch: 1605|steps:   30|Train Avg Loss: 0.8608 |Test Loss: 0.8157|lr = 0.00056\n",
      "Epoch: 1605|steps:   60|Train Avg Loss: 0.8944 |Test Loss: 0.8188|lr = 0.00056\n",
      "Epoch: 1606|steps:   30|Train Avg Loss: 0.9086 |Test Loss: 0.8221|lr = 0.00056\n",
      "Epoch: 1606|steps:   60|Train Avg Loss: 0.8522 |Test Loss: 0.8176|lr = 0.00056\n",
      "Epoch: 1607|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8173|lr = 0.00056\n",
      "Epoch: 1607|steps:   60|Train Avg Loss: 0.8734 |Test Loss: 0.8187|lr = 0.00056\n",
      "Epoch: 1608|steps:   30|Train Avg Loss: 0.8703 |Test Loss: 0.8173|lr = 0.00056\n",
      "Epoch: 1608|steps:   60|Train Avg Loss: 0.9023 |Test Loss: 0.8179|lr = 0.00056\n",
      "Epoch: 1609|steps:   30|Train Avg Loss: 0.8776 |Test Loss: 0.8183|lr = 0.00056\n",
      "Epoch: 1609|steps:   60|Train Avg Loss: 0.8821 |Test Loss: 0.8179|lr = 0.00056\n",
      "Epoch: 1610|steps:   30|Train Avg Loss: 0.8692 |Test Loss: 0.8151|lr = 0.00056\n",
      "Epoch: 1610|steps:   60|Train Avg Loss: 0.8718 |Test Loss: 0.8167|lr = 0.00056\n",
      "Epoch: 1611|steps:   30|Train Avg Loss: 0.8814 |Test Loss: 0.8197|lr = 0.00056\n",
      "Epoch: 1611|steps:   60|Train Avg Loss: 0.8738 |Test Loss: 0.8168|lr = 0.00056\n",
      "Epoch: 1612|steps:   30|Train Avg Loss: 0.8725 |Test Loss: 0.8165|lr = 0.00056\n",
      "Epoch: 1612|steps:   60|Train Avg Loss: 0.8920 |Test Loss: 0.8204|lr = 0.00056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1613|steps:   30|Train Avg Loss: 0.8811 |Test Loss: 0.8175|lr = 0.00056\n",
      "Epoch: 1613|steps:   60|Train Avg Loss: 0.8758 |Test Loss: 0.8179|lr = 0.00056\n",
      "Epoch: 1614|steps:   30|Train Avg Loss: 0.8799 |Test Loss: 0.8197|lr = 0.00056\n",
      "Epoch: 1614|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8192|lr = 0.00056\n",
      "Epoch: 1615|steps:   30|Train Avg Loss: 0.8838 |Test Loss: 0.8200|lr = 0.00055\n",
      "Epoch: 1615|steps:   60|Train Avg Loss: 0.8864 |Test Loss: 0.8174|lr = 0.00055\n",
      "Epoch: 1616|steps:   30|Train Avg Loss: 0.8625 |Test Loss: 0.8148|lr = 0.00055\n",
      "Epoch: 1616|steps:   60|Train Avg Loss: 0.8961 |Test Loss: 0.8170|lr = 0.00055\n",
      "Epoch: 1617|steps:   30|Train Avg Loss: 0.8782 |Test Loss: 0.8187|lr = 0.00055\n",
      "Epoch: 1617|steps:   60|Train Avg Loss: 0.8795 |Test Loss: 0.8175|lr = 0.00055\n",
      "Epoch: 1618|steps:   30|Train Avg Loss: 0.8798 |Test Loss: 0.8168|lr = 0.00055\n",
      "Epoch: 1618|steps:   60|Train Avg Loss: 0.8849 |Test Loss: 0.8173|lr = 0.00055\n",
      "Epoch: 1619|steps:   30|Train Avg Loss: 0.8772 |Test Loss: 0.8190|lr = 0.00055\n",
      "Epoch: 1619|steps:   60|Train Avg Loss: 0.8787 |Test Loss: 0.8172|lr = 0.00055\n",
      "Epoch: 1620|steps:   30|Train Avg Loss: 0.8764 |Test Loss: 0.8168|lr = 0.00055\n",
      "Epoch: 1620|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8187|lr = 0.00055\n",
      "Epoch: 1621|steps:   30|Train Avg Loss: 0.8691 |Test Loss: 0.8161|lr = 0.00055\n",
      "Epoch: 1621|steps:   60|Train Avg Loss: 0.8984 |Test Loss: 0.8180|lr = 0.00055\n",
      "Epoch: 1622|steps:   30|Train Avg Loss: 0.8826 |Test Loss: 0.8192|lr = 0.00055\n",
      "Epoch: 1622|steps:   60|Train Avg Loss: 0.8679 |Test Loss: 0.8173|lr = 0.00055\n",
      "Epoch: 1623|steps:   30|Train Avg Loss: 0.8949 |Test Loss: 0.8193|lr = 0.00055\n",
      "Epoch: 1623|steps:   60|Train Avg Loss: 0.8829 |Test Loss: 0.8198|lr = 0.00055\n",
      "Epoch: 1624|steps:   30|Train Avg Loss: 0.8793 |Test Loss: 0.8152|lr = 0.00055\n",
      "Epoch: 1624|steps:   60|Train Avg Loss: 0.8677 |Test Loss: 0.8156|lr = 0.00055\n",
      "Epoch: 1625|steps:   30|Train Avg Loss: 0.8949 |Test Loss: 0.8209|lr = 0.00055\n",
      "Epoch: 1625|steps:   60|Train Avg Loss: 0.8701 |Test Loss: 0.8166|lr = 0.00055\n",
      "Epoch: 1626|steps:   30|Train Avg Loss: 0.8695 |Test Loss: 0.8198|lr = 0.00053\n",
      "Epoch: 1626|steps:   60|Train Avg Loss: 0.8848 |Test Loss: 0.8166|lr = 0.00053\n",
      "Epoch: 1627|steps:   30|Train Avg Loss: 0.9017 |Test Loss: 0.8188|lr = 0.00053\n",
      "Epoch: 1627|steps:   60|Train Avg Loss: 0.8713 |Test Loss: 0.8190|lr = 0.00053\n",
      "Epoch: 1628|steps:   30|Train Avg Loss: 0.8878 |Test Loss: 0.8177|lr = 0.00053\n",
      "Epoch: 1628|steps:   60|Train Avg Loss: 0.8834 |Test Loss: 0.8208|lr = 0.00053\n",
      "Epoch: 1629|steps:   30|Train Avg Loss: 0.8855 |Test Loss: 0.8177|lr = 0.00053\n",
      "Epoch: 1629|steps:   60|Train Avg Loss: 0.8643 |Test Loss: 0.8162|lr = 0.00053\n",
      "Epoch: 1630|steps:   30|Train Avg Loss: 0.8626 |Test Loss: 0.8157|lr = 0.00053\n",
      "Epoch: 1630|steps:   60|Train Avg Loss: 0.8951 |Test Loss: 0.8187|lr = 0.00053\n",
      "Epoch: 1631|steps:   30|Train Avg Loss: 0.8705 |Test Loss: 0.8168|lr = 0.00053\n",
      "Epoch: 1631|steps:   60|Train Avg Loss: 0.8942 |Test Loss: 0.8199|lr = 0.00053\n",
      "Epoch: 1632|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8185|lr = 0.00053\n",
      "Epoch: 1632|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8188|lr = 0.00053\n",
      "Epoch: 1633|steps:   30|Train Avg Loss: 0.8866 |Test Loss: 0.8178|lr = 0.00053\n",
      "Epoch: 1633|steps:   60|Train Avg Loss: 0.8728 |Test Loss: 0.8202|lr = 0.00053\n",
      "Epoch: 1634|steps:   30|Train Avg Loss: 0.8668 |Test Loss: 0.8148|lr = 0.00053\n",
      "Epoch: 1634|steps:   60|Train Avg Loss: 0.8982 |Test Loss: 0.8186|lr = 0.00053\n",
      "Epoch: 1635|steps:   30|Train Avg Loss: 0.8906 |Test Loss: 0.8185|lr = 0.00053\n",
      "Epoch: 1635|steps:   60|Train Avg Loss: 0.8718 |Test Loss: 0.8194|lr = 0.00053\n",
      "Epoch: 1636|steps:   30|Train Avg Loss: 0.8818 |Test Loss: 0.8196|lr = 0.00053\n",
      "Epoch: 1636|steps:   60|Train Avg Loss: 0.8822 |Test Loss: 0.8177|lr = 0.00053\n",
      "Epoch: 1637|steps:   30|Train Avg Loss: 0.8730 |Test Loss: 0.8155|lr = 0.00052\n",
      "Epoch: 1637|steps:   60|Train Avg Loss: 0.8793 |Test Loss: 0.8169|lr = 0.00052\n",
      "Epoch: 1638|steps:   30|Train Avg Loss: 0.8760 |Test Loss: 0.8193|lr = 0.00052\n",
      "Epoch: 1638|steps:   60|Train Avg Loss: 0.8765 |Test Loss: 0.8167|lr = 0.00052\n",
      "Epoch: 1639|steps:   30|Train Avg Loss: 0.8617 |Test Loss: 0.8180|lr = 0.00052\n",
      "Epoch: 1639|steps:   60|Train Avg Loss: 0.8880 |Test Loss: 0.8199|lr = 0.00052\n",
      "Epoch: 1640|steps:   30|Train Avg Loss: 0.8836 |Test Loss: 0.8195|lr = 0.00052\n",
      "Epoch: 1640|steps:   60|Train Avg Loss: 0.8818 |Test Loss: 0.8187|lr = 0.00052\n",
      "Epoch: 1641|steps:   30|Train Avg Loss: 0.8676 |Test Loss: 0.8157|lr = 0.00052\n",
      "Epoch: 1641|steps:   60|Train Avg Loss: 0.8920 |Test Loss: 0.8190|lr = 0.00052\n",
      "Epoch: 1642|steps:   30|Train Avg Loss: 0.8704 |Test Loss: 0.8173|lr = 0.00052\n",
      "Epoch: 1642|steps:   60|Train Avg Loss: 0.8840 |Test Loss: 0.8177|lr = 0.00052\n",
      "Epoch: 1643|steps:   30|Train Avg Loss: 0.8698 |Test Loss: 0.8194|lr = 0.00052\n",
      "Epoch: 1643|steps:   60|Train Avg Loss: 0.8961 |Test Loss: 0.8191|lr = 0.00052\n",
      "Epoch: 1644|steps:   30|Train Avg Loss: 0.8816 |Test Loss: 0.8194|lr = 0.00052\n",
      "Epoch: 1644|steps:   60|Train Avg Loss: 0.8836 |Test Loss: 0.8179|lr = 0.00052\n",
      "Epoch: 1645|steps:   30|Train Avg Loss: 0.8901 |Test Loss: 0.8189|lr = 0.00052\n",
      "Epoch: 1645|steps:   60|Train Avg Loss: 0.8679 |Test Loss: 0.8176|lr = 0.00052\n",
      "Epoch: 1646|steps:   30|Train Avg Loss: 0.8554 |Test Loss: 0.8154|lr = 0.00052\n",
      "Epoch: 1646|steps:   60|Train Avg Loss: 0.9006 |Test Loss: 0.8180|lr = 0.00052\n",
      "Epoch: 1647|steps:   30|Train Avg Loss: 0.8979 |Test Loss: 0.8209|lr = 0.00052\n",
      "Epoch: 1647|steps:   60|Train Avg Loss: 0.8611 |Test Loss: 0.8179|lr = 0.00052\n",
      "Epoch: 1648|steps:   30|Train Avg Loss: 0.8739 |Test Loss: 0.8185|lr = 0.00051\n",
      "Epoch: 1648|steps:   60|Train Avg Loss: 0.8826 |Test Loss: 0.8165|lr = 0.00051\n",
      "Epoch: 1649|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8160|lr = 0.00051\n",
      "Epoch: 1649|steps:   60|Train Avg Loss: 0.8850 |Test Loss: 0.8175|lr = 0.00051\n",
      "Epoch: 1650|steps:   30|Train Avg Loss: 0.8825 |Test Loss: 0.8207|lr = 0.00051\n",
      "Epoch: 1650|steps:   60|Train Avg Loss: 0.8884 |Test Loss: 0.8183|lr = 0.00051\n",
      "Epoch: 1651|steps:   30|Train Avg Loss: 0.8641 |Test Loss: 0.8170|lr = 0.00051\n",
      "Epoch: 1651|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8184|lr = 0.00051\n",
      "Epoch: 1652|steps:   30|Train Avg Loss: 0.8893 |Test Loss: 0.8205|lr = 0.00051\n",
      "Epoch: 1652|steps:   60|Train Avg Loss: 0.8620 |Test Loss: 0.8163|lr = 0.00051\n",
      "Epoch: 1653|steps:   30|Train Avg Loss: 0.8742 |Test Loss: 0.8188|lr = 0.00051\n",
      "Epoch: 1653|steps:   60|Train Avg Loss: 0.8721 |Test Loss: 0.8163|lr = 0.00051\n",
      "Epoch: 1654|steps:   30|Train Avg Loss: 0.8821 |Test Loss: 0.8178|lr = 0.00051\n",
      "Epoch: 1654|steps:   60|Train Avg Loss: 0.8688 |Test Loss: 0.8167|lr = 0.00051\n",
      "Epoch: 1655|steps:   30|Train Avg Loss: 0.8695 |Test Loss: 0.8166|lr = 0.00051\n",
      "Epoch: 1655|steps:   60|Train Avg Loss: 0.8978 |Test Loss: 0.8186|lr = 0.00051\n",
      "Epoch: 1656|steps:   30|Train Avg Loss: 0.8766 |Test Loss: 0.8178|lr = 0.00051\n",
      "Epoch: 1656|steps:   60|Train Avg Loss: 0.8847 |Test Loss: 0.8194|lr = 0.00051\n",
      "Epoch: 1657|steps:   30|Train Avg Loss: 0.8881 |Test Loss: 0.8174|lr = 0.00051\n",
      "Epoch: 1657|steps:   60|Train Avg Loss: 0.8799 |Test Loss: 0.8189|lr = 0.00051\n",
      "Epoch: 1658|steps:   30|Train Avg Loss: 0.8801 |Test Loss: 0.8170|lr = 0.00051\n",
      "Epoch: 1658|steps:   60|Train Avg Loss: 0.8717 |Test Loss: 0.8184|lr = 0.00051\n",
      "Epoch: 1659|steps:   30|Train Avg Loss: 0.8728 |Test Loss: 0.8181|lr = 0.00050\n",
      "Epoch: 1659|steps:   60|Train Avg Loss: 0.8848 |Test Loss: 0.8172|lr = 0.00050\n",
      "Epoch: 1660|steps:   30|Train Avg Loss: 0.8776 |Test Loss: 0.8193|lr = 0.00050\n",
      "Epoch: 1660|steps:   60|Train Avg Loss: 0.8696 |Test Loss: 0.8157|lr = 0.00050\n",
      "Epoch: 1661|steps:   30|Train Avg Loss: 0.8761 |Test Loss: 0.8172|lr = 0.00050\n",
      "Epoch: 1661|steps:   60|Train Avg Loss: 0.8763 |Test Loss: 0.8196|lr = 0.00050\n",
      "Epoch: 1662|steps:   30|Train Avg Loss: 0.8562 |Test Loss: 0.8177|lr = 0.00050\n",
      "Epoch: 1662|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8168|lr = 0.00050\n",
      "Epoch: 1663|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8215|lr = 0.00050\n",
      "Epoch: 1663|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8181|lr = 0.00050\n",
      "Epoch: 1664|steps:   30|Train Avg Loss: 0.8581 |Test Loss: 0.8159|lr = 0.00050\n",
      "Epoch: 1664|steps:   60|Train Avg Loss: 0.9051 |Test Loss: 0.8204|lr = 0.00050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1665|steps:   30|Train Avg Loss: 0.8757 |Test Loss: 0.8191|lr = 0.00050\n",
      "Epoch: 1665|steps:   60|Train Avg Loss: 0.8674 |Test Loss: 0.8168|lr = 0.00050\n",
      "Epoch: 1666|steps:   30|Train Avg Loss: 0.8627 |Test Loss: 0.8178|lr = 0.00050\n",
      "Epoch: 1666|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8152|lr = 0.00050\n",
      "Epoch: 1667|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8186|lr = 0.00050\n",
      "Epoch: 1667|steps:   60|Train Avg Loss: 0.8834 |Test Loss: 0.8176|lr = 0.00050\n",
      "Epoch: 1668|steps:   30|Train Avg Loss: 0.8759 |Test Loss: 0.8198|lr = 0.00050\n",
      "Epoch: 1668|steps:   60|Train Avg Loss: 0.8781 |Test Loss: 0.8174|lr = 0.00050\n",
      "Epoch: 1669|steps:   30|Train Avg Loss: 0.8867 |Test Loss: 0.8196|lr = 0.00050\n",
      "Epoch: 1669|steps:   60|Train Avg Loss: 0.8704 |Test Loss: 0.8174|lr = 0.00050\n",
      "Epoch: 1670|steps:   30|Train Avg Loss: 0.8441 |Test Loss: 0.8148|lr = 0.00049\n",
      "Epoch: 1670|steps:   60|Train Avg Loss: 0.9010 |Test Loss: 0.8186|lr = 0.00049\n",
      "Epoch: 1671|steps:   30|Train Avg Loss: 0.8820 |Test Loss: 0.8178|lr = 0.00049\n",
      "Epoch: 1671|steps:   60|Train Avg Loss: 0.8743 |Test Loss: 0.8174|lr = 0.00049\n",
      "Epoch: 1672|steps:   30|Train Avg Loss: 0.8817 |Test Loss: 0.8204|lr = 0.00049\n",
      "Epoch: 1672|steps:   60|Train Avg Loss: 0.8745 |Test Loss: 0.8168|lr = 0.00049\n",
      "Epoch: 1673|steps:   30|Train Avg Loss: 0.8758 |Test Loss: 0.8150|lr = 0.00049\n",
      "Epoch: 1673|steps:   60|Train Avg Loss: 0.8902 |Test Loss: 0.8207|lr = 0.00049\n",
      "Epoch: 1674|steps:   30|Train Avg Loss: 0.8595 |Test Loss: 0.8150|lr = 0.00049\n",
      "Epoch: 1674|steps:   60|Train Avg Loss: 0.8905 |Test Loss: 0.8184|lr = 0.00049\n",
      "Epoch: 1675|steps:   30|Train Avg Loss: 0.8851 |Test Loss: 0.8212|lr = 0.00049\n",
      "Epoch: 1675|steps:   60|Train Avg Loss: 0.8690 |Test Loss: 0.8172|lr = 0.00049\n",
      "Epoch: 1676|steps:   30|Train Avg Loss: 0.9021 |Test Loss: 0.8197|lr = 0.00049\n",
      "Epoch: 1676|steps:   60|Train Avg Loss: 0.8492 |Test Loss: 0.8186|lr = 0.00049\n",
      "Epoch: 1677|steps:   30|Train Avg Loss: 0.9130 |Test Loss: 0.8197|lr = 0.00049\n",
      "Epoch: 1677|steps:   60|Train Avg Loss: 0.8489 |Test Loss: 0.8174|lr = 0.00049\n",
      "Epoch: 1678|steps:   30|Train Avg Loss: 0.8749 |Test Loss: 0.8150|lr = 0.00049\n",
      "Epoch: 1678|steps:   60|Train Avg Loss: 0.8880 |Test Loss: 0.8164|lr = 0.00049\n",
      "Epoch: 1679|steps:   30|Train Avg Loss: 0.8775 |Test Loss: 0.8176|lr = 0.00049\n",
      "Epoch: 1679|steps:   60|Train Avg Loss: 0.8804 |Test Loss: 0.8179|lr = 0.00049\n",
      "Epoch: 1680|steps:   30|Train Avg Loss: 0.8881 |Test Loss: 0.8191|lr = 0.00049\n",
      "Epoch: 1680|steps:   60|Train Avg Loss: 0.8649 |Test Loss: 0.8166|lr = 0.00049\n",
      "Epoch: 1681|steps:   30|Train Avg Loss: 0.8850 |Test Loss: 0.8183|lr = 0.00048\n",
      "Epoch: 1681|steps:   60|Train Avg Loss: 0.8674 |Test Loss: 0.8190|lr = 0.00048\n",
      "Epoch: 1682|steps:   30|Train Avg Loss: 0.8835 |Test Loss: 0.8164|lr = 0.00048\n",
      "Epoch: 1682|steps:   60|Train Avg Loss: 0.8875 |Test Loss: 0.8187|lr = 0.00048\n",
      "Epoch: 1683|steps:   30|Train Avg Loss: 0.8645 |Test Loss: 0.8169|lr = 0.00048\n",
      "Epoch: 1683|steps:   60|Train Avg Loss: 0.8841 |Test Loss: 0.8154|lr = 0.00048\n",
      "Epoch: 1684|steps:   30|Train Avg Loss: 0.8655 |Test Loss: 0.8178|lr = 0.00048\n",
      "Epoch: 1684|steps:   60|Train Avg Loss: 0.8850 |Test Loss: 0.8168|lr = 0.00048\n",
      "Epoch: 1685|steps:   30|Train Avg Loss: 0.8677 |Test Loss: 0.8152|lr = 0.00048\n",
      "Epoch: 1685|steps:   60|Train Avg Loss: 0.8919 |Test Loss: 0.8197|lr = 0.00048\n",
      "Epoch: 1686|steps:   30|Train Avg Loss: 0.8660 |Test Loss: 0.8162|lr = 0.00048\n",
      "Epoch: 1686|steps:   60|Train Avg Loss: 0.8632 |Test Loss: 0.8145|lr = 0.00048\n",
      "Epoch: 1687|steps:   30|Train Avg Loss: 0.8962 |Test Loss: 0.8193|lr = 0.00048\n",
      "Epoch: 1687|steps:   60|Train Avg Loss: 0.8619 |Test Loss: 0.8180|lr = 0.00048\n",
      "Epoch: 1688|steps:   30|Train Avg Loss: 0.8671 |Test Loss: 0.8165|lr = 0.00048\n",
      "Epoch: 1688|steps:   60|Train Avg Loss: 0.8717 |Test Loss: 0.8145|lr = 0.00048\n",
      "Epoch: 1689|steps:   30|Train Avg Loss: 0.8727 |Test Loss: 0.8161|lr = 0.00048\n",
      "Epoch: 1689|steps:   60|Train Avg Loss: 0.8824 |Test Loss: 0.8187|lr = 0.00048\n",
      "Epoch: 1690|steps:   30|Train Avg Loss: 0.8746 |Test Loss: 0.8154|lr = 0.00048\n",
      "Epoch: 1690|steps:   60|Train Avg Loss: 0.8792 |Test Loss: 0.8196|lr = 0.00048\n",
      "Epoch: 1691|steps:   30|Train Avg Loss: 0.8662 |Test Loss: 0.8194|lr = 0.00048\n",
      "Epoch: 1691|steps:   60|Train Avg Loss: 0.8876 |Test Loss: 0.8166|lr = 0.00048\n",
      "Epoch: 1692|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8167|lr = 0.00047\n",
      "Epoch: 1692|steps:   60|Train Avg Loss: 0.8748 |Test Loss: 0.8170|lr = 0.00047\n",
      "Epoch: 1693|steps:   30|Train Avg Loss: 0.8727 |Test Loss: 0.8180|lr = 0.00047\n",
      "Epoch: 1693|steps:   60|Train Avg Loss: 0.8739 |Test Loss: 0.8157|lr = 0.00047\n",
      "Epoch: 1694|steps:   30|Train Avg Loss: 0.8724 |Test Loss: 0.8161|lr = 0.00047\n",
      "Epoch: 1694|steps:   60|Train Avg Loss: 0.8847 |Test Loss: 0.8185|lr = 0.00047\n",
      "Epoch: 1695|steps:   30|Train Avg Loss: 0.8675 |Test Loss: 0.8153|lr = 0.00047\n",
      "Epoch: 1695|steps:   60|Train Avg Loss: 0.8753 |Test Loss: 0.8161|lr = 0.00047\n",
      "Epoch: 1696|steps:   30|Train Avg Loss: 0.8512 |Test Loss: 0.8152|lr = 0.00047\n",
      "Epoch: 1696|steps:   60|Train Avg Loss: 0.8892 |Test Loss: 0.8167|lr = 0.00047\n",
      "Epoch: 1697|steps:   30|Train Avg Loss: 0.8623 |Test Loss: 0.8171|lr = 0.00047\n",
      "Epoch: 1697|steps:   60|Train Avg Loss: 0.9001 |Test Loss: 0.8190|lr = 0.00047\n",
      "Epoch: 1698|steps:   30|Train Avg Loss: 0.8881 |Test Loss: 0.8171|lr = 0.00047\n",
      "Epoch: 1698|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8182|lr = 0.00047\n",
      "Epoch: 1699|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8154|lr = 0.00047\n",
      "Epoch: 1699|steps:   60|Train Avg Loss: 0.8756 |Test Loss: 0.8161|lr = 0.00047\n",
      "Epoch: 1700|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8169|lr = 0.00047\n",
      "Epoch: 1700|steps:   60|Train Avg Loss: 0.8879 |Test Loss: 0.8159|lr = 0.00047\n",
      "Epoch: 1701|steps:   30|Train Avg Loss: 0.8717 |Test Loss: 0.8177|lr = 0.00047\n",
      "Epoch: 1701|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8194|lr = 0.00047\n",
      "Epoch: 1702|steps:   30|Train Avg Loss: 0.8609 |Test Loss: 0.8156|lr = 0.00047\n",
      "Epoch: 1702|steps:   60|Train Avg Loss: 0.8722 |Test Loss: 0.8152|lr = 0.00047\n",
      "Epoch: 1703|steps:   30|Train Avg Loss: 0.8789 |Test Loss: 0.8168|lr = 0.00046\n",
      "Epoch: 1703|steps:   60|Train Avg Loss: 0.8662 |Test Loss: 0.8154|lr = 0.00046\n",
      "Epoch: 1704|steps:   30|Train Avg Loss: 0.8825 |Test Loss: 0.8188|lr = 0.00046\n",
      "Epoch: 1704|steps:   60|Train Avg Loss: 0.8637 |Test Loss: 0.8193|lr = 0.00046\n",
      "Epoch: 1705|steps:   30|Train Avg Loss: 0.8538 |Test Loss: 0.8128|lr = 0.00046\n",
      "Epoch: 1705|steps:   60|Train Avg Loss: 0.8895 |Test Loss: 0.8152|lr = 0.00046\n",
      "Epoch: 1706|steps:   30|Train Avg Loss: 0.8672 |Test Loss: 0.8210|lr = 0.00046\n",
      "Epoch: 1706|steps:   60|Train Avg Loss: 0.8762 |Test Loss: 0.8168|lr = 0.00046\n",
      "Epoch: 1707|steps:   30|Train Avg Loss: 0.8847 |Test Loss: 0.8193|lr = 0.00046\n",
      "Epoch: 1707|steps:   60|Train Avg Loss: 0.8670 |Test Loss: 0.8164|lr = 0.00046\n",
      "Epoch: 1708|steps:   30|Train Avg Loss: 0.8784 |Test Loss: 0.8172|lr = 0.00046\n",
      "Epoch: 1708|steps:   60|Train Avg Loss: 0.8610 |Test Loss: 0.8152|lr = 0.00046\n",
      "Epoch: 1709|steps:   30|Train Avg Loss: 0.8786 |Test Loss: 0.8163|lr = 0.00046\n",
      "Epoch: 1709|steps:   60|Train Avg Loss: 0.8841 |Test Loss: 0.8172|lr = 0.00046\n",
      "Epoch: 1710|steps:   30|Train Avg Loss: 0.8700 |Test Loss: 0.8148|lr = 0.00046\n",
      "Epoch: 1710|steps:   60|Train Avg Loss: 0.8678 |Test Loss: 0.8159|lr = 0.00046\n",
      "Epoch: 1711|steps:   30|Train Avg Loss: 0.8808 |Test Loss: 0.8153|lr = 0.00046\n",
      "Epoch: 1711|steps:   60|Train Avg Loss: 0.8739 |Test Loss: 0.8181|lr = 0.00046\n",
      "Epoch: 1712|steps:   30|Train Avg Loss: 0.8651 |Test Loss: 0.8153|lr = 0.00046\n",
      "Epoch: 1712|steps:   60|Train Avg Loss: 0.8889 |Test Loss: 0.8170|lr = 0.00046\n",
      "Epoch: 1713|steps:   30|Train Avg Loss: 0.8732 |Test Loss: 0.8165|lr = 0.00046\n",
      "Epoch: 1713|steps:   60|Train Avg Loss: 0.8721 |Test Loss: 0.8150|lr = 0.00046\n",
      "Epoch: 1714|steps:   30|Train Avg Loss: 0.8623 |Test Loss: 0.8137|lr = 0.00045\n",
      "Epoch: 1714|steps:   60|Train Avg Loss: 0.9063 |Test Loss: 0.8197|lr = 0.00045\n",
      "Epoch: 1715|steps:   30|Train Avg Loss: 0.8558 |Test Loss: 0.8130|lr = 0.00045\n",
      "Epoch: 1715|steps:   60|Train Avg Loss: 0.9008 |Test Loss: 0.8171|lr = 0.00045\n",
      "Epoch: 1716|steps:   30|Train Avg Loss: 0.8858 |Test Loss: 0.8176|lr = 0.00045\n",
      "Epoch: 1716|steps:   60|Train Avg Loss: 0.8735 |Test Loss: 0.8176|lr = 0.00045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1717|steps:   30|Train Avg Loss: 0.8752 |Test Loss: 0.8162|lr = 0.00045\n",
      "Epoch: 1717|steps:   60|Train Avg Loss: 0.8672 |Test Loss: 0.8153|lr = 0.00045\n",
      "Epoch: 1718|steps:   30|Train Avg Loss: 0.8793 |Test Loss: 0.8166|lr = 0.00045\n",
      "Epoch: 1718|steps:   60|Train Avg Loss: 0.8822 |Test Loss: 0.8150|lr = 0.00045\n",
      "Epoch: 1719|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8172|lr = 0.00045\n",
      "Epoch: 1719|steps:   60|Train Avg Loss: 0.8509 |Test Loss: 0.8141|lr = 0.00045\n",
      "Epoch: 1720|steps:   30|Train Avg Loss: 0.8587 |Test Loss: 0.8145|lr = 0.00045\n",
      "Epoch: 1720|steps:   60|Train Avg Loss: 0.8953 |Test Loss: 0.8157|lr = 0.00045\n",
      "Epoch: 1721|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8163|lr = 0.00045\n",
      "Epoch: 1721|steps:   60|Train Avg Loss: 0.8809 |Test Loss: 0.8146|lr = 0.00045\n",
      "Epoch: 1722|steps:   30|Train Avg Loss: 0.8874 |Test Loss: 0.8160|lr = 0.00045\n",
      "Epoch: 1722|steps:   60|Train Avg Loss: 0.8682 |Test Loss: 0.8165|lr = 0.00045\n",
      "Epoch: 1723|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8136|lr = 0.00045\n",
      "Epoch: 1723|steps:   60|Train Avg Loss: 0.8857 |Test Loss: 0.8134|lr = 0.00045\n",
      "Epoch: 1724|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8152|lr = 0.00045\n",
      "Epoch: 1724|steps:   60|Train Avg Loss: 0.8674 |Test Loss: 0.8162|lr = 0.00045\n",
      "Epoch: 1725|steps:   30|Train Avg Loss: 0.8643 |Test Loss: 0.8138|lr = 0.00045\n",
      "Epoch: 1725|steps:   60|Train Avg Loss: 0.8929 |Test Loss: 0.8176|lr = 0.00045\n",
      "Epoch: 1726|steps:   30|Train Avg Loss: 0.8872 |Test Loss: 0.8181|lr = 0.00045\n",
      "Epoch: 1726|steps:   60|Train Avg Loss: 0.8535 |Test Loss: 0.8139|lr = 0.00045\n",
      "Epoch: 1727|steps:   30|Train Avg Loss: 0.8768 |Test Loss: 0.8158|lr = 0.00045\n",
      "Epoch: 1727|steps:   60|Train Avg Loss: 0.8624 |Test Loss: 0.8154|lr = 0.00045\n",
      "Epoch: 1728|steps:   30|Train Avg Loss: 0.8756 |Test Loss: 0.8174|lr = 0.00045\n",
      "Epoch: 1728|steps:   60|Train Avg Loss: 0.8682 |Test Loss: 0.8142|lr = 0.00045\n",
      "Epoch: 1729|steps:   30|Train Avg Loss: 0.8804 |Test Loss: 0.8150|lr = 0.00045\n",
      "Epoch: 1729|steps:   60|Train Avg Loss: 0.8658 |Test Loss: 0.8157|lr = 0.00045\n",
      "Epoch: 1730|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8168|lr = 0.00045\n",
      "Epoch: 1730|steps:   60|Train Avg Loss: 0.8803 |Test Loss: 0.8154|lr = 0.00045\n",
      "Epoch: 1731|steps:   30|Train Avg Loss: 0.8888 |Test Loss: 0.8175|lr = 0.00045\n",
      "Epoch: 1731|steps:   60|Train Avg Loss: 0.8654 |Test Loss: 0.8143|lr = 0.00045\n",
      "Epoch: 1732|steps:   30|Train Avg Loss: 0.8589 |Test Loss: 0.8115|lr = 0.00045\n",
      "Epoch: 1732|steps:   60|Train Avg Loss: 0.8787 |Test Loss: 0.8172|lr = 0.00045\n",
      "Epoch: 1733|steps:   30|Train Avg Loss: 0.8713 |Test Loss: 0.8156|lr = 0.00045\n",
      "Epoch: 1733|steps:   60|Train Avg Loss: 0.8644 |Test Loss: 0.8152|lr = 0.00045\n",
      "Epoch: 1734|steps:   30|Train Avg Loss: 0.8868 |Test Loss: 0.8191|lr = 0.00045\n",
      "Epoch: 1734|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8141|lr = 0.00045\n",
      "Epoch: 1735|steps:   30|Train Avg Loss: 0.8553 |Test Loss: 0.8125|lr = 0.00045\n",
      "Epoch: 1735|steps:   60|Train Avg Loss: 0.9005 |Test Loss: 0.8175|lr = 0.00045\n",
      "Epoch: 1736|steps:   30|Train Avg Loss: 0.8743 |Test Loss: 0.8166|lr = 0.00044\n",
      "Epoch: 1736|steps:   60|Train Avg Loss: 0.8923 |Test Loss: 0.8146|lr = 0.00044\n",
      "Epoch: 1737|steps:   30|Train Avg Loss: 0.8695 |Test Loss: 0.8139|lr = 0.00044\n",
      "Epoch: 1737|steps:   60|Train Avg Loss: 0.8779 |Test Loss: 0.8155|lr = 0.00044\n",
      "Epoch: 1738|steps:   30|Train Avg Loss: 0.8481 |Test Loss: 0.8134|lr = 0.00044\n",
      "Epoch: 1738|steps:   60|Train Avg Loss: 0.9063 |Test Loss: 0.8162|lr = 0.00044\n",
      "Epoch: 1739|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.8155|lr = 0.00044\n",
      "Epoch: 1739|steps:   60|Train Avg Loss: 0.8621 |Test Loss: 0.8159|lr = 0.00044\n",
      "Epoch: 1740|steps:   30|Train Avg Loss: 0.8688 |Test Loss: 0.8135|lr = 0.00044\n",
      "Epoch: 1740|steps:   60|Train Avg Loss: 0.8688 |Test Loss: 0.8168|lr = 0.00044\n",
      "Epoch: 1741|steps:   30|Train Avg Loss: 0.8861 |Test Loss: 0.8161|lr = 0.00044\n",
      "Epoch: 1741|steps:   60|Train Avg Loss: 0.8571 |Test Loss: 0.8147|lr = 0.00044\n",
      "Epoch: 1742|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8156|lr = 0.00044\n",
      "Epoch: 1742|steps:   60|Train Avg Loss: 0.8850 |Test Loss: 0.8155|lr = 0.00044\n",
      "Epoch: 1743|steps:   30|Train Avg Loss: 0.8783 |Test Loss: 0.8131|lr = 0.00044\n",
      "Epoch: 1743|steps:   60|Train Avg Loss: 0.8657 |Test Loss: 0.8132|lr = 0.00044\n",
      "Epoch: 1744|steps:   30|Train Avg Loss: 0.8709 |Test Loss: 0.8133|lr = 0.00044\n",
      "Epoch: 1744|steps:   60|Train Avg Loss: 0.8775 |Test Loss: 0.8161|lr = 0.00044\n",
      "Epoch: 1745|steps:   30|Train Avg Loss: 0.8708 |Test Loss: 0.8140|lr = 0.00044\n",
      "Epoch: 1745|steps:   60|Train Avg Loss: 0.8832 |Test Loss: 0.8170|lr = 0.00044\n",
      "Epoch: 1746|steps:   30|Train Avg Loss: 0.8708 |Test Loss: 0.8147|lr = 0.00044\n",
      "Epoch: 1746|steps:   60|Train Avg Loss: 0.8835 |Test Loss: 0.8162|lr = 0.00044\n",
      "Epoch: 1747|steps:   30|Train Avg Loss: 0.8669 |Test Loss: 0.8134|lr = 0.00043\n",
      "Epoch: 1747|steps:   60|Train Avg Loss: 0.8830 |Test Loss: 0.8163|lr = 0.00043\n",
      "Epoch: 1748|steps:   30|Train Avg Loss: 0.8666 |Test Loss: 0.8133|lr = 0.00043\n",
      "Epoch: 1748|steps:   60|Train Avg Loss: 0.8899 |Test Loss: 0.8196|lr = 0.00043\n",
      "Epoch: 1749|steps:   30|Train Avg Loss: 0.8692 |Test Loss: 0.8136|lr = 0.00043\n",
      "Epoch: 1749|steps:   60|Train Avg Loss: 0.8882 |Test Loss: 0.8174|lr = 0.00043\n",
      "Epoch: 1750|steps:   30|Train Avg Loss: 0.8624 |Test Loss: 0.8133|lr = 0.00043\n",
      "Epoch: 1750|steps:   60|Train Avg Loss: 0.8898 |Test Loss: 0.8173|lr = 0.00043\n",
      "Epoch: 1751|steps:   30|Train Avg Loss: 0.8804 |Test Loss: 0.8172|lr = 0.00043\n",
      "Epoch: 1751|steps:   60|Train Avg Loss: 0.8713 |Test Loss: 0.8153|lr = 0.00043\n",
      "Epoch: 1752|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8145|lr = 0.00043\n",
      "Epoch: 1752|steps:   60|Train Avg Loss: 0.8731 |Test Loss: 0.8149|lr = 0.00043\n",
      "Epoch: 1753|steps:   30|Train Avg Loss: 0.8711 |Test Loss: 0.8161|lr = 0.00043\n",
      "Epoch: 1753|steps:   60|Train Avg Loss: 0.8739 |Test Loss: 0.8141|lr = 0.00043\n",
      "Epoch: 1754|steps:   30|Train Avg Loss: 0.8624 |Test Loss: 0.8143|lr = 0.00043\n",
      "Epoch: 1754|steps:   60|Train Avg Loss: 0.8859 |Test Loss: 0.8144|lr = 0.00043\n",
      "Epoch: 1755|steps:   30|Train Avg Loss: 0.8841 |Test Loss: 0.8165|lr = 0.00043\n",
      "Epoch: 1755|steps:   60|Train Avg Loss: 0.8702 |Test Loss: 0.8170|lr = 0.00043\n",
      "Epoch: 1756|steps:   30|Train Avg Loss: 0.8755 |Test Loss: 0.8137|lr = 0.00043\n",
      "Epoch: 1756|steps:   60|Train Avg Loss: 0.8660 |Test Loss: 0.8157|lr = 0.00043\n",
      "Epoch: 1757|steps:   30|Train Avg Loss: 0.8571 |Test Loss: 0.8119|lr = 0.00043\n",
      "Epoch: 1757|steps:   60|Train Avg Loss: 0.8962 |Test Loss: 0.8171|lr = 0.00043\n",
      "Epoch: 1758|steps:   30|Train Avg Loss: 0.8781 |Test Loss: 0.8146|lr = 0.00042\n",
      "Epoch: 1758|steps:   60|Train Avg Loss: 0.8791 |Test Loss: 0.8178|lr = 0.00042\n",
      "Epoch: 1759|steps:   30|Train Avg Loss: 0.8890 |Test Loss: 0.8166|lr = 0.00042\n",
      "Epoch: 1759|steps:   60|Train Avg Loss: 0.8730 |Test Loss: 0.8152|lr = 0.00042\n",
      "Epoch: 1760|steps:   30|Train Avg Loss: 0.8780 |Test Loss: 0.8145|lr = 0.00042\n",
      "Epoch: 1760|steps:   60|Train Avg Loss: 0.8704 |Test Loss: 0.8155|lr = 0.00042\n",
      "Epoch: 1761|steps:   30|Train Avg Loss: 0.8842 |Test Loss: 0.8163|lr = 0.00042\n",
      "Epoch: 1761|steps:   60|Train Avg Loss: 0.8640 |Test Loss: 0.8136|lr = 0.00042\n",
      "Epoch: 1762|steps:   30|Train Avg Loss: 0.8626 |Test Loss: 0.8138|lr = 0.00042\n",
      "Epoch: 1762|steps:   60|Train Avg Loss: 0.8783 |Test Loss: 0.8160|lr = 0.00042\n",
      "Epoch: 1763|steps:   30|Train Avg Loss: 0.8706 |Test Loss: 0.8166|lr = 0.00042\n",
      "Epoch: 1763|steps:   60|Train Avg Loss: 0.8721 |Test Loss: 0.8126|lr = 0.00042\n",
      "Epoch: 1764|steps:   30|Train Avg Loss: 0.8947 |Test Loss: 0.8176|lr = 0.00042\n",
      "Epoch: 1764|steps:   60|Train Avg Loss: 0.8736 |Test Loss: 0.8132|lr = 0.00042\n",
      "Epoch: 1765|steps:   30|Train Avg Loss: 0.8786 |Test Loss: 0.8142|lr = 0.00042\n",
      "Epoch: 1765|steps:   60|Train Avg Loss: 0.8712 |Test Loss: 0.8167|lr = 0.00042\n",
      "Epoch: 1766|steps:   30|Train Avg Loss: 0.8811 |Test Loss: 0.8165|lr = 0.00042\n",
      "Epoch: 1766|steps:   60|Train Avg Loss: 0.8597 |Test Loss: 0.8124|lr = 0.00042\n",
      "Epoch: 1767|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8138|lr = 0.00042\n",
      "Epoch: 1767|steps:   60|Train Avg Loss: 0.8805 |Test Loss: 0.8149|lr = 0.00042\n",
      "Epoch: 1768|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.8165|lr = 0.00042\n",
      "Epoch: 1768|steps:   60|Train Avg Loss: 0.8626 |Test Loss: 0.8150|lr = 0.00042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1769|steps:   30|Train Avg Loss: 0.8552 |Test Loss: 0.8133|lr = 0.00041\n",
      "Epoch: 1769|steps:   60|Train Avg Loss: 0.8875 |Test Loss: 0.8136|lr = 0.00041\n",
      "Epoch: 1770|steps:   30|Train Avg Loss: 0.8718 |Test Loss: 0.8168|lr = 0.00041\n",
      "Epoch: 1770|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8156|lr = 0.00041\n",
      "Epoch: 1771|steps:   30|Train Avg Loss: 0.8476 |Test Loss: 0.8110|lr = 0.00041\n",
      "Epoch: 1771|steps:   60|Train Avg Loss: 0.8930 |Test Loss: 0.8167|lr = 0.00041\n",
      "Epoch: 1772|steps:   30|Train Avg Loss: 0.8859 |Test Loss: 0.8158|lr = 0.00041\n",
      "Epoch: 1772|steps:   60|Train Avg Loss: 0.8732 |Test Loss: 0.8153|lr = 0.00041\n",
      "Epoch: 1773|steps:   30|Train Avg Loss: 0.8723 |Test Loss: 0.8153|lr = 0.00041\n",
      "Epoch: 1773|steps:   60|Train Avg Loss: 0.8851 |Test Loss: 0.8154|lr = 0.00041\n",
      "Epoch: 1774|steps:   30|Train Avg Loss: 0.8539 |Test Loss: 0.8130|lr = 0.00041\n",
      "Epoch: 1774|steps:   60|Train Avg Loss: 0.8942 |Test Loss: 0.8169|lr = 0.00041\n",
      "Epoch: 1775|steps:   30|Train Avg Loss: 0.8781 |Test Loss: 0.8157|lr = 0.00041\n",
      "Epoch: 1775|steps:   60|Train Avg Loss: 0.8690 |Test Loss: 0.8152|lr = 0.00041\n",
      "Epoch: 1776|steps:   30|Train Avg Loss: 0.8622 |Test Loss: 0.8142|lr = 0.00041\n",
      "Epoch: 1776|steps:   60|Train Avg Loss: 0.8760 |Test Loss: 0.8149|lr = 0.00041\n",
      "Epoch: 1777|steps:   30|Train Avg Loss: 0.8776 |Test Loss: 0.8142|lr = 0.00041\n",
      "Epoch: 1777|steps:   60|Train Avg Loss: 0.8705 |Test Loss: 0.8161|lr = 0.00041\n",
      "Epoch: 1778|steps:   30|Train Avg Loss: 0.8829 |Test Loss: 0.8161|lr = 0.00041\n",
      "Epoch: 1778|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8158|lr = 0.00041\n",
      "Epoch: 1779|steps:   30|Train Avg Loss: 0.8813 |Test Loss: 0.8159|lr = 0.00041\n",
      "Epoch: 1779|steps:   60|Train Avg Loss: 0.8552 |Test Loss: 0.8130|lr = 0.00041\n",
      "Epoch: 1780|steps:   30|Train Avg Loss: 0.8706 |Test Loss: 0.8137|lr = 0.00040\n",
      "Epoch: 1780|steps:   60|Train Avg Loss: 0.8639 |Test Loss: 0.8133|lr = 0.00040\n",
      "Epoch: 1781|steps:   30|Train Avg Loss: 0.8727 |Test Loss: 0.8161|lr = 0.00040\n",
      "Epoch: 1781|steps:   60|Train Avg Loss: 0.8804 |Test Loss: 0.8149|lr = 0.00040\n",
      "Epoch: 1782|steps:   30|Train Avg Loss: 0.8812 |Test Loss: 0.8160|lr = 0.00040\n",
      "Epoch: 1782|steps:   60|Train Avg Loss: 0.8632 |Test Loss: 0.8137|lr = 0.00040\n",
      "Epoch: 1783|steps:   30|Train Avg Loss: 0.8631 |Test Loss: 0.8123|lr = 0.00040\n",
      "Epoch: 1783|steps:   60|Train Avg Loss: 0.8885 |Test Loss: 0.8150|lr = 0.00040\n",
      "Epoch: 1784|steps:   30|Train Avg Loss: 0.8601 |Test Loss: 0.8159|lr = 0.00040\n",
      "Epoch: 1784|steps:   60|Train Avg Loss: 0.8794 |Test Loss: 0.8141|lr = 0.00040\n",
      "Epoch: 1785|steps:   30|Train Avg Loss: 0.8687 |Test Loss: 0.8148|lr = 0.00040\n",
      "Epoch: 1785|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8156|lr = 0.00040\n",
      "Epoch: 1786|steps:   30|Train Avg Loss: 0.8753 |Test Loss: 0.8152|lr = 0.00040\n",
      "Epoch: 1786|steps:   60|Train Avg Loss: 0.8859 |Test Loss: 0.8162|lr = 0.00040\n",
      "Epoch: 1787|steps:   30|Train Avg Loss: 0.8600 |Test Loss: 0.8118|lr = 0.00040\n",
      "Epoch: 1787|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8166|lr = 0.00040\n",
      "Epoch: 1788|steps:   30|Train Avg Loss: 0.8781 |Test Loss: 0.8162|lr = 0.00040\n",
      "Epoch: 1788|steps:   60|Train Avg Loss: 0.8793 |Test Loss: 0.8140|lr = 0.00040\n",
      "Epoch: 1789|steps:   30|Train Avg Loss: 0.8588 |Test Loss: 0.8128|lr = 0.00040\n",
      "Epoch: 1789|steps:   60|Train Avg Loss: 0.8894 |Test Loss: 0.8155|lr = 0.00040\n",
      "Epoch: 1790|steps:   30|Train Avg Loss: 0.8730 |Test Loss: 0.8157|lr = 0.00040\n",
      "Epoch: 1790|steps:   60|Train Avg Loss: 0.8538 |Test Loss: 0.8133|lr = 0.00040\n",
      "Epoch: 1791|steps:   30|Train Avg Loss: 0.8750 |Test Loss: 0.8161|lr = 0.00039\n",
      "Epoch: 1791|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8147|lr = 0.00039\n",
      "Epoch: 1792|steps:   30|Train Avg Loss: 0.8760 |Test Loss: 0.8148|lr = 0.00039\n",
      "Epoch: 1792|steps:   60|Train Avg Loss: 0.8576 |Test Loss: 0.8141|lr = 0.00039\n",
      "Epoch: 1793|steps:   30|Train Avg Loss: 0.8491 |Test Loss: 0.8116|lr = 0.00039\n",
      "Epoch: 1793|steps:   60|Train Avg Loss: 0.9021 |Test Loss: 0.8168|lr = 0.00039\n",
      "Epoch: 1794|steps:   30|Train Avg Loss: 0.8747 |Test Loss: 0.8157|lr = 0.00039\n",
      "Epoch: 1794|steps:   60|Train Avg Loss: 0.8828 |Test Loss: 0.8159|lr = 0.00039\n",
      "Epoch: 1795|steps:   30|Train Avg Loss: 0.8785 |Test Loss: 0.8155|lr = 0.00039\n",
      "Epoch: 1795|steps:   60|Train Avg Loss: 0.8609 |Test Loss: 0.8165|lr = 0.00039\n",
      "Epoch: 1796|steps:   30|Train Avg Loss: 0.8891 |Test Loss: 0.8172|lr = 0.00039\n",
      "Epoch: 1796|steps:   60|Train Avg Loss: 0.8711 |Test Loss: 0.8160|lr = 0.00039\n",
      "Epoch: 1797|steps:   30|Train Avg Loss: 0.8721 |Test Loss: 0.8126|lr = 0.00039\n",
      "Epoch: 1797|steps:   60|Train Avg Loss: 0.8750 |Test Loss: 0.8166|lr = 0.00039\n",
      "Epoch: 1798|steps:   30|Train Avg Loss: 0.8647 |Test Loss: 0.8142|lr = 0.00039\n",
      "Epoch: 1798|steps:   60|Train Avg Loss: 0.8861 |Test Loss: 0.8165|lr = 0.00039\n",
      "Epoch: 1799|steps:   30|Train Avg Loss: 0.8757 |Test Loss: 0.8155|lr = 0.00039\n",
      "Epoch: 1799|steps:   60|Train Avg Loss: 0.8652 |Test Loss: 0.8143|lr = 0.00039\n",
      "Epoch: 1800|steps:   30|Train Avg Loss: 0.8602 |Test Loss: 0.8139|lr = 0.00039\n",
      "Epoch: 1800|steps:   60|Train Avg Loss: 0.8747 |Test Loss: 0.8128|lr = 0.00039\n",
      "Epoch: 1801|steps:   30|Train Avg Loss: 0.8772 |Test Loss: 0.8174|lr = 0.00039\n",
      "Epoch: 1801|steps:   60|Train Avg Loss: 0.8780 |Test Loss: 0.8168|lr = 0.00039\n",
      "Epoch: 1802|steps:   30|Train Avg Loss: 0.8689 |Test Loss: 0.8161|lr = 0.00039\n",
      "Epoch: 1802|steps:   60|Train Avg Loss: 0.8659 |Test Loss: 0.8136|lr = 0.00039\n",
      "Epoch: 1803|steps:   30|Train Avg Loss: 0.8762 |Test Loss: 0.8156|lr = 0.00039\n",
      "Epoch: 1803|steps:   60|Train Avg Loss: 0.8672 |Test Loss: 0.8147|lr = 0.00039\n",
      "Epoch: 1804|steps:   30|Train Avg Loss: 0.8663 |Test Loss: 0.8140|lr = 0.00039\n",
      "Epoch: 1804|steps:   60|Train Avg Loss: 0.8894 |Test Loss: 0.8170|lr = 0.00039\n",
      "Epoch: 1805|steps:   30|Train Avg Loss: 0.8809 |Test Loss: 0.8163|lr = 0.00039\n",
      "Epoch: 1805|steps:   60|Train Avg Loss: 0.8649 |Test Loss: 0.8159|lr = 0.00039\n",
      "Epoch: 1806|steps:   30|Train Avg Loss: 0.8617 |Test Loss: 0.8135|lr = 0.00039\n",
      "Epoch: 1806|steps:   60|Train Avg Loss: 0.8676 |Test Loss: 0.8139|lr = 0.00039\n",
      "Epoch: 1807|steps:   30|Train Avg Loss: 0.8806 |Test Loss: 0.8185|lr = 0.00039\n",
      "Epoch: 1807|steps:   60|Train Avg Loss: 0.8663 |Test Loss: 0.8163|lr = 0.00039\n",
      "Epoch: 1808|steps:   30|Train Avg Loss: 0.8572 |Test Loss: 0.8129|lr = 0.00039\n",
      "Epoch: 1808|steps:   60|Train Avg Loss: 0.8801 |Test Loss: 0.8136|lr = 0.00039\n",
      "Epoch: 1809|steps:   30|Train Avg Loss: 0.8622 |Test Loss: 0.8151|lr = 0.00039\n",
      "Epoch: 1809|steps:   60|Train Avg Loss: 0.8829 |Test Loss: 0.8164|lr = 0.00039\n",
      "Epoch: 1810|steps:   30|Train Avg Loss: 0.8769 |Test Loss: 0.8154|lr = 0.00039\n",
      "Epoch: 1810|steps:   60|Train Avg Loss: 0.8715 |Test Loss: 0.8152|lr = 0.00039\n",
      "Epoch: 1811|steps:   30|Train Avg Loss: 0.8660 |Test Loss: 0.8145|lr = 0.00039\n",
      "Epoch: 1811|steps:   60|Train Avg Loss: 0.8758 |Test Loss: 0.8176|lr = 0.00039\n",
      "Epoch: 1812|steps:   30|Train Avg Loss: 0.8884 |Test Loss: 0.8165|lr = 0.00039\n",
      "Epoch: 1812|steps:   60|Train Avg Loss: 0.8516 |Test Loss: 0.8137|lr = 0.00039\n",
      "Epoch: 1813|steps:   30|Train Avg Loss: 0.8572 |Test Loss: 0.8126|lr = 0.00039\n",
      "Epoch: 1813|steps:   60|Train Avg Loss: 0.8933 |Test Loss: 0.8162|lr = 0.00039\n",
      "Epoch: 1814|steps:   30|Train Avg Loss: 0.8933 |Test Loss: 0.8182|lr = 0.00039\n",
      "Epoch: 1814|steps:   60|Train Avg Loss: 0.8492 |Test Loss: 0.8158|lr = 0.00039\n",
      "Epoch: 1815|steps:   30|Train Avg Loss: 0.8582 |Test Loss: 0.8140|lr = 0.00038\n",
      "Epoch: 1815|steps:   60|Train Avg Loss: 0.8914 |Test Loss: 0.8170|lr = 0.00038\n",
      "Epoch: 1816|steps:   30|Train Avg Loss: 0.8689 |Test Loss: 0.8158|lr = 0.00038\n",
      "Epoch: 1816|steps:   60|Train Avg Loss: 0.8671 |Test Loss: 0.8141|lr = 0.00038\n",
      "Epoch: 1817|steps:   30|Train Avg Loss: 0.8622 |Test Loss: 0.8152|lr = 0.00038\n",
      "Epoch: 1817|steps:   60|Train Avg Loss: 0.8717 |Test Loss: 0.8133|lr = 0.00038\n",
      "Epoch: 1818|steps:   30|Train Avg Loss: 0.8620 |Test Loss: 0.8181|lr = 0.00038\n",
      "Epoch: 1818|steps:   60|Train Avg Loss: 0.8765 |Test Loss: 0.8165|lr = 0.00038\n",
      "Epoch: 1819|steps:   30|Train Avg Loss: 0.8458 |Test Loss: 0.8136|lr = 0.00038\n",
      "Epoch: 1819|steps:   60|Train Avg Loss: 0.8893 |Test Loss: 0.8151|lr = 0.00038\n",
      "Epoch: 1820|steps:   30|Train Avg Loss: 0.8567 |Test Loss: 0.8135|lr = 0.00038\n",
      "Epoch: 1820|steps:   60|Train Avg Loss: 0.8667 |Test Loss: 0.8138|lr = 0.00038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1821|steps:   30|Train Avg Loss: 0.8701 |Test Loss: 0.8168|lr = 0.00038\n",
      "Epoch: 1821|steps:   60|Train Avg Loss: 0.8635 |Test Loss: 0.8162|lr = 0.00038\n",
      "Epoch: 1822|steps:   30|Train Avg Loss: 0.8656 |Test Loss: 0.8166|lr = 0.00038\n",
      "Epoch: 1822|steps:   60|Train Avg Loss: 0.8757 |Test Loss: 0.8151|lr = 0.00038\n",
      "Epoch: 1823|steps:   30|Train Avg Loss: 0.8800 |Test Loss: 0.8165|lr = 0.00038\n",
      "Epoch: 1823|steps:   60|Train Avg Loss: 0.8684 |Test Loss: 0.8168|lr = 0.00038\n",
      "Epoch: 1824|steps:   30|Train Avg Loss: 0.8716 |Test Loss: 0.8148|lr = 0.00038\n",
      "Epoch: 1824|steps:   60|Train Avg Loss: 0.8639 |Test Loss: 0.8148|lr = 0.00038\n",
      "Epoch: 1825|steps:   30|Train Avg Loss: 0.8473 |Test Loss: 0.8134|lr = 0.00038\n",
      "Epoch: 1825|steps:   60|Train Avg Loss: 0.8838 |Test Loss: 0.8147|lr = 0.00038\n",
      "Epoch: 1826|steps:   30|Train Avg Loss: 0.8567 |Test Loss: 0.8137|lr = 0.00037\n",
      "Epoch: 1826|steps:   60|Train Avg Loss: 0.8927 |Test Loss: 0.8192|lr = 0.00037\n",
      "Epoch: 1827|steps:   30|Train Avg Loss: 0.8720 |Test Loss: 0.8171|lr = 0.00037\n",
      "Epoch: 1827|steps:   60|Train Avg Loss: 0.8696 |Test Loss: 0.8162|lr = 0.00037\n",
      "Epoch: 1828|steps:   30|Train Avg Loss: 0.8482 |Test Loss: 0.8130|lr = 0.00037\n",
      "Epoch: 1828|steps:   60|Train Avg Loss: 0.8971 |Test Loss: 0.8179|lr = 0.00037\n",
      "Epoch: 1829|steps:   30|Train Avg Loss: 0.8820 |Test Loss: 0.8175|lr = 0.00037\n",
      "Epoch: 1829|steps:   60|Train Avg Loss: 0.8540 |Test Loss: 0.8134|lr = 0.00037\n",
      "Epoch: 1830|steps:   30|Train Avg Loss: 0.8788 |Test Loss: 0.8177|lr = 0.00037\n",
      "Epoch: 1830|steps:   60|Train Avg Loss: 0.8613 |Test Loss: 0.8158|lr = 0.00037\n",
      "Epoch: 1831|steps:   30|Train Avg Loss: 0.8744 |Test Loss: 0.8178|lr = 0.00037\n",
      "Epoch: 1831|steps:   60|Train Avg Loss: 0.8624 |Test Loss: 0.8170|lr = 0.00037\n",
      "Epoch: 1832|steps:   30|Train Avg Loss: 0.8740 |Test Loss: 0.8157|lr = 0.00037\n",
      "Epoch: 1832|steps:   60|Train Avg Loss: 0.8723 |Test Loss: 0.8164|lr = 0.00037\n",
      "Epoch: 1833|steps:   30|Train Avg Loss: 0.8754 |Test Loss: 0.8141|lr = 0.00037\n",
      "Epoch: 1833|steps:   60|Train Avg Loss: 0.8715 |Test Loss: 0.8167|lr = 0.00037\n",
      "Epoch: 1834|steps:   30|Train Avg Loss: 0.8576 |Test Loss: 0.8165|lr = 0.00037\n",
      "Epoch: 1834|steps:   60|Train Avg Loss: 0.8890 |Test Loss: 0.8175|lr = 0.00037\n",
      "Epoch: 1835|steps:   30|Train Avg Loss: 0.8570 |Test Loss: 0.8150|lr = 0.00037\n",
      "Epoch: 1835|steps:   60|Train Avg Loss: 0.8501 |Test Loss: 0.8123|lr = 0.00037\n",
      "Epoch: 1836|steps:   30|Train Avg Loss: 0.8683 |Test Loss: 0.8174|lr = 0.00037\n",
      "Epoch: 1836|steps:   60|Train Avg Loss: 0.8705 |Test Loss: 0.8182|lr = 0.00037\n",
      "Epoch: 1837|steps:   30|Train Avg Loss: 0.8608 |Test Loss: 0.8147|lr = 0.00036\n",
      "Epoch: 1837|steps:   60|Train Avg Loss: 0.8688 |Test Loss: 0.8135|lr = 0.00036\n",
      "Epoch: 1838|steps:   30|Train Avg Loss: 0.8576 |Test Loss: 0.8167|lr = 0.00036\n",
      "Epoch: 1838|steps:   60|Train Avg Loss: 0.8765 |Test Loss: 0.8170|lr = 0.00036\n",
      "Epoch: 1839|steps:   30|Train Avg Loss: 0.8575 |Test Loss: 0.8166|lr = 0.00036\n",
      "Epoch: 1839|steps:   60|Train Avg Loss: 0.8871 |Test Loss: 0.8184|lr = 0.00036\n",
      "Epoch: 1840|steps:   30|Train Avg Loss: 0.8720 |Test Loss: 0.8159|lr = 0.00036\n",
      "Epoch: 1840|steps:   60|Train Avg Loss: 0.8662 |Test Loss: 0.8163|lr = 0.00036\n",
      "Epoch: 1841|steps:   30|Train Avg Loss: 0.8621 |Test Loss: 0.8141|lr = 0.00036\n",
      "Epoch: 1841|steps:   60|Train Avg Loss: 0.8883 |Test Loss: 0.8170|lr = 0.00036\n",
      "Epoch: 1842|steps:   30|Train Avg Loss: 0.8658 |Test Loss: 0.8163|lr = 0.00036\n",
      "Epoch: 1842|steps:   60|Train Avg Loss: 0.8820 |Test Loss: 0.8174|lr = 0.00036\n",
      "Epoch: 1843|steps:   30|Train Avg Loss: 0.8594 |Test Loss: 0.8155|lr = 0.00036\n",
      "Epoch: 1843|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8156|lr = 0.00036\n",
      "Epoch: 1844|steps:   30|Train Avg Loss: 0.8982 |Test Loss: 0.8220|lr = 0.00036\n",
      "Epoch: 1844|steps:   60|Train Avg Loss: 0.8395 |Test Loss: 0.8151|lr = 0.00036\n",
      "Epoch: 1845|steps:   30|Train Avg Loss: 0.8522 |Test Loss: 0.8140|lr = 0.00036\n",
      "Epoch: 1845|steps:   60|Train Avg Loss: 0.8949 |Test Loss: 0.8175|lr = 0.00036\n",
      "Epoch: 1846|steps:   30|Train Avg Loss: 0.8620 |Test Loss: 0.8169|lr = 0.00036\n",
      "Epoch: 1846|steps:   60|Train Avg Loss: 0.8643 |Test Loss: 0.8153|lr = 0.00036\n",
      "Epoch: 1847|steps:   30|Train Avg Loss: 0.8709 |Test Loss: 0.8167|lr = 0.00036\n",
      "Epoch: 1847|steps:   60|Train Avg Loss: 0.8704 |Test Loss: 0.8173|lr = 0.00036\n",
      "Epoch: 1848|steps:   30|Train Avg Loss: 0.8830 |Test Loss: 0.8186|lr = 0.00036\n",
      "Epoch: 1848|steps:   60|Train Avg Loss: 0.8619 |Test Loss: 0.8171|lr = 0.00036\n",
      "Epoch: 1849|steps:   30|Train Avg Loss: 0.8666 |Test Loss: 0.8156|lr = 0.00036\n",
      "Epoch: 1849|steps:   60|Train Avg Loss: 0.8782 |Test Loss: 0.8160|lr = 0.00036\n",
      "Epoch: 1850|steps:   30|Train Avg Loss: 0.8714 |Test Loss: 0.8170|lr = 0.00036\n",
      "Epoch: 1850|steps:   60|Train Avg Loss: 0.8770 |Test Loss: 0.8198|lr = 0.00036\n",
      "Epoch: 1851|steps:   30|Train Avg Loss: 0.8669 |Test Loss: 0.8149|lr = 0.00036\n",
      "Epoch: 1851|steps:   60|Train Avg Loss: 0.8647 |Test Loss: 0.8158|lr = 0.00036\n",
      "Epoch: 1852|steps:   30|Train Avg Loss: 0.8465 |Test Loss: 0.8139|lr = 0.00036\n",
      "Epoch: 1852|steps:   60|Train Avg Loss: 0.8766 |Test Loss: 0.8160|lr = 0.00036\n",
      "Epoch: 1853|steps:   30|Train Avg Loss: 0.8639 |Test Loss: 0.8170|lr = 0.00036\n",
      "Epoch: 1853|steps:   60|Train Avg Loss: 0.8717 |Test Loss: 0.8159|lr = 0.00036\n",
      "Epoch: 1854|steps:   30|Train Avg Loss: 0.8598 |Test Loss: 0.8163|lr = 0.00036\n",
      "Epoch: 1854|steps:   60|Train Avg Loss: 0.8556 |Test Loss: 0.8150|lr = 0.00036\n",
      "Epoch: 1855|steps:   30|Train Avg Loss: 0.8548 |Test Loss: 0.8142|lr = 0.00036\n",
      "Epoch: 1855|steps:   60|Train Avg Loss: 0.8780 |Test Loss: 0.8174|lr = 0.00036\n",
      "Epoch: 1856|steps:   30|Train Avg Loss: 0.8776 |Test Loss: 0.8160|lr = 0.00036\n",
      "Epoch: 1856|steps:   60|Train Avg Loss: 0.8564 |Test Loss: 0.8162|lr = 0.00036\n",
      "Epoch: 1857|steps:   30|Train Avg Loss: 0.8693 |Test Loss: 0.8180|lr = 0.00036\n",
      "Epoch: 1857|steps:   60|Train Avg Loss: 0.8650 |Test Loss: 0.8180|lr = 0.00036\n",
      "Epoch: 1858|steps:   30|Train Avg Loss: 0.8718 |Test Loss: 0.8180|lr = 0.00036\n",
      "Epoch: 1858|steps:   60|Train Avg Loss: 0.8671 |Test Loss: 0.8183|lr = 0.00036\n",
      "Epoch: 1859|steps:   30|Train Avg Loss: 0.8479 |Test Loss: 0.8156|lr = 0.00035\n",
      "Epoch: 1859|steps:   60|Train Avg Loss: 0.8796 |Test Loss: 0.8177|lr = 0.00035\n",
      "Epoch: 1860|steps:   30|Train Avg Loss: 0.8657 |Test Loss: 0.8189|lr = 0.00035\n",
      "Epoch: 1860|steps:   60|Train Avg Loss: 0.8658 |Test Loss: 0.8168|lr = 0.00035\n",
      "Epoch: 1861|steps:   30|Train Avg Loss: 0.8668 |Test Loss: 0.8156|lr = 0.00035\n",
      "Epoch: 1861|steps:   60|Train Avg Loss: 0.8568 |Test Loss: 0.8159|lr = 0.00035\n",
      "Epoch: 1862|steps:   30|Train Avg Loss: 0.8767 |Test Loss: 0.8183|lr = 0.00035\n",
      "Epoch: 1862|steps:   60|Train Avg Loss: 0.8555 |Test Loss: 0.8168|lr = 0.00035\n",
      "Epoch: 1863|steps:   30|Train Avg Loss: 0.8523 |Test Loss: 0.8167|lr = 0.00035\n",
      "Epoch: 1863|steps:   60|Train Avg Loss: 0.8838 |Test Loss: 0.8173|lr = 0.00035\n",
      "Epoch: 1864|steps:   30|Train Avg Loss: 0.8629 |Test Loss: 0.8171|lr = 0.00035\n",
      "Epoch: 1864|steps:   60|Train Avg Loss: 0.8665 |Test Loss: 0.8172|lr = 0.00035\n",
      "Epoch: 1865|steps:   30|Train Avg Loss: 0.8474 |Test Loss: 0.8162|lr = 0.00035\n",
      "Epoch: 1865|steps:   60|Train Avg Loss: 0.8941 |Test Loss: 0.8180|lr = 0.00035\n",
      "Epoch: 1866|steps:   30|Train Avg Loss: 0.8793 |Test Loss: 0.8182|lr = 0.00035\n",
      "Epoch: 1866|steps:   60|Train Avg Loss: 0.8506 |Test Loss: 0.8167|lr = 0.00035\n",
      "Epoch: 1867|steps:   30|Train Avg Loss: 0.8478 |Test Loss: 0.8153|lr = 0.00035\n",
      "Epoch: 1867|steps:   60|Train Avg Loss: 0.8815 |Test Loss: 0.8182|lr = 0.00035\n",
      "Epoch: 1868|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8206|lr = 0.00035\n",
      "Epoch: 1868|steps:   60|Train Avg Loss: 0.8580 |Test Loss: 0.8174|lr = 0.00035\n",
      "Epoch: 1869|steps:   30|Train Avg Loss: 0.8814 |Test Loss: 0.8187|lr = 0.00035\n",
      "Epoch: 1869|steps:   60|Train Avg Loss: 0.8668 |Test Loss: 0.8209|lr = 0.00035\n",
      "Epoch: 1870|steps:   30|Train Avg Loss: 0.8711 |Test Loss: 0.8188|lr = 0.00034\n",
      "Epoch: 1870|steps:   60|Train Avg Loss: 0.8558 |Test Loss: 0.8146|lr = 0.00034\n",
      "Epoch: 1871|steps:   30|Train Avg Loss: 0.8501 |Test Loss: 0.8153|lr = 0.00034\n",
      "Epoch: 1871|steps:   60|Train Avg Loss: 0.8666 |Test Loss: 0.8176|lr = 0.00034\n",
      "Epoch: 1872|steps:   30|Train Avg Loss: 0.8567 |Test Loss: 0.8182|lr = 0.00034\n",
      "Epoch: 1872|steps:   60|Train Avg Loss: 0.8867 |Test Loss: 0.8194|lr = 0.00034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1873|steps:   30|Train Avg Loss: 0.8736 |Test Loss: 0.8187|lr = 0.00034\n",
      "Epoch: 1873|steps:   60|Train Avg Loss: 0.8587 |Test Loss: 0.8176|lr = 0.00034\n",
      "Epoch: 1874|steps:   30|Train Avg Loss: 0.8796 |Test Loss: 0.8175|lr = 0.00034\n",
      "Epoch: 1874|steps:   60|Train Avg Loss: 0.8439 |Test Loss: 0.8172|lr = 0.00034\n",
      "Epoch: 1875|steps:   30|Train Avg Loss: 0.8825 |Test Loss: 0.8195|lr = 0.00034\n",
      "Epoch: 1875|steps:   60|Train Avg Loss: 0.8570 |Test Loss: 0.8173|lr = 0.00034\n",
      "Epoch: 1876|steps:   30|Train Avg Loss: 0.8377 |Test Loss: 0.8142|lr = 0.00034\n",
      "Epoch: 1876|steps:   60|Train Avg Loss: 0.9075 |Test Loss: 0.8189|lr = 0.00034\n",
      "Epoch: 1877|steps:   30|Train Avg Loss: 0.8789 |Test Loss: 0.8198|lr = 0.00034\n",
      "Epoch: 1877|steps:   60|Train Avg Loss: 0.8545 |Test Loss: 0.8160|lr = 0.00034\n",
      "Epoch: 1878|steps:   30|Train Avg Loss: 0.8609 |Test Loss: 0.8165|lr = 0.00034\n",
      "Epoch: 1878|steps:   60|Train Avg Loss: 0.8665 |Test Loss: 0.8174|lr = 0.00034\n",
      "Epoch: 1879|steps:   30|Train Avg Loss: 0.8588 |Test Loss: 0.8183|lr = 0.00034\n",
      "Epoch: 1879|steps:   60|Train Avg Loss: 0.8672 |Test Loss: 0.8189|lr = 0.00034\n",
      "Epoch: 1880|steps:   30|Train Avg Loss: 0.8738 |Test Loss: 0.8174|lr = 0.00034\n",
      "Epoch: 1880|steps:   60|Train Avg Loss: 0.8507 |Test Loss: 0.8177|lr = 0.00034\n",
      "Epoch: 1881|steps:   30|Train Avg Loss: 0.8686 |Test Loss: 0.8190|lr = 0.00034\n",
      "Epoch: 1881|steps:   60|Train Avg Loss: 0.8599 |Test Loss: 0.8178|lr = 0.00034\n",
      "Epoch: 1882|steps:   30|Train Avg Loss: 0.8685 |Test Loss: 0.8197|lr = 0.00034\n",
      "Epoch: 1882|steps:   60|Train Avg Loss: 0.8667 |Test Loss: 0.8177|lr = 0.00034\n",
      "Epoch: 1883|steps:   30|Train Avg Loss: 0.8639 |Test Loss: 0.8196|lr = 0.00034\n",
      "Epoch: 1883|steps:   60|Train Avg Loss: 0.8723 |Test Loss: 0.8182|lr = 0.00034\n",
      "Epoch: 1884|steps:   30|Train Avg Loss: 0.8575 |Test Loss: 0.8168|lr = 0.00034\n",
      "Epoch: 1884|steps:   60|Train Avg Loss: 0.8710 |Test Loss: 0.8181|lr = 0.00034\n",
      "Epoch: 1885|steps:   30|Train Avg Loss: 0.8472 |Test Loss: 0.8189|lr = 0.00034\n",
      "Epoch: 1885|steps:   60|Train Avg Loss: 0.8864 |Test Loss: 0.8189|lr = 0.00034\n",
      "Epoch: 1886|steps:   30|Train Avg Loss: 0.8618 |Test Loss: 0.8189|lr = 0.00034\n",
      "Epoch: 1886|steps:   60|Train Avg Loss: 0.8585 |Test Loss: 0.8179|lr = 0.00034\n",
      "Epoch: 1887|steps:   30|Train Avg Loss: 0.8833 |Test Loss: 0.8195|lr = 0.00034\n",
      "Epoch: 1887|steps:   60|Train Avg Loss: 0.8394 |Test Loss: 0.8177|lr = 0.00034\n",
      "Epoch: 1888|steps:   30|Train Avg Loss: 0.8514 |Test Loss: 0.8174|lr = 0.00034\n",
      "Epoch: 1888|steps:   60|Train Avg Loss: 0.8747 |Test Loss: 0.8188|lr = 0.00034\n",
      "Epoch: 1889|steps:   30|Train Avg Loss: 0.8756 |Test Loss: 0.8207|lr = 0.00034\n",
      "Epoch: 1889|steps:   60|Train Avg Loss: 0.8545 |Test Loss: 0.8188|lr = 0.00034\n",
      "Epoch: 1890|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8188|lr = 0.00034\n",
      "Epoch: 1890|steps:   60|Train Avg Loss: 0.8648 |Test Loss: 0.8173|lr = 0.00034\n",
      "Epoch: 1891|steps:   30|Train Avg Loss: 0.8578 |Test Loss: 0.8181|lr = 0.00034\n",
      "Epoch: 1891|steps:   60|Train Avg Loss: 0.8778 |Test Loss: 0.8192|lr = 0.00034\n",
      "Epoch: 1892|steps:   30|Train Avg Loss: 0.8443 |Test Loss: 0.8168|lr = 0.00034\n",
      "Epoch: 1892|steps:   60|Train Avg Loss: 0.8663 |Test Loss: 0.8158|lr = 0.00034\n",
      "Epoch: 1893|steps:   30|Train Avg Loss: 0.8771 |Test Loss: 0.8220|lr = 0.00034\n",
      "Epoch: 1893|steps:   60|Train Avg Loss: 0.8552 |Test Loss: 0.8177|lr = 0.00034\n",
      "Epoch: 1894|steps:   30|Train Avg Loss: 0.8582 |Test Loss: 0.8172|lr = 0.00034\n",
      "Epoch: 1894|steps:   60|Train Avg Loss: 0.8602 |Test Loss: 0.8180|lr = 0.00034\n",
      "Epoch: 1895|steps:   30|Train Avg Loss: 0.8670 |Test Loss: 0.8203|lr = 0.00034\n",
      "Epoch: 1895|steps:   60|Train Avg Loss: 0.8664 |Test Loss: 0.8182|lr = 0.00034\n",
      "Epoch: 1896|steps:   30|Train Avg Loss: 0.8775 |Test Loss: 0.8197|lr = 0.00034\n",
      "Epoch: 1896|steps:   60|Train Avg Loss: 0.8639 |Test Loss: 0.8199|lr = 0.00034\n",
      "Epoch: 1897|steps:   30|Train Avg Loss: 0.8695 |Test Loss: 0.8196|lr = 0.00034\n",
      "Epoch: 1897|steps:   60|Train Avg Loss: 0.8597 |Test Loss: 0.8183|lr = 0.00034\n",
      "Epoch: 1898|steps:   30|Train Avg Loss: 0.8778 |Test Loss: 0.8194|lr = 0.00034\n",
      "Epoch: 1898|steps:   60|Train Avg Loss: 0.8622 |Test Loss: 0.8188|lr = 0.00034\n",
      "Epoch: 1899|steps:   30|Train Avg Loss: 0.8443 |Test Loss: 0.8167|lr = 0.00034\n",
      "Epoch: 1899|steps:   60|Train Avg Loss: 0.8847 |Test Loss: 0.8177|lr = 0.00034\n",
      "Epoch: 1900|steps:   30|Train Avg Loss: 0.8522 |Test Loss: 0.8207|lr = 0.00033\n",
      "Epoch: 1900|steps:   60|Train Avg Loss: 0.8747 |Test Loss: 0.8206|lr = 0.00033\n",
      "Epoch: 1901|steps:   30|Train Avg Loss: 0.8682 |Test Loss: 0.8209|lr = 0.00033\n",
      "Epoch: 1901|steps:   60|Train Avg Loss: 0.8616 |Test Loss: 0.8175|lr = 0.00033\n",
      "Epoch: 1902|steps:   30|Train Avg Loss: 0.8727 |Test Loss: 0.8193|lr = 0.00033\n",
      "Epoch: 1902|steps:   60|Train Avg Loss: 0.8659 |Test Loss: 0.8186|lr = 0.00033\n",
      "Epoch: 1903|steps:   30|Train Avg Loss: 0.8644 |Test Loss: 0.8199|lr = 0.00033\n",
      "Epoch: 1903|steps:   60|Train Avg Loss: 0.8770 |Test Loss: 0.8199|lr = 0.00033\n",
      "Epoch: 1904|steps:   30|Train Avg Loss: 0.8762 |Test Loss: 0.8203|lr = 0.00033\n",
      "Epoch: 1904|steps:   60|Train Avg Loss: 0.8687 |Test Loss: 0.8204|lr = 0.00033\n",
      "Epoch: 1905|steps:   30|Train Avg Loss: 0.8744 |Test Loss: 0.8201|lr = 0.00033\n",
      "Epoch: 1905|steps:   60|Train Avg Loss: 0.8560 |Test Loss: 0.8178|lr = 0.00033\n",
      "Epoch: 1906|steps:   30|Train Avg Loss: 0.8700 |Test Loss: 0.8189|lr = 0.00033\n",
      "Epoch: 1906|steps:   60|Train Avg Loss: 0.8721 |Test Loss: 0.8209|lr = 0.00033\n",
      "Epoch: 1907|steps:   30|Train Avg Loss: 0.8762 |Test Loss: 0.8213|lr = 0.00033\n",
      "Epoch: 1907|steps:   60|Train Avg Loss: 0.8650 |Test Loss: 0.8213|lr = 0.00033\n",
      "Epoch: 1908|steps:   30|Train Avg Loss: 0.8720 |Test Loss: 0.8191|lr = 0.00033\n",
      "Epoch: 1908|steps:   60|Train Avg Loss: 0.8716 |Test Loss: 0.8182|lr = 0.00033\n",
      "Epoch: 1909|steps:   30|Train Avg Loss: 0.8472 |Test Loss: 0.8188|lr = 0.00033\n",
      "Epoch: 1909|steps:   60|Train Avg Loss: 0.8720 |Test Loss: 0.8183|lr = 0.00033\n",
      "Epoch: 1910|steps:   30|Train Avg Loss: 0.8568 |Test Loss: 0.8207|lr = 0.00033\n",
      "Epoch: 1910|steps:   60|Train Avg Loss: 0.8624 |Test Loss: 0.8221|lr = 0.00033\n",
      "Epoch: 1911|steps:   30|Train Avg Loss: 0.8813 |Test Loss: 0.8213|lr = 0.00032\n",
      "Epoch: 1911|steps:   60|Train Avg Loss: 0.8535 |Test Loss: 0.8196|lr = 0.00032\n",
      "Epoch: 1912|steps:   30|Train Avg Loss: 0.8687 |Test Loss: 0.8181|lr = 0.00032\n",
      "Epoch: 1912|steps:   60|Train Avg Loss: 0.8660 |Test Loss: 0.8191|lr = 0.00032\n",
      "Epoch: 1913|steps:   30|Train Avg Loss: 0.8537 |Test Loss: 0.8173|lr = 0.00032\n",
      "Epoch: 1913|steps:   60|Train Avg Loss: 0.8677 |Test Loss: 0.8210|lr = 0.00032\n",
      "Epoch: 1914|steps:   30|Train Avg Loss: 0.8845 |Test Loss: 0.8257|lr = 0.00032\n",
      "Epoch: 1914|steps:   60|Train Avg Loss: 0.8542 |Test Loss: 0.8221|lr = 0.00032\n",
      "Epoch: 1915|steps:   30|Train Avg Loss: 0.8694 |Test Loss: 0.8169|lr = 0.00032\n",
      "Epoch: 1915|steps:   60|Train Avg Loss: 0.8604 |Test Loss: 0.8196|lr = 0.00032\n",
      "Epoch: 1916|steps:   30|Train Avg Loss: 0.8766 |Test Loss: 0.8203|lr = 0.00032\n",
      "Epoch: 1916|steps:   60|Train Avg Loss: 0.8558 |Test Loss: 0.8210|lr = 0.00032\n",
      "Epoch: 1917|steps:   30|Train Avg Loss: 0.8551 |Test Loss: 0.8159|lr = 0.00032\n",
      "Epoch: 1917|steps:   60|Train Avg Loss: 0.8798 |Test Loss: 0.8229|lr = 0.00032\n",
      "Epoch: 1918|steps:   30|Train Avg Loss: 0.8616 |Test Loss: 0.8213|lr = 0.00032\n",
      "Epoch: 1918|steps:   60|Train Avg Loss: 0.8610 |Test Loss: 0.8194|lr = 0.00032\n",
      "Epoch: 1919|steps:   30|Train Avg Loss: 0.8644 |Test Loss: 0.8192|lr = 0.00032\n",
      "Epoch: 1919|steps:   60|Train Avg Loss: 0.8665 |Test Loss: 0.8200|lr = 0.00032\n",
      "Epoch: 1920|steps:   30|Train Avg Loss: 0.8623 |Test Loss: 0.8190|lr = 0.00032\n",
      "Epoch: 1920|steps:   60|Train Avg Loss: 0.8608 |Test Loss: 0.8195|lr = 0.00032\n",
      "Epoch: 1921|steps:   30|Train Avg Loss: 0.8624 |Test Loss: 0.8207|lr = 0.00032\n",
      "Epoch: 1921|steps:   60|Train Avg Loss: 0.8612 |Test Loss: 0.8174|lr = 0.00032\n",
      "Epoch: 1922|steps:   30|Train Avg Loss: 0.8820 |Test Loss: 0.8231|lr = 0.00032\n",
      "Epoch: 1922|steps:   60|Train Avg Loss: 0.8443 |Test Loss: 0.8210|lr = 0.00032\n",
      "Epoch: 1923|steps:   30|Train Avg Loss: 0.8560 |Test Loss: 0.8194|lr = 0.00032\n",
      "Epoch: 1923|steps:   60|Train Avg Loss: 0.8720 |Test Loss: 0.8190|lr = 0.00032\n",
      "Epoch: 1924|steps:   30|Train Avg Loss: 0.8444 |Test Loss: 0.8175|lr = 0.00032\n",
      "Epoch: 1924|steps:   60|Train Avg Loss: 0.8757 |Test Loss: 0.8210|lr = 0.00032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1925|steps:   30|Train Avg Loss: 0.8557 |Test Loss: 0.8221|lr = 0.00032\n",
      "Epoch: 1925|steps:   60|Train Avg Loss: 0.8609 |Test Loss: 0.8196|lr = 0.00032\n",
      "Epoch: 1926|steps:   30|Train Avg Loss: 0.8634 |Test Loss: 0.8204|lr = 0.00032\n",
      "Epoch: 1926|steps:   60|Train Avg Loss: 0.8724 |Test Loss: 0.8219|lr = 0.00032\n",
      "Epoch: 1927|steps:   30|Train Avg Loss: 0.8758 |Test Loss: 0.8228|lr = 0.00032\n",
      "Epoch: 1927|steps:   60|Train Avg Loss: 0.8552 |Test Loss: 0.8218|lr = 0.00032\n",
      "Epoch: 1928|steps:   30|Train Avg Loss: 0.8708 |Test Loss: 0.8195|lr = 0.00032\n",
      "Epoch: 1928|steps:   60|Train Avg Loss: 0.8649 |Test Loss: 0.8206|lr = 0.00032\n",
      "Epoch: 1929|steps:   30|Train Avg Loss: 0.8452 |Test Loss: 0.8170|lr = 0.00032\n",
      "Epoch: 1929|steps:   60|Train Avg Loss: 0.8739 |Test Loss: 0.8194|lr = 0.00032\n",
      "Epoch: 1930|steps:   30|Train Avg Loss: 0.8677 |Test Loss: 0.8234|lr = 0.00032\n",
      "Epoch: 1930|steps:   60|Train Avg Loss: 0.8683 |Test Loss: 0.8220|lr = 0.00032\n",
      "Epoch: 1931|steps:   30|Train Avg Loss: 0.8526 |Test Loss: 0.8173|lr = 0.00032\n",
      "Epoch: 1931|steps:   60|Train Avg Loss: 0.8777 |Test Loss: 0.8204|lr = 0.00032\n",
      "Epoch: 1932|steps:   30|Train Avg Loss: 0.8699 |Test Loss: 0.8226|lr = 0.00032\n",
      "Epoch: 1932|steps:   60|Train Avg Loss: 0.8629 |Test Loss: 0.8224|lr = 0.00032\n",
      "Epoch: 1933|steps:   30|Train Avg Loss: 0.8815 |Test Loss: 0.8237|lr = 0.00031\n",
      "Epoch: 1933|steps:   60|Train Avg Loss: 0.8532 |Test Loss: 0.8197|lr = 0.00031\n",
      "Epoch: 1934|steps:   30|Train Avg Loss: 0.8655 |Test Loss: 0.8191|lr = 0.00031\n",
      "Epoch: 1934|steps:   60|Train Avg Loss: 0.8515 |Test Loss: 0.8202|lr = 0.00031\n",
      "Epoch: 1935|steps:   30|Train Avg Loss: 0.8819 |Test Loss: 0.8230|lr = 0.00031\n",
      "Epoch: 1935|steps:   60|Train Avg Loss: 0.8640 |Test Loss: 0.8236|lr = 0.00031\n",
      "Epoch: 1936|steps:   30|Train Avg Loss: 0.8451 |Test Loss: 0.8161|lr = 0.00031\n",
      "Epoch: 1936|steps:   60|Train Avg Loss: 0.8660 |Test Loss: 0.8199|lr = 0.00031\n",
      "Epoch: 1937|steps:   30|Train Avg Loss: 0.8489 |Test Loss: 0.8210|lr = 0.00031\n",
      "Epoch: 1937|steps:   60|Train Avg Loss: 0.8865 |Test Loss: 0.8214|lr = 0.00031\n",
      "Epoch: 1938|steps:   30|Train Avg Loss: 0.8718 |Test Loss: 0.8225|lr = 0.00031\n",
      "Epoch: 1938|steps:   60|Train Avg Loss: 0.8642 |Test Loss: 0.8225|lr = 0.00031\n",
      "Epoch: 1939|steps:   30|Train Avg Loss: 0.8560 |Test Loss: 0.8192|lr = 0.00031\n",
      "Epoch: 1939|steps:   60|Train Avg Loss: 0.8689 |Test Loss: 0.8201|lr = 0.00031\n",
      "Epoch: 1940|steps:   30|Train Avg Loss: 0.8729 |Test Loss: 0.8219|lr = 0.00031\n",
      "Epoch: 1940|steps:   60|Train Avg Loss: 0.8639 |Test Loss: 0.8213|lr = 0.00031\n",
      "Epoch: 1941|steps:   30|Train Avg Loss: 0.8513 |Test Loss: 0.8201|lr = 0.00031\n",
      "Epoch: 1941|steps:   60|Train Avg Loss: 0.8823 |Test Loss: 0.8200|lr = 0.00031\n",
      "Epoch: 1942|steps:   30|Train Avg Loss: 0.8527 |Test Loss: 0.8214|lr = 0.00031\n",
      "Epoch: 1942|steps:   60|Train Avg Loss: 0.8733 |Test Loss: 0.8206|lr = 0.00031\n",
      "Epoch: 1943|steps:   30|Train Avg Loss: 0.8553 |Test Loss: 0.8193|lr = 0.00031\n",
      "Epoch: 1943|steps:   60|Train Avg Loss: 0.8712 |Test Loss: 0.8205|lr = 0.00031\n",
      "Epoch: 1944|steps:   30|Train Avg Loss: 0.8587 |Test Loss: 0.8197|lr = 0.00030\n",
      "Epoch: 1944|steps:   60|Train Avg Loss: 0.8641 |Test Loss: 0.8202|lr = 0.00030\n",
      "Epoch: 1945|steps:   30|Train Avg Loss: 0.8545 |Test Loss: 0.8212|lr = 0.00030\n",
      "Epoch: 1945|steps:   60|Train Avg Loss: 0.8728 |Test Loss: 0.8231|lr = 0.00030\n",
      "Epoch: 1946|steps:   30|Train Avg Loss: 0.8832 |Test Loss: 0.8256|lr = 0.00030\n",
      "Epoch: 1946|steps:   60|Train Avg Loss: 0.8487 |Test Loss: 0.8220|lr = 0.00030\n",
      "Epoch: 1947|steps:   30|Train Avg Loss: 0.8456 |Test Loss: 0.8186|lr = 0.00030\n",
      "Epoch: 1947|steps:   60|Train Avg Loss: 0.8847 |Test Loss: 0.8209|lr = 0.00030\n",
      "Epoch: 1948|steps:   30|Train Avg Loss: 0.8487 |Test Loss: 0.8205|lr = 0.00030\n",
      "Epoch: 1948|steps:   60|Train Avg Loss: 0.8628 |Test Loss: 0.8212|lr = 0.00030\n",
      "Epoch: 1949|steps:   30|Train Avg Loss: 0.8642 |Test Loss: 0.8225|lr = 0.00030\n",
      "Epoch: 1949|steps:   60|Train Avg Loss: 0.8715 |Test Loss: 0.8235|lr = 0.00030\n",
      "Epoch: 1950|steps:   30|Train Avg Loss: 0.8487 |Test Loss: 0.8214|lr = 0.00030\n",
      "Epoch: 1950|steps:   60|Train Avg Loss: 0.8711 |Test Loss: 0.8205|lr = 0.00030\n",
      "Epoch: 1951|steps:   30|Train Avg Loss: 0.8515 |Test Loss: 0.8214|lr = 0.00030\n",
      "Epoch: 1951|steps:   60|Train Avg Loss: 0.8660 |Test Loss: 0.8223|lr = 0.00030\n",
      "Epoch: 1952|steps:   30|Train Avg Loss: 0.8604 |Test Loss: 0.8215|lr = 0.00030\n",
      "Epoch: 1952|steps:   60|Train Avg Loss: 0.8671 |Test Loss: 0.8231|lr = 0.00030\n",
      "Epoch: 1953|steps:   30|Train Avg Loss: 0.8674 |Test Loss: 0.8218|lr = 0.00030\n",
      "Epoch: 1953|steps:   60|Train Avg Loss: 0.8627 |Test Loss: 0.8242|lr = 0.00030\n",
      "Epoch: 1954|steps:   30|Train Avg Loss: 0.8797 |Test Loss: 0.8222|lr = 0.00030\n",
      "Epoch: 1954|steps:   60|Train Avg Loss: 0.8534 |Test Loss: 0.8226|lr = 0.00030\n",
      "Epoch: 1955|steps:   30|Train Avg Loss: 0.8776 |Test Loss: 0.8207|lr = 0.00030\n",
      "Epoch: 1955|steps:   60|Train Avg Loss: 0.8344 |Test Loss: 0.8197|lr = 0.00030\n",
      "Epoch: 1956|steps:   30|Train Avg Loss: 0.8635 |Test Loss: 0.8247|lr = 0.00030\n",
      "Epoch: 1956|steps:   60|Train Avg Loss: 0.8697 |Test Loss: 0.8247|lr = 0.00030\n",
      "Epoch: 1957|steps:   30|Train Avg Loss: 0.8428 |Test Loss: 0.8207|lr = 0.00030\n",
      "Epoch: 1957|steps:   60|Train Avg Loss: 0.8862 |Test Loss: 0.8230|lr = 0.00030\n",
      "Epoch: 1958|steps:   30|Train Avg Loss: 0.8600 |Test Loss: 0.8232|lr = 0.00030\n",
      "Epoch: 1958|steps:   60|Train Avg Loss: 0.8694 |Test Loss: 0.8226|lr = 0.00030\n",
      "Epoch: 1959|steps:   30|Train Avg Loss: 0.8676 |Test Loss: 0.8230|lr = 0.00030\n",
      "Epoch: 1959|steps:   60|Train Avg Loss: 0.8520 |Test Loss: 0.8224|lr = 0.00030\n",
      "Epoch: 1960|steps:   30|Train Avg Loss: 0.8654 |Test Loss: 0.8219|lr = 0.00030\n",
      "Epoch: 1960|steps:   60|Train Avg Loss: 0.8565 |Test Loss: 0.8226|lr = 0.00030\n",
      "Epoch: 1961|steps:   30|Train Avg Loss: 0.8473 |Test Loss: 0.8193|lr = 0.00030\n",
      "Epoch: 1961|steps:   60|Train Avg Loss: 0.8602 |Test Loss: 0.8201|lr = 0.00030\n",
      "Epoch: 1962|steps:   30|Train Avg Loss: 0.8281 |Test Loss: 0.8211|lr = 0.00030\n",
      "Epoch: 1962|steps:   60|Train Avg Loss: 0.8913 |Test Loss: 0.8228|lr = 0.00030\n",
      "Epoch: 1963|steps:   30|Train Avg Loss: 0.8674 |Test Loss: 0.8234|lr = 0.00030\n",
      "Epoch: 1963|steps:   60|Train Avg Loss: 0.8557 |Test Loss: 0.8227|lr = 0.00030\n",
      "Epoch: 1964|steps:   30|Train Avg Loss: 0.8532 |Test Loss: 0.8238|lr = 0.00030\n",
      "Epoch: 1964|steps:   60|Train Avg Loss: 0.8657 |Test Loss: 0.8228|lr = 0.00030\n",
      "Epoch: 1965|steps:   30|Train Avg Loss: 0.8597 |Test Loss: 0.8216|lr = 0.00030\n",
      "Epoch: 1965|steps:   60|Train Avg Loss: 0.8622 |Test Loss: 0.8229|lr = 0.00030\n",
      "Epoch: 1966|steps:   30|Train Avg Loss: 0.8796 |Test Loss: 0.8256|lr = 0.00029\n",
      "Epoch: 1966|steps:   60|Train Avg Loss: 0.8482 |Test Loss: 0.8212|lr = 0.00029\n",
      "Epoch: 1967|steps:   30|Train Avg Loss: 0.8756 |Test Loss: 0.8213|lr = 0.00029\n",
      "Epoch: 1967|steps:   60|Train Avg Loss: 0.8522 |Test Loss: 0.8225|lr = 0.00029\n",
      "Epoch: 1968|steps:   30|Train Avg Loss: 0.8597 |Test Loss: 0.8220|lr = 0.00029\n",
      "Epoch: 1968|steps:   60|Train Avg Loss: 0.8591 |Test Loss: 0.8247|lr = 0.00029\n",
      "Epoch: 1969|steps:   30|Train Avg Loss: 0.8499 |Test Loss: 0.8242|lr = 0.00029\n",
      "Epoch: 1969|steps:   60|Train Avg Loss: 0.8720 |Test Loss: 0.8232|lr = 0.00029\n",
      "Epoch: 1970|steps:   30|Train Avg Loss: 0.8820 |Test Loss: 0.8236|lr = 0.00029\n",
      "Epoch: 1970|steps:   60|Train Avg Loss: 0.8473 |Test Loss: 0.8221|lr = 0.00029\n",
      "Epoch: 1971|steps:   30|Train Avg Loss: 0.8533 |Test Loss: 0.8192|lr = 0.00029\n",
      "Epoch: 1971|steps:   60|Train Avg Loss: 0.8608 |Test Loss: 0.8212|lr = 0.00029\n",
      "Epoch: 1972|steps:   30|Train Avg Loss: 0.8509 |Test Loss: 0.8230|lr = 0.00029\n",
      "Epoch: 1972|steps:   60|Train Avg Loss: 0.8728 |Test Loss: 0.8245|lr = 0.00029\n",
      "Epoch: 1973|steps:   30|Train Avg Loss: 0.8408 |Test Loss: 0.8221|lr = 0.00029\n",
      "Epoch: 1973|steps:   60|Train Avg Loss: 0.8685 |Test Loss: 0.8212|lr = 0.00029\n",
      "Epoch: 1974|steps:   30|Train Avg Loss: 0.8559 |Test Loss: 0.8234|lr = 0.00029\n",
      "Epoch: 1974|steps:   60|Train Avg Loss: 0.8740 |Test Loss: 0.8238|lr = 0.00029\n",
      "Epoch: 1975|steps:   30|Train Avg Loss: 0.8499 |Test Loss: 0.8226|lr = 0.00029\n",
      "Epoch: 1975|steps:   60|Train Avg Loss: 0.8786 |Test Loss: 0.8243|lr = 0.00029\n",
      "Epoch: 1976|steps:   30|Train Avg Loss: 0.8569 |Test Loss: 0.8225|lr = 0.00029\n",
      "Epoch: 1976|steps:   60|Train Avg Loss: 0.8715 |Test Loss: 0.8244|lr = 0.00029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1977|steps:   30|Train Avg Loss: 0.8280 |Test Loss: 0.8213|lr = 0.00029\n",
      "Epoch: 1977|steps:   60|Train Avg Loss: 0.8810 |Test Loss: 0.8234|lr = 0.00029\n",
      "Epoch: 1978|steps:   30|Train Avg Loss: 0.8611 |Test Loss: 0.8264|lr = 0.00029\n",
      "Epoch: 1978|steps:   60|Train Avg Loss: 0.8557 |Test Loss: 0.8220|lr = 0.00029\n",
      "Epoch: 1979|steps:   30|Train Avg Loss: 0.8620 |Test Loss: 0.8237|lr = 0.00029\n",
      "Epoch: 1979|steps:   60|Train Avg Loss: 0.8730 |Test Loss: 0.8259|lr = 0.00029\n",
      "Epoch: 1980|steps:   30|Train Avg Loss: 0.8443 |Test Loss: 0.8206|lr = 0.00029\n",
      "Epoch: 1980|steps:   60|Train Avg Loss: 0.8754 |Test Loss: 0.8239|lr = 0.00029\n",
      "Epoch: 1981|steps:   30|Train Avg Loss: 0.8694 |Test Loss: 0.8226|lr = 0.00029\n",
      "Epoch: 1981|steps:   60|Train Avg Loss: 0.8529 |Test Loss: 0.8246|lr = 0.00029\n",
      "Epoch: 1982|steps:   30|Train Avg Loss: 0.8784 |Test Loss: 0.8250|lr = 0.00029\n",
      "Epoch: 1982|steps:   60|Train Avg Loss: 0.8473 |Test Loss: 0.8238|lr = 0.00029\n",
      "Epoch: 1983|steps:   30|Train Avg Loss: 0.8348 |Test Loss: 0.8214|lr = 0.00029\n",
      "Epoch: 1983|steps:   60|Train Avg Loss: 0.8699 |Test Loss: 0.8231|lr = 0.00029\n",
      "Epoch: 1984|steps:   30|Train Avg Loss: 0.8423 |Test Loss: 0.8217|lr = 0.00029\n",
      "Epoch: 1984|steps:   60|Train Avg Loss: 0.8785 |Test Loss: 0.8254|lr = 0.00029\n",
      "Epoch: 1985|steps:   30|Train Avg Loss: 0.8572 |Test Loss: 0.8236|lr = 0.00029\n",
      "Epoch: 1985|steps:   60|Train Avg Loss: 0.8703 |Test Loss: 0.8258|lr = 0.00029\n",
      "Epoch: 1986|steps:   30|Train Avg Loss: 0.8614 |Test Loss: 0.8264|lr = 0.00029\n",
      "Epoch: 1986|steps:   60|Train Avg Loss: 0.8570 |Test Loss: 0.8255|lr = 0.00029\n",
      "Epoch: 1987|steps:   30|Train Avg Loss: 0.8704 |Test Loss: 0.8241|lr = 0.00029\n",
      "Epoch: 1987|steps:   60|Train Avg Loss: 0.8527 |Test Loss: 0.8233|lr = 0.00029\n",
      "Epoch: 1988|steps:   30|Train Avg Loss: 0.8583 |Test Loss: 0.8244|lr = 0.00028\n",
      "Epoch: 1988|steps:   60|Train Avg Loss: 0.8751 |Test Loss: 0.8240|lr = 0.00028\n",
      "Epoch: 1989|steps:   30|Train Avg Loss: 0.8623 |Test Loss: 0.8256|lr = 0.00028\n",
      "Epoch: 1989|steps:   60|Train Avg Loss: 0.8739 |Test Loss: 0.8251|lr = 0.00028\n",
      "Epoch: 1990|steps:   30|Train Avg Loss: 0.8513 |Test Loss: 0.8206|lr = 0.00028\n",
      "Epoch: 1990|steps:   60|Train Avg Loss: 0.8691 |Test Loss: 0.8256|lr = 0.00028\n",
      "Epoch: 1991|steps:   30|Train Avg Loss: 0.8534 |Test Loss: 0.8248|lr = 0.00028\n",
      "Epoch: 1991|steps:   60|Train Avg Loss: 0.8643 |Test Loss: 0.8222|lr = 0.00028\n",
      "Epoch: 1992|steps:   30|Train Avg Loss: 0.8630 |Test Loss: 0.8246|lr = 0.00028\n",
      "Epoch: 1992|steps:   60|Train Avg Loss: 0.8563 |Test Loss: 0.8229|lr = 0.00028\n",
      "Epoch: 1993|steps:   30|Train Avg Loss: 0.8446 |Test Loss: 0.8217|lr = 0.00028\n",
      "Epoch: 1993|steps:   60|Train Avg Loss: 0.8798 |Test Loss: 0.8251|lr = 0.00028\n",
      "Epoch: 1994|steps:   30|Train Avg Loss: 0.8751 |Test Loss: 0.8285|lr = 0.00028\n",
      "Epoch: 1994|steps:   60|Train Avg Loss: 0.8543 |Test Loss: 0.8261|lr = 0.00028\n",
      "Epoch: 1995|steps:   30|Train Avg Loss: 0.8875 |Test Loss: 0.8252|lr = 0.00028\n",
      "Epoch: 1995|steps:   60|Train Avg Loss: 0.8388 |Test Loss: 0.8253|lr = 0.00028\n",
      "Epoch: 1996|steps:   30|Train Avg Loss: 0.8640 |Test Loss: 0.8232|lr = 0.00028\n",
      "Epoch: 1996|steps:   60|Train Avg Loss: 0.8598 |Test Loss: 0.8229|lr = 0.00028\n",
      "Epoch: 1997|steps:   30|Train Avg Loss: 0.8547 |Test Loss: 0.8258|lr = 0.00028\n",
      "Epoch: 1997|steps:   60|Train Avg Loss: 0.8746 |Test Loss: 0.8256|lr = 0.00028\n",
      "Epoch: 1998|steps:   30|Train Avg Loss: 0.8580 |Test Loss: 0.8233|lr = 0.00028\n",
      "Epoch: 1998|steps:   60|Train Avg Loss: 0.8465 |Test Loss: 0.8226|lr = 0.00028\n",
      "Epoch: 1999|steps:   30|Train Avg Loss: 0.8567 |Test Loss: 0.8248|lr = 0.00027\n",
      "Epoch: 1999|steps:   60|Train Avg Loss: 0.8732 |Test Loss: 0.8250|lr = 0.00027\n",
      "Epoch: 2000|steps:   30|Train Avg Loss: 0.8610 |Test Loss: 0.8237|lr = 0.00027\n",
      "Epoch: 2000|steps:   60|Train Avg Loss: 0.8545 |Test Loss: 0.8240|lr = 0.00027\n",
      "Epoch: 2001|steps:   30|Train Avg Loss: 0.8517 |Test Loss: 0.8244|lr = 0.00027\n",
      "Epoch: 2001|steps:   60|Train Avg Loss: 0.8689 |Test Loss: 0.8280|lr = 0.00027\n",
      "Epoch: 2002|steps:   30|Train Avg Loss: 0.8564 |Test Loss: 0.8245|lr = 0.00027\n",
      "Epoch: 2002|steps:   60|Train Avg Loss: 0.8641 |Test Loss: 0.8240|lr = 0.00027\n",
      "Epoch: 2003|steps:   30|Train Avg Loss: 0.8456 |Test Loss: 0.8251|lr = 0.00027\n",
      "Epoch: 2003|steps:   60|Train Avg Loss: 0.8670 |Test Loss: 0.8244|lr = 0.00027\n",
      "Epoch: 2004|steps:   30|Train Avg Loss: 0.8672 |Test Loss: 0.8260|lr = 0.00027\n",
      "Epoch: 2004|steps:   60|Train Avg Loss: 0.8538 |Test Loss: 0.8241|lr = 0.00027\n",
      "Epoch: 2005|steps:   30|Train Avg Loss: 0.8403 |Test Loss: 0.8243|lr = 0.00027\n",
      "Epoch: 2005|steps:   60|Train Avg Loss: 0.8749 |Test Loss: 0.8258|lr = 0.00027\n",
      "Epoch: 2006|steps:   30|Train Avg Loss: 0.8521 |Test Loss: 0.8242|lr = 0.00027\n",
      "Epoch: 2006|steps:   60|Train Avg Loss: 0.8642 |Test Loss: 0.8249|lr = 0.00027\n",
      "Epoch: 2007|steps:   30|Train Avg Loss: 0.8714 |Test Loss: 0.8235|lr = 0.00027\n",
      "Epoch: 2007|steps:   60|Train Avg Loss: 0.8498 |Test Loss: 0.8250|lr = 0.00027\n",
      "Epoch: 2008|steps:   30|Train Avg Loss: 0.8357 |Test Loss: 0.8204|lr = 0.00027\n",
      "Epoch: 2008|steps:   60|Train Avg Loss: 0.8770 |Test Loss: 0.8257|lr = 0.00027\n",
      "Epoch: 2009|steps:   30|Train Avg Loss: 0.8542 |Test Loss: 0.8268|lr = 0.00027\n",
      "Epoch: 2009|steps:   60|Train Avg Loss: 0.8500 |Test Loss: 0.8229|lr = 0.00027\n",
      "Epoch: 2010|steps:   30|Train Avg Loss: 0.8807 |Test Loss: 0.8264|lr = 0.00027\n",
      "Epoch: 2010|steps:   60|Train Avg Loss: 0.8487 |Test Loss: 0.8269|lr = 0.00027\n",
      "Epoch: 2011|steps:   30|Train Avg Loss: 0.8478 |Test Loss: 0.8200|lr = 0.00027\n",
      "Epoch: 2011|steps:   60|Train Avg Loss: 0.8744 |Test Loss: 0.8254|lr = 0.00027\n",
      "Epoch: 2012|steps:   30|Train Avg Loss: 0.8783 |Test Loss: 0.8284|lr = 0.00027\n",
      "Epoch: 2012|steps:   60|Train Avg Loss: 0.8365 |Test Loss: 0.8255|lr = 0.00027\n",
      "Epoch: 2013|steps:   30|Train Avg Loss: 0.8667 |Test Loss: 0.8259|lr = 0.00027\n",
      "Epoch: 2013|steps:   60|Train Avg Loss: 0.8593 |Test Loss: 0.8267|lr = 0.00027\n",
      "Epoch: 2014|steps:   30|Train Avg Loss: 0.8573 |Test Loss: 0.8258|lr = 0.00027\n",
      "Epoch: 2014|steps:   60|Train Avg Loss: 0.8608 |Test Loss: 0.8248|lr = 0.00027\n",
      "Epoch: 2015|steps:   30|Train Avg Loss: 0.8633 |Test Loss: 0.8244|lr = 0.00027\n",
      "Epoch: 2015|steps:   60|Train Avg Loss: 0.8577 |Test Loss: 0.8242|lr = 0.00027\n",
      "Epoch: 2016|steps:   30|Train Avg Loss: 0.8259 |Test Loss: 0.8204|lr = 0.00027\n",
      "Epoch: 2016|steps:   60|Train Avg Loss: 0.8798 |Test Loss: 0.8259|lr = 0.00027\n",
      "Epoch: 2017|steps:   30|Train Avg Loss: 0.8871 |Test Loss: 0.8323|lr = 0.00027\n",
      "Epoch: 2017|steps:   60|Train Avg Loss: 0.8372 |Test Loss: 0.8295|lr = 0.00027\n",
      "Epoch: 2018|steps:   30|Train Avg Loss: 0.8594 |Test Loss: 0.8242|lr = 0.00027\n",
      "Epoch: 2018|steps:   60|Train Avg Loss: 0.8595 |Test Loss: 0.8257|lr = 0.00027\n",
      "Epoch: 2019|steps:   30|Train Avg Loss: 0.8620 |Test Loss: 0.8257|lr = 0.00027\n",
      "Epoch: 2019|steps:   60|Train Avg Loss: 0.8572 |Test Loss: 0.8263|lr = 0.00027\n",
      "Epoch: 2020|steps:   30|Train Avg Loss: 0.8636 |Test Loss: 0.8251|lr = 0.00027\n",
      "Epoch: 2020|steps:   60|Train Avg Loss: 0.8561 |Test Loss: 0.8251|lr = 0.00027\n",
      "Epoch: 2021|steps:   30|Train Avg Loss: 0.8431 |Test Loss: 0.8240|lr = 0.00026\n",
      "Epoch: 2021|steps:   60|Train Avg Loss: 0.8616 |Test Loss: 0.8256|lr = 0.00026\n",
      "Epoch: 2022|steps:   30|Train Avg Loss: 0.8574 |Test Loss: 0.8258|lr = 0.00026\n",
      "Epoch: 2022|steps:   60|Train Avg Loss: 0.8553 |Test Loss: 0.8252|lr = 0.00026\n",
      "Epoch: 2023|steps:   30|Train Avg Loss: 0.8573 |Test Loss: 0.8245|lr = 0.00026\n",
      "Epoch: 2023|steps:   60|Train Avg Loss: 0.8491 |Test Loss: 0.8253|lr = 0.00026\n",
      "Epoch: 2024|steps:   30|Train Avg Loss: 0.8577 |Test Loss: 0.8270|lr = 0.00026\n",
      "Epoch: 2024|steps:   60|Train Avg Loss: 0.8575 |Test Loss: 0.8262|lr = 0.00026\n",
      "Epoch: 2025|steps:   30|Train Avg Loss: 0.8537 |Test Loss: 0.8251|lr = 0.00026\n",
      "Epoch: 2025|steps:   60|Train Avg Loss: 0.8610 |Test Loss: 0.8250|lr = 0.00026\n",
      "Epoch: 2026|steps:   30|Train Avg Loss: 0.8429 |Test Loss: 0.8248|lr = 0.00026\n",
      "Epoch: 2026|steps:   60|Train Avg Loss: 0.8673 |Test Loss: 0.8273|lr = 0.00026\n",
      "Epoch: 2027|steps:   30|Train Avg Loss: 0.8468 |Test Loss: 0.8249|lr = 0.00026\n",
      "Epoch: 2027|steps:   60|Train Avg Loss: 0.8696 |Test Loss: 0.8278|lr = 0.00026\n",
      "Epoch: 2028|steps:   30|Train Avg Loss: 0.8463 |Test Loss: 0.8244|lr = 0.00026\n",
      "Epoch: 2028|steps:   60|Train Avg Loss: 0.8699 |Test Loss: 0.8262|lr = 0.00026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2029|steps:   30|Train Avg Loss: 0.8605 |Test Loss: 0.8314|lr = 0.00026\n",
      "Epoch: 2029|steps:   60|Train Avg Loss: 0.8514 |Test Loss: 0.8251|lr = 0.00026\n",
      "Epoch: 2030|steps:   30|Train Avg Loss: 0.8399 |Test Loss: 0.8253|lr = 0.00026\n",
      "Epoch: 2030|steps:   60|Train Avg Loss: 0.8765 |Test Loss: 0.8250|lr = 0.00026\n",
      "Epoch: 2031|steps:   30|Train Avg Loss: 0.8444 |Test Loss: 0.8238|lr = 0.00026\n",
      "Epoch: 2031|steps:   60|Train Avg Loss: 0.8549 |Test Loss: 0.8238|lr = 0.00026\n",
      "Epoch: 2032|steps:   30|Train Avg Loss: 0.8490 |Test Loss: 0.8266|lr = 0.00026\n",
      "Epoch: 2032|steps:   60|Train Avg Loss: 0.8631 |Test Loss: 0.8279|lr = 0.00026\n",
      "Epoch: 2033|steps:   30|Train Avg Loss: 0.8711 |Test Loss: 0.8291|lr = 0.00026\n",
      "Epoch: 2033|steps:   60|Train Avg Loss: 0.8468 |Test Loss: 0.8269|lr = 0.00026\n",
      "Epoch: 2034|steps:   30|Train Avg Loss: 0.8634 |Test Loss: 0.8263|lr = 0.00026\n",
      "Epoch: 2034|steps:   60|Train Avg Loss: 0.8615 |Test Loss: 0.8291|lr = 0.00026\n",
      "Epoch: 2035|steps:   30|Train Avg Loss: 0.8500 |Test Loss: 0.8262|lr = 0.00026\n",
      "Epoch: 2035|steps:   60|Train Avg Loss: 0.8732 |Test Loss: 0.8296|lr = 0.00026\n",
      "Epoch: 2036|steps:   30|Train Avg Loss: 0.8412 |Test Loss: 0.8246|lr = 0.00026\n",
      "Epoch: 2036|steps:   60|Train Avg Loss: 0.8604 |Test Loss: 0.8253|lr = 0.00026\n",
      "Epoch: 2037|steps:   30|Train Avg Loss: 0.8667 |Test Loss: 0.8288|lr = 0.00026\n",
      "Epoch: 2037|steps:   60|Train Avg Loss: 0.8499 |Test Loss: 0.8263|lr = 0.00026\n",
      "Epoch: 2038|steps:   30|Train Avg Loss: 0.8438 |Test Loss: 0.8248|lr = 0.00026\n",
      "Epoch: 2038|steps:   60|Train Avg Loss: 0.8808 |Test Loss: 0.8311|lr = 0.00026\n",
      "Epoch: 2039|steps:   30|Train Avg Loss: 0.8572 |Test Loss: 0.8272|lr = 0.00026\n",
      "Epoch: 2039|steps:   60|Train Avg Loss: 0.8544 |Test Loss: 0.8264|lr = 0.00026\n",
      "Epoch: 2040|steps:   30|Train Avg Loss: 0.8478 |Test Loss: 0.8268|lr = 0.00026\n",
      "Epoch: 2040|steps:   60|Train Avg Loss: 0.8446 |Test Loss: 0.8266|lr = 0.00026\n",
      "Epoch: 2041|steps:   30|Train Avg Loss: 0.8536 |Test Loss: 0.8302|lr = 0.00026\n",
      "Epoch: 2041|steps:   60|Train Avg Loss: 0.8572 |Test Loss: 0.8267|lr = 0.00026\n",
      "Epoch: 2042|steps:   30|Train Avg Loss: 0.8563 |Test Loss: 0.8241|lr = 0.00026\n",
      "Epoch: 2042|steps:   60|Train Avg Loss: 0.8664 |Test Loss: 0.8256|lr = 0.00026\n",
      "Epoch: 2043|steps:   30|Train Avg Loss: 0.8357 |Test Loss: 0.8299|lr = 0.00025\n",
      "Epoch: 2043|steps:   60|Train Avg Loss: 0.8869 |Test Loss: 0.8291|lr = 0.00025\n",
      "Epoch: 2044|steps:   30|Train Avg Loss: 0.8545 |Test Loss: 0.8252|lr = 0.00025\n",
      "Epoch: 2044|steps:   60|Train Avg Loss: 0.8498 |Test Loss: 0.8260|lr = 0.00025\n",
      "Epoch: 2045|steps:   30|Train Avg Loss: 0.8363 |Test Loss: 0.8214|lr = 0.00025\n",
      "Epoch: 2045|steps:   60|Train Avg Loss: 0.8887 |Test Loss: 0.8314|lr = 0.00025\n",
      "Epoch: 2046|steps:   30|Train Avg Loss: 0.8702 |Test Loss: 0.8301|lr = 0.00025\n",
      "Epoch: 2046|steps:   60|Train Avg Loss: 0.8401 |Test Loss: 0.8272|lr = 0.00025\n",
      "Epoch: 2047|steps:   30|Train Avg Loss: 0.8451 |Test Loss: 0.8275|lr = 0.00025\n",
      "Epoch: 2047|steps:   60|Train Avg Loss: 0.8615 |Test Loss: 0.8247|lr = 0.00025\n",
      "Epoch: 2048|steps:   30|Train Avg Loss: 0.8422 |Test Loss: 0.8241|lr = 0.00025\n",
      "Epoch: 2048|steps:   60|Train Avg Loss: 0.8669 |Test Loss: 0.8271|lr = 0.00025\n",
      "Epoch: 2049|steps:   30|Train Avg Loss: 0.8392 |Test Loss: 0.8250|lr = 0.00025\n",
      "Epoch: 2049|steps:   60|Train Avg Loss: 0.8749 |Test Loss: 0.8281|lr = 0.00025\n",
      "Epoch: 2050|steps:   30|Train Avg Loss: 0.8548 |Test Loss: 0.8252|lr = 0.00025\n",
      "Epoch: 2050|steps:   60|Train Avg Loss: 0.8529 |Test Loss: 0.8268|lr = 0.00025\n",
      "Epoch: 2051|steps:   30|Train Avg Loss: 0.8614 |Test Loss: 0.8275|lr = 0.00025\n",
      "Epoch: 2051|steps:   60|Train Avg Loss: 0.8526 |Test Loss: 0.8310|lr = 0.00025\n",
      "Epoch: 2052|steps:   30|Train Avg Loss: 0.8550 |Test Loss: 0.8291|lr = 0.00025\n",
      "Epoch: 2052|steps:   60|Train Avg Loss: 0.8538 |Test Loss: 0.8271|lr = 0.00025\n",
      "Epoch: 2053|steps:   30|Train Avg Loss: 0.8681 |Test Loss: 0.8269|lr = 0.00025\n",
      "Epoch: 2053|steps:   60|Train Avg Loss: 0.8496 |Test Loss: 0.8262|lr = 0.00025\n",
      "Epoch: 2054|steps:   30|Train Avg Loss: 0.8497 |Test Loss: 0.8228|lr = 0.00025\n",
      "Epoch: 2054|steps:   60|Train Avg Loss: 0.8670 |Test Loss: 0.8287|lr = 0.00025\n",
      "Epoch: 2055|steps:   30|Train Avg Loss: 0.8519 |Test Loss: 0.8252|lr = 0.00025\n",
      "Epoch: 2055|steps:   60|Train Avg Loss: 0.8684 |Test Loss: 0.8248|lr = 0.00025\n",
      "Epoch: 2056|steps:   30|Train Avg Loss: 0.8584 |Test Loss: 0.8280|lr = 0.00025\n",
      "Epoch: 2056|steps:   60|Train Avg Loss: 0.8557 |Test Loss: 0.8273|lr = 0.00025\n",
      "Epoch: 2057|steps:   30|Train Avg Loss: 0.8355 |Test Loss: 0.8239|lr = 0.00025\n",
      "Epoch: 2057|steps:   60|Train Avg Loss: 0.8672 |Test Loss: 0.8274|lr = 0.00025\n",
      "Epoch: 2058|steps:   30|Train Avg Loss: 0.8702 |Test Loss: 0.8322|lr = 0.00025\n",
      "Epoch: 2058|steps:   60|Train Avg Loss: 0.8443 |Test Loss: 0.8265|lr = 0.00025\n",
      "Epoch: 2059|steps:   30|Train Avg Loss: 0.8622 |Test Loss: 0.8288|lr = 0.00025\n",
      "Epoch: 2059|steps:   60|Train Avg Loss: 0.8520 |Test Loss: 0.8270|lr = 0.00025\n",
      "Epoch: 2060|steps:   30|Train Avg Loss: 0.8515 |Test Loss: 0.8272|lr = 0.00025\n",
      "Epoch: 2060|steps:   60|Train Avg Loss: 0.8599 |Test Loss: 0.8253|lr = 0.00025\n",
      "Epoch: 2061|steps:   30|Train Avg Loss: 0.8743 |Test Loss: 0.8331|lr = 0.00025\n",
      "Epoch: 2061|steps:   60|Train Avg Loss: 0.8263 |Test Loss: 0.8262|lr = 0.00025\n",
      "Epoch: 2062|steps:   30|Train Avg Loss: 0.8415 |Test Loss: 0.8230|lr = 0.00025\n",
      "Epoch: 2062|steps:   60|Train Avg Loss: 0.8695 |Test Loss: 0.8272|lr = 0.00025\n",
      "Epoch: 2063|steps:   30|Train Avg Loss: 0.8298 |Test Loss: 0.8248|lr = 0.00025\n",
      "Epoch: 2063|steps:   60|Train Avg Loss: 0.8820 |Test Loss: 0.8272|lr = 0.00025\n",
      "Epoch: 2064|steps:   30|Train Avg Loss: 0.8549 |Test Loss: 0.8310|lr = 0.00025\n",
      "Epoch: 2064|steps:   60|Train Avg Loss: 0.8608 |Test Loss: 0.8293|lr = 0.00025\n",
      "Epoch: 2065|steps:   30|Train Avg Loss: 0.8708 |Test Loss: 0.8290|lr = 0.00024\n",
      "Epoch: 2065|steps:   60|Train Avg Loss: 0.8435 |Test Loss: 0.8262|lr = 0.00024\n",
      "Epoch: 2066|steps:   30|Train Avg Loss: 0.8541 |Test Loss: 0.8277|lr = 0.00024\n",
      "Epoch: 2066|steps:   60|Train Avg Loss: 0.8519 |Test Loss: 0.8316|lr = 0.00024\n",
      "Epoch: 2067|steps:   30|Train Avg Loss: 0.8422 |Test Loss: 0.8283|lr = 0.00024\n",
      "Epoch: 2067|steps:   60|Train Avg Loss: 0.8470 |Test Loss: 0.8250|lr = 0.00024\n",
      "Epoch: 2068|steps:   30|Train Avg Loss: 0.8374 |Test Loss: 0.8247|lr = 0.00024\n",
      "Epoch: 2068|steps:   60|Train Avg Loss: 0.8668 |Test Loss: 0.8282|lr = 0.00024\n",
      "Epoch: 2069|steps:   30|Train Avg Loss: 0.8379 |Test Loss: 0.8298|lr = 0.00024\n",
      "Epoch: 2069|steps:   60|Train Avg Loss: 0.8570 |Test Loss: 0.8275|lr = 0.00024\n",
      "Epoch: 2070|steps:   30|Train Avg Loss: 0.8462 |Test Loss: 0.8264|lr = 0.00024\n",
      "Epoch: 2070|steps:   60|Train Avg Loss: 0.8564 |Test Loss: 0.8301|lr = 0.00024\n",
      "Epoch: 2071|steps:   30|Train Avg Loss: 0.8737 |Test Loss: 0.8290|lr = 0.00024\n",
      "Epoch: 2071|steps:   60|Train Avg Loss: 0.8255 |Test Loss: 0.8275|lr = 0.00024\n",
      "Epoch: 2072|steps:   30|Train Avg Loss: 0.8497 |Test Loss: 0.8285|lr = 0.00024\n",
      "Epoch: 2072|steps:   60|Train Avg Loss: 0.8568 |Test Loss: 0.8287|lr = 0.00024\n",
      "Epoch: 2073|steps:   30|Train Avg Loss: 0.8366 |Test Loss: 0.8269|lr = 0.00024\n",
      "Epoch: 2073|steps:   60|Train Avg Loss: 0.8651 |Test Loss: 0.8273|lr = 0.00024\n",
      "Epoch: 2074|steps:   30|Train Avg Loss: 0.8501 |Test Loss: 0.8310|lr = 0.00024\n",
      "Epoch: 2074|steps:   60|Train Avg Loss: 0.8410 |Test Loss: 0.8285|lr = 0.00024\n",
      "Epoch: 2075|steps:   30|Train Avg Loss: 0.8696 |Test Loss: 0.8333|lr = 0.00024\n",
      "Epoch: 2075|steps:   60|Train Avg Loss: 0.8517 |Test Loss: 0.8270|lr = 0.00024\n",
      "Epoch: 2076|steps:   30|Train Avg Loss: 0.8509 |Test Loss: 0.8284|lr = 0.00024\n",
      "Epoch: 2076|steps:   60|Train Avg Loss: 0.8474 |Test Loss: 0.8259|lr = 0.00024\n",
      "Epoch: 2077|steps:   30|Train Avg Loss: 0.8371 |Test Loss: 0.8264|lr = 0.00024\n",
      "Epoch: 2077|steps:   60|Train Avg Loss: 0.8443 |Test Loss: 0.8260|lr = 0.00024\n",
      "Epoch: 2078|steps:   30|Train Avg Loss: 0.8424 |Test Loss: 0.8266|lr = 0.00024\n",
      "Epoch: 2078|steps:   60|Train Avg Loss: 0.8626 |Test Loss: 0.8322|lr = 0.00024\n",
      "Epoch: 2079|steps:   30|Train Avg Loss: 0.8400 |Test Loss: 0.8272|lr = 0.00024\n",
      "Epoch: 2079|steps:   60|Train Avg Loss: 0.8635 |Test Loss: 0.8242|lr = 0.00024\n",
      "Epoch: 2080|steps:   30|Train Avg Loss: 0.8757 |Test Loss: 0.8342|lr = 0.00024\n",
      "Epoch: 2080|steps:   60|Train Avg Loss: 0.8367 |Test Loss: 0.8297|lr = 0.00024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2081|steps:   30|Train Avg Loss: 0.8613 |Test Loss: 0.8291|lr = 0.00024\n",
      "Epoch: 2081|steps:   60|Train Avg Loss: 0.8465 |Test Loss: 0.8281|lr = 0.00024\n",
      "Epoch: 2082|steps:   30|Train Avg Loss: 0.8726 |Test Loss: 0.8283|lr = 0.00024\n",
      "Epoch: 2082|steps:   60|Train Avg Loss: 0.8343 |Test Loss: 0.8297|lr = 0.00024\n",
      "Epoch: 2083|steps:   30|Train Avg Loss: 0.8613 |Test Loss: 0.8293|lr = 0.00024\n",
      "Epoch: 2083|steps:   60|Train Avg Loss: 0.8295 |Test Loss: 0.8291|lr = 0.00024\n",
      "Epoch: 2084|steps:   30|Train Avg Loss: 0.8736 |Test Loss: 0.8332|lr = 0.00024\n",
      "Epoch: 2084|steps:   60|Train Avg Loss: 0.8374 |Test Loss: 0.8292|lr = 0.00024\n",
      "Epoch: 2085|steps:   30|Train Avg Loss: 0.8574 |Test Loss: 0.8277|lr = 0.00024\n",
      "Epoch: 2085|steps:   60|Train Avg Loss: 0.8464 |Test Loss: 0.8276|lr = 0.00024\n",
      "Epoch: 2086|steps:   30|Train Avg Loss: 0.8446 |Test Loss: 0.8267|lr = 0.00024\n",
      "Epoch: 2086|steps:   60|Train Avg Loss: 0.8670 |Test Loss: 0.8296|lr = 0.00024\n",
      "Epoch: 2087|steps:   30|Train Avg Loss: 0.8545 |Test Loss: 0.8288|lr = 0.00023\n",
      "Epoch: 2087|steps:   60|Train Avg Loss: 0.8540 |Test Loss: 0.8293|lr = 0.00023\n",
      "Epoch: 2088|steps:   30|Train Avg Loss: 0.8419 |Test Loss: 0.8275|lr = 0.00023\n",
      "Epoch: 2088|steps:   60|Train Avg Loss: 0.8451 |Test Loss: 0.8322|lr = 0.00023\n",
      "Epoch: 2089|steps:   30|Train Avg Loss: 0.8527 |Test Loss: 0.8369|lr = 0.00023\n",
      "Epoch: 2089|steps:   60|Train Avg Loss: 0.8525 |Test Loss: 0.8278|lr = 0.00023\n",
      "Epoch: 2090|steps:   30|Train Avg Loss: 0.8443 |Test Loss: 0.8250|lr = 0.00023\n",
      "Epoch: 2090|steps:   60|Train Avg Loss: 0.8593 |Test Loss: 0.8292|lr = 0.00023\n",
      "Epoch: 2091|steps:   30|Train Avg Loss: 0.8632 |Test Loss: 0.8310|lr = 0.00023\n",
      "Epoch: 2091|steps:   60|Train Avg Loss: 0.8244 |Test Loss: 0.8297|lr = 0.00023\n",
      "Epoch: 2092|steps:   30|Train Avg Loss: 0.8602 |Test Loss: 0.8333|lr = 0.00023\n",
      "Epoch: 2092|steps:   60|Train Avg Loss: 0.8538 |Test Loss: 0.8306|lr = 0.00023\n",
      "Epoch: 2093|steps:   30|Train Avg Loss: 0.8574 |Test Loss: 0.8258|lr = 0.00023\n",
      "Epoch: 2093|steps:   60|Train Avg Loss: 0.8502 |Test Loss: 0.8289|lr = 0.00023\n",
      "Epoch: 2094|steps:   30|Train Avg Loss: 0.8393 |Test Loss: 0.8244|lr = 0.00023\n",
      "Epoch: 2094|steps:   60|Train Avg Loss: 0.8764 |Test Loss: 0.8292|lr = 0.00023\n",
      "Epoch: 2095|steps:   30|Train Avg Loss: 0.8503 |Test Loss: 0.8317|lr = 0.00023\n",
      "Epoch: 2095|steps:   60|Train Avg Loss: 0.8543 |Test Loss: 0.8316|lr = 0.00023\n",
      "Epoch: 2096|steps:   30|Train Avg Loss: 0.8526 |Test Loss: 0.8273|lr = 0.00023\n",
      "Epoch: 2096|steps:   60|Train Avg Loss: 0.8498 |Test Loss: 0.8285|lr = 0.00023\n",
      "Epoch: 2097|steps:   30|Train Avg Loss: 0.8652 |Test Loss: 0.8278|lr = 0.00023\n",
      "Epoch: 2097|steps:   60|Train Avg Loss: 0.8422 |Test Loss: 0.8323|lr = 0.00023\n",
      "Epoch: 2098|steps:   30|Train Avg Loss: 0.8748 |Test Loss: 0.8310|lr = 0.00023\n",
      "Epoch: 2098|steps:   60|Train Avg Loss: 0.8308 |Test Loss: 0.8312|lr = 0.00023\n",
      "Epoch: 2099|steps:   30|Train Avg Loss: 0.8556 |Test Loss: 0.8303|lr = 0.00023\n",
      "Epoch: 2099|steps:   60|Train Avg Loss: 0.8449 |Test Loss: 0.8280|lr = 0.00023\n",
      "Epoch: 2100|steps:   30|Train Avg Loss: 0.8607 |Test Loss: 0.8282|lr = 0.00023\n",
      "Epoch: 2100|steps:   60|Train Avg Loss: 0.8351 |Test Loss: 0.8271|lr = 0.00023\n",
      "Epoch: 2101|steps:   30|Train Avg Loss: 0.8476 |Test Loss: 0.8297|lr = 0.00023\n",
      "Epoch: 2101|steps:   60|Train Avg Loss: 0.8598 |Test Loss: 0.8295|lr = 0.00023\n",
      "Epoch: 2102|steps:   30|Train Avg Loss: 0.8628 |Test Loss: 0.8331|lr = 0.00023\n",
      "Epoch: 2102|steps:   60|Train Avg Loss: 0.8505 |Test Loss: 0.8296|lr = 0.00023\n",
      "Epoch: 2103|steps:   30|Train Avg Loss: 0.8436 |Test Loss: 0.8275|lr = 0.00023\n",
      "Epoch: 2103|steps:   60|Train Avg Loss: 0.8527 |Test Loss: 0.8286|lr = 0.00023\n",
      "Epoch: 2104|steps:   30|Train Avg Loss: 0.8401 |Test Loss: 0.8278|lr = 0.00023\n",
      "Epoch: 2104|steps:   60|Train Avg Loss: 0.8580 |Test Loss: 0.8291|lr = 0.00023\n",
      "Epoch: 2105|steps:   30|Train Avg Loss: 0.8505 |Test Loss: 0.8334|lr = 0.00023\n",
      "Epoch: 2105|steps:   60|Train Avg Loss: 0.8533 |Test Loss: 0.8304|lr = 0.00023\n",
      "Epoch: 2106|steps:   30|Train Avg Loss: 0.8527 |Test Loss: 0.8260|lr = 0.00023\n",
      "Epoch: 2106|steps:   60|Train Avg Loss: 0.8419 |Test Loss: 0.8301|lr = 0.00023\n",
      "Epoch: 2107|steps:   30|Train Avg Loss: 0.8604 |Test Loss: 0.8312|lr = 0.00023\n",
      "Epoch: 2107|steps:   60|Train Avg Loss: 0.8388 |Test Loss: 0.8288|lr = 0.00023\n",
      "Epoch: 2108|steps:   30|Train Avg Loss: 0.8497 |Test Loss: 0.8291|lr = 0.00023\n",
      "Epoch: 2108|steps:   60|Train Avg Loss: 0.8482 |Test Loss: 0.8301|lr = 0.00023\n",
      "Epoch: 2109|steps:   30|Train Avg Loss: 0.8485 |Test Loss: 0.8295|lr = 0.00023\n",
      "Epoch: 2109|steps:   60|Train Avg Loss: 0.8385 |Test Loss: 0.8246|lr = 0.00023\n",
      "Epoch: 2110|steps:   30|Train Avg Loss: 0.8348 |Test Loss: 0.8296|lr = 0.00023\n",
      "Epoch: 2110|steps:   60|Train Avg Loss: 0.8745 |Test Loss: 0.8311|lr = 0.00023\n",
      "Epoch: 2111|steps:   30|Train Avg Loss: 0.8244 |Test Loss: 0.8271|lr = 0.00023\n",
      "Epoch: 2111|steps:   60|Train Avg Loss: 0.8694 |Test Loss: 0.8316|lr = 0.00023\n",
      "Epoch: 2112|steps:   30|Train Avg Loss: 0.8488 |Test Loss: 0.8342|lr = 0.00023\n",
      "Epoch: 2112|steps:   60|Train Avg Loss: 0.8513 |Test Loss: 0.8291|lr = 0.00023\n",
      "Epoch: 2113|steps:   30|Train Avg Loss: 0.8582 |Test Loss: 0.8326|lr = 0.00023\n",
      "Epoch: 2113|steps:   60|Train Avg Loss: 0.8347 |Test Loss: 0.8274|lr = 0.00023\n",
      "Epoch: 2114|steps:   30|Train Avg Loss: 0.8408 |Test Loss: 0.8341|lr = 0.00023\n",
      "Epoch: 2114|steps:   60|Train Avg Loss: 0.8411 |Test Loss: 0.8288|lr = 0.00023\n",
      "Epoch: 2115|steps:   30|Train Avg Loss: 0.8550 |Test Loss: 0.8325|lr = 0.00023\n",
      "Epoch: 2115|steps:   60|Train Avg Loss: 0.8594 |Test Loss: 0.8303|lr = 0.00023\n",
      "Epoch: 2116|steps:   30|Train Avg Loss: 0.8654 |Test Loss: 0.8309|lr = 0.00023\n",
      "Epoch: 2116|steps:   60|Train Avg Loss: 0.8310 |Test Loss: 0.8308|lr = 0.00023\n",
      "Epoch: 2117|steps:   30|Train Avg Loss: 0.8307 |Test Loss: 0.8281|lr = 0.00022\n",
      "Epoch: 2117|steps:   60|Train Avg Loss: 0.8652 |Test Loss: 0.8372|lr = 0.00022\n",
      "Epoch: 2118|steps:   30|Train Avg Loss: 0.8323 |Test Loss: 0.8291|lr = 0.00022\n",
      "Epoch: 2118|steps:   60|Train Avg Loss: 0.8574 |Test Loss: 0.8308|lr = 0.00022\n",
      "Epoch: 2119|steps:   30|Train Avg Loss: 0.8664 |Test Loss: 0.8334|lr = 0.00022\n",
      "Epoch: 2119|steps:   60|Train Avg Loss: 0.8343 |Test Loss: 0.8316|lr = 0.00022\n",
      "Epoch: 2120|steps:   30|Train Avg Loss: 0.8458 |Test Loss: 0.8325|lr = 0.00022\n",
      "Epoch: 2120|steps:   60|Train Avg Loss: 0.8432 |Test Loss: 0.8294|lr = 0.00022\n",
      "Epoch: 2121|steps:   30|Train Avg Loss: 0.8409 |Test Loss: 0.8282|lr = 0.00022\n",
      "Epoch: 2121|steps:   60|Train Avg Loss: 0.8539 |Test Loss: 0.8310|lr = 0.00022\n",
      "Epoch: 2122|steps:   30|Train Avg Loss: 0.8373 |Test Loss: 0.8297|lr = 0.00022\n",
      "Epoch: 2122|steps:   60|Train Avg Loss: 0.8550 |Test Loss: 0.8325|lr = 0.00022\n",
      "Epoch: 2123|steps:   30|Train Avg Loss: 0.8612 |Test Loss: 0.8318|lr = 0.00022\n",
      "Epoch: 2123|steps:   60|Train Avg Loss: 0.8498 |Test Loss: 0.8324|lr = 0.00022\n",
      "Epoch: 2124|steps:   30|Train Avg Loss: 0.8535 |Test Loss: 0.8319|lr = 0.00022\n",
      "Epoch: 2124|steps:   60|Train Avg Loss: 0.8289 |Test Loss: 0.8283|lr = 0.00022\n",
      "Epoch: 2125|steps:   30|Train Avg Loss: 0.8673 |Test Loss: 0.8319|lr = 0.00022\n",
      "Epoch: 2125|steps:   60|Train Avg Loss: 0.8409 |Test Loss: 0.8302|lr = 0.00022\n",
      "Epoch: 2126|steps:   30|Train Avg Loss: 0.8526 |Test Loss: 0.8316|lr = 0.00022\n",
      "Epoch: 2126|steps:   60|Train Avg Loss: 0.8521 |Test Loss: 0.8304|lr = 0.00022\n",
      "Epoch: 2127|steps:   30|Train Avg Loss: 0.8453 |Test Loss: 0.8306|lr = 0.00022\n",
      "Epoch: 2127|steps:   60|Train Avg Loss: 0.8397 |Test Loss: 0.8283|lr = 0.00022\n",
      "Epoch: 2128|steps:   30|Train Avg Loss: 0.8350 |Test Loss: 0.8295|lr = 0.00022\n",
      "Epoch: 2128|steps:   60|Train Avg Loss: 0.8554 |Test Loss: 0.8314|lr = 0.00022\n",
      "Epoch: 2129|steps:   30|Train Avg Loss: 0.8545 |Test Loss: 0.8324|lr = 0.00022\n",
      "Epoch: 2129|steps:   60|Train Avg Loss: 0.8328 |Test Loss: 0.8313|lr = 0.00022\n",
      "Epoch: 2130|steps:   30|Train Avg Loss: 0.8240 |Test Loss: 0.8299|lr = 0.00022\n",
      "Epoch: 2130|steps:   60|Train Avg Loss: 0.8694 |Test Loss: 0.8312|lr = 0.00022\n",
      "Epoch: 2131|steps:   30|Train Avg Loss: 0.8241 |Test Loss: 0.8299|lr = 0.00022\n",
      "Epoch: 2131|steps:   60|Train Avg Loss: 0.8525 |Test Loss: 0.8322|lr = 0.00022\n",
      "Epoch: 2132|steps:   30|Train Avg Loss: 0.8356 |Test Loss: 0.8315|lr = 0.00022\n",
      "Epoch: 2132|steps:   60|Train Avg Loss: 0.8529 |Test Loss: 0.8316|lr = 0.00022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2133|steps:   30|Train Avg Loss: 0.8499 |Test Loss: 0.8333|lr = 0.00022\n",
      "Epoch: 2133|steps:   60|Train Avg Loss: 0.8577 |Test Loss: 0.8355|lr = 0.00022\n",
      "Epoch: 2134|steps:   30|Train Avg Loss: 0.8498 |Test Loss: 0.8332|lr = 0.00022\n",
      "Epoch: 2134|steps:   60|Train Avg Loss: 0.8494 |Test Loss: 0.8311|lr = 0.00022\n",
      "Epoch: 2135|steps:   30|Train Avg Loss: 0.8613 |Test Loss: 0.8322|lr = 0.00022\n",
      "Epoch: 2135|steps:   60|Train Avg Loss: 0.8349 |Test Loss: 0.8298|lr = 0.00022\n",
      "Epoch: 2136|steps:   30|Train Avg Loss: 0.8582 |Test Loss: 0.8330|lr = 0.00022\n",
      "Epoch: 2136|steps:   60|Train Avg Loss: 0.8398 |Test Loss: 0.8331|lr = 0.00022\n",
      "Epoch: 2137|steps:   30|Train Avg Loss: 0.8598 |Test Loss: 0.8294|lr = 0.00022\n",
      "Epoch: 2137|steps:   60|Train Avg Loss: 0.8377 |Test Loss: 0.8338|lr = 0.00022\n",
      "Epoch: 2138|steps:   30|Train Avg Loss: 0.8412 |Test Loss: 0.8314|lr = 0.00022\n",
      "Epoch: 2138|steps:   60|Train Avg Loss: 0.8623 |Test Loss: 0.8307|lr = 0.00022\n",
      "Epoch: 2139|steps:   30|Train Avg Loss: 0.8340 |Test Loss: 0.8294|lr = 0.00022\n",
      "Epoch: 2139|steps:   60|Train Avg Loss: 0.8630 |Test Loss: 0.8319|lr = 0.00022\n",
      "Epoch: 2140|steps:   30|Train Avg Loss: 0.8345 |Test Loss: 0.8301|lr = 0.00022\n",
      "Epoch: 2140|steps:   60|Train Avg Loss: 0.8578 |Test Loss: 0.8345|lr = 0.00022\n",
      "Epoch: 2141|steps:   30|Train Avg Loss: 0.8413 |Test Loss: 0.8369|lr = 0.00022\n",
      "Epoch: 2141|steps:   60|Train Avg Loss: 0.8356 |Test Loss: 0.8324|lr = 0.00022\n",
      "Epoch: 2142|steps:   30|Train Avg Loss: 0.8318 |Test Loss: 0.8308|lr = 0.00022\n",
      "Epoch: 2142|steps:   60|Train Avg Loss: 0.8548 |Test Loss: 0.8327|lr = 0.00022\n",
      "Epoch: 2143|steps:   30|Train Avg Loss: 0.8552 |Test Loss: 0.8275|lr = 0.00022\n",
      "Epoch: 2143|steps:   60|Train Avg Loss: 0.8421 |Test Loss: 0.8311|lr = 0.00022\n",
      "Epoch: 2144|steps:   30|Train Avg Loss: 0.8536 |Test Loss: 0.8325|lr = 0.00022\n",
      "Epoch: 2144|steps:   60|Train Avg Loss: 0.8416 |Test Loss: 0.8342|lr = 0.00022\n",
      "Epoch: 2145|steps:   30|Train Avg Loss: 0.8482 |Test Loss: 0.8312|lr = 0.00022\n",
      "Epoch: 2145|steps:   60|Train Avg Loss: 0.8442 |Test Loss: 0.8292|lr = 0.00022\n",
      "Epoch: 2146|steps:   30|Train Avg Loss: 0.8526 |Test Loss: 0.8329|lr = 0.00022\n",
      "Epoch: 2146|steps:   60|Train Avg Loss: 0.8336 |Test Loss: 0.8353|lr = 0.00022\n",
      "Epoch: 2147|steps:   30|Train Avg Loss: 0.8520 |Test Loss: 0.8352|lr = 0.00022\n",
      "Epoch: 2147|steps:   60|Train Avg Loss: 0.8411 |Test Loss: 0.8347|lr = 0.00022\n",
      "Epoch: 2148|steps:   30|Train Avg Loss: 0.8354 |Test Loss: 0.8300|lr = 0.00022\n",
      "Epoch: 2148|steps:   60|Train Avg Loss: 0.8475 |Test Loss: 0.8332|lr = 0.00022\n",
      "Epoch: 2149|steps:   30|Train Avg Loss: 0.8511 |Test Loss: 0.8386|lr = 0.00022\n",
      "Epoch: 2149|steps:   60|Train Avg Loss: 0.8477 |Test Loss: 0.8304|lr = 0.00022\n",
      "Epoch: 2150|steps:   30|Train Avg Loss: 0.8534 |Test Loss: 0.8336|lr = 0.00022\n",
      "Epoch: 2150|steps:   60|Train Avg Loss: 0.8398 |Test Loss: 0.8299|lr = 0.00022\n",
      "Epoch: 2151|steps:   30|Train Avg Loss: 0.8438 |Test Loss: 0.8335|lr = 0.00022\n",
      "Epoch: 2151|steps:   60|Train Avg Loss: 0.8409 |Test Loss: 0.8308|lr = 0.00022\n",
      "Epoch: 2152|steps:   30|Train Avg Loss: 0.8280 |Test Loss: 0.8332|lr = 0.00022\n",
      "Epoch: 2152|steps:   60|Train Avg Loss: 0.8534 |Test Loss: 0.8313|lr = 0.00022\n",
      "Epoch: 2153|steps:   30|Train Avg Loss: 0.8361 |Test Loss: 0.8332|lr = 0.00022\n",
      "Epoch: 2153|steps:   60|Train Avg Loss: 0.8695 |Test Loss: 0.8323|lr = 0.00022\n",
      "Epoch: 2154|steps:   30|Train Avg Loss: 0.8531 |Test Loss: 0.8303|lr = 0.00022\n",
      "Epoch: 2154|steps:   60|Train Avg Loss: 0.8269 |Test Loss: 0.8328|lr = 0.00022\n",
      "Epoch: 2155|steps:   30|Train Avg Loss: 0.8357 |Test Loss: 0.8300|lr = 0.00022\n",
      "Epoch: 2155|steps:   60|Train Avg Loss: 0.8491 |Test Loss: 0.8364|lr = 0.00022\n",
      "Epoch: 2156|steps:   30|Train Avg Loss: 0.8457 |Test Loss: 0.8351|lr = 0.00022\n",
      "Epoch: 2156|steps:   60|Train Avg Loss: 0.8336 |Test Loss: 0.8316|lr = 0.00022\n",
      "Epoch: 2157|steps:   30|Train Avg Loss: 0.8290 |Test Loss: 0.8295|lr = 0.00022\n",
      "Epoch: 2157|steps:   60|Train Avg Loss: 0.8564 |Test Loss: 0.8334|lr = 0.00022\n",
      "Epoch: 2158|steps:   30|Train Avg Loss: 0.8349 |Test Loss: 0.8320|lr = 0.00022\n",
      "Epoch: 2158|steps:   60|Train Avg Loss: 0.8724 |Test Loss: 0.8349|lr = 0.00022\n",
      "Epoch: 2159|steps:   30|Train Avg Loss: 0.8369 |Test Loss: 0.8326|lr = 0.00022\n",
      "Epoch: 2159|steps:   60|Train Avg Loss: 0.8531 |Test Loss: 0.8330|lr = 0.00022\n",
      "Epoch: 2160|steps:   30|Train Avg Loss: 0.8417 |Test Loss: 0.8384|lr = 0.00022\n",
      "Epoch: 2160|steps:   60|Train Avg Loss: 0.8498 |Test Loss: 0.8336|lr = 0.00022\n",
      "Epoch: 2161|steps:   30|Train Avg Loss: 0.8522 |Test Loss: 0.8345|lr = 0.00021\n",
      "Epoch: 2161|steps:   60|Train Avg Loss: 0.8302 |Test Loss: 0.8301|lr = 0.00021\n",
      "Epoch: 2162|steps:   30|Train Avg Loss: 0.8499 |Test Loss: 0.8329|lr = 0.00021\n",
      "Epoch: 2162|steps:   60|Train Avg Loss: 0.8468 |Test Loss: 0.8329|lr = 0.00021\n",
      "Epoch: 2163|steps:   30|Train Avg Loss: 0.8473 |Test Loss: 0.8352|lr = 0.00021\n",
      "Epoch: 2163|steps:   60|Train Avg Loss: 0.8304 |Test Loss: 0.8302|lr = 0.00021\n",
      "Epoch: 2164|steps:   30|Train Avg Loss: 0.8382 |Test Loss: 0.8362|lr = 0.00021\n",
      "Epoch: 2164|steps:   60|Train Avg Loss: 0.8322 |Test Loss: 0.8324|lr = 0.00021\n",
      "Epoch: 2165|steps:   30|Train Avg Loss: 0.8345 |Test Loss: 0.8404|lr = 0.00021\n",
      "Epoch: 2165|steps:   60|Train Avg Loss: 0.8484 |Test Loss: 0.8319|lr = 0.00021\n",
      "Epoch: 2166|steps:   30|Train Avg Loss: 0.8366 |Test Loss: 0.8292|lr = 0.00021\n",
      "Epoch: 2166|steps:   60|Train Avg Loss: 0.8535 |Test Loss: 0.8326|lr = 0.00021\n",
      "Epoch: 2167|steps:   30|Train Avg Loss: 0.8475 |Test Loss: 0.8342|lr = 0.00021\n",
      "Epoch: 2167|steps:   60|Train Avg Loss: 0.8408 |Test Loss: 0.8357|lr = 0.00021\n",
      "Epoch: 2168|steps:   30|Train Avg Loss: 0.8358 |Test Loss: 0.8312|lr = 0.00021\n",
      "Epoch: 2168|steps:   60|Train Avg Loss: 0.8519 |Test Loss: 0.8326|lr = 0.00021\n",
      "Epoch: 2169|steps:   30|Train Avg Loss: 0.8229 |Test Loss: 0.8320|lr = 0.00021\n",
      "Epoch: 2169|steps:   60|Train Avg Loss: 0.8513 |Test Loss: 0.8333|lr = 0.00021\n",
      "Epoch: 2170|steps:   30|Train Avg Loss: 0.8570 |Test Loss: 0.8318|lr = 0.00021\n",
      "Epoch: 2170|steps:   60|Train Avg Loss: 0.8385 |Test Loss: 0.8363|lr = 0.00021\n",
      "Epoch: 2171|steps:   30|Train Avg Loss: 0.8568 |Test Loss: 0.8320|lr = 0.00021\n",
      "Epoch: 2171|steps:   60|Train Avg Loss: 0.8345 |Test Loss: 0.8358|lr = 0.00021\n",
      "Epoch: 2172|steps:   30|Train Avg Loss: 0.8371 |Test Loss: 0.8355|lr = 0.00021\n",
      "Epoch: 2172|steps:   60|Train Avg Loss: 0.8455 |Test Loss: 0.8331|lr = 0.00021\n",
      "Epoch: 2173|steps:   30|Train Avg Loss: 0.8250 |Test Loss: 0.8334|lr = 0.00021\n",
      "Epoch: 2173|steps:   60|Train Avg Loss: 0.8696 |Test Loss: 0.8345|lr = 0.00021\n",
      "Epoch: 2174|steps:   30|Train Avg Loss: 0.8350 |Test Loss: 0.8297|lr = 0.00021\n",
      "Epoch: 2174|steps:   60|Train Avg Loss: 0.8437 |Test Loss: 0.8345|lr = 0.00021\n",
      "Epoch: 2175|steps:   30|Train Avg Loss: 0.8544 |Test Loss: 0.8375|lr = 0.00021\n",
      "Epoch: 2175|steps:   60|Train Avg Loss: 0.8248 |Test Loss: 0.8314|lr = 0.00021\n",
      "Epoch: 2176|steps:   30|Train Avg Loss: 0.8303 |Test Loss: 0.8319|lr = 0.00021\n",
      "Epoch: 2176|steps:   60|Train Avg Loss: 0.8551 |Test Loss: 0.8377|lr = 0.00021\n",
      "Epoch: 2177|steps:   30|Train Avg Loss: 0.8398 |Test Loss: 0.8328|lr = 0.00021\n",
      "Epoch: 2177|steps:   60|Train Avg Loss: 0.8475 |Test Loss: 0.8310|lr = 0.00021\n",
      "Epoch: 2178|steps:   30|Train Avg Loss: 0.8596 |Test Loss: 0.8416|lr = 0.00021\n",
      "Epoch: 2178|steps:   60|Train Avg Loss: 0.8217 |Test Loss: 0.8346|lr = 0.00021\n",
      "Epoch: 2179|steps:   30|Train Avg Loss: 0.8276 |Test Loss: 0.8318|lr = 0.00021\n",
      "Epoch: 2179|steps:   60|Train Avg Loss: 0.8562 |Test Loss: 0.8378|lr = 0.00021\n",
      "Epoch: 2180|steps:   30|Train Avg Loss: 0.8364 |Test Loss: 0.8345|lr = 0.00021\n",
      "Epoch: 2180|steps:   60|Train Avg Loss: 0.8421 |Test Loss: 0.8363|lr = 0.00021\n",
      "Epoch: 2181|steps:   30|Train Avg Loss: 0.8411 |Test Loss: 0.8375|lr = 0.00021\n",
      "Epoch: 2181|steps:   60|Train Avg Loss: 0.8427 |Test Loss: 0.8359|lr = 0.00021\n",
      "Epoch: 2182|steps:   30|Train Avg Loss: 0.8402 |Test Loss: 0.8310|lr = 0.00021\n",
      "Epoch: 2182|steps:   60|Train Avg Loss: 0.8537 |Test Loss: 0.8388|lr = 0.00021\n",
      "Epoch: 2183|steps:   30|Train Avg Loss: 0.8214 |Test Loss: 0.8345|lr = 0.00020\n",
      "Epoch: 2183|steps:   60|Train Avg Loss: 0.8507 |Test Loss: 0.8345|lr = 0.00020\n",
      "Epoch: 2184|steps:   30|Train Avg Loss: 0.8557 |Test Loss: 0.8424|lr = 0.00020\n",
      "Epoch: 2184|steps:   60|Train Avg Loss: 0.8365 |Test Loss: 0.8339|lr = 0.00020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2185|steps:   30|Train Avg Loss: 0.8275 |Test Loss: 0.8367|lr = 0.00020\n",
      "Epoch: 2185|steps:   60|Train Avg Loss: 0.8470 |Test Loss: 0.8320|lr = 0.00020\n",
      "Epoch: 2186|steps:   30|Train Avg Loss: 0.8464 |Test Loss: 0.8374|lr = 0.00020\n",
      "Epoch: 2186|steps:   60|Train Avg Loss: 0.8147 |Test Loss: 0.8337|lr = 0.00020\n",
      "Epoch: 2187|steps:   30|Train Avg Loss: 0.8387 |Test Loss: 0.8366|lr = 0.00020\n",
      "Epoch: 2187|steps:   60|Train Avg Loss: 0.8235 |Test Loss: 0.8325|lr = 0.00020\n",
      "Epoch: 2188|steps:   30|Train Avg Loss: 0.8456 |Test Loss: 0.8318|lr = 0.00020\n",
      "Epoch: 2188|steps:   60|Train Avg Loss: 0.8445 |Test Loss: 0.8370|lr = 0.00020\n",
      "Epoch: 2189|steps:   30|Train Avg Loss: 0.8359 |Test Loss: 0.8345|lr = 0.00020\n",
      "Epoch: 2189|steps:   60|Train Avg Loss: 0.8391 |Test Loss: 0.8368|lr = 0.00020\n",
      "Epoch: 2190|steps:   30|Train Avg Loss: 0.8230 |Test Loss: 0.8330|lr = 0.00020\n",
      "Epoch: 2190|steps:   60|Train Avg Loss: 0.8522 |Test Loss: 0.8360|lr = 0.00020\n",
      "Epoch: 2191|steps:   30|Train Avg Loss: 0.8474 |Test Loss: 0.8370|lr = 0.00020\n",
      "Epoch: 2191|steps:   60|Train Avg Loss: 0.8307 |Test Loss: 0.8404|lr = 0.00020\n",
      "Epoch: 2192|steps:   30|Train Avg Loss: 0.8373 |Test Loss: 0.8387|lr = 0.00020\n",
      "Epoch: 2192|steps:   60|Train Avg Loss: 0.8448 |Test Loss: 0.8311|lr = 0.00020\n",
      "Epoch: 2193|steps:   30|Train Avg Loss: 0.8332 |Test Loss: 0.8336|lr = 0.00020\n",
      "Epoch: 2193|steps:   60|Train Avg Loss: 0.8497 |Test Loss: 0.8348|lr = 0.00020\n",
      "Epoch: 2194|steps:   30|Train Avg Loss: 0.8502 |Test Loss: 0.8348|lr = 0.00020\n",
      "Epoch: 2194|steps:   60|Train Avg Loss: 0.8393 |Test Loss: 0.8427|lr = 0.00020\n",
      "Epoch: 2195|steps:   30|Train Avg Loss: 0.8563 |Test Loss: 0.8393|lr = 0.00020\n",
      "Epoch: 2195|steps:   60|Train Avg Loss: 0.8187 |Test Loss: 0.8334|lr = 0.00020\n",
      "Epoch: 2196|steps:   30|Train Avg Loss: 0.8316 |Test Loss: 0.8347|lr = 0.00020\n",
      "Epoch: 2196|steps:   60|Train Avg Loss: 0.8594 |Test Loss: 0.8373|lr = 0.00020\n",
      "Epoch: 2197|steps:   30|Train Avg Loss: 0.8418 |Test Loss: 0.8346|lr = 0.00020\n",
      "Epoch: 2197|steps:   60|Train Avg Loss: 0.8361 |Test Loss: 0.8358|lr = 0.00020\n",
      "Epoch: 2198|steps:   30|Train Avg Loss: 0.8414 |Test Loss: 0.8391|lr = 0.00020\n",
      "Epoch: 2198|steps:   60|Train Avg Loss: 0.8539 |Test Loss: 0.8370|lr = 0.00020\n",
      "Epoch: 2199|steps:   30|Train Avg Loss: 0.8389 |Test Loss: 0.8332|lr = 0.00020\n",
      "Epoch: 2199|steps:   60|Train Avg Loss: 0.8326 |Test Loss: 0.8345|lr = 0.00020\n",
      "Epoch: 2200|steps:   30|Train Avg Loss: 0.8233 |Test Loss: 0.8346|lr = 0.00020\n",
      "Epoch: 2200|steps:   60|Train Avg Loss: 0.8472 |Test Loss: 0.8393|lr = 0.00020\n",
      "Epoch: 2201|steps:   30|Train Avg Loss: 0.8365 |Test Loss: 0.8349|lr = 0.00020\n",
      "Epoch: 2201|steps:   60|Train Avg Loss: 0.8511 |Test Loss: 0.8433|lr = 0.00020\n",
      "Epoch: 2202|steps:   30|Train Avg Loss: 0.8511 |Test Loss: 0.8376|lr = 0.00020\n",
      "Epoch: 2202|steps:   60|Train Avg Loss: 0.8388 |Test Loss: 0.8332|lr = 0.00020\n",
      "Epoch: 2203|steps:   30|Train Avg Loss: 0.8459 |Test Loss: 0.8406|lr = 0.00020\n",
      "Epoch: 2203|steps:   60|Train Avg Loss: 0.8333 |Test Loss: 0.8362|lr = 0.00020\n",
      "Epoch: 2204|steps:   30|Train Avg Loss: 0.8273 |Test Loss: 0.8334|lr = 0.00020\n",
      "Epoch: 2204|steps:   60|Train Avg Loss: 0.8568 |Test Loss: 0.8396|lr = 0.00020\n",
      "Epoch: 2205|steps:   30|Train Avg Loss: 0.8268 |Test Loss: 0.8351|lr = 0.00019\n",
      "Epoch: 2205|steps:   60|Train Avg Loss: 0.8357 |Test Loss: 0.8331|lr = 0.00019\n",
      "Epoch: 2206|steps:   30|Train Avg Loss: 0.8527 |Test Loss: 0.8387|lr = 0.00019\n",
      "Epoch: 2206|steps:   60|Train Avg Loss: 0.8169 |Test Loss: 0.8403|lr = 0.00019\n",
      "Epoch: 2207|steps:   30|Train Avg Loss: 0.8408 |Test Loss: 0.8382|lr = 0.00019\n",
      "Epoch: 2207|steps:   60|Train Avg Loss: 0.8368 |Test Loss: 0.8352|lr = 0.00019\n",
      "Epoch: 2208|steps:   30|Train Avg Loss: 0.8352 |Test Loss: 0.8343|lr = 0.00019\n",
      "Epoch: 2208|steps:   60|Train Avg Loss: 0.8419 |Test Loss: 0.8389|lr = 0.00019\n",
      "Epoch: 2209|steps:   30|Train Avg Loss: 0.8447 |Test Loss: 0.8331|lr = 0.00019\n",
      "Epoch: 2209|steps:   60|Train Avg Loss: 0.8305 |Test Loss: 0.8394|lr = 0.00019\n",
      "Epoch: 2210|steps:   30|Train Avg Loss: 0.8494 |Test Loss: 0.8358|lr = 0.00019\n",
      "Epoch: 2210|steps:   60|Train Avg Loss: 0.8280 |Test Loss: 0.8350|lr = 0.00019\n",
      "Epoch: 2211|steps:   30|Train Avg Loss: 0.8480 |Test Loss: 0.8382|lr = 0.00019\n",
      "Epoch: 2211|steps:   60|Train Avg Loss: 0.8359 |Test Loss: 0.8367|lr = 0.00019\n",
      "Epoch: 2212|steps:   30|Train Avg Loss: 0.8172 |Test Loss: 0.8318|lr = 0.00019\n",
      "Epoch: 2212|steps:   60|Train Avg Loss: 0.8510 |Test Loss: 0.8323|lr = 0.00019\n",
      "Epoch: 2213|steps:   30|Train Avg Loss: 0.8304 |Test Loss: 0.8416|lr = 0.00019\n",
      "Epoch: 2213|steps:   60|Train Avg Loss: 0.8489 |Test Loss: 0.8323|lr = 0.00019\n",
      "Epoch: 2214|steps:   30|Train Avg Loss: 0.8492 |Test Loss: 0.8399|lr = 0.00019\n",
      "Epoch: 2214|steps:   60|Train Avg Loss: 0.8376 |Test Loss: 0.8337|lr = 0.00019\n",
      "Epoch: 2215|steps:   30|Train Avg Loss: 0.8414 |Test Loss: 0.8413|lr = 0.00019\n",
      "Epoch: 2215|steps:   60|Train Avg Loss: 0.8393 |Test Loss: 0.8345|lr = 0.00019\n",
      "Epoch: 2216|steps:   30|Train Avg Loss: 0.8342 |Test Loss: 0.8337|lr = 0.00019\n",
      "Epoch: 2216|steps:   60|Train Avg Loss: 0.8378 |Test Loss: 0.8415|lr = 0.00019\n",
      "Epoch: 2217|steps:   30|Train Avg Loss: 0.8417 |Test Loss: 0.8355|lr = 0.00019\n",
      "Epoch: 2217|steps:   60|Train Avg Loss: 0.8403 |Test Loss: 0.8339|lr = 0.00019\n",
      "Epoch: 2218|steps:   30|Train Avg Loss: 0.8451 |Test Loss: 0.8406|lr = 0.00019\n",
      "Epoch: 2218|steps:   60|Train Avg Loss: 0.8290 |Test Loss: 0.8344|lr = 0.00019\n",
      "Epoch: 2219|steps:   30|Train Avg Loss: 0.8312 |Test Loss: 0.8374|lr = 0.00019\n",
      "Epoch: 2219|steps:   60|Train Avg Loss: 0.8411 |Test Loss: 0.8348|lr = 0.00019\n",
      "Epoch: 2220|steps:   30|Train Avg Loss: 0.8380 |Test Loss: 0.8398|lr = 0.00019\n",
      "Epoch: 2220|steps:   60|Train Avg Loss: 0.8380 |Test Loss: 0.8340|lr = 0.00019\n",
      "Epoch: 2221|steps:   30|Train Avg Loss: 0.8249 |Test Loss: 0.8348|lr = 0.00019\n",
      "Epoch: 2221|steps:   60|Train Avg Loss: 0.8501 |Test Loss: 0.8440|lr = 0.00019\n",
      "Epoch: 2222|steps:   30|Train Avg Loss: 0.8460 |Test Loss: 0.8358|lr = 0.00019\n",
      "Epoch: 2222|steps:   60|Train Avg Loss: 0.8300 |Test Loss: 0.8401|lr = 0.00019\n",
      "Epoch: 2223|steps:   30|Train Avg Loss: 0.8498 |Test Loss: 0.8368|lr = 0.00019\n",
      "Epoch: 2223|steps:   60|Train Avg Loss: 0.8276 |Test Loss: 0.8357|lr = 0.00019\n",
      "Epoch: 2224|steps:   30|Train Avg Loss: 0.8417 |Test Loss: 0.8338|lr = 0.00019\n",
      "Epoch: 2224|steps:   60|Train Avg Loss: 0.8293 |Test Loss: 0.8382|lr = 0.00019\n",
      "Epoch: 2225|steps:   30|Train Avg Loss: 0.8251 |Test Loss: 0.8347|lr = 0.00019\n",
      "Epoch: 2225|steps:   60|Train Avg Loss: 0.8485 |Test Loss: 0.8338|lr = 0.00019\n",
      "Epoch: 2226|steps:   30|Train Avg Loss: 0.8208 |Test Loss: 0.8372|lr = 0.00019\n",
      "Epoch: 2226|steps:   60|Train Avg Loss: 0.8562 |Test Loss: 0.8426|lr = 0.00019\n",
      "Epoch: 2227|steps:   30|Train Avg Loss: 0.8384 |Test Loss: 0.8362|lr = 0.00019\n",
      "Epoch: 2227|steps:   60|Train Avg Loss: 0.8341 |Test Loss: 0.8354|lr = 0.00019\n",
      "Epoch: 2228|steps:   30|Train Avg Loss: 0.8298 |Test Loss: 0.8321|lr = 0.00019\n",
      "Epoch: 2228|steps:   60|Train Avg Loss: 0.8444 |Test Loss: 0.8359|lr = 0.00019\n",
      "Epoch: 2229|steps:   30|Train Avg Loss: 0.8423 |Test Loss: 0.8413|lr = 0.00019\n",
      "Epoch: 2229|steps:   60|Train Avg Loss: 0.8386 |Test Loss: 0.8360|lr = 0.00019\n",
      "Epoch: 2230|steps:   30|Train Avg Loss: 0.8441 |Test Loss: 0.8378|lr = 0.00019\n",
      "Epoch: 2230|steps:   60|Train Avg Loss: 0.8361 |Test Loss: 0.8395|lr = 0.00019\n",
      "Epoch: 2231|steps:   30|Train Avg Loss: 0.8344 |Test Loss: 0.8392|lr = 0.00019\n",
      "Epoch: 2231|steps:   60|Train Avg Loss: 0.8492 |Test Loss: 0.8400|lr = 0.00019\n",
      "Epoch: 2232|steps:   30|Train Avg Loss: 0.8394 |Test Loss: 0.8375|lr = 0.00019\n",
      "Epoch: 2232|steps:   60|Train Avg Loss: 0.8315 |Test Loss: 0.8386|lr = 0.00019\n",
      "Epoch: 2233|steps:   30|Train Avg Loss: 0.8363 |Test Loss: 0.8355|lr = 0.00019\n",
      "Epoch: 2233|steps:   60|Train Avg Loss: 0.8358 |Test Loss: 0.8350|lr = 0.00019\n",
      "Epoch: 2234|steps:   30|Train Avg Loss: 0.8260 |Test Loss: 0.8413|lr = 0.00019\n",
      "Epoch: 2234|steps:   60|Train Avg Loss: 0.8413 |Test Loss: 0.8345|lr = 0.00019\n",
      "Epoch: 2235|steps:   30|Train Avg Loss: 0.8277 |Test Loss: 0.8388|lr = 0.00019\n",
      "Epoch: 2235|steps:   60|Train Avg Loss: 0.8508 |Test Loss: 0.8368|lr = 0.00019\n",
      "Epoch: 2236|steps:   30|Train Avg Loss: 0.8379 |Test Loss: 0.8354|lr = 0.00019\n",
      "Epoch: 2236|steps:   60|Train Avg Loss: 0.8227 |Test Loss: 0.8406|lr = 0.00019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2237|steps:   30|Train Avg Loss: 0.8430 |Test Loss: 0.8426|lr = 0.00019\n",
      "Epoch: 2237|steps:   60|Train Avg Loss: 0.8343 |Test Loss: 0.8402|lr = 0.00019\n",
      "Epoch: 2238|steps:   30|Train Avg Loss: 0.8424 |Test Loss: 0.8420|lr = 0.00018\n",
      "Epoch: 2238|steps:   60|Train Avg Loss: 0.8283 |Test Loss: 0.8391|lr = 0.00018\n",
      "Epoch: 2239|steps:   30|Train Avg Loss: 0.8540 |Test Loss: 0.8445|lr = 0.00018\n",
      "Epoch: 2239|steps:   60|Train Avg Loss: 0.8417 |Test Loss: 0.8375|lr = 0.00018\n",
      "Epoch: 2240|steps:   30|Train Avg Loss: 0.8167 |Test Loss: 0.8390|lr = 0.00018\n",
      "Epoch: 2240|steps:   60|Train Avg Loss: 0.8524 |Test Loss: 0.8356|lr = 0.00018\n",
      "Epoch: 2241|steps:   30|Train Avg Loss: 0.8341 |Test Loss: 0.8369|lr = 0.00018\n",
      "Epoch: 2241|steps:   60|Train Avg Loss: 0.8363 |Test Loss: 0.8350|lr = 0.00018\n",
      "Epoch: 2242|steps:   30|Train Avg Loss: 0.8348 |Test Loss: 0.8355|lr = 0.00018\n",
      "Epoch: 2242|steps:   60|Train Avg Loss: 0.8405 |Test Loss: 0.8365|lr = 0.00018\n",
      "Epoch: 2243|steps:   30|Train Avg Loss: 0.8384 |Test Loss: 0.8366|lr = 0.00018\n",
      "Epoch: 2243|steps:   60|Train Avg Loss: 0.8368 |Test Loss: 0.8361|lr = 0.00018\n",
      "Epoch: 2244|steps:   30|Train Avg Loss: 0.8229 |Test Loss: 0.8320|lr = 0.00018\n",
      "Epoch: 2244|steps:   60|Train Avg Loss: 0.8435 |Test Loss: 0.8391|lr = 0.00018\n",
      "Epoch: 2245|steps:   30|Train Avg Loss: 0.8514 |Test Loss: 0.8400|lr = 0.00018\n",
      "Epoch: 2245|steps:   60|Train Avg Loss: 0.8261 |Test Loss: 0.8386|lr = 0.00018\n",
      "Epoch: 2246|steps:   30|Train Avg Loss: 0.8539 |Test Loss: 0.8355|lr = 0.00018\n",
      "Epoch: 2246|steps:   60|Train Avg Loss: 0.8231 |Test Loss: 0.8417|lr = 0.00018\n",
      "Epoch: 2247|steps:   30|Train Avg Loss: 0.8359 |Test Loss: 0.8404|lr = 0.00018\n",
      "Epoch: 2247|steps:   60|Train Avg Loss: 0.8415 |Test Loss: 0.8377|lr = 0.00018\n",
      "Epoch: 2248|steps:   30|Train Avg Loss: 0.8360 |Test Loss: 0.8408|lr = 0.00018\n",
      "Epoch: 2248|steps:   60|Train Avg Loss: 0.8416 |Test Loss: 0.8367|lr = 0.00018\n",
      "Epoch: 2249|steps:   30|Train Avg Loss: 0.8138 |Test Loss: 0.8287|lr = 0.00018\n",
      "Epoch: 2249|steps:   60|Train Avg Loss: 0.8504 |Test Loss: 0.8340|lr = 0.00018\n",
      "Epoch: 2250|steps:   30|Train Avg Loss: 0.8272 |Test Loss: 0.8349|lr = 0.00018\n",
      "Epoch: 2250|steps:   60|Train Avg Loss: 0.8415 |Test Loss: 0.8419|lr = 0.00018\n",
      "Epoch: 2251|steps:   30|Train Avg Loss: 0.8346 |Test Loss: 0.8412|lr = 0.00018\n",
      "Epoch: 2251|steps:   60|Train Avg Loss: 0.8349 |Test Loss: 0.8382|lr = 0.00018\n",
      "Epoch: 2252|steps:   30|Train Avg Loss: 0.8146 |Test Loss: 0.8342|lr = 0.00018\n",
      "Epoch: 2252|steps:   60|Train Avg Loss: 0.8407 |Test Loss: 0.8411|lr = 0.00018\n",
      "Epoch: 2253|steps:   30|Train Avg Loss: 0.8443 |Test Loss: 0.8415|lr = 0.00018\n",
      "Epoch: 2253|steps:   60|Train Avg Loss: 0.8225 |Test Loss: 0.8432|lr = 0.00018\n",
      "Epoch: 2254|steps:   30|Train Avg Loss: 0.8604 |Test Loss: 0.8501|lr = 0.00018\n",
      "Epoch: 2254|steps:   60|Train Avg Loss: 0.8114 |Test Loss: 0.8357|lr = 0.00018\n",
      "Epoch: 2255|steps:   30|Train Avg Loss: 0.8291 |Test Loss: 0.8378|lr = 0.00018\n",
      "Epoch: 2255|steps:   60|Train Avg Loss: 0.8306 |Test Loss: 0.8427|lr = 0.00018\n",
      "Epoch: 2256|steps:   30|Train Avg Loss: 0.7988 |Test Loss: 0.8324|lr = 0.00018\n",
      "Epoch: 2256|steps:   60|Train Avg Loss: 0.8547 |Test Loss: 0.8369|lr = 0.00018\n",
      "Epoch: 2257|steps:   30|Train Avg Loss: 0.8598 |Test Loss: 0.8449|lr = 0.00018\n",
      "Epoch: 2257|steps:   60|Train Avg Loss: 0.8069 |Test Loss: 0.8377|lr = 0.00018\n",
      "Epoch: 2258|steps:   30|Train Avg Loss: 0.8496 |Test Loss: 0.8363|lr = 0.00018\n",
      "Epoch: 2258|steps:   60|Train Avg Loss: 0.8250 |Test Loss: 0.8395|lr = 0.00018\n",
      "Epoch: 2259|steps:   30|Train Avg Loss: 0.8356 |Test Loss: 0.8370|lr = 0.00018\n",
      "Epoch: 2259|steps:   60|Train Avg Loss: 0.8467 |Test Loss: 0.8457|lr = 0.00018\n",
      "Epoch: 2260|steps:   30|Train Avg Loss: 0.8288 |Test Loss: 0.8325|lr = 0.00018\n",
      "Epoch: 2260|steps:   60|Train Avg Loss: 0.8316 |Test Loss: 0.8360|lr = 0.00018\n",
      "Epoch: 2261|steps:   30|Train Avg Loss: 0.8523 |Test Loss: 0.8434|lr = 0.00018\n",
      "Epoch: 2261|steps:   60|Train Avg Loss: 0.8280 |Test Loss: 0.8411|lr = 0.00018\n",
      "Epoch: 2262|steps:   30|Train Avg Loss: 0.8448 |Test Loss: 0.8429|lr = 0.00018\n",
      "Epoch: 2262|steps:   60|Train Avg Loss: 0.8207 |Test Loss: 0.8389|lr = 0.00018\n",
      "Epoch: 2263|steps:   30|Train Avg Loss: 0.8413 |Test Loss: 0.8372|lr = 0.00018\n",
      "Epoch: 2263|steps:   60|Train Avg Loss: 0.8208 |Test Loss: 0.8322|lr = 0.00018\n",
      "Epoch: 2264|steps:   30|Train Avg Loss: 0.8403 |Test Loss: 0.8414|lr = 0.00018\n",
      "Epoch: 2264|steps:   60|Train Avg Loss: 0.8416 |Test Loss: 0.8407|lr = 0.00018\n",
      "Epoch: 2265|steps:   30|Train Avg Loss: 0.8183 |Test Loss: 0.8350|lr = 0.00018\n",
      "Epoch: 2265|steps:   60|Train Avg Loss: 0.8435 |Test Loss: 0.8368|lr = 0.00018\n",
      "Epoch: 2266|steps:   30|Train Avg Loss: 0.8242 |Test Loss: 0.8381|lr = 0.00018\n",
      "Epoch: 2266|steps:   60|Train Avg Loss: 0.8296 |Test Loss: 0.8379|lr = 0.00018\n",
      "Epoch: 2267|steps:   30|Train Avg Loss: 0.8286 |Test Loss: 0.8429|lr = 0.00018\n",
      "Epoch: 2267|steps:   60|Train Avg Loss: 0.8282 |Test Loss: 0.8386|lr = 0.00018\n",
      "Epoch: 2268|steps:   30|Train Avg Loss: 0.8275 |Test Loss: 0.8381|lr = 0.00018\n",
      "Epoch: 2268|steps:   60|Train Avg Loss: 0.8365 |Test Loss: 0.8413|lr = 0.00018\n",
      "Epoch: 2269|steps:   30|Train Avg Loss: 0.8192 |Test Loss: 0.8446|lr = 0.00018\n",
      "Epoch: 2269|steps:   60|Train Avg Loss: 0.8411 |Test Loss: 0.8352|lr = 0.00018\n",
      "Epoch: 2270|steps:   30|Train Avg Loss: 0.8373 |Test Loss: 0.8439|lr = 0.00018\n",
      "Epoch: 2270|steps:   60|Train Avg Loss: 0.8153 |Test Loss: 0.8312|lr = 0.00018\n",
      "Epoch: 2271|steps:   30|Train Avg Loss: 0.8197 |Test Loss: 0.8388|lr = 0.00017\n",
      "Epoch: 2271|steps:   60|Train Avg Loss: 0.8377 |Test Loss: 0.8397|lr = 0.00017\n",
      "Epoch: 2272|steps:   30|Train Avg Loss: 0.8388 |Test Loss: 0.8396|lr = 0.00017\n",
      "Epoch: 2272|steps:   60|Train Avg Loss: 0.8205 |Test Loss: 0.8444|lr = 0.00017\n",
      "Epoch: 2273|steps:   30|Train Avg Loss: 0.8342 |Test Loss: 0.8382|lr = 0.00017\n",
      "Epoch: 2273|steps:   60|Train Avg Loss: 0.8130 |Test Loss: 0.8367|lr = 0.00017\n",
      "Epoch: 2274|steps:   30|Train Avg Loss: 0.8296 |Test Loss: 0.8441|lr = 0.00017\n",
      "Epoch: 2274|steps:   60|Train Avg Loss: 0.8294 |Test Loss: 0.8307|lr = 0.00017\n",
      "Epoch: 2275|steps:   30|Train Avg Loss: 0.8319 |Test Loss: 0.8379|lr = 0.00017\n",
      "Epoch: 2275|steps:   60|Train Avg Loss: 0.8437 |Test Loss: 0.8367|lr = 0.00017\n",
      "Epoch: 2276|steps:   30|Train Avg Loss: 0.8365 |Test Loss: 0.8416|lr = 0.00017\n",
      "Epoch: 2276|steps:   60|Train Avg Loss: 0.8376 |Test Loss: 0.8423|lr = 0.00017\n",
      "Epoch: 2277|steps:   30|Train Avg Loss: 0.8269 |Test Loss: 0.8391|lr = 0.00017\n",
      "Epoch: 2277|steps:   60|Train Avg Loss: 0.8313 |Test Loss: 0.8361|lr = 0.00017\n",
      "Epoch: 2278|steps:   30|Train Avg Loss: 0.8097 |Test Loss: 0.8297|lr = 0.00017\n",
      "Epoch: 2278|steps:   60|Train Avg Loss: 0.8399 |Test Loss: 0.8366|lr = 0.00017\n",
      "Epoch: 2279|steps:   30|Train Avg Loss: 0.8331 |Test Loss: 0.8412|lr = 0.00017\n",
      "Epoch: 2279|steps:   60|Train Avg Loss: 0.8182 |Test Loss: 0.8384|lr = 0.00017\n",
      "Epoch: 2280|steps:   30|Train Avg Loss: 0.8442 |Test Loss: 0.8443|lr = 0.00017\n",
      "Epoch: 2280|steps:   60|Train Avg Loss: 0.8221 |Test Loss: 0.8421|lr = 0.00017\n",
      "Epoch: 2281|steps:   30|Train Avg Loss: 0.8314 |Test Loss: 0.8406|lr = 0.00017\n",
      "Epoch: 2281|steps:   60|Train Avg Loss: 0.8277 |Test Loss: 0.8408|lr = 0.00017\n",
      "Epoch: 2282|steps:   30|Train Avg Loss: 0.8266 |Test Loss: 0.8404|lr = 0.00017\n",
      "Epoch: 2282|steps:   60|Train Avg Loss: 0.8364 |Test Loss: 0.8428|lr = 0.00017\n",
      "Epoch: 2283|steps:   30|Train Avg Loss: 0.8465 |Test Loss: 0.8442|lr = 0.00017\n",
      "Epoch: 2283|steps:   60|Train Avg Loss: 0.8157 |Test Loss: 0.8376|lr = 0.00017\n",
      "Epoch: 2284|steps:   30|Train Avg Loss: 0.8320 |Test Loss: 0.8403|lr = 0.00017\n",
      "Epoch: 2284|steps:   60|Train Avg Loss: 0.8313 |Test Loss: 0.8398|lr = 0.00017\n",
      "Epoch: 2285|steps:   30|Train Avg Loss: 0.8367 |Test Loss: 0.8440|lr = 0.00017\n",
      "Epoch: 2285|steps:   60|Train Avg Loss: 0.8353 |Test Loss: 0.8324|lr = 0.00017\n",
      "Epoch: 2286|steps:   30|Train Avg Loss: 0.8312 |Test Loss: 0.8423|lr = 0.00017\n",
      "Epoch: 2286|steps:   60|Train Avg Loss: 0.8250 |Test Loss: 0.8388|lr = 0.00017\n",
      "Epoch: 2287|steps:   30|Train Avg Loss: 0.8170 |Test Loss: 0.8381|lr = 0.00017\n",
      "Epoch: 2287|steps:   60|Train Avg Loss: 0.8397 |Test Loss: 0.8408|lr = 0.00017\n",
      "Epoch: 2288|steps:   30|Train Avg Loss: 0.8191 |Test Loss: 0.8329|lr = 0.00017\n",
      "Epoch: 2288|steps:   60|Train Avg Loss: 0.8365 |Test Loss: 0.8384|lr = 0.00017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2289|steps:   30|Train Avg Loss: 0.8127 |Test Loss: 0.8410|lr = 0.00017\n",
      "Epoch: 2289|steps:   60|Train Avg Loss: 0.8444 |Test Loss: 0.8394|lr = 0.00017\n",
      "Epoch: 2290|steps:   30|Train Avg Loss: 0.8233 |Test Loss: 0.8343|lr = 0.00017\n",
      "Epoch: 2290|steps:   60|Train Avg Loss: 0.8185 |Test Loss: 0.8359|lr = 0.00017\n",
      "Epoch: 2291|steps:   30|Train Avg Loss: 0.8175 |Test Loss: 0.8415|lr = 0.00017\n",
      "Epoch: 2291|steps:   60|Train Avg Loss: 0.8353 |Test Loss: 0.8399|lr = 0.00017\n",
      "Epoch: 2292|steps:   30|Train Avg Loss: 0.8387 |Test Loss: 0.8469|lr = 0.00017\n",
      "Epoch: 2292|steps:   60|Train Avg Loss: 0.8134 |Test Loss: 0.8382|lr = 0.00017\n",
      "Epoch: 2293|steps:   30|Train Avg Loss: 0.8257 |Test Loss: 0.8401|lr = 0.00017\n",
      "Epoch: 2293|steps:   60|Train Avg Loss: 0.8318 |Test Loss: 0.8400|lr = 0.00017\n",
      "Epoch: 2294|steps:   30|Train Avg Loss: 0.8066 |Test Loss: 0.8459|lr = 0.00017\n",
      "Epoch: 2294|steps:   60|Train Avg Loss: 0.8545 |Test Loss: 0.8381|lr = 0.00017\n",
      "Epoch: 2295|steps:   30|Train Avg Loss: 0.8241 |Test Loss: 0.8414|lr = 0.00017\n",
      "Epoch: 2295|steps:   60|Train Avg Loss: 0.8418 |Test Loss: 0.8342|lr = 0.00017\n",
      "Epoch: 2296|steps:   30|Train Avg Loss: 0.8243 |Test Loss: 0.8455|lr = 0.00017\n",
      "Epoch: 2296|steps:   60|Train Avg Loss: 0.8190 |Test Loss: 0.8436|lr = 0.00017\n",
      "Epoch: 2297|steps:   30|Train Avg Loss: 0.8496 |Test Loss: 0.8428|lr = 0.00017\n",
      "Epoch: 2297|steps:   60|Train Avg Loss: 0.8041 |Test Loss: 0.8382|lr = 0.00017\n",
      "Epoch: 2298|steps:   30|Train Avg Loss: 0.8244 |Test Loss: 0.8398|lr = 0.00017\n",
      "Epoch: 2298|steps:   60|Train Avg Loss: 0.8357 |Test Loss: 0.8414|lr = 0.00017\n",
      "Epoch: 2299|steps:   30|Train Avg Loss: 0.7915 |Test Loss: 0.8345|lr = 0.00017\n",
      "Epoch: 2299|steps:   60|Train Avg Loss: 0.8695 |Test Loss: 0.8442|lr = 0.00017\n",
      "Epoch: 2300|steps:   30|Train Avg Loss: 0.8240 |Test Loss: 0.8376|lr = 0.00017\n",
      "Epoch: 2300|steps:   60|Train Avg Loss: 0.8333 |Test Loss: 0.8401|lr = 0.00017\n",
      "Epoch: 2301|steps:   30|Train Avg Loss: 0.8480 |Test Loss: 0.8418|lr = 0.00017\n",
      "Epoch: 2301|steps:   60|Train Avg Loss: 0.8104 |Test Loss: 0.8383|lr = 0.00017\n",
      "Epoch: 2302|steps:   30|Train Avg Loss: 0.8041 |Test Loss: 0.8373|lr = 0.00017\n",
      "Epoch: 2302|steps:   60|Train Avg Loss: 0.8526 |Test Loss: 0.8388|lr = 0.00017\n",
      "Epoch: 2303|steps:   30|Train Avg Loss: 0.8266 |Test Loss: 0.8444|lr = 0.00017\n",
      "Epoch: 2303|steps:   60|Train Avg Loss: 0.8264 |Test Loss: 0.8364|lr = 0.00017\n",
      "Epoch: 2304|steps:   30|Train Avg Loss: 0.8361 |Test Loss: 0.8441|lr = 0.00016\n",
      "Epoch: 2304|steps:   60|Train Avg Loss: 0.8171 |Test Loss: 0.8354|lr = 0.00016\n",
      "Epoch: 2305|steps:   30|Train Avg Loss: 0.8233 |Test Loss: 0.8385|lr = 0.00016\n",
      "Epoch: 2305|steps:   60|Train Avg Loss: 0.8339 |Test Loss: 0.8387|lr = 0.00016\n",
      "Epoch: 2306|steps:   30|Train Avg Loss: 0.8120 |Test Loss: 0.8409|lr = 0.00016\n",
      "Epoch: 2306|steps:   60|Train Avg Loss: 0.8412 |Test Loss: 0.8427|lr = 0.00016\n",
      "Epoch: 2307|steps:   30|Train Avg Loss: 0.8401 |Test Loss: 0.8450|lr = 0.00016\n",
      "Epoch: 2307|steps:   60|Train Avg Loss: 0.8213 |Test Loss: 0.8381|lr = 0.00016\n",
      "Epoch: 2308|steps:   30|Train Avg Loss: 0.8211 |Test Loss: 0.8362|lr = 0.00016\n",
      "Epoch: 2308|steps:   60|Train Avg Loss: 0.8284 |Test Loss: 0.8436|lr = 0.00016\n",
      "Epoch: 2309|steps:   30|Train Avg Loss: 0.8501 |Test Loss: 0.8455|lr = 0.00016\n",
      "Epoch: 2309|steps:   60|Train Avg Loss: 0.8100 |Test Loss: 0.8415|lr = 0.00016\n",
      "Epoch: 2310|steps:   30|Train Avg Loss: 0.8287 |Test Loss: 0.8439|lr = 0.00016\n",
      "Epoch: 2310|steps:   60|Train Avg Loss: 0.8184 |Test Loss: 0.8356|lr = 0.00016\n",
      "Epoch: 2311|steps:   30|Train Avg Loss: 0.8394 |Test Loss: 0.8507|lr = 0.00016\n",
      "Epoch: 2311|steps:   60|Train Avg Loss: 0.8204 |Test Loss: 0.8372|lr = 0.00016\n",
      "Epoch: 2312|steps:   30|Train Avg Loss: 0.8196 |Test Loss: 0.8361|lr = 0.00016\n",
      "Epoch: 2312|steps:   60|Train Avg Loss: 0.8374 |Test Loss: 0.8415|lr = 0.00016\n",
      "Epoch: 2313|steps:   30|Train Avg Loss: 0.8239 |Test Loss: 0.8428|lr = 0.00016\n",
      "Epoch: 2313|steps:   60|Train Avg Loss: 0.8276 |Test Loss: 0.8477|lr = 0.00016\n",
      "Epoch: 2314|steps:   30|Train Avg Loss: 0.8156 |Test Loss: 0.8354|lr = 0.00016\n",
      "Epoch: 2314|steps:   60|Train Avg Loss: 0.8290 |Test Loss: 0.8421|lr = 0.00016\n",
      "Epoch: 2315|steps:   30|Train Avg Loss: 0.8274 |Test Loss: 0.8451|lr = 0.00016\n",
      "Epoch: 2315|steps:   60|Train Avg Loss: 0.8318 |Test Loss: 0.8444|lr = 0.00016\n",
      "Epoch: 2316|steps:   30|Train Avg Loss: 0.8215 |Test Loss: 0.8435|lr = 0.00016\n",
      "Epoch: 2316|steps:   60|Train Avg Loss: 0.8337 |Test Loss: 0.8369|lr = 0.00016\n",
      "Epoch: 2317|steps:   30|Train Avg Loss: 0.8085 |Test Loss: 0.8409|lr = 0.00016\n",
      "Epoch: 2317|steps:   60|Train Avg Loss: 0.8452 |Test Loss: 0.8446|lr = 0.00016\n",
      "Epoch: 2318|steps:   30|Train Avg Loss: 0.8270 |Test Loss: 0.8465|lr = 0.00016\n",
      "Epoch: 2318|steps:   60|Train Avg Loss: 0.8089 |Test Loss: 0.8389|lr = 0.00016\n",
      "Epoch: 2319|steps:   30|Train Avg Loss: 0.8192 |Test Loss: 0.8381|lr = 0.00016\n",
      "Epoch: 2319|steps:   60|Train Avg Loss: 0.8142 |Test Loss: 0.8427|lr = 0.00016\n",
      "Epoch: 2320|steps:   30|Train Avg Loss: 0.8180 |Test Loss: 0.8414|lr = 0.00016\n",
      "Epoch: 2320|steps:   60|Train Avg Loss: 0.8271 |Test Loss: 0.8411|lr = 0.00016\n",
      "Epoch: 2321|steps:   30|Train Avg Loss: 0.8132 |Test Loss: 0.8383|lr = 0.00016\n",
      "Epoch: 2321|steps:   60|Train Avg Loss: 0.8356 |Test Loss: 0.8417|lr = 0.00016\n",
      "Epoch: 2322|steps:   30|Train Avg Loss: 0.8057 |Test Loss: 0.8388|lr = 0.00016\n",
      "Epoch: 2322|steps:   60|Train Avg Loss: 0.8342 |Test Loss: 0.8464|lr = 0.00016\n",
      "Epoch: 2323|steps:   30|Train Avg Loss: 0.8174 |Test Loss: 0.8406|lr = 0.00016\n",
      "Epoch: 2323|steps:   60|Train Avg Loss: 0.8476 |Test Loss: 0.8456|lr = 0.00016\n",
      "Epoch: 2324|steps:   30|Train Avg Loss: 0.8239 |Test Loss: 0.8472|lr = 0.00016\n",
      "Epoch: 2324|steps:   60|Train Avg Loss: 0.8276 |Test Loss: 0.8414|lr = 0.00016\n",
      "Epoch: 2325|steps:   30|Train Avg Loss: 0.8209 |Test Loss: 0.8377|lr = 0.00016\n",
      "Epoch: 2325|steps:   60|Train Avg Loss: 0.8210 |Test Loss: 0.8416|lr = 0.00016\n",
      "Epoch: 2326|steps:   30|Train Avg Loss: 0.8234 |Test Loss: 0.8433|lr = 0.00016\n",
      "Epoch: 2326|steps:   60|Train Avg Loss: 0.8231 |Test Loss: 0.8409|lr = 0.00016\n",
      "Epoch: 2327|steps:   30|Train Avg Loss: 0.8069 |Test Loss: 0.8377|lr = 0.00016\n",
      "Epoch: 2327|steps:   60|Train Avg Loss: 0.8455 |Test Loss: 0.8451|lr = 0.00016\n",
      "Epoch: 2328|steps:   30|Train Avg Loss: 0.8138 |Test Loss: 0.8383|lr = 0.00016\n",
      "Epoch: 2328|steps:   60|Train Avg Loss: 0.8300 |Test Loss: 0.8505|lr = 0.00016\n",
      "Epoch: 2329|steps:   30|Train Avg Loss: 0.8280 |Test Loss: 0.8375|lr = 0.00016\n",
      "Epoch: 2329|steps:   60|Train Avg Loss: 0.8324 |Test Loss: 0.8461|lr = 0.00016\n",
      "Epoch: 2330|steps:   30|Train Avg Loss: 0.8183 |Test Loss: 0.8455|lr = 0.00016\n",
      "Epoch: 2330|steps:   60|Train Avg Loss: 0.8206 |Test Loss: 0.8372|lr = 0.00016\n",
      "Epoch: 2331|steps:   30|Train Avg Loss: 0.8293 |Test Loss: 0.8433|lr = 0.00016\n",
      "Epoch: 2331|steps:   60|Train Avg Loss: 0.8259 |Test Loss: 0.8394|lr = 0.00016\n",
      "Epoch: 2332|steps:   30|Train Avg Loss: 0.8041 |Test Loss: 0.8445|lr = 0.00016\n",
      "Epoch: 2332|steps:   60|Train Avg Loss: 0.8365 |Test Loss: 0.8435|lr = 0.00016\n",
      "Epoch: 2333|steps:   30|Train Avg Loss: 0.8380 |Test Loss: 0.8428|lr = 0.00016\n",
      "Epoch: 2333|steps:   60|Train Avg Loss: 0.8112 |Test Loss: 0.8436|lr = 0.00016\n",
      "Epoch: 2334|steps:   30|Train Avg Loss: 0.8331 |Test Loss: 0.8386|lr = 0.00016\n",
      "Epoch: 2334|steps:   60|Train Avg Loss: 0.8106 |Test Loss: 0.8405|lr = 0.00016\n",
      "Epoch: 2335|steps:   30|Train Avg Loss: 0.8227 |Test Loss: 0.8430|lr = 0.00016\n",
      "Epoch: 2335|steps:   60|Train Avg Loss: 0.8156 |Test Loss: 0.8410|lr = 0.00016\n",
      "Epoch: 2336|steps:   30|Train Avg Loss: 0.8265 |Test Loss: 0.8425|lr = 0.00016\n",
      "Epoch: 2336|steps:   60|Train Avg Loss: 0.8310 |Test Loss: 0.8420|lr = 0.00016\n",
      "Epoch: 2337|steps:   30|Train Avg Loss: 0.8215 |Test Loss: 0.8338|lr = 0.00015\n",
      "Epoch: 2337|steps:   60|Train Avg Loss: 0.8311 |Test Loss: 0.8443|lr = 0.00015\n",
      "Epoch: 2338|steps:   30|Train Avg Loss: 0.8278 |Test Loss: 0.8419|lr = 0.00015\n",
      "Epoch: 2338|steps:   60|Train Avg Loss: 0.8266 |Test Loss: 0.8450|lr = 0.00015\n",
      "Epoch: 2339|steps:   30|Train Avg Loss: 0.8425 |Test Loss: 0.8422|lr = 0.00015\n",
      "Epoch: 2339|steps:   60|Train Avg Loss: 0.8159 |Test Loss: 0.8413|lr = 0.00015\n",
      "Epoch: 2340|steps:   30|Train Avg Loss: 0.8155 |Test Loss: 0.8374|lr = 0.00015\n",
      "Epoch: 2340|steps:   60|Train Avg Loss: 0.8296 |Test Loss: 0.8447|lr = 0.00015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2341|steps:   30|Train Avg Loss: 0.8072 |Test Loss: 0.8412|lr = 0.00015\n",
      "Epoch: 2341|steps:   60|Train Avg Loss: 0.8451 |Test Loss: 0.8458|lr = 0.00015\n",
      "Epoch: 2342|steps:   30|Train Avg Loss: 0.8099 |Test Loss: 0.8408|lr = 0.00015\n",
      "Epoch: 2342|steps:   60|Train Avg Loss: 0.8298 |Test Loss: 0.8376|lr = 0.00015\n",
      "Epoch: 2343|steps:   30|Train Avg Loss: 0.8235 |Test Loss: 0.8435|lr = 0.00015\n",
      "Epoch: 2343|steps:   60|Train Avg Loss: 0.8170 |Test Loss: 0.8434|lr = 0.00015\n",
      "Epoch: 2344|steps:   30|Train Avg Loss: 0.8300 |Test Loss: 0.8410|lr = 0.00015\n",
      "Epoch: 2344|steps:   60|Train Avg Loss: 0.8288 |Test Loss: 0.8410|lr = 0.00015\n",
      "Epoch: 2345|steps:   30|Train Avg Loss: 0.8161 |Test Loss: 0.8398|lr = 0.00015\n",
      "Epoch: 2345|steps:   60|Train Avg Loss: 0.8187 |Test Loss: 0.8403|lr = 0.00015\n",
      "Epoch: 2346|steps:   30|Train Avg Loss: 0.7993 |Test Loss: 0.8454|lr = 0.00015\n",
      "Epoch: 2346|steps:   60|Train Avg Loss: 0.8232 |Test Loss: 0.8386|lr = 0.00015\n",
      "Epoch: 2347|steps:   30|Train Avg Loss: 0.8153 |Test Loss: 0.8393|lr = 0.00015\n",
      "Epoch: 2347|steps:   60|Train Avg Loss: 0.8230 |Test Loss: 0.8351|lr = 0.00015\n",
      "Epoch: 2348|steps:   30|Train Avg Loss: 0.8099 |Test Loss: 0.8381|lr = 0.00015\n",
      "Epoch: 2348|steps:   60|Train Avg Loss: 0.8347 |Test Loss: 0.8459|lr = 0.00015\n",
      "Epoch: 2349|steps:   30|Train Avg Loss: 0.8418 |Test Loss: 0.8480|lr = 0.00015\n",
      "Epoch: 2349|steps:   60|Train Avg Loss: 0.8016 |Test Loss: 0.8434|lr = 0.00015\n",
      "Epoch: 2350|steps:   30|Train Avg Loss: 0.8050 |Test Loss: 0.8388|lr = 0.00015\n",
      "Epoch: 2350|steps:   60|Train Avg Loss: 0.8270 |Test Loss: 0.8428|lr = 0.00015\n",
      "Epoch: 2351|steps:   30|Train Avg Loss: 0.8201 |Test Loss: 0.8458|lr = 0.00015\n",
      "Epoch: 2351|steps:   60|Train Avg Loss: 0.8308 |Test Loss: 0.8474|lr = 0.00015\n",
      "Epoch: 2352|steps:   30|Train Avg Loss: 0.8135 |Test Loss: 0.8516|lr = 0.00015\n",
      "Epoch: 2352|steps:   60|Train Avg Loss: 0.8303 |Test Loss: 0.8392|lr = 0.00015\n",
      "Epoch: 2353|steps:   30|Train Avg Loss: 0.8229 |Test Loss: 0.8415|lr = 0.00015\n",
      "Epoch: 2353|steps:   60|Train Avg Loss: 0.8241 |Test Loss: 0.8410|lr = 0.00015\n",
      "Epoch: 2354|steps:   30|Train Avg Loss: 0.8117 |Test Loss: 0.8438|lr = 0.00015\n",
      "Epoch: 2354|steps:   60|Train Avg Loss: 0.8327 |Test Loss: 0.8432|lr = 0.00015\n",
      "Epoch: 2355|steps:   30|Train Avg Loss: 0.8190 |Test Loss: 0.8387|lr = 0.00015\n",
      "Epoch: 2355|steps:   60|Train Avg Loss: 0.8297 |Test Loss: 0.8472|lr = 0.00015\n",
      "Epoch: 2356|steps:   30|Train Avg Loss: 0.8357 |Test Loss: 0.8399|lr = 0.00015\n",
      "Epoch: 2356|steps:   60|Train Avg Loss: 0.8214 |Test Loss: 0.8442|lr = 0.00015\n",
      "Epoch: 2357|steps:   30|Train Avg Loss: 0.8040 |Test Loss: 0.8358|lr = 0.00015\n",
      "Epoch: 2357|steps:   60|Train Avg Loss: 0.8382 |Test Loss: 0.8445|lr = 0.00015\n",
      "Epoch: 2358|steps:   30|Train Avg Loss: 0.8179 |Test Loss: 0.8444|lr = 0.00015\n",
      "Epoch: 2358|steps:   60|Train Avg Loss: 0.8318 |Test Loss: 0.8467|lr = 0.00015\n",
      "Epoch: 2359|steps:   30|Train Avg Loss: 0.8541 |Test Loss: 0.8452|lr = 0.00015\n",
      "Epoch: 2359|steps:   60|Train Avg Loss: 0.7949 |Test Loss: 0.8396|lr = 0.00015\n",
      "Epoch: 2360|steps:   30|Train Avg Loss: 0.8397 |Test Loss: 0.8458|lr = 0.00015\n",
      "Epoch: 2360|steps:   60|Train Avg Loss: 0.8180 |Test Loss: 0.8500|lr = 0.00015\n",
      "Epoch: 2361|steps:   30|Train Avg Loss: 0.8311 |Test Loss: 0.8438|lr = 0.00015\n",
      "Epoch: 2361|steps:   60|Train Avg Loss: 0.8145 |Test Loss: 0.8445|lr = 0.00015\n",
      "Epoch: 2362|steps:   30|Train Avg Loss: 0.8158 |Test Loss: 0.8467|lr = 0.00015\n",
      "Epoch: 2362|steps:   60|Train Avg Loss: 0.8210 |Test Loss: 0.8457|lr = 0.00015\n",
      "Epoch: 2363|steps:   30|Train Avg Loss: 0.8234 |Test Loss: 0.8487|lr = 0.00015\n",
      "Epoch: 2363|steps:   60|Train Avg Loss: 0.8281 |Test Loss: 0.8430|lr = 0.00015\n",
      "Epoch: 2364|steps:   30|Train Avg Loss: 0.8290 |Test Loss: 0.8449|lr = 0.00015\n",
      "Epoch: 2364|steps:   60|Train Avg Loss: 0.8080 |Test Loss: 0.8425|lr = 0.00015\n",
      "Epoch: 2365|steps:   30|Train Avg Loss: 0.8412 |Test Loss: 0.8458|lr = 0.00015\n",
      "Epoch: 2365|steps:   60|Train Avg Loss: 0.8012 |Test Loss: 0.8407|lr = 0.00015\n",
      "Epoch: 2366|steps:   30|Train Avg Loss: 0.8143 |Test Loss: 0.8420|lr = 0.00015\n",
      "Epoch: 2366|steps:   60|Train Avg Loss: 0.8252 |Test Loss: 0.8473|lr = 0.00015\n",
      "Epoch: 2367|steps:   30|Train Avg Loss: 0.8234 |Test Loss: 0.8530|lr = 0.00015\n",
      "Epoch: 2367|steps:   60|Train Avg Loss: 0.8376 |Test Loss: 0.8450|lr = 0.00015\n",
      "Epoch: 2368|steps:   30|Train Avg Loss: 0.8209 |Test Loss: 0.8366|lr = 0.00015\n",
      "Epoch: 2368|steps:   60|Train Avg Loss: 0.8222 |Test Loss: 0.8482|lr = 0.00015\n",
      "Epoch: 2369|steps:   30|Train Avg Loss: 0.8030 |Test Loss: 0.8371|lr = 0.00015\n",
      "Epoch: 2369|steps:   60|Train Avg Loss: 0.8354 |Test Loss: 0.8439|lr = 0.00015\n",
      "Epoch: 2370|steps:   30|Train Avg Loss: 0.8146 |Test Loss: 0.8503|lr = 0.00015\n",
      "Epoch: 2370|steps:   60|Train Avg Loss: 0.8209 |Test Loss: 0.8365|lr = 0.00015\n",
      "Epoch: 2371|steps:   30|Train Avg Loss: 0.8120 |Test Loss: 0.8407|lr = 0.00015\n",
      "Epoch: 2371|steps:   60|Train Avg Loss: 0.8377 |Test Loss: 0.8405|lr = 0.00015\n",
      "Epoch: 2372|steps:   30|Train Avg Loss: 0.8153 |Test Loss: 0.8385|lr = 0.00015\n",
      "Epoch: 2372|steps:   60|Train Avg Loss: 0.8304 |Test Loss: 0.8429|lr = 0.00015\n",
      "Epoch: 2373|steps:   30|Train Avg Loss: 0.8271 |Test Loss: 0.8438|lr = 0.00014\n",
      "Epoch: 2373|steps:   60|Train Avg Loss: 0.8297 |Test Loss: 0.8449|lr = 0.00014\n",
      "Epoch: 2374|steps:   30|Train Avg Loss: 0.8028 |Test Loss: 0.8435|lr = 0.00014\n",
      "Epoch: 2374|steps:   60|Train Avg Loss: 0.8404 |Test Loss: 0.8452|lr = 0.00014\n",
      "Epoch: 2375|steps:   30|Train Avg Loss: 0.8226 |Test Loss: 0.8526|lr = 0.00014\n",
      "Epoch: 2375|steps:   60|Train Avg Loss: 0.8197 |Test Loss: 0.8427|lr = 0.00014\n",
      "Epoch: 2376|steps:   30|Train Avg Loss: 0.8210 |Test Loss: 0.8414|lr = 0.00014\n",
      "Epoch: 2376|steps:   60|Train Avg Loss: 0.8261 |Test Loss: 0.8478|lr = 0.00014\n",
      "Epoch: 2377|steps:   30|Train Avg Loss: 0.8007 |Test Loss: 0.8437|lr = 0.00014\n",
      "Epoch: 2377|steps:   60|Train Avg Loss: 0.8244 |Test Loss: 0.8419|lr = 0.00014\n",
      "Epoch: 2378|steps:   30|Train Avg Loss: 0.8258 |Test Loss: 0.8402|lr = 0.00014\n",
      "Epoch: 2378|steps:   60|Train Avg Loss: 0.7988 |Test Loss: 0.8408|lr = 0.00014\n",
      "Epoch: 2379|steps:   30|Train Avg Loss: 0.8076 |Test Loss: 0.8487|lr = 0.00014\n",
      "Epoch: 2379|steps:   60|Train Avg Loss: 0.8325 |Test Loss: 0.8436|lr = 0.00014\n",
      "Epoch: 2380|steps:   30|Train Avg Loss: 0.8207 |Test Loss: 0.8377|lr = 0.00014\n",
      "Epoch: 2380|steps:   60|Train Avg Loss: 0.8224 |Test Loss: 0.8459|lr = 0.00014\n",
      "Epoch: 2381|steps:   30|Train Avg Loss: 0.8185 |Test Loss: 0.8477|lr = 0.00014\n",
      "Epoch: 2381|steps:   60|Train Avg Loss: 0.8253 |Test Loss: 0.8464|lr = 0.00014\n",
      "Epoch: 2382|steps:   30|Train Avg Loss: 0.8240 |Test Loss: 0.8402|lr = 0.00014\n",
      "Epoch: 2382|steps:   60|Train Avg Loss: 0.8265 |Test Loss: 0.8476|lr = 0.00014\n",
      "Epoch: 2383|steps:   30|Train Avg Loss: 0.8119 |Test Loss: 0.8451|lr = 0.00014\n",
      "Epoch: 2383|steps:   60|Train Avg Loss: 0.8366 |Test Loss: 0.8484|lr = 0.00014\n",
      "Epoch: 2384|steps:   30|Train Avg Loss: 0.8292 |Test Loss: 0.8412|lr = 0.00014\n",
      "Epoch: 2384|steps:   60|Train Avg Loss: 0.8067 |Test Loss: 0.8493|lr = 0.00014\n",
      "Epoch: 2385|steps:   30|Train Avg Loss: 0.8049 |Test Loss: 0.8390|lr = 0.00014\n",
      "Epoch: 2385|steps:   60|Train Avg Loss: 0.8160 |Test Loss: 0.8489|lr = 0.00014\n",
      "Epoch: 2386|steps:   30|Train Avg Loss: 0.8176 |Test Loss: 0.8502|lr = 0.00014\n",
      "Epoch: 2386|steps:   60|Train Avg Loss: 0.8129 |Test Loss: 0.8509|lr = 0.00014\n",
      "Epoch: 2387|steps:   30|Train Avg Loss: 0.8191 |Test Loss: 0.8459|lr = 0.00014\n",
      "Epoch: 2387|steps:   60|Train Avg Loss: 0.8196 |Test Loss: 0.8452|lr = 0.00014\n",
      "Epoch: 2388|steps:   30|Train Avg Loss: 0.8102 |Test Loss: 0.8474|lr = 0.00014\n",
      "Epoch: 2388|steps:   60|Train Avg Loss: 0.8204 |Test Loss: 0.8448|lr = 0.00014\n",
      "Epoch: 2389|steps:   30|Train Avg Loss: 0.8279 |Test Loss: 0.8491|lr = 0.00014\n",
      "Epoch: 2389|steps:   60|Train Avg Loss: 0.8077 |Test Loss: 0.8467|lr = 0.00014\n",
      "Epoch: 2390|steps:   30|Train Avg Loss: 0.8204 |Test Loss: 0.8461|lr = 0.00014\n",
      "Epoch: 2390|steps:   60|Train Avg Loss: 0.8182 |Test Loss: 0.8477|lr = 0.00014\n",
      "Epoch: 2391|steps:   30|Train Avg Loss: 0.7928 |Test Loss: 0.8440|lr = 0.00014\n",
      "Epoch: 2391|steps:   60|Train Avg Loss: 0.8325 |Test Loss: 0.8477|lr = 0.00014\n",
      "Epoch: 2392|steps:   30|Train Avg Loss: 0.7992 |Test Loss: 0.8517|lr = 0.00014\n",
      "Epoch: 2392|steps:   60|Train Avg Loss: 0.8353 |Test Loss: 0.8418|lr = 0.00014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2393|steps:   30|Train Avg Loss: 0.8051 |Test Loss: 0.8475|lr = 0.00014\n",
      "Epoch: 2393|steps:   60|Train Avg Loss: 0.8178 |Test Loss: 0.8421|lr = 0.00014\n",
      "Epoch: 2394|steps:   30|Train Avg Loss: 0.8164 |Test Loss: 0.8485|lr = 0.00014\n",
      "Epoch: 2394|steps:   60|Train Avg Loss: 0.8127 |Test Loss: 0.8493|lr = 0.00014\n",
      "Epoch: 2395|steps:   30|Train Avg Loss: 0.8272 |Test Loss: 0.8416|lr = 0.00014\n",
      "Epoch: 2395|steps:   60|Train Avg Loss: 0.8108 |Test Loss: 0.8525|lr = 0.00014\n",
      "Epoch: 2396|steps:   30|Train Avg Loss: 0.8042 |Test Loss: 0.8405|lr = 0.00014\n",
      "Epoch: 2396|steps:   60|Train Avg Loss: 0.8310 |Test Loss: 0.8494|lr = 0.00014\n",
      "Epoch: 2397|steps:   30|Train Avg Loss: 0.8181 |Test Loss: 0.8505|lr = 0.00014\n",
      "Epoch: 2397|steps:   60|Train Avg Loss: 0.8131 |Test Loss: 0.8447|lr = 0.00014\n",
      "Epoch: 2398|steps:   30|Train Avg Loss: 0.8215 |Test Loss: 0.8552|lr = 0.00014\n",
      "Epoch: 2398|steps:   60|Train Avg Loss: 0.8044 |Test Loss: 0.8463|lr = 0.00014\n",
      "Epoch: 2399|steps:   30|Train Avg Loss: 0.8304 |Test Loss: 0.8480|lr = 0.00014\n",
      "Epoch: 2399|steps:   60|Train Avg Loss: 0.8015 |Test Loss: 0.8467|lr = 0.00014\n",
      "Epoch: 2400|steps:   30|Train Avg Loss: 0.8140 |Test Loss: 0.8535|lr = 0.00014\n",
      "Epoch: 2400|steps:   60|Train Avg Loss: 0.8115 |Test Loss: 0.8457|lr = 0.00014\n",
      "Epoch: 2401|steps:   30|Train Avg Loss: 0.8038 |Test Loss: 0.8446|lr = 0.00014\n",
      "Epoch: 2401|steps:   60|Train Avg Loss: 0.8278 |Test Loss: 0.8488|lr = 0.00014\n",
      "Epoch: 2402|steps:   30|Train Avg Loss: 0.8116 |Test Loss: 0.8431|lr = 0.00014\n",
      "Epoch: 2402|steps:   60|Train Avg Loss: 0.8180 |Test Loss: 0.8513|lr = 0.00014\n",
      "Epoch: 2403|steps:   30|Train Avg Loss: 0.8166 |Test Loss: 0.8542|lr = 0.00014\n",
      "Epoch: 2403|steps:   60|Train Avg Loss: 0.8129 |Test Loss: 0.8415|lr = 0.00014\n",
      "Epoch: 2404|steps:   30|Train Avg Loss: 0.8205 |Test Loss: 0.8450|lr = 0.00014\n",
      "Epoch: 2404|steps:   60|Train Avg Loss: 0.7993 |Test Loss: 0.8481|lr = 0.00014\n",
      "Epoch: 2405|steps:   30|Train Avg Loss: 0.7832 |Test Loss: 0.8475|lr = 0.00014\n",
      "Epoch: 2405|steps:   60|Train Avg Loss: 0.8450 |Test Loss: 0.8543|lr = 0.00014\n",
      "Epoch: 2406|steps:   30|Train Avg Loss: 0.7972 |Test Loss: 0.8462|lr = 0.00014\n",
      "Epoch: 2406|steps:   60|Train Avg Loss: 0.8329 |Test Loss: 0.8521|lr = 0.00014\n",
      "Epoch: 2407|steps:   30|Train Avg Loss: 0.8097 |Test Loss: 0.8483|lr = 0.00014\n",
      "Epoch: 2407|steps:   60|Train Avg Loss: 0.8196 |Test Loss: 0.8510|lr = 0.00014\n",
      "Epoch: 2408|steps:   30|Train Avg Loss: 0.8026 |Test Loss: 0.8466|lr = 0.00014\n",
      "Epoch: 2408|steps:   60|Train Avg Loss: 0.8126 |Test Loss: 0.8465|lr = 0.00014\n",
      "Epoch: 2409|steps:   30|Train Avg Loss: 0.8038 |Test Loss: 0.8493|lr = 0.00014\n",
      "Epoch: 2409|steps:   60|Train Avg Loss: 0.8203 |Test Loss: 0.8505|lr = 0.00014\n",
      "Epoch: 2410|steps:   30|Train Avg Loss: 0.8254 |Test Loss: 0.8524|lr = 0.00014\n",
      "Epoch: 2410|steps:   60|Train Avg Loss: 0.8015 |Test Loss: 0.8514|lr = 0.00014\n",
      "Epoch: 2411|steps:   30|Train Avg Loss: 0.8122 |Test Loss: 0.8485|lr = 0.00014\n",
      "Epoch: 2411|steps:   60|Train Avg Loss: 0.8294 |Test Loss: 0.8547|lr = 0.00014\n",
      "Epoch: 2412|steps:   30|Train Avg Loss: 0.8182 |Test Loss: 0.8504|lr = 0.00014\n",
      "Epoch: 2412|steps:   60|Train Avg Loss: 0.8086 |Test Loss: 0.8485|lr = 0.00014\n",
      "Epoch: 2413|steps:   30|Train Avg Loss: 0.8189 |Test Loss: 0.8462|lr = 0.00014\n",
      "Epoch: 2413|steps:   60|Train Avg Loss: 0.8114 |Test Loss: 0.8535|lr = 0.00014\n",
      "Epoch: 2414|steps:   30|Train Avg Loss: 0.8160 |Test Loss: 0.8545|lr = 0.00014\n",
      "Epoch: 2414|steps:   60|Train Avg Loss: 0.8261 |Test Loss: 0.8496|lr = 0.00014\n",
      "Epoch: 2415|steps:   30|Train Avg Loss: 0.8231 |Test Loss: 0.8467|lr = 0.00014\n",
      "Epoch: 2415|steps:   60|Train Avg Loss: 0.8010 |Test Loss: 0.8486|lr = 0.00014\n",
      "Epoch: 2416|steps:   30|Train Avg Loss: 0.7915 |Test Loss: 0.8502|lr = 0.00014\n",
      "Epoch: 2416|steps:   60|Train Avg Loss: 0.8449 |Test Loss: 0.8554|lr = 0.00014\n",
      "Epoch: 2417|steps:   30|Train Avg Loss: 0.8218 |Test Loss: 0.8503|lr = 0.00013\n",
      "Epoch: 2417|steps:   60|Train Avg Loss: 0.8071 |Test Loss: 0.8460|lr = 0.00013\n",
      "Epoch: 2418|steps:   30|Train Avg Loss: 0.7996 |Test Loss: 0.8433|lr = 0.00013\n",
      "Epoch: 2418|steps:   60|Train Avg Loss: 0.8172 |Test Loss: 0.8455|lr = 0.00013\n",
      "Epoch: 2419|steps:   30|Train Avg Loss: 0.7861 |Test Loss: 0.8563|lr = 0.00013\n",
      "Epoch: 2419|steps:   60|Train Avg Loss: 0.8343 |Test Loss: 0.8498|lr = 0.00013\n",
      "Epoch: 2420|steps:   30|Train Avg Loss: 0.8013 |Test Loss: 0.8442|lr = 0.00013\n",
      "Epoch: 2420|steps:   60|Train Avg Loss: 0.8230 |Test Loss: 0.8522|lr = 0.00013\n",
      "Epoch: 2421|steps:   30|Train Avg Loss: 0.7964 |Test Loss: 0.8439|lr = 0.00013\n",
      "Epoch: 2421|steps:   60|Train Avg Loss: 0.8262 |Test Loss: 0.8505|lr = 0.00013\n",
      "Epoch: 2422|steps:   30|Train Avg Loss: 0.7932 |Test Loss: 0.8495|lr = 0.00013\n",
      "Epoch: 2422|steps:   60|Train Avg Loss: 0.8205 |Test Loss: 0.8492|lr = 0.00013\n",
      "Epoch: 2423|steps:   30|Train Avg Loss: 0.8125 |Test Loss: 0.8534|lr = 0.00013\n",
      "Epoch: 2423|steps:   60|Train Avg Loss: 0.8233 |Test Loss: 0.8524|lr = 0.00013\n",
      "Epoch: 2424|steps:   30|Train Avg Loss: 0.8134 |Test Loss: 0.8506|lr = 0.00013\n",
      "Epoch: 2424|steps:   60|Train Avg Loss: 0.8010 |Test Loss: 0.8471|lr = 0.00013\n",
      "Epoch: 2425|steps:   30|Train Avg Loss: 0.8312 |Test Loss: 0.8508|lr = 0.00013\n",
      "Epoch: 2425|steps:   60|Train Avg Loss: 0.7945 |Test Loss: 0.8490|lr = 0.00013\n",
      "Epoch: 2426|steps:   30|Train Avg Loss: 0.7999 |Test Loss: 0.8457|lr = 0.00013\n",
      "Epoch: 2426|steps:   60|Train Avg Loss: 0.8277 |Test Loss: 0.8533|lr = 0.00013\n",
      "Epoch: 2427|steps:   30|Train Avg Loss: 0.8145 |Test Loss: 0.8447|lr = 0.00013\n",
      "Epoch: 2427|steps:   60|Train Avg Loss: 0.8087 |Test Loss: 0.8560|lr = 0.00013\n",
      "Epoch: 2428|steps:   30|Train Avg Loss: 0.8177 |Test Loss: 0.8485|lr = 0.00013\n",
      "Epoch: 2428|steps:   60|Train Avg Loss: 0.8102 |Test Loss: 0.8529|lr = 0.00013\n",
      "Epoch: 2429|steps:   30|Train Avg Loss: 0.7888 |Test Loss: 0.8490|lr = 0.00013\n",
      "Epoch: 2429|steps:   60|Train Avg Loss: 0.8289 |Test Loss: 0.8554|lr = 0.00013\n",
      "Epoch: 2430|steps:   30|Train Avg Loss: 0.8209 |Test Loss: 0.8526|lr = 0.00013\n",
      "Epoch: 2430|steps:   60|Train Avg Loss: 0.7970 |Test Loss: 0.8444|lr = 0.00013\n",
      "Epoch: 2431|steps:   30|Train Avg Loss: 0.8128 |Test Loss: 0.8570|lr = 0.00013\n",
      "Epoch: 2431|steps:   60|Train Avg Loss: 0.8133 |Test Loss: 0.8439|lr = 0.00013\n",
      "Epoch: 2432|steps:   30|Train Avg Loss: 0.8109 |Test Loss: 0.8497|lr = 0.00013\n",
      "Epoch: 2432|steps:   60|Train Avg Loss: 0.8146 |Test Loss: 0.8505|lr = 0.00013\n",
      "Epoch: 2433|steps:   30|Train Avg Loss: 0.8170 |Test Loss: 0.8605|lr = 0.00013\n",
      "Epoch: 2433|steps:   60|Train Avg Loss: 0.7965 |Test Loss: 0.8472|lr = 0.00013\n",
      "Epoch: 2434|steps:   30|Train Avg Loss: 0.7994 |Test Loss: 0.8502|lr = 0.00013\n",
      "Epoch: 2434|steps:   60|Train Avg Loss: 0.8179 |Test Loss: 0.8457|lr = 0.00013\n",
      "Epoch: 2435|steps:   30|Train Avg Loss: 0.7878 |Test Loss: 0.8501|lr = 0.00013\n",
      "Epoch: 2435|steps:   60|Train Avg Loss: 0.8330 |Test Loss: 0.8506|lr = 0.00013\n",
      "Epoch: 2436|steps:   30|Train Avg Loss: 0.8145 |Test Loss: 0.8581|lr = 0.00013\n",
      "Epoch: 2436|steps:   60|Train Avg Loss: 0.7971 |Test Loss: 0.8507|lr = 0.00013\n",
      "Epoch: 2437|steps:   30|Train Avg Loss: 0.8044 |Test Loss: 0.8478|lr = 0.00013\n",
      "Epoch: 2437|steps:   60|Train Avg Loss: 0.8266 |Test Loss: 0.8517|lr = 0.00013\n",
      "Epoch: 2438|steps:   30|Train Avg Loss: 0.8155 |Test Loss: 0.8476|lr = 0.00013\n",
      "Epoch: 2438|steps:   60|Train Avg Loss: 0.7960 |Test Loss: 0.8555|lr = 0.00013\n",
      "Epoch: 2439|steps:   30|Train Avg Loss: 0.8145 |Test Loss: 0.8513|lr = 0.00013\n",
      "Epoch: 2439|steps:   60|Train Avg Loss: 0.8049 |Test Loss: 0.8509|lr = 0.00013\n",
      "Epoch: 2440|steps:   30|Train Avg Loss: 0.7944 |Test Loss: 0.8497|lr = 0.00013\n",
      "Epoch: 2440|steps:   60|Train Avg Loss: 0.8141 |Test Loss: 0.8483|lr = 0.00013\n",
      "Epoch: 2441|steps:   30|Train Avg Loss: 0.7926 |Test Loss: 0.8472|lr = 0.00013\n",
      "Epoch: 2441|steps:   60|Train Avg Loss: 0.8262 |Test Loss: 0.8547|lr = 0.00013\n",
      "Epoch: 2442|steps:   30|Train Avg Loss: 0.7963 |Test Loss: 0.8526|lr = 0.00013\n",
      "Epoch: 2442|steps:   60|Train Avg Loss: 0.8155 |Test Loss: 0.8555|lr = 0.00013\n",
      "Epoch: 2443|steps:   30|Train Avg Loss: 0.8021 |Test Loss: 0.8476|lr = 0.00013\n",
      "Epoch: 2443|steps:   60|Train Avg Loss: 0.8188 |Test Loss: 0.8569|lr = 0.00013\n",
      "Epoch: 2444|steps:   30|Train Avg Loss: 0.7947 |Test Loss: 0.8492|lr = 0.00013\n",
      "Epoch: 2444|steps:   60|Train Avg Loss: 0.8150 |Test Loss: 0.8522|lr = 0.00013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2445|steps:   30|Train Avg Loss: 0.7874 |Test Loss: 0.8512|lr = 0.00013\n",
      "Epoch: 2445|steps:   60|Train Avg Loss: 0.8250 |Test Loss: 0.8570|lr = 0.00013\n",
      "Epoch: 2446|steps:   30|Train Avg Loss: 0.7878 |Test Loss: 0.8506|lr = 0.00013\n",
      "Epoch: 2446|steps:   60|Train Avg Loss: 0.8089 |Test Loss: 0.8503|lr = 0.00013\n",
      "Epoch: 2447|steps:   30|Train Avg Loss: 0.7969 |Test Loss: 0.8540|lr = 0.00013\n",
      "Epoch: 2447|steps:   60|Train Avg Loss: 0.8147 |Test Loss: 0.8509|lr = 0.00013\n",
      "Epoch: 2448|steps:   30|Train Avg Loss: 0.8216 |Test Loss: 0.8607|lr = 0.00013\n",
      "Epoch: 2448|steps:   60|Train Avg Loss: 0.8011 |Test Loss: 0.8463|lr = 0.00013\n",
      "Epoch: 2449|steps:   30|Train Avg Loss: 0.7844 |Test Loss: 0.8520|lr = 0.00013\n",
      "Epoch: 2449|steps:   60|Train Avg Loss: 0.8183 |Test Loss: 0.8493|lr = 0.00013\n",
      "Epoch: 2450|steps:   30|Train Avg Loss: 0.8024 |Test Loss: 0.8555|lr = 0.00012\n",
      "Epoch: 2450|steps:   60|Train Avg Loss: 0.8169 |Test Loss: 0.8527|lr = 0.00012\n",
      "Epoch: 2451|steps:   30|Train Avg Loss: 0.8021 |Test Loss: 0.8500|lr = 0.00012\n",
      "Epoch: 2451|steps:   60|Train Avg Loss: 0.8085 |Test Loss: 0.8464|lr = 0.00012\n",
      "Epoch: 2452|steps:   30|Train Avg Loss: 0.8232 |Test Loss: 0.8644|lr = 0.00012\n",
      "Epoch: 2452|steps:   60|Train Avg Loss: 0.7972 |Test Loss: 0.8542|lr = 0.00012\n",
      "Epoch: 2453|steps:   30|Train Avg Loss: 0.8029 |Test Loss: 0.8480|lr = 0.00012\n",
      "Epoch: 2453|steps:   60|Train Avg Loss: 0.8035 |Test Loss: 0.8549|lr = 0.00012\n",
      "Epoch: 2454|steps:   30|Train Avg Loss: 0.8155 |Test Loss: 0.8558|lr = 0.00012\n",
      "Epoch: 2454|steps:   60|Train Avg Loss: 0.7824 |Test Loss: 0.8549|lr = 0.00012\n",
      "Epoch: 2455|steps:   30|Train Avg Loss: 0.7907 |Test Loss: 0.8538|lr = 0.00012\n",
      "Epoch: 2455|steps:   60|Train Avg Loss: 0.8190 |Test Loss: 0.8562|lr = 0.00012\n",
      "Epoch: 2456|steps:   30|Train Avg Loss: 0.8142 |Test Loss: 0.8474|lr = 0.00012\n",
      "Epoch: 2456|steps:   60|Train Avg Loss: 0.7987 |Test Loss: 0.8598|lr = 0.00012\n",
      "Epoch: 2457|steps:   30|Train Avg Loss: 0.8173 |Test Loss: 0.8468|lr = 0.00012\n",
      "Epoch: 2457|steps:   60|Train Avg Loss: 0.7920 |Test Loss: 0.8559|lr = 0.00012\n",
      "Epoch: 2458|steps:   30|Train Avg Loss: 0.8128 |Test Loss: 0.8601|lr = 0.00012\n",
      "Epoch: 2458|steps:   60|Train Avg Loss: 0.7978 |Test Loss: 0.8560|lr = 0.00012\n",
      "Epoch: 2459|steps:   30|Train Avg Loss: 0.8082 |Test Loss: 0.8564|lr = 0.00012\n",
      "Epoch: 2459|steps:   60|Train Avg Loss: 0.8031 |Test Loss: 0.8522|lr = 0.00012\n",
      "Epoch: 2460|steps:   30|Train Avg Loss: 0.8047 |Test Loss: 0.8566|lr = 0.00012\n",
      "Epoch: 2460|steps:   60|Train Avg Loss: 0.8125 |Test Loss: 0.8567|lr = 0.00012\n",
      "Epoch: 2461|steps:   30|Train Avg Loss: 0.7961 |Test Loss: 0.8609|lr = 0.00012\n",
      "Epoch: 2461|steps:   60|Train Avg Loss: 0.7994 |Test Loss: 0.8537|lr = 0.00012\n",
      "Epoch: 2462|steps:   30|Train Avg Loss: 0.7854 |Test Loss: 0.8492|lr = 0.00012\n",
      "Epoch: 2462|steps:   60|Train Avg Loss: 0.8170 |Test Loss: 0.8509|lr = 0.00012\n",
      "Epoch: 2463|steps:   30|Train Avg Loss: 0.8422 |Test Loss: 0.8661|lr = 0.00012\n",
      "Epoch: 2463|steps:   60|Train Avg Loss: 0.7723 |Test Loss: 0.8559|lr = 0.00012\n",
      "Epoch: 2464|steps:   30|Train Avg Loss: 0.8003 |Test Loss: 0.8516|lr = 0.00012\n",
      "Epoch: 2464|steps:   60|Train Avg Loss: 0.8019 |Test Loss: 0.8592|lr = 0.00012\n",
      "Epoch: 2465|steps:   30|Train Avg Loss: 0.8128 |Test Loss: 0.8541|lr = 0.00012\n",
      "Epoch: 2465|steps:   60|Train Avg Loss: 0.7919 |Test Loss: 0.8480|lr = 0.00012\n",
      "Epoch: 2466|steps:   30|Train Avg Loss: 0.7972 |Test Loss: 0.8547|lr = 0.00012\n",
      "Epoch: 2466|steps:   60|Train Avg Loss: 0.8116 |Test Loss: 0.8554|lr = 0.00012\n",
      "Epoch: 2467|steps:   30|Train Avg Loss: 0.7877 |Test Loss: 0.8584|lr = 0.00012\n",
      "Epoch: 2467|steps:   60|Train Avg Loss: 0.8159 |Test Loss: 0.8588|lr = 0.00012\n",
      "Epoch: 2468|steps:   30|Train Avg Loss: 0.7926 |Test Loss: 0.8522|lr = 0.00012\n",
      "Epoch: 2468|steps:   60|Train Avg Loss: 0.8205 |Test Loss: 0.8597|lr = 0.00012\n",
      "Epoch: 2469|steps:   30|Train Avg Loss: 0.8062 |Test Loss: 0.8576|lr = 0.00012\n",
      "Epoch: 2469|steps:   60|Train Avg Loss: 0.8049 |Test Loss: 0.8436|lr = 0.00012\n",
      "Epoch: 2470|steps:   30|Train Avg Loss: 0.8173 |Test Loss: 0.8558|lr = 0.00012\n",
      "Epoch: 2470|steps:   60|Train Avg Loss: 0.8048 |Test Loss: 0.8599|lr = 0.00012\n",
      "Epoch: 2471|steps:   30|Train Avg Loss: 0.8154 |Test Loss: 0.8494|lr = 0.00012\n",
      "Epoch: 2471|steps:   60|Train Avg Loss: 0.7976 |Test Loss: 0.8557|lr = 0.00012\n",
      "Epoch: 2472|steps:   30|Train Avg Loss: 0.8027 |Test Loss: 0.8545|lr = 0.00012\n",
      "Epoch: 2472|steps:   60|Train Avg Loss: 0.8042 |Test Loss: 0.8537|lr = 0.00012\n",
      "Epoch: 2473|steps:   30|Train Avg Loss: 0.8212 |Test Loss: 0.8538|lr = 0.00012\n",
      "Epoch: 2473|steps:   60|Train Avg Loss: 0.7882 |Test Loss: 0.8577|lr = 0.00012\n",
      "Epoch: 2474|steps:   30|Train Avg Loss: 0.7947 |Test Loss: 0.8559|lr = 0.00012\n",
      "Epoch: 2474|steps:   60|Train Avg Loss: 0.8171 |Test Loss: 0.8622|lr = 0.00012\n",
      "Epoch: 2475|steps:   30|Train Avg Loss: 0.8096 |Test Loss: 0.8560|lr = 0.00012\n",
      "Epoch: 2475|steps:   60|Train Avg Loss: 0.8000 |Test Loss: 0.8535|lr = 0.00012\n",
      "Epoch: 2476|steps:   30|Train Avg Loss: 0.8130 |Test Loss: 0.8521|lr = 0.00012\n",
      "Epoch: 2476|steps:   60|Train Avg Loss: 0.8026 |Test Loss: 0.8423|lr = 0.00012\n",
      "Epoch: 2477|steps:   30|Train Avg Loss: 0.7962 |Test Loss: 0.8525|lr = 0.00012\n",
      "Epoch: 2477|steps:   60|Train Avg Loss: 0.8076 |Test Loss: 0.8567|lr = 0.00012\n",
      "Epoch: 2478|steps:   30|Train Avg Loss: 0.7940 |Test Loss: 0.8551|lr = 0.00012\n",
      "Epoch: 2478|steps:   60|Train Avg Loss: 0.8158 |Test Loss: 0.8554|lr = 0.00012\n",
      "Epoch: 2479|steps:   30|Train Avg Loss: 0.8238 |Test Loss: 0.8664|lr = 0.00012\n",
      "Epoch: 2479|steps:   60|Train Avg Loss: 0.7765 |Test Loss: 0.8467|lr = 0.00012\n",
      "Epoch: 2480|steps:   30|Train Avg Loss: 0.7955 |Test Loss: 0.8460|lr = 0.00012\n",
      "Epoch: 2480|steps:   60|Train Avg Loss: 0.8141 |Test Loss: 0.8578|lr = 0.00012\n",
      "Epoch: 2481|steps:   30|Train Avg Loss: 0.8091 |Test Loss: 0.8629|lr = 0.00012\n",
      "Epoch: 2481|steps:   60|Train Avg Loss: 0.7998 |Test Loss: 0.8545|lr = 0.00012\n",
      "Epoch: 2482|steps:   30|Train Avg Loss: 0.7970 |Test Loss: 0.8559|lr = 0.00012\n",
      "Epoch: 2482|steps:   60|Train Avg Loss: 0.8047 |Test Loss: 0.8552|lr = 0.00012\n",
      "Epoch: 2483|steps:   30|Train Avg Loss: 0.7971 |Test Loss: 0.8565|lr = 0.00012\n",
      "Epoch: 2483|steps:   60|Train Avg Loss: 0.7973 |Test Loss: 0.8601|lr = 0.00012\n",
      "Epoch: 2484|steps:   30|Train Avg Loss: 0.8094 |Test Loss: 0.8687|lr = 0.00012\n",
      "Epoch: 2484|steps:   60|Train Avg Loss: 0.8021 |Test Loss: 0.8501|lr = 0.00012\n",
      "Epoch: 2485|steps:   30|Train Avg Loss: 0.7961 |Test Loss: 0.8534|lr = 0.00012\n",
      "Epoch: 2485|steps:   60|Train Avg Loss: 0.8117 |Test Loss: 0.8537|lr = 0.00012\n",
      "Epoch: 2486|steps:   30|Train Avg Loss: 0.7802 |Test Loss: 0.8589|lr = 0.00012\n",
      "Epoch: 2486|steps:   60|Train Avg Loss: 0.8122 |Test Loss: 0.8573|lr = 0.00012\n",
      "Epoch: 2487|steps:   30|Train Avg Loss: 0.8099 |Test Loss: 0.8549|lr = 0.00012\n",
      "Epoch: 2487|steps:   60|Train Avg Loss: 0.7912 |Test Loss: 0.8569|lr = 0.00012\n",
      "Epoch: 2488|steps:   30|Train Avg Loss: 0.8028 |Test Loss: 0.8626|lr = 0.00012\n",
      "Epoch: 2488|steps:   60|Train Avg Loss: 0.8039 |Test Loss: 0.8562|lr = 0.00012\n",
      "Epoch: 2489|steps:   30|Train Avg Loss: 0.8037 |Test Loss: 0.8620|lr = 0.00012\n",
      "Epoch: 2489|steps:   60|Train Avg Loss: 0.8144 |Test Loss: 0.8570|lr = 0.00012\n",
      "Epoch: 2490|steps:   30|Train Avg Loss: 0.8012 |Test Loss: 0.8587|lr = 0.00012\n",
      "Epoch: 2490|steps:   60|Train Avg Loss: 0.8062 |Test Loss: 0.8580|lr = 0.00012\n",
      "Epoch: 2491|steps:   30|Train Avg Loss: 0.7774 |Test Loss: 0.8612|lr = 0.00012\n",
      "Epoch: 2491|steps:   60|Train Avg Loss: 0.8069 |Test Loss: 0.8537|lr = 0.00012\n",
      "Epoch: 2492|steps:   30|Train Avg Loss: 0.7907 |Test Loss: 0.8617|lr = 0.00012\n",
      "Epoch: 2492|steps:   60|Train Avg Loss: 0.8093 |Test Loss: 0.8512|lr = 0.00012\n",
      "Epoch: 2493|steps:   30|Train Avg Loss: 0.8027 |Test Loss: 0.8566|lr = 0.00012\n",
      "Epoch: 2493|steps:   60|Train Avg Loss: 0.7953 |Test Loss: 0.8551|lr = 0.00012\n",
      "Epoch: 2494|steps:   30|Train Avg Loss: 0.7822 |Test Loss: 0.8578|lr = 0.00012\n",
      "Epoch: 2494|steps:   60|Train Avg Loss: 0.8158 |Test Loss: 0.8601|lr = 0.00012\n",
      "Epoch: 2495|steps:   30|Train Avg Loss: 0.7986 |Test Loss: 0.8591|lr = 0.00012\n",
      "Epoch: 2495|steps:   60|Train Avg Loss: 0.7987 |Test Loss: 0.8651|lr = 0.00012\n",
      "Epoch: 2496|steps:   30|Train Avg Loss: 0.8297 |Test Loss: 0.8607|lr = 0.00012\n",
      "Epoch: 2496|steps:   60|Train Avg Loss: 0.7644 |Test Loss: 0.8475|lr = 0.00012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2497|steps:   30|Train Avg Loss: 0.8098 |Test Loss: 0.8574|lr = 0.00012\n",
      "Epoch: 2497|steps:   60|Train Avg Loss: 0.7732 |Test Loss: 0.8581|lr = 0.00012\n",
      "Epoch: 2498|steps:   30|Train Avg Loss: 0.8078 |Test Loss: 0.8614|lr = 0.00012\n",
      "Epoch: 2498|steps:   60|Train Avg Loss: 0.7902 |Test Loss: 0.8608|lr = 0.00012\n",
      "Epoch: 2499|steps:   30|Train Avg Loss: 0.8183 |Test Loss: 0.8666|lr = 0.00012\n",
      "Epoch: 2499|steps:   60|Train Avg Loss: 0.7864 |Test Loss: 0.8601|lr = 0.00012\n",
      "Epoch: 2500|steps:   30|Train Avg Loss: 0.7937 |Test Loss: 0.8568|lr = 0.00012\n",
      "Epoch: 2500|steps:   60|Train Avg Loss: 0.7937 |Test Loss: 0.8540|lr = 0.00012\n",
      "Epoch: 2501|steps:   30|Train Avg Loss: 0.7732 |Test Loss: 0.8537|lr = 0.00012\n",
      "Epoch: 2501|steps:   60|Train Avg Loss: 0.8227 |Test Loss: 0.8563|lr = 0.00012\n",
      "Epoch: 2502|steps:   30|Train Avg Loss: 0.7847 |Test Loss: 0.8563|lr = 0.00012\n",
      "Epoch: 2502|steps:   60|Train Avg Loss: 0.8098 |Test Loss: 0.8553|lr = 0.00012\n",
      "Epoch: 2503|steps:   30|Train Avg Loss: 0.7829 |Test Loss: 0.8549|lr = 0.00012\n",
      "Epoch: 2503|steps:   60|Train Avg Loss: 0.8205 |Test Loss: 0.8618|lr = 0.00012\n",
      "Epoch: 2504|steps:   30|Train Avg Loss: 0.7861 |Test Loss: 0.8569|lr = 0.00012\n",
      "Epoch: 2504|steps:   60|Train Avg Loss: 0.8041 |Test Loss: 0.8620|lr = 0.00012\n",
      "Epoch: 2505|steps:   30|Train Avg Loss: 0.8082 |Test Loss: 0.8603|lr = 0.00011\n",
      "Epoch: 2505|steps:   60|Train Avg Loss: 0.7773 |Test Loss: 0.8646|lr = 0.00011\n",
      "Epoch: 2506|steps:   30|Train Avg Loss: 0.7737 |Test Loss: 0.8515|lr = 0.00011\n",
      "Epoch: 2506|steps:   60|Train Avg Loss: 0.8091 |Test Loss: 0.8612|lr = 0.00011\n",
      "Epoch: 2507|steps:   30|Train Avg Loss: 0.7809 |Test Loss: 0.8616|lr = 0.00011\n",
      "Epoch: 2507|steps:   60|Train Avg Loss: 0.7944 |Test Loss: 0.8530|lr = 0.00011\n",
      "Epoch: 2508|steps:   30|Train Avg Loss: 0.7938 |Test Loss: 0.8609|lr = 0.00011\n",
      "Epoch: 2508|steps:   60|Train Avg Loss: 0.8059 |Test Loss: 0.8686|lr = 0.00011\n",
      "Epoch: 2509|steps:   30|Train Avg Loss: 0.8083 |Test Loss: 0.8561|lr = 0.00011\n",
      "Epoch: 2509|steps:   60|Train Avg Loss: 0.7857 |Test Loss: 0.8555|lr = 0.00011\n",
      "Epoch: 2510|steps:   30|Train Avg Loss: 0.7856 |Test Loss: 0.8546|lr = 0.00011\n",
      "Epoch: 2510|steps:   60|Train Avg Loss: 0.8076 |Test Loss: 0.8641|lr = 0.00011\n",
      "Epoch: 2511|steps:   30|Train Avg Loss: 0.7831 |Test Loss: 0.8633|lr = 0.00011\n",
      "Epoch: 2511|steps:   60|Train Avg Loss: 0.8090 |Test Loss: 0.8609|lr = 0.00011\n",
      "Epoch: 2512|steps:   30|Train Avg Loss: 0.7740 |Test Loss: 0.8550|lr = 0.00011\n",
      "Epoch: 2512|steps:   60|Train Avg Loss: 0.8223 |Test Loss: 0.8601|lr = 0.00011\n",
      "Epoch: 2513|steps:   30|Train Avg Loss: 0.7945 |Test Loss: 0.8662|lr = 0.00011\n",
      "Epoch: 2513|steps:   60|Train Avg Loss: 0.7960 |Test Loss: 0.8528|lr = 0.00011\n",
      "Epoch: 2514|steps:   30|Train Avg Loss: 0.7958 |Test Loss: 0.8593|lr = 0.00011\n",
      "Epoch: 2514|steps:   60|Train Avg Loss: 0.7884 |Test Loss: 0.8588|lr = 0.00011\n",
      "Epoch: 2515|steps:   30|Train Avg Loss: 0.7960 |Test Loss: 0.8614|lr = 0.00011\n",
      "Epoch: 2515|steps:   60|Train Avg Loss: 0.7837 |Test Loss: 0.8534|lr = 0.00011\n",
      "Epoch: 2516|steps:   30|Train Avg Loss: 0.7904 |Test Loss: 0.8578|lr = 0.00011\n",
      "Epoch: 2516|steps:   60|Train Avg Loss: 0.7970 |Test Loss: 0.8593|lr = 0.00011\n",
      "Epoch: 2517|steps:   30|Train Avg Loss: 0.7903 |Test Loss: 0.8521|lr = 0.00011\n",
      "Epoch: 2517|steps:   60|Train Avg Loss: 0.7961 |Test Loss: 0.8653|lr = 0.00011\n",
      "Epoch: 2518|steps:   30|Train Avg Loss: 0.7967 |Test Loss: 0.8631|lr = 0.00011\n",
      "Epoch: 2518|steps:   60|Train Avg Loss: 0.7823 |Test Loss: 0.8469|lr = 0.00011\n",
      "Epoch: 2519|steps:   30|Train Avg Loss: 0.7778 |Test Loss: 0.8564|lr = 0.00011\n",
      "Epoch: 2519|steps:   60|Train Avg Loss: 0.8004 |Test Loss: 0.8663|lr = 0.00011\n",
      "Epoch: 2520|steps:   30|Train Avg Loss: 0.7958 |Test Loss: 0.8679|lr = 0.00011\n",
      "Epoch: 2520|steps:   60|Train Avg Loss: 0.7877 |Test Loss: 0.8620|lr = 0.00011\n",
      "Epoch: 2521|steps:   30|Train Avg Loss: 0.7686 |Test Loss: 0.8562|lr = 0.00011\n",
      "Epoch: 2521|steps:   60|Train Avg Loss: 0.8322 |Test Loss: 0.8568|lr = 0.00011\n",
      "Epoch: 2522|steps:   30|Train Avg Loss: 0.7913 |Test Loss: 0.8633|lr = 0.00011\n",
      "Epoch: 2522|steps:   60|Train Avg Loss: 0.8011 |Test Loss: 0.8635|lr = 0.00011\n",
      "Epoch: 2523|steps:   30|Train Avg Loss: 0.7949 |Test Loss: 0.8532|lr = 0.00011\n",
      "Epoch: 2523|steps:   60|Train Avg Loss: 0.7918 |Test Loss: 0.8667|lr = 0.00011\n",
      "Epoch: 2524|steps:   30|Train Avg Loss: 0.8033 |Test Loss: 0.8633|lr = 0.00011\n",
      "Epoch: 2524|steps:   60|Train Avg Loss: 0.7849 |Test Loss: 0.8591|lr = 0.00011\n",
      "Epoch: 2525|steps:   30|Train Avg Loss: 0.7841 |Test Loss: 0.8615|lr = 0.00011\n",
      "Epoch: 2525|steps:   60|Train Avg Loss: 0.7950 |Test Loss: 0.8620|lr = 0.00011\n",
      "Epoch: 2526|steps:   30|Train Avg Loss: 0.7902 |Test Loss: 0.8555|lr = 0.00011\n",
      "Epoch: 2526|steps:   60|Train Avg Loss: 0.7997 |Test Loss: 0.8564|lr = 0.00011\n",
      "Epoch: 2527|steps:   30|Train Avg Loss: 0.7705 |Test Loss: 0.8618|lr = 0.00011\n",
      "Epoch: 2527|steps:   60|Train Avg Loss: 0.8087 |Test Loss: 0.8596|lr = 0.00011\n",
      "Epoch: 2528|steps:   30|Train Avg Loss: 0.7911 |Test Loss: 0.8541|lr = 0.00011\n",
      "Epoch: 2528|steps:   60|Train Avg Loss: 0.7879 |Test Loss: 0.8583|lr = 0.00011\n",
      "Epoch: 2529|steps:   30|Train Avg Loss: 0.8024 |Test Loss: 0.8733|lr = 0.00011\n",
      "Epoch: 2529|steps:   60|Train Avg Loss: 0.7815 |Test Loss: 0.8493|lr = 0.00011\n",
      "Epoch: 2530|steps:   30|Train Avg Loss: 0.7890 |Test Loss: 0.8694|lr = 0.00011\n",
      "Epoch: 2530|steps:   60|Train Avg Loss: 0.7889 |Test Loss: 0.8608|lr = 0.00011\n",
      "Epoch: 2531|steps:   30|Train Avg Loss: 0.7814 |Test Loss: 0.8562|lr = 0.00011\n",
      "Epoch: 2531|steps:   60|Train Avg Loss: 0.7971 |Test Loss: 0.8599|lr = 0.00011\n",
      "Epoch: 2532|steps:   30|Train Avg Loss: 0.7775 |Test Loss: 0.8625|lr = 0.00011\n",
      "Epoch: 2532|steps:   60|Train Avg Loss: 0.7948 |Test Loss: 0.8586|lr = 0.00011\n",
      "Epoch: 2533|steps:   30|Train Avg Loss: 0.7889 |Test Loss: 0.8597|lr = 0.00011\n",
      "Epoch: 2533|steps:   60|Train Avg Loss: 0.7867 |Test Loss: 0.8623|lr = 0.00011\n",
      "Epoch: 2534|steps:   30|Train Avg Loss: 0.7952 |Test Loss: 0.8597|lr = 0.00011\n",
      "Epoch: 2534|steps:   60|Train Avg Loss: 0.7893 |Test Loss: 0.8615|lr = 0.00011\n",
      "Epoch: 2535|steps:   30|Train Avg Loss: 0.8010 |Test Loss: 0.8644|lr = 0.00011\n",
      "Epoch: 2535|steps:   60|Train Avg Loss: 0.7821 |Test Loss: 0.8506|lr = 0.00011\n",
      "Epoch: 2536|steps:   30|Train Avg Loss: 0.8045 |Test Loss: 0.8658|lr = 0.00011\n",
      "Epoch: 2536|steps:   60|Train Avg Loss: 0.7822 |Test Loss: 0.8602|lr = 0.00011\n",
      "Epoch: 2537|steps:   30|Train Avg Loss: 0.8067 |Test Loss: 0.8689|lr = 0.00011\n",
      "Epoch: 2537|steps:   60|Train Avg Loss: 0.7800 |Test Loss: 0.8631|lr = 0.00011\n",
      "Epoch: 2538|steps:   30|Train Avg Loss: 0.7731 |Test Loss: 0.8611|lr = 0.00011\n",
      "Epoch: 2538|steps:   60|Train Avg Loss: 0.8003 |Test Loss: 0.8629|lr = 0.00011\n",
      "Epoch: 2539|steps:   30|Train Avg Loss: 0.7827 |Test Loss: 0.8648|lr = 0.00011\n",
      "Epoch: 2539|steps:   60|Train Avg Loss: 0.8114 |Test Loss: 0.8665|lr = 0.00011\n",
      "Epoch: 2540|steps:   30|Train Avg Loss: 0.7802 |Test Loss: 0.8557|lr = 0.00011\n",
      "Epoch: 2540|steps:   60|Train Avg Loss: 0.7876 |Test Loss: 0.8615|lr = 0.00011\n",
      "Epoch: 2541|steps:   30|Train Avg Loss: 0.7833 |Test Loss: 0.8609|lr = 0.00011\n",
      "Epoch: 2541|steps:   60|Train Avg Loss: 0.7959 |Test Loss: 0.8655|lr = 0.00011\n",
      "Epoch: 2542|steps:   30|Train Avg Loss: 0.7746 |Test Loss: 0.8575|lr = 0.00011\n",
      "Epoch: 2542|steps:   60|Train Avg Loss: 0.8036 |Test Loss: 0.8692|lr = 0.00011\n",
      "Epoch: 2543|steps:   30|Train Avg Loss: 0.7688 |Test Loss: 0.8483|lr = 0.00011\n",
      "Epoch: 2543|steps:   60|Train Avg Loss: 0.8257 |Test Loss: 0.8663|lr = 0.00011\n",
      "Epoch: 2544|steps:   30|Train Avg Loss: 0.7830 |Test Loss: 0.8643|lr = 0.00011\n",
      "Epoch: 2544|steps:   60|Train Avg Loss: 0.8012 |Test Loss: 0.8622|lr = 0.00011\n",
      "Epoch: 2545|steps:   30|Train Avg Loss: 0.7665 |Test Loss: 0.8631|lr = 0.00011\n",
      "Epoch: 2545|steps:   60|Train Avg Loss: 0.8042 |Test Loss: 0.8652|lr = 0.00011\n",
      "Epoch: 2546|steps:   30|Train Avg Loss: 0.7742 |Test Loss: 0.8458|lr = 0.00011\n",
      "Epoch: 2546|steps:   60|Train Avg Loss: 0.8084 |Test Loss: 0.8711|lr = 0.00011\n",
      "Epoch: 2547|steps:   30|Train Avg Loss: 0.7852 |Test Loss: 0.8648|lr = 0.00011\n",
      "Epoch: 2547|steps:   60|Train Avg Loss: 0.7806 |Test Loss: 0.8548|lr = 0.00011\n",
      "Epoch: 2548|steps:   30|Train Avg Loss: 0.8084 |Test Loss: 0.8726|lr = 0.00011\n",
      "Epoch: 2548|steps:   60|Train Avg Loss: 0.7784 |Test Loss: 0.8622|lr = 0.00011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2549|steps:   30|Train Avg Loss: 0.7937 |Test Loss: 0.8559|lr = 0.00011\n",
      "Epoch: 2549|steps:   60|Train Avg Loss: 0.7949 |Test Loss: 0.8655|lr = 0.00011\n",
      "Epoch: 2550|steps:   30|Train Avg Loss: 0.7686 |Test Loss: 0.8594|lr = 0.00011\n",
      "Epoch: 2550|steps:   60|Train Avg Loss: 0.8132 |Test Loss: 0.8636|lr = 0.00011\n",
      "Epoch: 2551|steps:   30|Train Avg Loss: 0.7777 |Test Loss: 0.8569|lr = 0.00011\n",
      "Epoch: 2551|steps:   60|Train Avg Loss: 0.7911 |Test Loss: 0.8644|lr = 0.00011\n",
      "Epoch: 2552|steps:   30|Train Avg Loss: 0.7738 |Test Loss: 0.8614|lr = 0.00011\n",
      "Epoch: 2552|steps:   60|Train Avg Loss: 0.7997 |Test Loss: 0.8569|lr = 0.00011\n",
      "Epoch: 2553|steps:   30|Train Avg Loss: 0.7960 |Test Loss: 0.8650|lr = 0.00011\n",
      "Epoch: 2553|steps:   60|Train Avg Loss: 0.7832 |Test Loss: 0.8639|lr = 0.00011\n",
      "Epoch: 2554|steps:   30|Train Avg Loss: 0.7741 |Test Loss: 0.8528|lr = 0.00011\n",
      "Epoch: 2554|steps:   60|Train Avg Loss: 0.7919 |Test Loss: 0.8523|lr = 0.00011\n",
      "Epoch: 2555|steps:   30|Train Avg Loss: 0.7566 |Test Loss: 0.8653|lr = 0.00011\n",
      "Epoch: 2555|steps:   60|Train Avg Loss: 0.8168 |Test Loss: 0.8657|lr = 0.00011\n",
      "Epoch: 2556|steps:   30|Train Avg Loss: 0.7948 |Test Loss: 0.8734|lr = 0.00011\n",
      "Epoch: 2556|steps:   60|Train Avg Loss: 0.7823 |Test Loss: 0.8597|lr = 0.00011\n",
      "Epoch: 2557|steps:   30|Train Avg Loss: 0.7750 |Test Loss: 0.8649|lr = 0.00011\n",
      "Epoch: 2557|steps:   60|Train Avg Loss: 0.7944 |Test Loss: 0.8534|lr = 0.00011\n",
      "Epoch: 2558|steps:   30|Train Avg Loss: 0.7933 |Test Loss: 0.8633|lr = 0.00011\n",
      "Epoch: 2558|steps:   60|Train Avg Loss: 0.7754 |Test Loss: 0.8586|lr = 0.00011\n",
      "Epoch: 2559|steps:   30|Train Avg Loss: 0.7852 |Test Loss: 0.8659|lr = 0.00011\n",
      "Epoch: 2559|steps:   60|Train Avg Loss: 0.7861 |Test Loss: 0.8664|lr = 0.00011\n",
      "Epoch: 2560|steps:   30|Train Avg Loss: 0.7884 |Test Loss: 0.8562|lr = 0.00011\n",
      "Epoch: 2560|steps:   60|Train Avg Loss: 0.7797 |Test Loss: 0.8620|lr = 0.00011\n",
      "Epoch: 2561|steps:   30|Train Avg Loss: 0.7674 |Test Loss: 0.8601|lr = 0.00011\n",
      "Epoch: 2561|steps:   60|Train Avg Loss: 0.8082 |Test Loss: 0.8682|lr = 0.00011\n",
      "Epoch: 2562|steps:   30|Train Avg Loss: 0.7757 |Test Loss: 0.8607|lr = 0.00010\n",
      "Epoch: 2562|steps:   60|Train Avg Loss: 0.7858 |Test Loss: 0.8697|lr = 0.00010\n",
      "Epoch: 2563|steps:   30|Train Avg Loss: 0.7916 |Test Loss: 0.8693|lr = 0.00010\n",
      "Epoch: 2563|steps:   60|Train Avg Loss: 0.7647 |Test Loss: 0.8594|lr = 0.00010\n",
      "Epoch: 2564|steps:   30|Train Avg Loss: 0.7553 |Test Loss: 0.8568|lr = 0.00010\n",
      "Epoch: 2564|steps:   60|Train Avg Loss: 0.7859 |Test Loss: 0.8698|lr = 0.00010\n",
      "Epoch: 2565|steps:   30|Train Avg Loss: 0.7835 |Test Loss: 0.8635|lr = 0.00010\n",
      "Epoch: 2565|steps:   60|Train Avg Loss: 0.7928 |Test Loss: 0.8740|lr = 0.00010\n",
      "Epoch: 2566|steps:   30|Train Avg Loss: 0.7760 |Test Loss: 0.8665|lr = 0.00010\n",
      "Epoch: 2566|steps:   60|Train Avg Loss: 0.7852 |Test Loss: 0.8653|lr = 0.00010\n",
      "Epoch: 2567|steps:   30|Train Avg Loss: 0.7650 |Test Loss: 0.8629|lr = 0.00010\n",
      "Epoch: 2567|steps:   60|Train Avg Loss: 0.7965 |Test Loss: 0.8580|lr = 0.00010\n",
      "Epoch: 2568|steps:   30|Train Avg Loss: 0.8103 |Test Loss: 0.8674|lr = 0.00010\n",
      "Epoch: 2568|steps:   60|Train Avg Loss: 0.7501 |Test Loss: 0.8547|lr = 0.00010\n",
      "Epoch: 2569|steps:   30|Train Avg Loss: 0.7944 |Test Loss: 0.8640|lr = 0.00010\n",
      "Epoch: 2569|steps:   60|Train Avg Loss: 0.7774 |Test Loss: 0.8632|lr = 0.00010\n",
      "Epoch: 2570|steps:   30|Train Avg Loss: 0.7703 |Test Loss: 0.8557|lr = 0.00010\n",
      "Epoch: 2570|steps:   60|Train Avg Loss: 0.7924 |Test Loss: 0.8723|lr = 0.00010\n",
      "Epoch: 2571|steps:   30|Train Avg Loss: 0.7714 |Test Loss: 0.8652|lr = 0.00010\n",
      "Epoch: 2571|steps:   60|Train Avg Loss: 0.7896 |Test Loss: 0.8578|lr = 0.00010\n",
      "Epoch: 2572|steps:   30|Train Avg Loss: 0.7809 |Test Loss: 0.8613|lr = 0.00010\n",
      "Epoch: 2572|steps:   60|Train Avg Loss: 0.7865 |Test Loss: 0.8716|lr = 0.00010\n",
      "Epoch: 2573|steps:   30|Train Avg Loss: 0.7807 |Test Loss: 0.8647|lr = 0.00010\n",
      "Epoch: 2573|steps:   60|Train Avg Loss: 0.7976 |Test Loss: 0.8749|lr = 0.00010\n",
      "Epoch: 2574|steps:   30|Train Avg Loss: 0.7919 |Test Loss: 0.8638|lr = 0.00010\n",
      "Epoch: 2574|steps:   60|Train Avg Loss: 0.7735 |Test Loss: 0.8760|lr = 0.00010\n",
      "Epoch: 2575|steps:   30|Train Avg Loss: 0.7749 |Test Loss: 0.8612|lr = 0.00010\n",
      "Epoch: 2575|steps:   60|Train Avg Loss: 0.7844 |Test Loss: 0.8614|lr = 0.00010\n",
      "Epoch: 2576|steps:   30|Train Avg Loss: 0.7666 |Test Loss: 0.8664|lr = 0.00010\n",
      "Epoch: 2576|steps:   60|Train Avg Loss: 0.7802 |Test Loss: 0.8580|lr = 0.00010\n",
      "Epoch: 2577|steps:   30|Train Avg Loss: 0.7612 |Test Loss: 0.8609|lr = 0.00010\n",
      "Epoch: 2577|steps:   60|Train Avg Loss: 0.7972 |Test Loss: 0.8634|lr = 0.00010\n",
      "Epoch: 2578|steps:   30|Train Avg Loss: 0.7692 |Test Loss: 0.8579|lr = 0.00010\n",
      "Epoch: 2578|steps:   60|Train Avg Loss: 0.7922 |Test Loss: 0.8669|lr = 0.00010\n",
      "Epoch: 2579|steps:   30|Train Avg Loss: 0.7764 |Test Loss: 0.8720|lr = 0.00010\n",
      "Epoch: 2579|steps:   60|Train Avg Loss: 0.7722 |Test Loss: 0.8587|lr = 0.00010\n",
      "Epoch: 2580|steps:   30|Train Avg Loss: 0.7800 |Test Loss: 0.8599|lr = 0.00010\n",
      "Epoch: 2580|steps:   60|Train Avg Loss: 0.7880 |Test Loss: 0.8690|lr = 0.00010\n",
      "Epoch: 2581|steps:   30|Train Avg Loss: 0.7761 |Test Loss: 0.8645|lr = 0.00010\n",
      "Epoch: 2581|steps:   60|Train Avg Loss: 0.7784 |Test Loss: 0.8675|lr = 0.00010\n",
      "Epoch: 2582|steps:   30|Train Avg Loss: 0.7830 |Test Loss: 0.8719|lr = 0.00010\n",
      "Epoch: 2582|steps:   60|Train Avg Loss: 0.7720 |Test Loss: 0.8584|lr = 0.00010\n",
      "Epoch: 2583|steps:   30|Train Avg Loss: 0.7786 |Test Loss: 0.8638|lr = 0.00010\n",
      "Epoch: 2583|steps:   60|Train Avg Loss: 0.7751 |Test Loss: 0.8599|lr = 0.00010\n",
      "Epoch: 2584|steps:   30|Train Avg Loss: 0.7890 |Test Loss: 0.8697|lr = 0.00010\n",
      "Epoch: 2584|steps:   60|Train Avg Loss: 0.7772 |Test Loss: 0.8753|lr = 0.00010\n",
      "Epoch: 2585|steps:   30|Train Avg Loss: 0.7936 |Test Loss: 0.8717|lr = 0.00010\n",
      "Epoch: 2585|steps:   60|Train Avg Loss: 0.7734 |Test Loss: 0.8615|lr = 0.00010\n",
      "Epoch: 2586|steps:   30|Train Avg Loss: 0.7704 |Test Loss: 0.8572|lr = 0.00010\n",
      "Epoch: 2586|steps:   60|Train Avg Loss: 0.7747 |Test Loss: 0.8599|lr = 0.00010\n",
      "Epoch: 2587|steps:   30|Train Avg Loss: 0.7820 |Test Loss: 0.8740|lr = 0.00010\n",
      "Epoch: 2587|steps:   60|Train Avg Loss: 0.7791 |Test Loss: 0.8564|lr = 0.00010\n",
      "Epoch: 2588|steps:   30|Train Avg Loss: 0.7545 |Test Loss: 0.8604|lr = 0.00010\n",
      "Epoch: 2588|steps:   60|Train Avg Loss: 0.7842 |Test Loss: 0.8659|lr = 0.00010\n",
      "Epoch: 2589|steps:   30|Train Avg Loss: 0.7618 |Test Loss: 0.8634|lr = 0.00010\n",
      "Epoch: 2589|steps:   60|Train Avg Loss: 0.8017 |Test Loss: 0.8668|lr = 0.00010\n",
      "Epoch: 2590|steps:   30|Train Avg Loss: 0.7610 |Test Loss: 0.8596|lr = 0.00010\n",
      "Epoch: 2590|steps:   60|Train Avg Loss: 0.7948 |Test Loss: 0.8714|lr = 0.00010\n",
      "Epoch: 2591|steps:   30|Train Avg Loss: 0.7891 |Test Loss: 0.8630|lr = 0.00010\n",
      "Epoch: 2591|steps:   60|Train Avg Loss: 0.7606 |Test Loss: 0.8680|lr = 0.00010\n",
      "Epoch: 2592|steps:   30|Train Avg Loss: 0.7933 |Test Loss: 0.8741|lr = 0.00010\n",
      "Epoch: 2592|steps:   60|Train Avg Loss: 0.7680 |Test Loss: 0.8675|lr = 0.00010\n",
      "Epoch: 2593|steps:   30|Train Avg Loss: 0.7918 |Test Loss: 0.8646|lr = 0.00010\n",
      "Epoch: 2593|steps:   60|Train Avg Loss: 0.7513 |Test Loss: 0.8713|lr = 0.00010\n",
      "Epoch: 2594|steps:   30|Train Avg Loss: 0.7663 |Test Loss: 0.8581|lr = 0.00010\n",
      "Epoch: 2594|steps:   60|Train Avg Loss: 0.7763 |Test Loss: 0.8650|lr = 0.00010\n",
      "Epoch: 2595|steps:   30|Train Avg Loss: 0.7702 |Test Loss: 0.8634|lr = 0.00010\n",
      "Epoch: 2595|steps:   60|Train Avg Loss: 0.7742 |Test Loss: 0.8628|lr = 0.00010\n",
      "Epoch: 2596|steps:   30|Train Avg Loss: 0.7874 |Test Loss: 0.8724|lr = 0.00010\n",
      "Epoch: 2596|steps:   60|Train Avg Loss: 0.7765 |Test Loss: 0.8767|lr = 0.00010\n",
      "Epoch: 2597|steps:   30|Train Avg Loss: 0.7930 |Test Loss: 0.8741|lr = 0.00010\n",
      "Epoch: 2597|steps:   60|Train Avg Loss: 0.7683 |Test Loss: 0.8556|lr = 0.00010\n",
      "Epoch: 2598|steps:   30|Train Avg Loss: 0.7808 |Test Loss: 0.8735|lr = 0.00010\n",
      "Epoch: 2598|steps:   60|Train Avg Loss: 0.7817 |Test Loss: 0.8699|lr = 0.00010\n",
      "Epoch: 2599|steps:   30|Train Avg Loss: 0.7797 |Test Loss: 0.8623|lr = 0.00010\n",
      "Epoch: 2599|steps:   60|Train Avg Loss: 0.7750 |Test Loss: 0.8677|lr = 0.00010\n",
      "Epoch: 2600|steps:   30|Train Avg Loss: 0.7682 |Test Loss: 0.8638|lr = 0.00010\n",
      "Epoch: 2600|steps:   60|Train Avg Loss: 0.7743 |Test Loss: 0.8713|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2601|steps:   30|Train Avg Loss: 0.7579 |Test Loss: 0.8639|lr = 0.00010\n",
      "Epoch: 2601|steps:   60|Train Avg Loss: 0.7905 |Test Loss: 0.8701|lr = 0.00010\n",
      "Epoch: 2602|steps:   30|Train Avg Loss: 0.7597 |Test Loss: 0.8708|lr = 0.00010\n",
      "Epoch: 2602|steps:   60|Train Avg Loss: 0.7898 |Test Loss: 0.8629|lr = 0.00010\n",
      "Epoch: 2603|steps:   30|Train Avg Loss: 0.7790 |Test Loss: 0.8675|lr = 0.00010\n",
      "Epoch: 2603|steps:   60|Train Avg Loss: 0.7723 |Test Loss: 0.8685|lr = 0.00010\n",
      "Epoch: 2604|steps:   30|Train Avg Loss: 0.7744 |Test Loss: 0.8671|lr = 0.00010\n",
      "Epoch: 2604|steps:   60|Train Avg Loss: 0.7708 |Test Loss: 0.8652|lr = 0.00010\n",
      "Epoch: 2605|steps:   30|Train Avg Loss: 0.7610 |Test Loss: 0.8711|lr = 0.00010\n",
      "Epoch: 2605|steps:   60|Train Avg Loss: 0.7671 |Test Loss: 0.8690|lr = 0.00010\n",
      "Epoch: 2606|steps:   30|Train Avg Loss: 0.7616 |Test Loss: 0.8773|lr = 0.00010\n",
      "Epoch: 2606|steps:   60|Train Avg Loss: 0.7723 |Test Loss: 0.8573|lr = 0.00010\n",
      "Epoch: 2607|steps:   30|Train Avg Loss: 0.7663 |Test Loss: 0.8775|lr = 0.00010\n",
      "Epoch: 2607|steps:   60|Train Avg Loss: 0.7906 |Test Loss: 0.8696|lr = 0.00010\n",
      "Epoch: 2608|steps:   30|Train Avg Loss: 0.7791 |Test Loss: 0.8641|lr = 0.00010\n",
      "Epoch: 2608|steps:   60|Train Avg Loss: 0.7667 |Test Loss: 0.8660|lr = 0.00010\n",
      "Epoch: 2609|steps:   30|Train Avg Loss: 0.7858 |Test Loss: 0.8765|lr = 0.00010\n",
      "Epoch: 2609|steps:   60|Train Avg Loss: 0.7719 |Test Loss: 0.8659|lr = 0.00010\n",
      "Epoch: 2610|steps:   30|Train Avg Loss: 0.7806 |Test Loss: 0.8705|lr = 0.00010\n",
      "Epoch: 2610|steps:   60|Train Avg Loss: 0.7654 |Test Loss: 0.8701|lr = 0.00010\n",
      "Epoch: 2611|steps:   30|Train Avg Loss: 0.7725 |Test Loss: 0.8770|lr = 0.00010\n",
      "Epoch: 2611|steps:   60|Train Avg Loss: 0.7582 |Test Loss: 0.8667|lr = 0.00010\n",
      "Epoch: 2612|steps:   30|Train Avg Loss: 0.7679 |Test Loss: 0.8675|lr = 0.00010\n",
      "Epoch: 2612|steps:   60|Train Avg Loss: 0.7894 |Test Loss: 0.8756|lr = 0.00010\n",
      "Epoch: 2613|steps:   30|Train Avg Loss: 0.7570 |Test Loss: 0.8785|lr = 0.00010\n",
      "Epoch: 2613|steps:   60|Train Avg Loss: 0.7871 |Test Loss: 0.8719|lr = 0.00010\n",
      "Epoch: 2614|steps:   30|Train Avg Loss: 0.7731 |Test Loss: 0.8763|lr = 0.00010\n",
      "Epoch: 2614|steps:   60|Train Avg Loss: 0.7820 |Test Loss: 0.8746|lr = 0.00010\n",
      "Epoch: 2615|steps:   30|Train Avg Loss: 0.7814 |Test Loss: 0.8726|lr = 0.00010\n",
      "Epoch: 2615|steps:   60|Train Avg Loss: 0.7581 |Test Loss: 0.8662|lr = 0.00010\n",
      "Epoch: 2616|steps:   30|Train Avg Loss: 0.7728 |Test Loss: 0.8797|lr = 0.00010\n",
      "Epoch: 2616|steps:   60|Train Avg Loss: 0.7838 |Test Loss: 0.8632|lr = 0.00010\n",
      "Epoch: 2617|steps:   30|Train Avg Loss: 0.7821 |Test Loss: 0.8697|lr = 0.00010\n",
      "Epoch: 2617|steps:   60|Train Avg Loss: 0.7568 |Test Loss: 0.8694|lr = 0.00010\n",
      "Epoch: 2618|steps:   30|Train Avg Loss: 0.7620 |Test Loss: 0.8765|lr = 0.00010\n",
      "Epoch: 2618|steps:   60|Train Avg Loss: 0.7744 |Test Loss: 0.8684|lr = 0.00010\n",
      "Epoch: 2619|steps:   30|Train Avg Loss: 0.7514 |Test Loss: 0.8747|lr = 0.00010\n",
      "Epoch: 2619|steps:   60|Train Avg Loss: 0.7872 |Test Loss: 0.8681|lr = 0.00010\n",
      "Epoch: 2620|steps:   30|Train Avg Loss: 0.7727 |Test Loss: 0.8663|lr = 0.00010\n",
      "Epoch: 2620|steps:   60|Train Avg Loss: 0.7592 |Test Loss: 0.8730|lr = 0.00010\n",
      "Epoch: 2621|steps:   30|Train Avg Loss: 0.7860 |Test Loss: 0.8731|lr = 0.00010\n",
      "Epoch: 2621|steps:   60|Train Avg Loss: 0.7671 |Test Loss: 0.8715|lr = 0.00010\n",
      "Epoch: 2622|steps:   30|Train Avg Loss: 0.7750 |Test Loss: 0.8700|lr = 0.00010\n",
      "Epoch: 2622|steps:   60|Train Avg Loss: 0.7579 |Test Loss: 0.8606|lr = 0.00010\n",
      "Epoch: 2623|steps:   30|Train Avg Loss: 0.7875 |Test Loss: 0.8750|lr = 0.00010\n",
      "Epoch: 2623|steps:   60|Train Avg Loss: 0.7615 |Test Loss: 0.8724|lr = 0.00010\n",
      "Epoch: 2624|steps:   30|Train Avg Loss: 0.7772 |Test Loss: 0.8771|lr = 0.00010\n",
      "Epoch: 2624|steps:   60|Train Avg Loss: 0.7572 |Test Loss: 0.8702|lr = 0.00010\n",
      "Epoch: 2625|steps:   30|Train Avg Loss: 0.7795 |Test Loss: 0.8787|lr = 0.00010\n",
      "Epoch: 2625|steps:   60|Train Avg Loss: 0.7587 |Test Loss: 0.8678|lr = 0.00010\n",
      "Epoch: 2626|steps:   30|Train Avg Loss: 0.7535 |Test Loss: 0.8617|lr = 0.00010\n",
      "Epoch: 2626|steps:   60|Train Avg Loss: 0.7881 |Test Loss: 0.8753|lr = 0.00010\n",
      "Epoch: 2627|steps:   30|Train Avg Loss: 0.7627 |Test Loss: 0.8769|lr = 0.00010\n",
      "Epoch: 2627|steps:   60|Train Avg Loss: 0.7617 |Test Loss: 0.8716|lr = 0.00010\n",
      "Epoch: 2628|steps:   30|Train Avg Loss: 0.7474 |Test Loss: 0.8714|lr = 0.00010\n",
      "Epoch: 2628|steps:   60|Train Avg Loss: 0.7809 |Test Loss: 0.8651|lr = 0.00010\n",
      "Epoch: 2629|steps:   30|Train Avg Loss: 0.7708 |Test Loss: 0.8741|lr = 0.00010\n",
      "Epoch: 2629|steps:   60|Train Avg Loss: 0.7664 |Test Loss: 0.8670|lr = 0.00010\n",
      "Epoch: 2630|steps:   30|Train Avg Loss: 0.7494 |Test Loss: 0.8581|lr = 0.00010\n",
      "Epoch: 2630|steps:   60|Train Avg Loss: 0.7868 |Test Loss: 0.8819|lr = 0.00010\n",
      "Epoch: 2631|steps:   30|Train Avg Loss: 0.7511 |Test Loss: 0.8673|lr = 0.00010\n",
      "Epoch: 2631|steps:   60|Train Avg Loss: 0.7845 |Test Loss: 0.8757|lr = 0.00010\n",
      "Epoch: 2632|steps:   30|Train Avg Loss: 0.7811 |Test Loss: 0.8799|lr = 0.00010\n",
      "Epoch: 2632|steps:   60|Train Avg Loss: 0.7564 |Test Loss: 0.8707|lr = 0.00010\n",
      "Epoch: 2633|steps:   30|Train Avg Loss: 0.7468 |Test Loss: 0.8691|lr = 0.00010\n",
      "Epoch: 2633|steps:   60|Train Avg Loss: 0.7813 |Test Loss: 0.8734|lr = 0.00010\n",
      "Epoch: 2634|steps:   30|Train Avg Loss: 0.7713 |Test Loss: 0.8709|lr = 0.00010\n",
      "Epoch: 2634|steps:   60|Train Avg Loss: 0.7713 |Test Loss: 0.8671|lr = 0.00010\n",
      "Epoch: 2635|steps:   30|Train Avg Loss: 0.7579 |Test Loss: 0.8753|lr = 0.00010\n",
      "Epoch: 2635|steps:   60|Train Avg Loss: 0.7794 |Test Loss: 0.8779|lr = 0.00010\n",
      "Epoch: 2636|steps:   30|Train Avg Loss: 0.7665 |Test Loss: 0.8645|lr = 0.00010\n",
      "Epoch: 2636|steps:   60|Train Avg Loss: 0.7707 |Test Loss: 0.8819|lr = 0.00010\n",
      "Epoch: 2637|steps:   30|Train Avg Loss: 0.7547 |Test Loss: 0.8638|lr = 0.00010\n",
      "Epoch: 2637|steps:   60|Train Avg Loss: 0.7956 |Test Loss: 0.8821|lr = 0.00010\n",
      "Epoch: 2638|steps:   30|Train Avg Loss: 0.7688 |Test Loss: 0.8809|lr = 0.00010\n",
      "Epoch: 2638|steps:   60|Train Avg Loss: 0.7609 |Test Loss: 0.8761|lr = 0.00010\n",
      "Epoch: 2639|steps:   30|Train Avg Loss: 0.7574 |Test Loss: 0.8719|lr = 0.00010\n",
      "Epoch: 2639|steps:   60|Train Avg Loss: 0.7759 |Test Loss: 0.8756|lr = 0.00010\n",
      "Epoch: 2640|steps:   30|Train Avg Loss: 0.7485 |Test Loss: 0.8755|lr = 0.00010\n",
      "Epoch: 2640|steps:   60|Train Avg Loss: 0.7832 |Test Loss: 0.8802|lr = 0.00010\n",
      "Epoch: 2641|steps:   30|Train Avg Loss: 0.7685 |Test Loss: 0.8848|lr = 0.00010\n",
      "Epoch: 2641|steps:   60|Train Avg Loss: 0.7691 |Test Loss: 0.8727|lr = 0.00010\n",
      "Epoch: 2642|steps:   30|Train Avg Loss: 0.7586 |Test Loss: 0.8690|lr = 0.00010\n",
      "Epoch: 2642|steps:   60|Train Avg Loss: 0.7671 |Test Loss: 0.8711|lr = 0.00010\n",
      "Epoch: 2643|steps:   30|Train Avg Loss: 0.7682 |Test Loss: 0.8857|lr = 0.00010\n",
      "Epoch: 2643|steps:   60|Train Avg Loss: 0.7636 |Test Loss: 0.8774|lr = 0.00010\n",
      "Epoch: 2644|steps:   30|Train Avg Loss: 0.7677 |Test Loss: 0.8695|lr = 0.00010\n",
      "Epoch: 2644|steps:   60|Train Avg Loss: 0.7652 |Test Loss: 0.8717|lr = 0.00010\n",
      "Epoch: 2645|steps:   30|Train Avg Loss: 0.7534 |Test Loss: 0.8783|lr = 0.00010\n",
      "Epoch: 2645|steps:   60|Train Avg Loss: 0.7834 |Test Loss: 0.8781|lr = 0.00010\n",
      "Epoch: 2646|steps:   30|Train Avg Loss: 0.7649 |Test Loss: 0.8782|lr = 0.00010\n",
      "Epoch: 2646|steps:   60|Train Avg Loss: 0.7736 |Test Loss: 0.8703|lr = 0.00010\n",
      "Epoch: 2647|steps:   30|Train Avg Loss: 0.7675 |Test Loss: 0.8755|lr = 0.00010\n",
      "Epoch: 2647|steps:   60|Train Avg Loss: 0.7689 |Test Loss: 0.8752|lr = 0.00010\n",
      "Epoch: 2648|steps:   30|Train Avg Loss: 0.7857 |Test Loss: 0.8855|lr = 0.00010\n",
      "Epoch: 2648|steps:   60|Train Avg Loss: 0.7588 |Test Loss: 0.8704|lr = 0.00010\n",
      "Epoch: 2649|steps:   30|Train Avg Loss: 0.7630 |Test Loss: 0.8759|lr = 0.00010\n",
      "Epoch: 2649|steps:   60|Train Avg Loss: 0.7719 |Test Loss: 0.8732|lr = 0.00010\n",
      "Epoch: 2650|steps:   30|Train Avg Loss: 0.7574 |Test Loss: 0.8830|lr = 0.00010\n",
      "Epoch: 2650|steps:   60|Train Avg Loss: 0.7727 |Test Loss: 0.8805|lr = 0.00010\n",
      "Epoch: 2651|steps:   30|Train Avg Loss: 0.7499 |Test Loss: 0.8795|lr = 0.00010\n",
      "Epoch: 2651|steps:   60|Train Avg Loss: 0.7735 |Test Loss: 0.8806|lr = 0.00010\n",
      "Epoch: 2652|steps:   30|Train Avg Loss: 0.7526 |Test Loss: 0.8761|lr = 0.00010\n",
      "Epoch: 2652|steps:   60|Train Avg Loss: 0.7699 |Test Loss: 0.8810|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2653|steps:   30|Train Avg Loss: 0.7605 |Test Loss: 0.8836|lr = 0.00010\n",
      "Epoch: 2653|steps:   60|Train Avg Loss: 0.7541 |Test Loss: 0.8743|lr = 0.00010\n",
      "Epoch: 2654|steps:   30|Train Avg Loss: 0.7628 |Test Loss: 0.8765|lr = 0.00010\n",
      "Epoch: 2654|steps:   60|Train Avg Loss: 0.7589 |Test Loss: 0.8751|lr = 0.00010\n",
      "Epoch: 2655|steps:   30|Train Avg Loss: 0.7747 |Test Loss: 0.8704|lr = 0.00010\n",
      "Epoch: 2655|steps:   60|Train Avg Loss: 0.7476 |Test Loss: 0.8768|lr = 0.00010\n",
      "Epoch: 2656|steps:   30|Train Avg Loss: 0.7703 |Test Loss: 0.8772|lr = 0.00010\n",
      "Epoch: 2656|steps:   60|Train Avg Loss: 0.7529 |Test Loss: 0.8761|lr = 0.00010\n",
      "Epoch: 2657|steps:   30|Train Avg Loss: 0.7380 |Test Loss: 0.8876|lr = 0.00010\n",
      "Epoch: 2657|steps:   60|Train Avg Loss: 0.7838 |Test Loss: 0.8828|lr = 0.00010\n",
      "Epoch: 2658|steps:   30|Train Avg Loss: 0.7651 |Test Loss: 0.8781|lr = 0.00010\n",
      "Epoch: 2658|steps:   60|Train Avg Loss: 0.7569 |Test Loss: 0.8770|lr = 0.00010\n",
      "Epoch: 2659|steps:   30|Train Avg Loss: 0.7641 |Test Loss: 0.8833|lr = 0.00010\n",
      "Epoch: 2659|steps:   60|Train Avg Loss: 0.7481 |Test Loss: 0.8660|lr = 0.00010\n",
      "Epoch: 2660|steps:   30|Train Avg Loss: 0.7750 |Test Loss: 0.8884|lr = 0.00010\n",
      "Epoch: 2660|steps:   60|Train Avg Loss: 0.7472 |Test Loss: 0.8692|lr = 0.00010\n",
      "Epoch: 2661|steps:   30|Train Avg Loss: 0.7501 |Test Loss: 0.8756|lr = 0.00010\n",
      "Epoch: 2661|steps:   60|Train Avg Loss: 0.7790 |Test Loss: 0.8893|lr = 0.00010\n",
      "Epoch: 2662|steps:   30|Train Avg Loss: 0.7585 |Test Loss: 0.8771|lr = 0.00010\n",
      "Epoch: 2662|steps:   60|Train Avg Loss: 0.7568 |Test Loss: 0.8804|lr = 0.00010\n",
      "Epoch: 2663|steps:   30|Train Avg Loss: 0.7690 |Test Loss: 0.8852|lr = 0.00010\n",
      "Epoch: 2663|steps:   60|Train Avg Loss: 0.7595 |Test Loss: 0.8830|lr = 0.00010\n",
      "Epoch: 2664|steps:   30|Train Avg Loss: 0.7392 |Test Loss: 0.8816|lr = 0.00010\n",
      "Epoch: 2664|steps:   60|Train Avg Loss: 0.7870 |Test Loss: 0.8796|lr = 0.00010\n",
      "Epoch: 2665|steps:   30|Train Avg Loss: 0.7784 |Test Loss: 0.8753|lr = 0.00010\n",
      "Epoch: 2665|steps:   60|Train Avg Loss: 0.7537 |Test Loss: 0.8872|lr = 0.00010\n",
      "Epoch: 2666|steps:   30|Train Avg Loss: 0.7201 |Test Loss: 0.8781|lr = 0.00010\n",
      "Epoch: 2666|steps:   60|Train Avg Loss: 0.7843 |Test Loss: 0.8801|lr = 0.00010\n",
      "Epoch: 2667|steps:   30|Train Avg Loss: 0.7439 |Test Loss: 0.8780|lr = 0.00010\n",
      "Epoch: 2667|steps:   60|Train Avg Loss: 0.7749 |Test Loss: 0.8768|lr = 0.00010\n",
      "Epoch: 2668|steps:   30|Train Avg Loss: 0.7394 |Test Loss: 0.8772|lr = 0.00010\n",
      "Epoch: 2668|steps:   60|Train Avg Loss: 0.7811 |Test Loss: 0.8881|lr = 0.00010\n",
      "Epoch: 2669|steps:   30|Train Avg Loss: 0.7526 |Test Loss: 0.8783|lr = 0.00010\n",
      "Epoch: 2669|steps:   60|Train Avg Loss: 0.7682 |Test Loss: 0.8830|lr = 0.00010\n",
      "Epoch: 2670|steps:   30|Train Avg Loss: 0.7432 |Test Loss: 0.8777|lr = 0.00010\n",
      "Epoch: 2670|steps:   60|Train Avg Loss: 0.7667 |Test Loss: 0.8794|lr = 0.00010\n",
      "Epoch: 2671|steps:   30|Train Avg Loss: 0.7635 |Test Loss: 0.8783|lr = 0.00010\n",
      "Epoch: 2671|steps:   60|Train Avg Loss: 0.7394 |Test Loss: 0.8862|lr = 0.00010\n",
      "Epoch: 2672|steps:   30|Train Avg Loss: 0.7666 |Test Loss: 0.8823|lr = 0.00010\n",
      "Epoch: 2672|steps:   60|Train Avg Loss: 0.7405 |Test Loss: 0.8804|lr = 0.00010\n",
      "Epoch: 2673|steps:   30|Train Avg Loss: 0.7466 |Test Loss: 0.8848|lr = 0.00010\n",
      "Epoch: 2673|steps:   60|Train Avg Loss: 0.7660 |Test Loss: 0.8761|lr = 0.00010\n",
      "Epoch: 2674|steps:   30|Train Avg Loss: 0.7581 |Test Loss: 0.8892|lr = 0.00010\n",
      "Epoch: 2674|steps:   60|Train Avg Loss: 0.7629 |Test Loss: 0.8758|lr = 0.00010\n",
      "Epoch: 2675|steps:   30|Train Avg Loss: 0.7659 |Test Loss: 0.8828|lr = 0.00010\n",
      "Epoch: 2675|steps:   60|Train Avg Loss: 0.7363 |Test Loss: 0.8788|lr = 0.00010\n",
      "Epoch: 2676|steps:   30|Train Avg Loss: 0.7670 |Test Loss: 0.8881|lr = 0.00010\n",
      "Epoch: 2676|steps:   60|Train Avg Loss: 0.7462 |Test Loss: 0.8804|lr = 0.00010\n",
      "Epoch: 2677|steps:   30|Train Avg Loss: 0.7618 |Test Loss: 0.8837|lr = 0.00010\n",
      "Epoch: 2677|steps:   60|Train Avg Loss: 0.7435 |Test Loss: 0.8813|lr = 0.00010\n",
      "Epoch: 2678|steps:   30|Train Avg Loss: 0.7520 |Test Loss: 0.8845|lr = 0.00010\n",
      "Epoch: 2678|steps:   60|Train Avg Loss: 0.7535 |Test Loss: 0.8848|lr = 0.00010\n",
      "Epoch: 2679|steps:   30|Train Avg Loss: 0.7587 |Test Loss: 0.8864|lr = 0.00010\n",
      "Epoch: 2679|steps:   60|Train Avg Loss: 0.7478 |Test Loss: 0.8868|lr = 0.00010\n",
      "Epoch: 2680|steps:   30|Train Avg Loss: 0.7575 |Test Loss: 0.8897|lr = 0.00010\n",
      "Epoch: 2680|steps:   60|Train Avg Loss: 0.7512 |Test Loss: 0.8787|lr = 0.00010\n",
      "Epoch: 2681|steps:   30|Train Avg Loss: 0.7353 |Test Loss: 0.8777|lr = 0.00010\n",
      "Epoch: 2681|steps:   60|Train Avg Loss: 0.7746 |Test Loss: 0.8844|lr = 0.00010\n",
      "Epoch: 2682|steps:   30|Train Avg Loss: 0.7429 |Test Loss: 0.8820|lr = 0.00010\n",
      "Epoch: 2682|steps:   60|Train Avg Loss: 0.7598 |Test Loss: 0.8857|lr = 0.00010\n",
      "Epoch: 2683|steps:   30|Train Avg Loss: 0.7733 |Test Loss: 0.8939|lr = 0.00010\n",
      "Epoch: 2683|steps:   60|Train Avg Loss: 0.7415 |Test Loss: 0.8822|lr = 0.00010\n",
      "Epoch: 2684|steps:   30|Train Avg Loss: 0.7548 |Test Loss: 0.8853|lr = 0.00010\n",
      "Epoch: 2684|steps:   60|Train Avg Loss: 0.7521 |Test Loss: 0.8950|lr = 0.00010\n",
      "Epoch: 2685|steps:   30|Train Avg Loss: 0.7517 |Test Loss: 0.8758|lr = 0.00010\n",
      "Epoch: 2685|steps:   60|Train Avg Loss: 0.7481 |Test Loss: 0.8890|lr = 0.00010\n",
      "Epoch: 2686|steps:   30|Train Avg Loss: 0.7616 |Test Loss: 0.9003|lr = 0.00010\n",
      "Epoch: 2686|steps:   60|Train Avg Loss: 0.7603 |Test Loss: 0.8858|lr = 0.00010\n",
      "Epoch: 2687|steps:   30|Train Avg Loss: 0.7530 |Test Loss: 0.8863|lr = 0.00010\n",
      "Epoch: 2687|steps:   60|Train Avg Loss: 0.7434 |Test Loss: 0.8815|lr = 0.00010\n",
      "Epoch: 2688|steps:   30|Train Avg Loss: 0.7322 |Test Loss: 0.8851|lr = 0.00010\n",
      "Epoch: 2688|steps:   60|Train Avg Loss: 0.7811 |Test Loss: 0.8874|lr = 0.00010\n",
      "Epoch: 2689|steps:   30|Train Avg Loss: 0.7821 |Test Loss: 0.8910|lr = 0.00010\n",
      "Epoch: 2689|steps:   60|Train Avg Loss: 0.7257 |Test Loss: 0.8764|lr = 0.00010\n",
      "Epoch: 2690|steps:   30|Train Avg Loss: 0.7633 |Test Loss: 0.8905|lr = 0.00010\n",
      "Epoch: 2690|steps:   60|Train Avg Loss: 0.7507 |Test Loss: 0.8867|lr = 0.00010\n",
      "Epoch: 2691|steps:   30|Train Avg Loss: 0.7554 |Test Loss: 0.8680|lr = 0.00010\n",
      "Epoch: 2691|steps:   60|Train Avg Loss: 0.7569 |Test Loss: 0.8931|lr = 0.00010\n",
      "Epoch: 2692|steps:   30|Train Avg Loss: 0.7487 |Test Loss: 0.8865|lr = 0.00010\n",
      "Epoch: 2692|steps:   60|Train Avg Loss: 0.7483 |Test Loss: 0.8911|lr = 0.00010\n",
      "Epoch: 2693|steps:   30|Train Avg Loss: 0.7463 |Test Loss: 0.8948|lr = 0.00010\n",
      "Epoch: 2693|steps:   60|Train Avg Loss: 0.7635 |Test Loss: 0.8813|lr = 0.00010\n",
      "Epoch: 2694|steps:   30|Train Avg Loss: 0.7457 |Test Loss: 0.8832|lr = 0.00010\n",
      "Epoch: 2694|steps:   60|Train Avg Loss: 0.7575 |Test Loss: 0.8920|lr = 0.00010\n",
      "Epoch: 2695|steps:   30|Train Avg Loss: 0.7506 |Test Loss: 0.8793|lr = 0.00010\n",
      "Epoch: 2695|steps:   60|Train Avg Loss: 0.7492 |Test Loss: 0.8947|lr = 0.00010\n",
      "Epoch: 2696|steps:   30|Train Avg Loss: 0.7472 |Test Loss: 0.8957|lr = 0.00010\n",
      "Epoch: 2696|steps:   60|Train Avg Loss: 0.7640 |Test Loss: 0.8936|lr = 0.00010\n",
      "Epoch: 2697|steps:   30|Train Avg Loss: 0.7579 |Test Loss: 0.8894|lr = 0.00010\n",
      "Epoch: 2697|steps:   60|Train Avg Loss: 0.7524 |Test Loss: 0.8879|lr = 0.00010\n",
      "Epoch: 2698|steps:   30|Train Avg Loss: 0.7486 |Test Loss: 0.8823|lr = 0.00010\n",
      "Epoch: 2698|steps:   60|Train Avg Loss: 0.7494 |Test Loss: 0.8968|lr = 0.00010\n",
      "Epoch: 2699|steps:   30|Train Avg Loss: 0.7498 |Test Loss: 0.9001|lr = 0.00010\n",
      "Epoch: 2699|steps:   60|Train Avg Loss: 0.7550 |Test Loss: 0.8817|lr = 0.00010\n",
      "Epoch: 2700|steps:   30|Train Avg Loss: 0.7466 |Test Loss: 0.8805|lr = 0.00010\n",
      "Epoch: 2700|steps:   60|Train Avg Loss: 0.7605 |Test Loss: 0.8819|lr = 0.00010\n",
      "Epoch: 2701|steps:   30|Train Avg Loss: 0.7267 |Test Loss: 0.8872|lr = 0.00010\n",
      "Epoch: 2701|steps:   60|Train Avg Loss: 0.7608 |Test Loss: 0.8830|lr = 0.00010\n",
      "Epoch: 2702|steps:   30|Train Avg Loss: 0.7436 |Test Loss: 0.8938|lr = 0.00010\n",
      "Epoch: 2702|steps:   60|Train Avg Loss: 0.7600 |Test Loss: 0.8954|lr = 0.00010\n",
      "Epoch: 2703|steps:   30|Train Avg Loss: 0.7402 |Test Loss: 0.8864|lr = 0.00010\n",
      "Epoch: 2703|steps:   60|Train Avg Loss: 0.7491 |Test Loss: 0.8914|lr = 0.00010\n",
      "Epoch: 2704|steps:   30|Train Avg Loss: 0.7443 |Test Loss: 0.8941|lr = 0.00010\n",
      "Epoch: 2704|steps:   60|Train Avg Loss: 0.7402 |Test Loss: 0.8944|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2705|steps:   30|Train Avg Loss: 0.7504 |Test Loss: 0.8946|lr = 0.00010\n",
      "Epoch: 2705|steps:   60|Train Avg Loss: 0.7541 |Test Loss: 0.8940|lr = 0.00010\n",
      "Epoch: 2706|steps:   30|Train Avg Loss: 0.7578 |Test Loss: 0.8904|lr = 0.00010\n",
      "Epoch: 2706|steps:   60|Train Avg Loss: 0.7358 |Test Loss: 0.8840|lr = 0.00010\n",
      "Epoch: 2707|steps:   30|Train Avg Loss: 0.7582 |Test Loss: 0.8904|lr = 0.00010\n",
      "Epoch: 2707|steps:   60|Train Avg Loss: 0.7458 |Test Loss: 0.8789|lr = 0.00010\n",
      "Epoch: 2708|steps:   30|Train Avg Loss: 0.7506 |Test Loss: 0.8837|lr = 0.00010\n",
      "Epoch: 2708|steps:   60|Train Avg Loss: 0.7466 |Test Loss: 0.8943|lr = 0.00010\n",
      "Epoch: 2709|steps:   30|Train Avg Loss: 0.7569 |Test Loss: 0.8904|lr = 0.00010\n",
      "Epoch: 2709|steps:   60|Train Avg Loss: 0.7483 |Test Loss: 0.8851|lr = 0.00010\n",
      "Epoch: 2710|steps:   30|Train Avg Loss: 0.7140 |Test Loss: 0.8852|lr = 0.00010\n",
      "Epoch: 2710|steps:   60|Train Avg Loss: 0.7704 |Test Loss: 0.8980|lr = 0.00010\n",
      "Epoch: 2711|steps:   30|Train Avg Loss: 0.7402 |Test Loss: 0.8855|lr = 0.00010\n",
      "Epoch: 2711|steps:   60|Train Avg Loss: 0.7456 |Test Loss: 0.8891|lr = 0.00010\n",
      "Epoch: 2712|steps:   30|Train Avg Loss: 0.7532 |Test Loss: 0.8921|lr = 0.00010\n",
      "Epoch: 2712|steps:   60|Train Avg Loss: 0.7408 |Test Loss: 0.8919|lr = 0.00010\n",
      "Epoch: 2713|steps:   30|Train Avg Loss: 0.7391 |Test Loss: 0.8842|lr = 0.00010\n",
      "Epoch: 2713|steps:   60|Train Avg Loss: 0.7436 |Test Loss: 0.8971|lr = 0.00010\n",
      "Epoch: 2714|steps:   30|Train Avg Loss: 0.7242 |Test Loss: 0.8889|lr = 0.00010\n",
      "Epoch: 2714|steps:   60|Train Avg Loss: 0.7623 |Test Loss: 0.8976|lr = 0.00010\n",
      "Epoch: 2715|steps:   30|Train Avg Loss: 0.7463 |Test Loss: 0.8996|lr = 0.00010\n",
      "Epoch: 2715|steps:   60|Train Avg Loss: 0.7352 |Test Loss: 0.8940|lr = 0.00010\n",
      "Epoch: 2716|steps:   30|Train Avg Loss: 0.7339 |Test Loss: 0.8895|lr = 0.00010\n",
      "Epoch: 2716|steps:   60|Train Avg Loss: 0.7554 |Test Loss: 0.9016|lr = 0.00010\n",
      "Epoch: 2717|steps:   30|Train Avg Loss: 0.7413 |Test Loss: 0.8986|lr = 0.00010\n",
      "Epoch: 2717|steps:   60|Train Avg Loss: 0.7545 |Test Loss: 0.8973|lr = 0.00010\n",
      "Epoch: 2718|steps:   30|Train Avg Loss: 0.7293 |Test Loss: 0.8887|lr = 0.00010\n",
      "Epoch: 2718|steps:   60|Train Avg Loss: 0.7524 |Test Loss: 0.8955|lr = 0.00010\n",
      "Epoch: 2719|steps:   30|Train Avg Loss: 0.7315 |Test Loss: 0.8818|lr = 0.00010\n",
      "Epoch: 2719|steps:   60|Train Avg Loss: 0.7661 |Test Loss: 0.9102|lr = 0.00010\n",
      "Epoch: 2720|steps:   30|Train Avg Loss: 0.7489 |Test Loss: 0.9014|lr = 0.00010\n",
      "Epoch: 2720|steps:   60|Train Avg Loss: 0.7366 |Test Loss: 0.8924|lr = 0.00010\n",
      "Epoch: 2721|steps:   30|Train Avg Loss: 0.7346 |Test Loss: 0.8930|lr = 0.00010\n",
      "Epoch: 2721|steps:   60|Train Avg Loss: 0.7429 |Test Loss: 0.8874|lr = 0.00010\n",
      "Epoch: 2722|steps:   30|Train Avg Loss: 0.7368 |Test Loss: 0.8956|lr = 0.00010\n",
      "Epoch: 2722|steps:   60|Train Avg Loss: 0.7618 |Test Loss: 0.9124|lr = 0.00010\n",
      "Epoch: 2723|steps:   30|Train Avg Loss: 0.7326 |Test Loss: 0.8870|lr = 0.00010\n",
      "Epoch: 2723|steps:   60|Train Avg Loss: 0.7527 |Test Loss: 0.9045|lr = 0.00010\n",
      "Epoch: 2724|steps:   30|Train Avg Loss: 0.7484 |Test Loss: 0.8955|lr = 0.00010\n",
      "Epoch: 2724|steps:   60|Train Avg Loss: 0.7362 |Test Loss: 0.8977|lr = 0.00010\n",
      "Epoch: 2725|steps:   30|Train Avg Loss: 0.7424 |Test Loss: 0.8980|lr = 0.00010\n",
      "Epoch: 2725|steps:   60|Train Avg Loss: 0.7573 |Test Loss: 0.8959|lr = 0.00010\n",
      "Epoch: 2726|steps:   30|Train Avg Loss: 0.7472 |Test Loss: 0.9007|lr = 0.00010\n",
      "Epoch: 2726|steps:   60|Train Avg Loss: 0.7289 |Test Loss: 0.8925|lr = 0.00010\n",
      "Epoch: 2727|steps:   30|Train Avg Loss: 0.7460 |Test Loss: 0.9006|lr = 0.00010\n",
      "Epoch: 2727|steps:   60|Train Avg Loss: 0.7412 |Test Loss: 0.8961|lr = 0.00010\n",
      "Epoch: 2728|steps:   30|Train Avg Loss: 0.7258 |Test Loss: 0.8910|lr = 0.00010\n",
      "Epoch: 2728|steps:   60|Train Avg Loss: 0.7448 |Test Loss: 0.8978|lr = 0.00010\n",
      "Epoch: 2729|steps:   30|Train Avg Loss: 0.7607 |Test Loss: 0.9153|lr = 0.00010\n",
      "Epoch: 2729|steps:   60|Train Avg Loss: 0.7277 |Test Loss: 0.8872|lr = 0.00010\n",
      "Epoch: 2730|steps:   30|Train Avg Loss: 0.7307 |Test Loss: 0.8936|lr = 0.00010\n",
      "Epoch: 2730|steps:   60|Train Avg Loss: 0.7483 |Test Loss: 0.8983|lr = 0.00010\n",
      "Epoch: 2731|steps:   30|Train Avg Loss: 0.7366 |Test Loss: 0.8963|lr = 0.00010\n",
      "Epoch: 2731|steps:   60|Train Avg Loss: 0.7447 |Test Loss: 0.9001|lr = 0.00010\n",
      "Epoch: 2732|steps:   30|Train Avg Loss: 0.7419 |Test Loss: 0.8928|lr = 0.00010\n",
      "Epoch: 2732|steps:   60|Train Avg Loss: 0.7403 |Test Loss: 0.9012|lr = 0.00010\n",
      "Epoch: 2733|steps:   30|Train Avg Loss: 0.7120 |Test Loss: 0.8900|lr = 0.00010\n",
      "Epoch: 2733|steps:   60|Train Avg Loss: 0.7617 |Test Loss: 0.8999|lr = 0.00010\n",
      "Epoch: 2734|steps:   30|Train Avg Loss: 0.7392 |Test Loss: 0.9019|lr = 0.00010\n",
      "Epoch: 2734|steps:   60|Train Avg Loss: 0.7399 |Test Loss: 0.8872|lr = 0.00010\n",
      "Epoch: 2735|steps:   30|Train Avg Loss: 0.7303 |Test Loss: 0.8976|lr = 0.00010\n",
      "Epoch: 2735|steps:   60|Train Avg Loss: 0.7435 |Test Loss: 0.8971|lr = 0.00010\n",
      "Epoch: 2736|steps:   30|Train Avg Loss: 0.7197 |Test Loss: 0.8949|lr = 0.00010\n",
      "Epoch: 2736|steps:   60|Train Avg Loss: 0.7556 |Test Loss: 0.8928|lr = 0.00010\n",
      "Epoch: 2737|steps:   30|Train Avg Loss: 0.7546 |Test Loss: 0.9026|lr = 0.00010\n",
      "Epoch: 2737|steps:   60|Train Avg Loss: 0.7205 |Test Loss: 0.8925|lr = 0.00010\n",
      "Epoch: 2738|steps:   30|Train Avg Loss: 0.7368 |Test Loss: 0.9011|lr = 0.00010\n",
      "Epoch: 2738|steps:   60|Train Avg Loss: 0.7270 |Test Loss: 0.8971|lr = 0.00010\n",
      "Epoch: 2739|steps:   30|Train Avg Loss: 0.7445 |Test Loss: 0.8953|lr = 0.00010\n",
      "Epoch: 2739|steps:   60|Train Avg Loss: 0.7265 |Test Loss: 0.9007|lr = 0.00010\n",
      "Epoch: 2740|steps:   30|Train Avg Loss: 0.7457 |Test Loss: 0.9058|lr = 0.00010\n",
      "Epoch: 2740|steps:   60|Train Avg Loss: 0.7430 |Test Loss: 0.8951|lr = 0.00010\n",
      "Epoch: 2741|steps:   30|Train Avg Loss: 0.7329 |Test Loss: 0.8943|lr = 0.00010\n",
      "Epoch: 2741|steps:   60|Train Avg Loss: 0.7416 |Test Loss: 0.9016|lr = 0.00010\n",
      "Epoch: 2742|steps:   30|Train Avg Loss: 0.7150 |Test Loss: 0.8941|lr = 0.00010\n",
      "Epoch: 2742|steps:   60|Train Avg Loss: 0.7604 |Test Loss: 0.9016|lr = 0.00010\n",
      "Epoch: 2743|steps:   30|Train Avg Loss: 0.7151 |Test Loss: 0.8864|lr = 0.00010\n",
      "Epoch: 2743|steps:   60|Train Avg Loss: 0.7651 |Test Loss: 0.9096|lr = 0.00010\n",
      "Epoch: 2744|steps:   30|Train Avg Loss: 0.7132 |Test Loss: 0.8945|lr = 0.00010\n",
      "Epoch: 2744|steps:   60|Train Avg Loss: 0.7369 |Test Loss: 0.9044|lr = 0.00010\n",
      "Epoch: 2745|steps:   30|Train Avg Loss: 0.7387 |Test Loss: 0.9057|lr = 0.00010\n",
      "Epoch: 2745|steps:   60|Train Avg Loss: 0.7162 |Test Loss: 0.8918|lr = 0.00010\n",
      "Epoch: 2746|steps:   30|Train Avg Loss: 0.7279 |Test Loss: 0.8957|lr = 0.00010\n",
      "Epoch: 2746|steps:   60|Train Avg Loss: 0.7291 |Test Loss: 0.8962|lr = 0.00010\n",
      "Epoch: 2747|steps:   30|Train Avg Loss: 0.7517 |Test Loss: 0.8967|lr = 0.00010\n",
      "Epoch: 2747|steps:   60|Train Avg Loss: 0.7319 |Test Loss: 0.8929|lr = 0.00010\n",
      "Epoch: 2748|steps:   30|Train Avg Loss: 0.7322 |Test Loss: 0.9015|lr = 0.00010\n",
      "Epoch: 2748|steps:   60|Train Avg Loss: 0.7350 |Test Loss: 0.9013|lr = 0.00010\n",
      "Epoch: 2749|steps:   30|Train Avg Loss: 0.7260 |Test Loss: 0.8945|lr = 0.00010\n",
      "Epoch: 2749|steps:   60|Train Avg Loss: 0.7248 |Test Loss: 0.9003|lr = 0.00010\n",
      "Epoch: 2750|steps:   30|Train Avg Loss: 0.7021 |Test Loss: 0.9090|lr = 0.00010\n",
      "Epoch: 2750|steps:   60|Train Avg Loss: 0.7525 |Test Loss: 0.8945|lr = 0.00010\n",
      "Epoch: 2751|steps:   30|Train Avg Loss: 0.7321 |Test Loss: 0.8999|lr = 0.00010\n",
      "Epoch: 2751|steps:   60|Train Avg Loss: 0.7394 |Test Loss: 0.8989|lr = 0.00010\n",
      "Epoch: 2752|steps:   30|Train Avg Loss: 0.7183 |Test Loss: 0.8962|lr = 0.00010\n",
      "Epoch: 2752|steps:   60|Train Avg Loss: 0.7349 |Test Loss: 0.9039|lr = 0.00010\n",
      "Epoch: 2753|steps:   30|Train Avg Loss: 0.7036 |Test Loss: 0.9031|lr = 0.00010\n",
      "Epoch: 2753|steps:   60|Train Avg Loss: 0.7525 |Test Loss: 0.8977|lr = 0.00010\n",
      "Epoch: 2754|steps:   30|Train Avg Loss: 0.7198 |Test Loss: 0.8957|lr = 0.00010\n",
      "Epoch: 2754|steps:   60|Train Avg Loss: 0.7343 |Test Loss: 0.9033|lr = 0.00010\n",
      "Epoch: 2755|steps:   30|Train Avg Loss: 0.7268 |Test Loss: 0.9000|lr = 0.00010\n",
      "Epoch: 2755|steps:   60|Train Avg Loss: 0.7189 |Test Loss: 0.8897|lr = 0.00010\n",
      "Epoch: 2756|steps:   30|Train Avg Loss: 0.7369 |Test Loss: 0.8997|lr = 0.00010\n",
      "Epoch: 2756|steps:   60|Train Avg Loss: 0.7342 |Test Loss: 0.9030|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2757|steps:   30|Train Avg Loss: 0.7125 |Test Loss: 0.9136|lr = 0.00010\n",
      "Epoch: 2757|steps:   60|Train Avg Loss: 0.7272 |Test Loss: 0.8907|lr = 0.00010\n",
      "Epoch: 2758|steps:   30|Train Avg Loss: 0.7298 |Test Loss: 0.8944|lr = 0.00010\n",
      "Epoch: 2758|steps:   60|Train Avg Loss: 0.7317 |Test Loss: 0.9125|lr = 0.00010\n",
      "Epoch: 2759|steps:   30|Train Avg Loss: 0.7257 |Test Loss: 0.9028|lr = 0.00010\n",
      "Epoch: 2759|steps:   60|Train Avg Loss: 0.7262 |Test Loss: 0.9030|lr = 0.00010\n",
      "Epoch: 2760|steps:   30|Train Avg Loss: 0.7365 |Test Loss: 0.9113|lr = 0.00010\n",
      "Epoch: 2760|steps:   60|Train Avg Loss: 0.7264 |Test Loss: 0.8947|lr = 0.00010\n",
      "Epoch: 2761|steps:   30|Train Avg Loss: 0.6998 |Test Loss: 0.8920|lr = 0.00010\n",
      "Epoch: 2761|steps:   60|Train Avg Loss: 0.7623 |Test Loss: 0.9002|lr = 0.00010\n",
      "Epoch: 2762|steps:   30|Train Avg Loss: 0.7324 |Test Loss: 0.9040|lr = 0.00010\n",
      "Epoch: 2762|steps:   60|Train Avg Loss: 0.7378 |Test Loss: 0.9069|lr = 0.00010\n",
      "Epoch: 2763|steps:   30|Train Avg Loss: 0.7503 |Test Loss: 0.9067|lr = 0.00010\n",
      "Epoch: 2763|steps:   60|Train Avg Loss: 0.7193 |Test Loss: 0.9084|lr = 0.00010\n",
      "Epoch: 2764|steps:   30|Train Avg Loss: 0.7374 |Test Loss: 0.9026|lr = 0.00010\n",
      "Epoch: 2764|steps:   60|Train Avg Loss: 0.7216 |Test Loss: 0.8930|lr = 0.00010\n",
      "Epoch: 2765|steps:   30|Train Avg Loss: 0.7084 |Test Loss: 0.8950|lr = 0.00010\n",
      "Epoch: 2765|steps:   60|Train Avg Loss: 0.7542 |Test Loss: 0.9132|lr = 0.00010\n",
      "Epoch: 2766|steps:   30|Train Avg Loss: 0.7105 |Test Loss: 0.9088|lr = 0.00010\n",
      "Epoch: 2766|steps:   60|Train Avg Loss: 0.7408 |Test Loss: 0.8915|lr = 0.00010\n",
      "Epoch: 2767|steps:   30|Train Avg Loss: 0.7290 |Test Loss: 0.9006|lr = 0.00010\n",
      "Epoch: 2767|steps:   60|Train Avg Loss: 0.7261 |Test Loss: 0.9066|lr = 0.00010\n",
      "Epoch: 2768|steps:   30|Train Avg Loss: 0.7370 |Test Loss: 0.8842|lr = 0.00010\n",
      "Epoch: 2768|steps:   60|Train Avg Loss: 0.7326 |Test Loss: 0.9176|lr = 0.00010\n",
      "Epoch: 2769|steps:   30|Train Avg Loss: 0.7110 |Test Loss: 0.9027|lr = 0.00010\n",
      "Epoch: 2769|steps:   60|Train Avg Loss: 0.7375 |Test Loss: 0.9114|lr = 0.00010\n",
      "Epoch: 2770|steps:   30|Train Avg Loss: 0.7281 |Test Loss: 0.8967|lr = 0.00010\n",
      "Epoch: 2770|steps:   60|Train Avg Loss: 0.7218 |Test Loss: 0.9075|lr = 0.00010\n",
      "Epoch: 2771|steps:   30|Train Avg Loss: 0.7205 |Test Loss: 0.9030|lr = 0.00010\n",
      "Epoch: 2771|steps:   60|Train Avg Loss: 0.7316 |Test Loss: 0.9040|lr = 0.00010\n",
      "Epoch: 2772|steps:   30|Train Avg Loss: 0.7133 |Test Loss: 0.9043|lr = 0.00010\n",
      "Epoch: 2772|steps:   60|Train Avg Loss: 0.7298 |Test Loss: 0.8938|lr = 0.00010\n",
      "Epoch: 2773|steps:   30|Train Avg Loss: 0.7089 |Test Loss: 0.9074|lr = 0.00010\n",
      "Epoch: 2773|steps:   60|Train Avg Loss: 0.7507 |Test Loss: 0.9034|lr = 0.00010\n",
      "Epoch: 2774|steps:   30|Train Avg Loss: 0.7491 |Test Loss: 0.9114|lr = 0.00010\n",
      "Epoch: 2774|steps:   60|Train Avg Loss: 0.7203 |Test Loss: 0.8967|lr = 0.00010\n",
      "Epoch: 2775|steps:   30|Train Avg Loss: 0.7097 |Test Loss: 0.8866|lr = 0.00010\n",
      "Epoch: 2775|steps:   60|Train Avg Loss: 0.7435 |Test Loss: 0.9114|lr = 0.00010\n",
      "Epoch: 2776|steps:   30|Train Avg Loss: 0.7253 |Test Loss: 0.9104|lr = 0.00010\n",
      "Epoch: 2776|steps:   60|Train Avg Loss: 0.7225 |Test Loss: 0.8975|lr = 0.00010\n",
      "Epoch: 2777|steps:   30|Train Avg Loss: 0.7257 |Test Loss: 0.9046|lr = 0.00010\n",
      "Epoch: 2777|steps:   60|Train Avg Loss: 0.7166 |Test Loss: 0.9012|lr = 0.00010\n",
      "Epoch: 2778|steps:   30|Train Avg Loss: 0.7284 |Test Loss: 0.9051|lr = 0.00010\n",
      "Epoch: 2778|steps:   60|Train Avg Loss: 0.7317 |Test Loss: 0.9105|lr = 0.00010\n",
      "Epoch: 2779|steps:   30|Train Avg Loss: 0.7224 |Test Loss: 0.9047|lr = 0.00010\n",
      "Epoch: 2779|steps:   60|Train Avg Loss: 0.7209 |Test Loss: 0.8978|lr = 0.00010\n",
      "Epoch: 2780|steps:   30|Train Avg Loss: 0.7548 |Test Loss: 0.9142|lr = 0.00010\n",
      "Epoch: 2780|steps:   60|Train Avg Loss: 0.6904 |Test Loss: 0.8962|lr = 0.00010\n",
      "Epoch: 2781|steps:   30|Train Avg Loss: 0.7293 |Test Loss: 0.9131|lr = 0.00010\n",
      "Epoch: 2781|steps:   60|Train Avg Loss: 0.6985 |Test Loss: 0.9087|lr = 0.00010\n",
      "Epoch: 2782|steps:   30|Train Avg Loss: 0.7036 |Test Loss: 0.9008|lr = 0.00010\n",
      "Epoch: 2782|steps:   60|Train Avg Loss: 0.7358 |Test Loss: 0.9023|lr = 0.00010\n",
      "Epoch: 2783|steps:   30|Train Avg Loss: 0.7262 |Test Loss: 0.9134|lr = 0.00010\n",
      "Epoch: 2783|steps:   60|Train Avg Loss: 0.7231 |Test Loss: 0.9042|lr = 0.00010\n",
      "Epoch: 2784|steps:   30|Train Avg Loss: 0.7222 |Test Loss: 0.9080|lr = 0.00010\n",
      "Epoch: 2784|steps:   60|Train Avg Loss: 0.7279 |Test Loss: 0.9062|lr = 0.00010\n",
      "Epoch: 2785|steps:   30|Train Avg Loss: 0.7306 |Test Loss: 0.9230|lr = 0.00010\n",
      "Epoch: 2785|steps:   60|Train Avg Loss: 0.7259 |Test Loss: 0.8969|lr = 0.00010\n",
      "Epoch: 2786|steps:   30|Train Avg Loss: 0.7193 |Test Loss: 0.9071|lr = 0.00010\n",
      "Epoch: 2786|steps:   60|Train Avg Loss: 0.7357 |Test Loss: 0.9038|lr = 0.00010\n",
      "Epoch: 2787|steps:   30|Train Avg Loss: 0.7096 |Test Loss: 0.9210|lr = 0.00010\n",
      "Epoch: 2787|steps:   60|Train Avg Loss: 0.7290 |Test Loss: 0.9117|lr = 0.00010\n",
      "Epoch: 2788|steps:   30|Train Avg Loss: 0.7105 |Test Loss: 0.9052|lr = 0.00010\n",
      "Epoch: 2788|steps:   60|Train Avg Loss: 0.7353 |Test Loss: 0.9075|lr = 0.00010\n",
      "Epoch: 2789|steps:   30|Train Avg Loss: 0.7061 |Test Loss: 0.9067|lr = 0.00010\n",
      "Epoch: 2789|steps:   60|Train Avg Loss: 0.7196 |Test Loss: 0.9089|lr = 0.00010\n",
      "Epoch: 2790|steps:   30|Train Avg Loss: 0.7022 |Test Loss: 0.8971|lr = 0.00010\n",
      "Epoch: 2790|steps:   60|Train Avg Loss: 0.7270 |Test Loss: 0.9097|lr = 0.00010\n",
      "Epoch: 2791|steps:   30|Train Avg Loss: 0.7083 |Test Loss: 0.8993|lr = 0.00010\n",
      "Epoch: 2791|steps:   60|Train Avg Loss: 0.7279 |Test Loss: 0.9266|lr = 0.00010\n",
      "Epoch: 2792|steps:   30|Train Avg Loss: 0.7129 |Test Loss: 0.9051|lr = 0.00010\n",
      "Epoch: 2792|steps:   60|Train Avg Loss: 0.7258 |Test Loss: 0.9070|lr = 0.00010\n",
      "Epoch: 2793|steps:   30|Train Avg Loss: 0.7199 |Test Loss: 0.9058|lr = 0.00010\n",
      "Epoch: 2793|steps:   60|Train Avg Loss: 0.7189 |Test Loss: 0.9250|lr = 0.00010\n",
      "Epoch: 2794|steps:   30|Train Avg Loss: 0.7145 |Test Loss: 0.9119|lr = 0.00010\n",
      "Epoch: 2794|steps:   60|Train Avg Loss: 0.7144 |Test Loss: 0.8975|lr = 0.00010\n",
      "Epoch: 2795|steps:   30|Train Avg Loss: 0.7250 |Test Loss: 0.9199|lr = 0.00010\n",
      "Epoch: 2795|steps:   60|Train Avg Loss: 0.7177 |Test Loss: 0.9018|lr = 0.00010\n",
      "Epoch: 2796|steps:   30|Train Avg Loss: 0.7075 |Test Loss: 0.8977|lr = 0.00010\n",
      "Epoch: 2796|steps:   60|Train Avg Loss: 0.7329 |Test Loss: 0.9239|lr = 0.00010\n",
      "Epoch: 2797|steps:   30|Train Avg Loss: 0.7232 |Test Loss: 0.9082|lr = 0.00010\n",
      "Epoch: 2797|steps:   60|Train Avg Loss: 0.7082 |Test Loss: 0.9054|lr = 0.00010\n",
      "Epoch: 2798|steps:   30|Train Avg Loss: 0.7020 |Test Loss: 0.9017|lr = 0.00010\n",
      "Epoch: 2798|steps:   60|Train Avg Loss: 0.7325 |Test Loss: 0.9146|lr = 0.00010\n",
      "Epoch: 2799|steps:   30|Train Avg Loss: 0.6876 |Test Loss: 0.8927|lr = 0.00010\n",
      "Epoch: 2799|steps:   60|Train Avg Loss: 0.7350 |Test Loss: 0.9156|lr = 0.00010\n",
      "Epoch: 2800|steps:   30|Train Avg Loss: 0.7245 |Test Loss: 0.9104|lr = 0.00010\n",
      "Epoch: 2800|steps:   60|Train Avg Loss: 0.7216 |Test Loss: 0.9007|lr = 0.00010\n",
      "Epoch: 2801|steps:   30|Train Avg Loss: 0.7220 |Test Loss: 0.9226|lr = 0.00010\n",
      "Epoch: 2801|steps:   60|Train Avg Loss: 0.7030 |Test Loss: 0.9146|lr = 0.00010\n",
      "Epoch: 2802|steps:   30|Train Avg Loss: 0.7163 |Test Loss: 0.9010|lr = 0.00010\n",
      "Epoch: 2802|steps:   60|Train Avg Loss: 0.7319 |Test Loss: 0.9211|lr = 0.00010\n",
      "Epoch: 2803|steps:   30|Train Avg Loss: 0.6819 |Test Loss: 0.9008|lr = 0.00010\n",
      "Epoch: 2803|steps:   60|Train Avg Loss: 0.7396 |Test Loss: 0.9366|lr = 0.00010\n",
      "Epoch: 2804|steps:   30|Train Avg Loss: 0.7363 |Test Loss: 0.9023|lr = 0.00010\n",
      "Epoch: 2804|steps:   60|Train Avg Loss: 0.7082 |Test Loss: 0.9116|lr = 0.00010\n",
      "Epoch: 2805|steps:   30|Train Avg Loss: 0.7011 |Test Loss: 0.8961|lr = 0.00010\n",
      "Epoch: 2805|steps:   60|Train Avg Loss: 0.7363 |Test Loss: 0.9162|lr = 0.00010\n",
      "Epoch: 2806|steps:   30|Train Avg Loss: 0.6893 |Test Loss: 0.9073|lr = 0.00010\n",
      "Epoch: 2806|steps:   60|Train Avg Loss: 0.7303 |Test Loss: 0.9093|lr = 0.00010\n",
      "Epoch: 2807|steps:   30|Train Avg Loss: 0.7234 |Test Loss: 0.9017|lr = 0.00010\n",
      "Epoch: 2807|steps:   60|Train Avg Loss: 0.7042 |Test Loss: 0.9125|lr = 0.00010\n",
      "Epoch: 2808|steps:   30|Train Avg Loss: 0.7082 |Test Loss: 0.9081|lr = 0.00010\n",
      "Epoch: 2808|steps:   60|Train Avg Loss: 0.7267 |Test Loss: 0.9121|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2809|steps:   30|Train Avg Loss: 0.7145 |Test Loss: 0.9006|lr = 0.00010\n",
      "Epoch: 2809|steps:   60|Train Avg Loss: 0.6983 |Test Loss: 0.9197|lr = 0.00010\n",
      "Epoch: 2810|steps:   30|Train Avg Loss: 0.7241 |Test Loss: 0.9196|lr = 0.00010\n",
      "Epoch: 2810|steps:   60|Train Avg Loss: 0.6994 |Test Loss: 0.9052|lr = 0.00010\n",
      "Epoch: 2811|steps:   30|Train Avg Loss: 0.7070 |Test Loss: 0.9130|lr = 0.00010\n",
      "Epoch: 2811|steps:   60|Train Avg Loss: 0.7232 |Test Loss: 0.9136|lr = 0.00010\n",
      "Epoch: 2812|steps:   30|Train Avg Loss: 0.7143 |Test Loss: 0.9393|lr = 0.00010\n",
      "Epoch: 2812|steps:   60|Train Avg Loss: 0.7126 |Test Loss: 0.8913|lr = 0.00010\n",
      "Epoch: 2813|steps:   30|Train Avg Loss: 0.7196 |Test Loss: 0.9128|lr = 0.00010\n",
      "Epoch: 2813|steps:   60|Train Avg Loss: 0.7222 |Test Loss: 0.9065|lr = 0.00010\n",
      "Epoch: 2814|steps:   30|Train Avg Loss: 0.7128 |Test Loss: 0.9158|lr = 0.00010\n",
      "Epoch: 2814|steps:   60|Train Avg Loss: 0.7145 |Test Loss: 0.9141|lr = 0.00010\n",
      "Epoch: 2815|steps:   30|Train Avg Loss: 0.6932 |Test Loss: 0.9139|lr = 0.00010\n",
      "Epoch: 2815|steps:   60|Train Avg Loss: 0.7342 |Test Loss: 0.9131|lr = 0.00010\n",
      "Epoch: 2816|steps:   30|Train Avg Loss: 0.7222 |Test Loss: 0.9354|lr = 0.00010\n",
      "Epoch: 2816|steps:   60|Train Avg Loss: 0.7008 |Test Loss: 0.8976|lr = 0.00010\n",
      "Epoch: 2817|steps:   30|Train Avg Loss: 0.7162 |Test Loss: 0.9220|lr = 0.00010\n",
      "Epoch: 2817|steps:   60|Train Avg Loss: 0.7094 |Test Loss: 0.9070|lr = 0.00010\n",
      "Epoch: 2818|steps:   30|Train Avg Loss: 0.7157 |Test Loss: 0.9167|lr = 0.00010\n",
      "Epoch: 2818|steps:   60|Train Avg Loss: 0.6920 |Test Loss: 0.9067|lr = 0.00010\n",
      "Epoch: 2819|steps:   30|Train Avg Loss: 0.7037 |Test Loss: 0.9118|lr = 0.00010\n",
      "Epoch: 2819|steps:   60|Train Avg Loss: 0.7076 |Test Loss: 0.9061|lr = 0.00010\n",
      "Epoch: 2820|steps:   30|Train Avg Loss: 0.6941 |Test Loss: 0.9072|lr = 0.00010\n",
      "Epoch: 2820|steps:   60|Train Avg Loss: 0.7126 |Test Loss: 0.9215|lr = 0.00010\n",
      "Epoch: 2821|steps:   30|Train Avg Loss: 0.7157 |Test Loss: 0.9188|lr = 0.00010\n",
      "Epoch: 2821|steps:   60|Train Avg Loss: 0.7026 |Test Loss: 0.9177|lr = 0.00010\n",
      "Epoch: 2822|steps:   30|Train Avg Loss: 0.7236 |Test Loss: 0.9142|lr = 0.00010\n",
      "Epoch: 2822|steps:   60|Train Avg Loss: 0.6940 |Test Loss: 0.9137|lr = 0.00010\n",
      "Epoch: 2823|steps:   30|Train Avg Loss: 0.7138 |Test Loss: 0.9230|lr = 0.00010\n",
      "Epoch: 2823|steps:   60|Train Avg Loss: 0.7053 |Test Loss: 0.9213|lr = 0.00010\n",
      "Epoch: 2824|steps:   30|Train Avg Loss: 0.6999 |Test Loss: 0.9153|lr = 0.00010\n",
      "Epoch: 2824|steps:   60|Train Avg Loss: 0.7048 |Test Loss: 0.9182|lr = 0.00010\n",
      "Epoch: 2825|steps:   30|Train Avg Loss: 0.7049 |Test Loss: 0.9112|lr = 0.00010\n",
      "Epoch: 2825|steps:   60|Train Avg Loss: 0.6975 |Test Loss: 0.9172|lr = 0.00010\n",
      "Epoch: 2826|steps:   30|Train Avg Loss: 0.6816 |Test Loss: 0.9149|lr = 0.00010\n",
      "Epoch: 2826|steps:   60|Train Avg Loss: 0.7319 |Test Loss: 0.9247|lr = 0.00010\n",
      "Epoch: 2827|steps:   30|Train Avg Loss: 0.6922 |Test Loss: 0.9034|lr = 0.00010\n",
      "Epoch: 2827|steps:   60|Train Avg Loss: 0.7142 |Test Loss: 0.9226|lr = 0.00010\n",
      "Epoch: 2828|steps:   30|Train Avg Loss: 0.6802 |Test Loss: 0.9168|lr = 0.00010\n",
      "Epoch: 2828|steps:   60|Train Avg Loss: 0.7183 |Test Loss: 0.9051|lr = 0.00010\n",
      "Epoch: 2829|steps:   30|Train Avg Loss: 0.6865 |Test Loss: 0.9195|lr = 0.00010\n",
      "Epoch: 2829|steps:   60|Train Avg Loss: 0.7185 |Test Loss: 0.9120|lr = 0.00010\n",
      "Epoch: 2830|steps:   30|Train Avg Loss: 0.7110 |Test Loss: 0.9173|lr = 0.00010\n",
      "Epoch: 2830|steps:   60|Train Avg Loss: 0.7012 |Test Loss: 0.9162|lr = 0.00010\n",
      "Epoch: 2831|steps:   30|Train Avg Loss: 0.7147 |Test Loss: 0.9243|lr = 0.00010\n",
      "Epoch: 2831|steps:   60|Train Avg Loss: 0.6922 |Test Loss: 0.9127|lr = 0.00010\n",
      "Epoch: 2832|steps:   30|Train Avg Loss: 0.7042 |Test Loss: 0.9162|lr = 0.00010\n",
      "Epoch: 2832|steps:   60|Train Avg Loss: 0.7008 |Test Loss: 0.9129|lr = 0.00010\n",
      "Epoch: 2833|steps:   30|Train Avg Loss: 0.6959 |Test Loss: 0.9297|lr = 0.00010\n",
      "Epoch: 2833|steps:   60|Train Avg Loss: 0.7022 |Test Loss: 0.9325|lr = 0.00010\n",
      "Epoch: 2834|steps:   30|Train Avg Loss: 0.7107 |Test Loss: 0.9137|lr = 0.00010\n",
      "Epoch: 2834|steps:   60|Train Avg Loss: 0.6887 |Test Loss: 0.9056|lr = 0.00010\n",
      "Epoch: 2835|steps:   30|Train Avg Loss: 0.6911 |Test Loss: 0.9160|lr = 0.00010\n",
      "Epoch: 2835|steps:   60|Train Avg Loss: 0.7157 |Test Loss: 0.9153|lr = 0.00010\n",
      "Epoch: 2836|steps:   30|Train Avg Loss: 0.7171 |Test Loss: 0.9093|lr = 0.00010\n",
      "Epoch: 2836|steps:   60|Train Avg Loss: 0.6914 |Test Loss: 0.9164|lr = 0.00010\n",
      "Epoch: 2837|steps:   30|Train Avg Loss: 0.6916 |Test Loss: 0.9133|lr = 0.00010\n",
      "Epoch: 2837|steps:   60|Train Avg Loss: 0.7177 |Test Loss: 0.9215|lr = 0.00010\n",
      "Epoch: 2838|steps:   30|Train Avg Loss: 0.7115 |Test Loss: 0.9479|lr = 0.00010\n",
      "Epoch: 2838|steps:   60|Train Avg Loss: 0.7045 |Test Loss: 0.9138|lr = 0.00010\n",
      "Epoch: 2839|steps:   30|Train Avg Loss: 0.6943 |Test Loss: 0.9150|lr = 0.00010\n",
      "Epoch: 2839|steps:   60|Train Avg Loss: 0.7095 |Test Loss: 0.9177|lr = 0.00010\n",
      "Epoch: 2840|steps:   30|Train Avg Loss: 0.7122 |Test Loss: 0.9108|lr = 0.00010\n",
      "Epoch: 2840|steps:   60|Train Avg Loss: 0.6888 |Test Loss: 0.9218|lr = 0.00010\n",
      "Epoch: 2841|steps:   30|Train Avg Loss: 0.6940 |Test Loss: 0.8990|lr = 0.00010\n",
      "Epoch: 2841|steps:   60|Train Avg Loss: 0.7066 |Test Loss: 0.9338|lr = 0.00010\n",
      "Epoch: 2842|steps:   30|Train Avg Loss: 0.7026 |Test Loss: 0.9147|lr = 0.00010\n",
      "Epoch: 2842|steps:   60|Train Avg Loss: 0.6844 |Test Loss: 0.9322|lr = 0.00010\n",
      "Epoch: 2843|steps:   30|Train Avg Loss: 0.6992 |Test Loss: 0.9277|lr = 0.00010\n",
      "Epoch: 2843|steps:   60|Train Avg Loss: 0.6902 |Test Loss: 0.9098|lr = 0.00010\n",
      "Epoch: 2844|steps:   30|Train Avg Loss: 0.6930 |Test Loss: 0.9140|lr = 0.00010\n",
      "Epoch: 2844|steps:   60|Train Avg Loss: 0.6985 |Test Loss: 0.9225|lr = 0.00010\n",
      "Epoch: 2845|steps:   30|Train Avg Loss: 0.7026 |Test Loss: 0.9160|lr = 0.00010\n",
      "Epoch: 2845|steps:   60|Train Avg Loss: 0.6859 |Test Loss: 0.9186|lr = 0.00010\n",
      "Epoch: 2846|steps:   30|Train Avg Loss: 0.6775 |Test Loss: 0.9030|lr = 0.00010\n",
      "Epoch: 2846|steps:   60|Train Avg Loss: 0.7131 |Test Loss: 0.9144|lr = 0.00010\n",
      "Epoch: 2847|steps:   30|Train Avg Loss: 0.7041 |Test Loss: 0.9368|lr = 0.00010\n",
      "Epoch: 2847|steps:   60|Train Avg Loss: 0.7096 |Test Loss: 0.9190|lr = 0.00010\n",
      "Epoch: 2848|steps:   30|Train Avg Loss: 0.6961 |Test Loss: 0.9085|lr = 0.00010\n",
      "Epoch: 2848|steps:   60|Train Avg Loss: 0.6990 |Test Loss: 0.9409|lr = 0.00010\n",
      "Epoch: 2849|steps:   30|Train Avg Loss: 0.6971 |Test Loss: 0.9136|lr = 0.00010\n",
      "Epoch: 2849|steps:   60|Train Avg Loss: 0.6888 |Test Loss: 0.9218|lr = 0.00010\n",
      "Epoch: 2850|steps:   30|Train Avg Loss: 0.6891 |Test Loss: 0.9178|lr = 0.00010\n",
      "Epoch: 2850|steps:   60|Train Avg Loss: 0.6992 |Test Loss: 0.9159|lr = 0.00010\n",
      "Epoch: 2851|steps:   30|Train Avg Loss: 0.6904 |Test Loss: 0.9193|lr = 0.00010\n",
      "Epoch: 2851|steps:   60|Train Avg Loss: 0.7051 |Test Loss: 0.9192|lr = 0.00010\n",
      "Epoch: 2852|steps:   30|Train Avg Loss: 0.6902 |Test Loss: 0.9082|lr = 0.00010\n",
      "Epoch: 2852|steps:   60|Train Avg Loss: 0.7134 |Test Loss: 0.9426|lr = 0.00010\n",
      "Epoch: 2853|steps:   30|Train Avg Loss: 0.6895 |Test Loss: 0.9121|lr = 0.00010\n",
      "Epoch: 2853|steps:   60|Train Avg Loss: 0.6917 |Test Loss: 0.9098|lr = 0.00010\n",
      "Epoch: 2854|steps:   30|Train Avg Loss: 0.6873 |Test Loss: 0.9283|lr = 0.00010\n",
      "Epoch: 2854|steps:   60|Train Avg Loss: 0.7060 |Test Loss: 0.9222|lr = 0.00010\n",
      "Epoch: 2855|steps:   30|Train Avg Loss: 0.7110 |Test Loss: 0.9266|lr = 0.00010\n",
      "Epoch: 2855|steps:   60|Train Avg Loss: 0.6821 |Test Loss: 0.9157|lr = 0.00010\n",
      "Epoch: 2856|steps:   30|Train Avg Loss: 0.7013 |Test Loss: 0.9225|lr = 0.00010\n",
      "Epoch: 2856|steps:   60|Train Avg Loss: 0.6882 |Test Loss: 0.9178|lr = 0.00010\n",
      "Epoch: 2857|steps:   30|Train Avg Loss: 0.6695 |Test Loss: 0.9257|lr = 0.00010\n",
      "Epoch: 2857|steps:   60|Train Avg Loss: 0.7162 |Test Loss: 0.9201|lr = 0.00010\n",
      "Epoch: 2858|steps:   30|Train Avg Loss: 0.6965 |Test Loss: 0.9322|lr = 0.00010\n",
      "Epoch: 2858|steps:   60|Train Avg Loss: 0.6876 |Test Loss: 0.9218|lr = 0.00010\n",
      "Epoch: 2859|steps:   30|Train Avg Loss: 0.7002 |Test Loss: 0.9195|lr = 0.00010\n",
      "Epoch: 2859|steps:   60|Train Avg Loss: 0.6779 |Test Loss: 0.9171|lr = 0.00010\n",
      "Epoch: 2860|steps:   30|Train Avg Loss: 0.6852 |Test Loss: 0.9255|lr = 0.00010\n",
      "Epoch: 2860|steps:   60|Train Avg Loss: 0.7000 |Test Loss: 0.9392|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2861|steps:   30|Train Avg Loss: 0.6967 |Test Loss: 0.9161|lr = 0.00010\n",
      "Epoch: 2861|steps:   60|Train Avg Loss: 0.6882 |Test Loss: 0.9153|lr = 0.00010\n",
      "Epoch: 2862|steps:   30|Train Avg Loss: 0.6770 |Test Loss: 0.9174|lr = 0.00010\n",
      "Epoch: 2862|steps:   60|Train Avg Loss: 0.7043 |Test Loss: 0.9298|lr = 0.00010\n",
      "Epoch: 2863|steps:   30|Train Avg Loss: 0.6629 |Test Loss: 0.9102|lr = 0.00010\n",
      "Epoch: 2863|steps:   60|Train Avg Loss: 0.7308 |Test Loss: 0.9499|lr = 0.00010\n",
      "Epoch: 2864|steps:   30|Train Avg Loss: 0.6817 |Test Loss: 0.9202|lr = 0.00010\n",
      "Epoch: 2864|steps:   60|Train Avg Loss: 0.7066 |Test Loss: 0.9275|lr = 0.00010\n",
      "Epoch: 2865|steps:   30|Train Avg Loss: 0.6882 |Test Loss: 0.9171|lr = 0.00010\n",
      "Epoch: 2865|steps:   60|Train Avg Loss: 0.6869 |Test Loss: 0.9326|lr = 0.00010\n",
      "Epoch: 2866|steps:   30|Train Avg Loss: 0.6721 |Test Loss: 0.9289|lr = 0.00010\n",
      "Epoch: 2866|steps:   60|Train Avg Loss: 0.6930 |Test Loss: 0.9188|lr = 0.00010\n",
      "Epoch: 2867|steps:   30|Train Avg Loss: 0.6615 |Test Loss: 0.9237|lr = 0.00010\n",
      "Epoch: 2867|steps:   60|Train Avg Loss: 0.7261 |Test Loss: 0.9289|lr = 0.00010\n",
      "Epoch: 2868|steps:   30|Train Avg Loss: 0.6654 |Test Loss: 0.9262|lr = 0.00010\n",
      "Epoch: 2868|steps:   60|Train Avg Loss: 0.7006 |Test Loss: 0.9285|lr = 0.00010\n",
      "Epoch: 2869|steps:   30|Train Avg Loss: 0.6975 |Test Loss: 0.9274|lr = 0.00010\n",
      "Epoch: 2869|steps:   60|Train Avg Loss: 0.6660 |Test Loss: 0.9180|lr = 0.00010\n",
      "Epoch: 2870|steps:   30|Train Avg Loss: 0.6822 |Test Loss: 0.9383|lr = 0.00010\n",
      "Epoch: 2870|steps:   60|Train Avg Loss: 0.6967 |Test Loss: 0.9241|lr = 0.00010\n",
      "Epoch: 2871|steps:   30|Train Avg Loss: 0.6732 |Test Loss: 0.9329|lr = 0.00010\n",
      "Epoch: 2871|steps:   60|Train Avg Loss: 0.6966 |Test Loss: 0.9317|lr = 0.00010\n",
      "Epoch: 2872|steps:   30|Train Avg Loss: 0.6912 |Test Loss: 0.9210|lr = 0.00010\n",
      "Epoch: 2872|steps:   60|Train Avg Loss: 0.6835 |Test Loss: 0.9338|lr = 0.00010\n",
      "Epoch: 2873|steps:   30|Train Avg Loss: 0.6856 |Test Loss: 0.9208|lr = 0.00010\n",
      "Epoch: 2873|steps:   60|Train Avg Loss: 0.6842 |Test Loss: 0.9426|lr = 0.00010\n",
      "Epoch: 2874|steps:   30|Train Avg Loss: 0.6879 |Test Loss: 0.9190|lr = 0.00010\n",
      "Epoch: 2874|steps:   60|Train Avg Loss: 0.6887 |Test Loss: 0.9221|lr = 0.00010\n",
      "Epoch: 2875|steps:   30|Train Avg Loss: 0.6899 |Test Loss: 0.9341|lr = 0.00010\n",
      "Epoch: 2875|steps:   60|Train Avg Loss: 0.6846 |Test Loss: 0.9301|lr = 0.00010\n",
      "Epoch: 2876|steps:   30|Train Avg Loss: 0.6840 |Test Loss: 0.9325|lr = 0.00010\n",
      "Epoch: 2876|steps:   60|Train Avg Loss: 0.6952 |Test Loss: 0.9244|lr = 0.00010\n",
      "Epoch: 2877|steps:   30|Train Avg Loss: 0.6865 |Test Loss: 0.9392|lr = 0.00010\n",
      "Epoch: 2877|steps:   60|Train Avg Loss: 0.6829 |Test Loss: 0.9347|lr = 0.00010\n",
      "Epoch: 2878|steps:   30|Train Avg Loss: 0.6611 |Test Loss: 0.9212|lr = 0.00010\n",
      "Epoch: 2878|steps:   60|Train Avg Loss: 0.6950 |Test Loss: 0.9401|lr = 0.00010\n",
      "Epoch: 2879|steps:   30|Train Avg Loss: 0.6971 |Test Loss: 0.9192|lr = 0.00010\n",
      "Epoch: 2879|steps:   60|Train Avg Loss: 0.6646 |Test Loss: 0.9242|lr = 0.00010\n",
      "Epoch: 2880|steps:   30|Train Avg Loss: 0.6743 |Test Loss: 0.9217|lr = 0.00010\n",
      "Epoch: 2880|steps:   60|Train Avg Loss: 0.6962 |Test Loss: 0.9369|lr = 0.00010\n",
      "Epoch: 2881|steps:   30|Train Avg Loss: 0.6861 |Test Loss: 0.9348|lr = 0.00010\n",
      "Epoch: 2881|steps:   60|Train Avg Loss: 0.6861 |Test Loss: 0.9346|lr = 0.00010\n",
      "Epoch: 2882|steps:   30|Train Avg Loss: 0.6682 |Test Loss: 0.9478|lr = 0.00010\n",
      "Epoch: 2882|steps:   60|Train Avg Loss: 0.7026 |Test Loss: 0.9258|lr = 0.00010\n",
      "Epoch: 2883|steps:   30|Train Avg Loss: 0.6733 |Test Loss: 0.9362|lr = 0.00010\n",
      "Epoch: 2883|steps:   60|Train Avg Loss: 0.6842 |Test Loss: 0.9237|lr = 0.00010\n",
      "Epoch: 2884|steps:   30|Train Avg Loss: 0.6859 |Test Loss: 0.9420|lr = 0.00010\n",
      "Epoch: 2884|steps:   60|Train Avg Loss: 0.6566 |Test Loss: 0.9191|lr = 0.00010\n",
      "Epoch: 2885|steps:   30|Train Avg Loss: 0.6668 |Test Loss: 0.9352|lr = 0.00010\n",
      "Epoch: 2885|steps:   60|Train Avg Loss: 0.6852 |Test Loss: 0.9307|lr = 0.00010\n",
      "Epoch: 2886|steps:   30|Train Avg Loss: 0.6845 |Test Loss: 0.9320|lr = 0.00010\n",
      "Epoch: 2886|steps:   60|Train Avg Loss: 0.6799 |Test Loss: 0.9224|lr = 0.00010\n",
      "Epoch: 2887|steps:   30|Train Avg Loss: 0.6949 |Test Loss: 0.9376|lr = 0.00010\n",
      "Epoch: 2887|steps:   60|Train Avg Loss: 0.6662 |Test Loss: 0.9333|lr = 0.00010\n",
      "Epoch: 2888|steps:   30|Train Avg Loss: 0.6745 |Test Loss: 0.9350|lr = 0.00010\n",
      "Epoch: 2888|steps:   60|Train Avg Loss: 0.6803 |Test Loss: 0.9231|lr = 0.00010\n",
      "Epoch: 2889|steps:   30|Train Avg Loss: 0.6683 |Test Loss: 0.9348|lr = 0.00010\n",
      "Epoch: 2889|steps:   60|Train Avg Loss: 0.6676 |Test Loss: 0.9310|lr = 0.00010\n",
      "Epoch: 2890|steps:   30|Train Avg Loss: 0.6867 |Test Loss: 0.9466|lr = 0.00010\n",
      "Epoch: 2890|steps:   60|Train Avg Loss: 0.6642 |Test Loss: 0.9242|lr = 0.00010\n",
      "Epoch: 2891|steps:   30|Train Avg Loss: 0.6482 |Test Loss: 0.9191|lr = 0.00010\n",
      "Epoch: 2891|steps:   60|Train Avg Loss: 0.6886 |Test Loss: 0.9451|lr = 0.00010\n",
      "Epoch: 2892|steps:   30|Train Avg Loss: 0.6810 |Test Loss: 0.9459|lr = 0.00010\n",
      "Epoch: 2892|steps:   60|Train Avg Loss: 0.6776 |Test Loss: 0.9315|lr = 0.00010\n",
      "Epoch: 2893|steps:   30|Train Avg Loss: 0.6784 |Test Loss: 0.9398|lr = 0.00010\n",
      "Epoch: 2893|steps:   60|Train Avg Loss: 0.6901 |Test Loss: 0.9402|lr = 0.00010\n",
      "Epoch: 2894|steps:   30|Train Avg Loss: 0.6662 |Test Loss: 0.9332|lr = 0.00010\n",
      "Epoch: 2894|steps:   60|Train Avg Loss: 0.6965 |Test Loss: 0.9429|lr = 0.00010\n",
      "Epoch: 2895|steps:   30|Train Avg Loss: 0.6630 |Test Loss: 0.9503|lr = 0.00010\n",
      "Epoch: 2895|steps:   60|Train Avg Loss: 0.6724 |Test Loss: 0.9273|lr = 0.00010\n",
      "Epoch: 2896|steps:   30|Train Avg Loss: 0.6960 |Test Loss: 0.9473|lr = 0.00010\n",
      "Epoch: 2896|steps:   60|Train Avg Loss: 0.6656 |Test Loss: 0.9422|lr = 0.00010\n",
      "Epoch: 2897|steps:   30|Train Avg Loss: 0.6795 |Test Loss: 0.9303|lr = 0.00010\n",
      "Epoch: 2897|steps:   60|Train Avg Loss: 0.6665 |Test Loss: 0.9374|lr = 0.00010\n",
      "Epoch: 2898|steps:   30|Train Avg Loss: 0.6841 |Test Loss: 0.9293|lr = 0.00010\n",
      "Epoch: 2898|steps:   60|Train Avg Loss: 0.6721 |Test Loss: 0.9455|lr = 0.00010\n",
      "Epoch: 2899|steps:   30|Train Avg Loss: 0.6649 |Test Loss: 0.9208|lr = 0.00010\n",
      "Epoch: 2899|steps:   60|Train Avg Loss: 0.6796 |Test Loss: 0.9440|lr = 0.00010\n",
      "Epoch: 2900|steps:   30|Train Avg Loss: 0.6630 |Test Loss: 0.9350|lr = 0.00010\n",
      "Epoch: 2900|steps:   60|Train Avg Loss: 0.6832 |Test Loss: 0.9375|lr = 0.00010\n",
      "Epoch: 2901|steps:   30|Train Avg Loss: 0.6621 |Test Loss: 0.9500|lr = 0.00010\n",
      "Epoch: 2901|steps:   60|Train Avg Loss: 0.6643 |Test Loss: 0.9273|lr = 0.00010\n",
      "Epoch: 2902|steps:   30|Train Avg Loss: 0.6675 |Test Loss: 0.9479|lr = 0.00010\n",
      "Epoch: 2902|steps:   60|Train Avg Loss: 0.6732 |Test Loss: 0.9439|lr = 0.00010\n",
      "Epoch: 2903|steps:   30|Train Avg Loss: 0.6680 |Test Loss: 0.9576|lr = 0.00010\n",
      "Epoch: 2903|steps:   60|Train Avg Loss: 0.6664 |Test Loss: 0.9410|lr = 0.00010\n",
      "Epoch: 2904|steps:   30|Train Avg Loss: 0.6796 |Test Loss: 0.9351|lr = 0.00010\n",
      "Epoch: 2904|steps:   60|Train Avg Loss: 0.6582 |Test Loss: 0.9370|lr = 0.00010\n",
      "Epoch: 2905|steps:   30|Train Avg Loss: 0.6724 |Test Loss: 0.9444|lr = 0.00010\n",
      "Epoch: 2905|steps:   60|Train Avg Loss: 0.6714 |Test Loss: 0.9411|lr = 0.00010\n",
      "Epoch: 2906|steps:   30|Train Avg Loss: 0.6728 |Test Loss: 0.9369|lr = 0.00010\n",
      "Epoch: 2906|steps:   60|Train Avg Loss: 0.6625 |Test Loss: 0.9334|lr = 0.00010\n",
      "Epoch: 2907|steps:   30|Train Avg Loss: 0.6664 |Test Loss: 0.9450|lr = 0.00010\n",
      "Epoch: 2907|steps:   60|Train Avg Loss: 0.6634 |Test Loss: 0.9279|lr = 0.00010\n",
      "Epoch: 2908|steps:   30|Train Avg Loss: 0.6585 |Test Loss: 0.9369|lr = 0.00010\n",
      "Epoch: 2908|steps:   60|Train Avg Loss: 0.6755 |Test Loss: 0.9442|lr = 0.00010\n",
      "Epoch: 2909|steps:   30|Train Avg Loss: 0.6656 |Test Loss: 0.9516|lr = 0.00010\n",
      "Epoch: 2909|steps:   60|Train Avg Loss: 0.6625 |Test Loss: 0.9304|lr = 0.00010\n",
      "Epoch: 2910|steps:   30|Train Avg Loss: 0.6513 |Test Loss: 0.9368|lr = 0.00010\n",
      "Epoch: 2910|steps:   60|Train Avg Loss: 0.6813 |Test Loss: 0.9454|lr = 0.00010\n",
      "Epoch: 2911|steps:   30|Train Avg Loss: 0.6536 |Test Loss: 0.9624|lr = 0.00010\n",
      "Epoch: 2911|steps:   60|Train Avg Loss: 0.6740 |Test Loss: 0.9319|lr = 0.00010\n",
      "Epoch: 2912|steps:   30|Train Avg Loss: 0.6571 |Test Loss: 0.9441|lr = 0.00010\n",
      "Epoch: 2912|steps:   60|Train Avg Loss: 0.6746 |Test Loss: 0.9327|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2913|steps:   30|Train Avg Loss: 0.6519 |Test Loss: 0.9505|lr = 0.00010\n",
      "Epoch: 2913|steps:   60|Train Avg Loss: 0.6667 |Test Loss: 0.9429|lr = 0.00010\n",
      "Epoch: 2914|steps:   30|Train Avg Loss: 0.6673 |Test Loss: 0.9364|lr = 0.00010\n",
      "Epoch: 2914|steps:   60|Train Avg Loss: 0.6687 |Test Loss: 0.9455|lr = 0.00010\n",
      "Epoch: 2915|steps:   30|Train Avg Loss: 0.6654 |Test Loss: 0.9353|lr = 0.00010\n",
      "Epoch: 2915|steps:   60|Train Avg Loss: 0.6746 |Test Loss: 0.9518|lr = 0.00010\n",
      "Epoch: 2916|steps:   30|Train Avg Loss: 0.6540 |Test Loss: 0.9343|lr = 0.00010\n",
      "Epoch: 2916|steps:   60|Train Avg Loss: 0.6751 |Test Loss: 0.9492|lr = 0.00010\n",
      "Epoch: 2917|steps:   30|Train Avg Loss: 0.6603 |Test Loss: 0.9524|lr = 0.00010\n",
      "Epoch: 2917|steps:   60|Train Avg Loss: 0.6858 |Test Loss: 0.9535|lr = 0.00010\n",
      "Epoch: 2918|steps:   30|Train Avg Loss: 0.6575 |Test Loss: 0.9293|lr = 0.00010\n",
      "Epoch: 2918|steps:   60|Train Avg Loss: 0.6636 |Test Loss: 0.9495|lr = 0.00010\n",
      "Epoch: 2919|steps:   30|Train Avg Loss: 0.6596 |Test Loss: 0.9302|lr = 0.00010\n",
      "Epoch: 2919|steps:   60|Train Avg Loss: 0.6740 |Test Loss: 0.9462|lr = 0.00010\n",
      "Epoch: 2920|steps:   30|Train Avg Loss: 0.6567 |Test Loss: 0.9317|lr = 0.00010\n",
      "Epoch: 2920|steps:   60|Train Avg Loss: 0.6736 |Test Loss: 0.9481|lr = 0.00010\n",
      "Epoch: 2921|steps:   30|Train Avg Loss: 0.6420 |Test Loss: 0.9324|lr = 0.00010\n",
      "Epoch: 2921|steps:   60|Train Avg Loss: 0.6832 |Test Loss: 0.9537|lr = 0.00010\n",
      "Epoch: 2922|steps:   30|Train Avg Loss: 0.6573 |Test Loss: 0.9505|lr = 0.00010\n",
      "Epoch: 2922|steps:   60|Train Avg Loss: 0.6688 |Test Loss: 0.9349|lr = 0.00010\n",
      "Epoch: 2923|steps:   30|Train Avg Loss: 0.6444 |Test Loss: 0.9508|lr = 0.00010\n",
      "Epoch: 2923|steps:   60|Train Avg Loss: 0.6816 |Test Loss: 0.9482|lr = 0.00010\n",
      "Epoch: 2924|steps:   30|Train Avg Loss: 0.6510 |Test Loss: 0.9406|lr = 0.00010\n",
      "Epoch: 2924|steps:   60|Train Avg Loss: 0.6836 |Test Loss: 0.9590|lr = 0.00010\n",
      "Epoch: 2925|steps:   30|Train Avg Loss: 0.6713 |Test Loss: 0.9513|lr = 0.00010\n",
      "Epoch: 2925|steps:   60|Train Avg Loss: 0.6580 |Test Loss: 0.9455|lr = 0.00010\n",
      "Epoch: 2926|steps:   30|Train Avg Loss: 0.6533 |Test Loss: 0.9349|lr = 0.00010\n",
      "Epoch: 2926|steps:   60|Train Avg Loss: 0.6620 |Test Loss: 0.9566|lr = 0.00010\n",
      "Epoch: 2927|steps:   30|Train Avg Loss: 0.6827 |Test Loss: 0.9548|lr = 0.00010\n",
      "Epoch: 2927|steps:   60|Train Avg Loss: 0.6358 |Test Loss: 0.9491|lr = 0.00010\n",
      "Epoch: 2928|steps:   30|Train Avg Loss: 0.6511 |Test Loss: 0.9449|lr = 0.00010\n",
      "Epoch: 2928|steps:   60|Train Avg Loss: 0.6713 |Test Loss: 0.9421|lr = 0.00010\n",
      "Epoch: 2929|steps:   30|Train Avg Loss: 0.6564 |Test Loss: 0.9590|lr = 0.00010\n",
      "Epoch: 2929|steps:   60|Train Avg Loss: 0.6739 |Test Loss: 0.9632|lr = 0.00010\n",
      "Epoch: 2930|steps:   30|Train Avg Loss: 0.6442 |Test Loss: 0.9462|lr = 0.00010\n",
      "Epoch: 2930|steps:   60|Train Avg Loss: 0.6796 |Test Loss: 0.9540|lr = 0.00010\n",
      "Epoch: 2931|steps:   30|Train Avg Loss: 0.6465 |Test Loss: 0.9386|lr = 0.00010\n",
      "Epoch: 2931|steps:   60|Train Avg Loss: 0.6648 |Test Loss: 0.9500|lr = 0.00010\n",
      "Epoch: 2932|steps:   30|Train Avg Loss: 0.6531 |Test Loss: 0.9513|lr = 0.00010\n",
      "Epoch: 2932|steps:   60|Train Avg Loss: 0.6458 |Test Loss: 0.9576|lr = 0.00010\n",
      "Epoch: 2933|steps:   30|Train Avg Loss: 0.6354 |Test Loss: 0.9467|lr = 0.00010\n",
      "Epoch: 2933|steps:   60|Train Avg Loss: 0.6773 |Test Loss: 0.9549|lr = 0.00010\n",
      "Epoch: 2934|steps:   30|Train Avg Loss: 0.6693 |Test Loss: 0.9567|lr = 0.00010\n",
      "Epoch: 2934|steps:   60|Train Avg Loss: 0.6355 |Test Loss: 0.9357|lr = 0.00010\n",
      "Epoch: 2935|steps:   30|Train Avg Loss: 0.6506 |Test Loss: 0.9832|lr = 0.00010\n",
      "Epoch: 2935|steps:   60|Train Avg Loss: 0.6516 |Test Loss: 0.9293|lr = 0.00010\n",
      "Epoch: 2936|steps:   30|Train Avg Loss: 0.6499 |Test Loss: 0.9513|lr = 0.00010\n",
      "Epoch: 2936|steps:   60|Train Avg Loss: 0.6537 |Test Loss: 0.9581|lr = 0.00010\n",
      "Epoch: 2937|steps:   30|Train Avg Loss: 0.6586 |Test Loss: 0.9618|lr = 0.00010\n",
      "Epoch: 2937|steps:   60|Train Avg Loss: 0.6497 |Test Loss: 0.9490|lr = 0.00010\n",
      "Epoch: 2938|steps:   30|Train Avg Loss: 0.6530 |Test Loss: 0.9453|lr = 0.00010\n",
      "Epoch: 2938|steps:   60|Train Avg Loss: 0.6593 |Test Loss: 0.9573|lr = 0.00010\n",
      "Epoch: 2939|steps:   30|Train Avg Loss: 0.6433 |Test Loss: 0.9635|lr = 0.00010\n",
      "Epoch: 2939|steps:   60|Train Avg Loss: 0.6566 |Test Loss: 0.9454|lr = 0.00010\n",
      "Epoch: 2940|steps:   30|Train Avg Loss: 0.6215 |Test Loss: 0.9354|lr = 0.00010\n",
      "Epoch: 2940|steps:   60|Train Avg Loss: 0.6674 |Test Loss: 0.9467|lr = 0.00010\n",
      "Epoch: 2941|steps:   30|Train Avg Loss: 0.6433 |Test Loss: 0.9464|lr = 0.00010\n",
      "Epoch: 2941|steps:   60|Train Avg Loss: 0.6671 |Test Loss: 0.9531|lr = 0.00010\n",
      "Epoch: 2942|steps:   30|Train Avg Loss: 0.6469 |Test Loss: 0.9499|lr = 0.00010\n",
      "Epoch: 2942|steps:   60|Train Avg Loss: 0.6548 |Test Loss: 0.9533|lr = 0.00010\n",
      "Epoch: 2943|steps:   30|Train Avg Loss: 0.6479 |Test Loss: 0.9405|lr = 0.00010\n",
      "Epoch: 2943|steps:   60|Train Avg Loss: 0.6689 |Test Loss: 0.9520|lr = 0.00010\n",
      "Epoch: 2944|steps:   30|Train Avg Loss: 0.6370 |Test Loss: 0.9571|lr = 0.00010\n",
      "Epoch: 2944|steps:   60|Train Avg Loss: 0.6479 |Test Loss: 0.9475|lr = 0.00010\n",
      "Epoch: 2945|steps:   30|Train Avg Loss: 0.6572 |Test Loss: 0.9608|lr = 0.00010\n",
      "Epoch: 2945|steps:   60|Train Avg Loss: 0.6425 |Test Loss: 0.9488|lr = 0.00010\n",
      "Epoch: 2946|steps:   30|Train Avg Loss: 0.6629 |Test Loss: 0.9651|lr = 0.00010\n",
      "Epoch: 2946|steps:   60|Train Avg Loss: 0.6561 |Test Loss: 0.9475|lr = 0.00010\n",
      "Epoch: 2947|steps:   30|Train Avg Loss: 0.6458 |Test Loss: 0.9539|lr = 0.00010\n",
      "Epoch: 2947|steps:   60|Train Avg Loss: 0.6469 |Test Loss: 0.9438|lr = 0.00010\n",
      "Epoch: 2948|steps:   30|Train Avg Loss: 0.6488 |Test Loss: 0.9540|lr = 0.00010\n",
      "Epoch: 2948|steps:   60|Train Avg Loss: 0.6492 |Test Loss: 0.9549|lr = 0.00010\n",
      "Epoch: 2949|steps:   30|Train Avg Loss: 0.6657 |Test Loss: 0.9651|lr = 0.00010\n",
      "Epoch: 2949|steps:   60|Train Avg Loss: 0.6411 |Test Loss: 0.9487|lr = 0.00010\n",
      "Epoch: 2950|steps:   30|Train Avg Loss: 0.6191 |Test Loss: 0.9476|lr = 0.00010\n",
      "Epoch: 2950|steps:   60|Train Avg Loss: 0.6874 |Test Loss: 0.9698|lr = 0.00010\n",
      "Epoch: 2951|steps:   30|Train Avg Loss: 0.6311 |Test Loss: 0.9542|lr = 0.00010\n",
      "Epoch: 2951|steps:   60|Train Avg Loss: 0.6511 |Test Loss: 0.9408|lr = 0.00010\n",
      "Epoch: 2952|steps:   30|Train Avg Loss: 0.6407 |Test Loss: 0.9801|lr = 0.00010\n",
      "Epoch: 2952|steps:   60|Train Avg Loss: 0.6574 |Test Loss: 0.9455|lr = 0.00010\n",
      "Epoch: 2953|steps:   30|Train Avg Loss: 0.6383 |Test Loss: 0.9632|lr = 0.00010\n",
      "Epoch: 2953|steps:   60|Train Avg Loss: 0.6430 |Test Loss: 0.9522|lr = 0.00010\n",
      "Epoch: 2954|steps:   30|Train Avg Loss: 0.6540 |Test Loss: 0.9607|lr = 0.00010\n",
      "Epoch: 2954|steps:   60|Train Avg Loss: 0.6397 |Test Loss: 0.9589|lr = 0.00010\n",
      "Epoch: 2955|steps:   30|Train Avg Loss: 0.6348 |Test Loss: 0.9634|lr = 0.00010\n",
      "Epoch: 2955|steps:   60|Train Avg Loss: 0.6608 |Test Loss: 0.9641|lr = 0.00010\n",
      "Epoch: 2956|steps:   30|Train Avg Loss: 0.6232 |Test Loss: 0.9525|lr = 0.00010\n",
      "Epoch: 2956|steps:   60|Train Avg Loss: 0.6668 |Test Loss: 0.9689|lr = 0.00010\n",
      "Epoch: 2957|steps:   30|Train Avg Loss: 0.6483 |Test Loss: 0.9670|lr = 0.00010\n",
      "Epoch: 2957|steps:   60|Train Avg Loss: 0.6294 |Test Loss: 0.9646|lr = 0.00010\n",
      "Epoch: 2958|steps:   30|Train Avg Loss: 0.6273 |Test Loss: 0.9699|lr = 0.00010\n",
      "Epoch: 2958|steps:   60|Train Avg Loss: 0.6545 |Test Loss: 0.9604|lr = 0.00010\n",
      "Epoch: 2959|steps:   30|Train Avg Loss: 0.6261 |Test Loss: 0.9520|lr = 0.00010\n",
      "Epoch: 2959|steps:   60|Train Avg Loss: 0.6767 |Test Loss: 0.9722|lr = 0.00010\n",
      "Epoch: 2960|steps:   30|Train Avg Loss: 0.6417 |Test Loss: 0.9566|lr = 0.00010\n",
      "Epoch: 2960|steps:   60|Train Avg Loss: 0.6202 |Test Loss: 0.9651|lr = 0.00010\n",
      "Epoch: 2961|steps:   30|Train Avg Loss: 0.6407 |Test Loss: 0.9510|lr = 0.00010\n",
      "Epoch: 2961|steps:   60|Train Avg Loss: 0.6476 |Test Loss: 0.9649|lr = 0.00010\n",
      "Epoch: 2962|steps:   30|Train Avg Loss: 0.6548 |Test Loss: 0.9663|lr = 0.00010\n",
      "Epoch: 2962|steps:   60|Train Avg Loss: 0.6272 |Test Loss: 0.9609|lr = 0.00010\n",
      "Epoch: 2963|steps:   30|Train Avg Loss: 0.6510 |Test Loss: 0.9626|lr = 0.00010\n",
      "Epoch: 2963|steps:   60|Train Avg Loss: 0.6288 |Test Loss: 0.9717|lr = 0.00010\n",
      "Epoch: 2964|steps:   30|Train Avg Loss: 0.6289 |Test Loss: 0.9444|lr = 0.00010\n",
      "Epoch: 2964|steps:   60|Train Avg Loss: 0.6475 |Test Loss: 0.9888|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2965|steps:   30|Train Avg Loss: 0.6489 |Test Loss: 0.9856|lr = 0.00010\n",
      "Epoch: 2965|steps:   60|Train Avg Loss: 0.6258 |Test Loss: 0.9460|lr = 0.00010\n",
      "Epoch: 2966|steps:   30|Train Avg Loss: 0.6321 |Test Loss: 0.9644|lr = 0.00010\n",
      "Epoch: 2966|steps:   60|Train Avg Loss: 0.6369 |Test Loss: 0.9767|lr = 0.00010\n",
      "Epoch: 2967|steps:   30|Train Avg Loss: 0.6530 |Test Loss: 0.9670|lr = 0.00010\n",
      "Epoch: 2967|steps:   60|Train Avg Loss: 0.6273 |Test Loss: 0.9634|lr = 0.00010\n",
      "Epoch: 2968|steps:   30|Train Avg Loss: 0.6239 |Test Loss: 0.9523|lr = 0.00010\n",
      "Epoch: 2968|steps:   60|Train Avg Loss: 0.6503 |Test Loss: 0.9683|lr = 0.00010\n",
      "Epoch: 2969|steps:   30|Train Avg Loss: 0.6441 |Test Loss: 0.9710|lr = 0.00010\n",
      "Epoch: 2969|steps:   60|Train Avg Loss: 0.6461 |Test Loss: 0.9711|lr = 0.00010\n",
      "Epoch: 2970|steps:   30|Train Avg Loss: 0.6210 |Test Loss: 0.9743|lr = 0.00010\n",
      "Epoch: 2970|steps:   60|Train Avg Loss: 0.6461 |Test Loss: 0.9746|lr = 0.00010\n",
      "Epoch: 2971|steps:   30|Train Avg Loss: 0.6429 |Test Loss: 0.9777|lr = 0.00010\n",
      "Epoch: 2971|steps:   60|Train Avg Loss: 0.6359 |Test Loss: 0.9550|lr = 0.00010\n",
      "Epoch: 2972|steps:   30|Train Avg Loss: 0.6187 |Test Loss: 0.9824|lr = 0.00010\n",
      "Epoch: 2972|steps:   60|Train Avg Loss: 0.6468 |Test Loss: 0.9453|lr = 0.00010\n",
      "Epoch: 2973|steps:   30|Train Avg Loss: 0.6284 |Test Loss: 0.9628|lr = 0.00010\n",
      "Epoch: 2973|steps:   60|Train Avg Loss: 0.6565 |Test Loss: 0.9831|lr = 0.00010\n",
      "Epoch: 2974|steps:   30|Train Avg Loss: 0.6071 |Test Loss: 0.9594|lr = 0.00010\n",
      "Epoch: 2974|steps:   60|Train Avg Loss: 0.6541 |Test Loss: 0.9826|lr = 0.00010\n",
      "Epoch: 2975|steps:   30|Train Avg Loss: 0.6281 |Test Loss: 0.9646|lr = 0.00010\n",
      "Epoch: 2975|steps:   60|Train Avg Loss: 0.6632 |Test Loss: 0.9935|lr = 0.00010\n",
      "Epoch: 2976|steps:   30|Train Avg Loss: 0.6123 |Test Loss: 0.9783|lr = 0.00010\n",
      "Epoch: 2976|steps:   60|Train Avg Loss: 0.6404 |Test Loss: 0.9637|lr = 0.00010\n",
      "Epoch: 2977|steps:   30|Train Avg Loss: 0.6544 |Test Loss: 0.9545|lr = 0.00010\n",
      "Epoch: 2977|steps:   60|Train Avg Loss: 0.6187 |Test Loss: 0.9638|lr = 0.00010\n",
      "Epoch: 2978|steps:   30|Train Avg Loss: 0.6042 |Test Loss: 0.9663|lr = 0.00010\n",
      "Epoch: 2978|steps:   60|Train Avg Loss: 0.6509 |Test Loss: 0.9664|lr = 0.00010\n",
      "Epoch: 2979|steps:   30|Train Avg Loss: 0.6093 |Test Loss: 0.9908|lr = 0.00010\n",
      "Epoch: 2979|steps:   60|Train Avg Loss: 0.6411 |Test Loss: 0.9608|lr = 0.00010\n",
      "Epoch: 2980|steps:   30|Train Avg Loss: 0.6302 |Test Loss: 0.9801|lr = 0.00010\n",
      "Epoch: 2980|steps:   60|Train Avg Loss: 0.6593 |Test Loss: 0.9524|lr = 0.00010\n",
      "Epoch: 2981|steps:   30|Train Avg Loss: 0.6198 |Test Loss: 0.9692|lr = 0.00010\n",
      "Epoch: 2981|steps:   60|Train Avg Loss: 0.6326 |Test Loss: 0.9894|lr = 0.00010\n",
      "Epoch: 2982|steps:   30|Train Avg Loss: 0.6420 |Test Loss: 0.9741|lr = 0.00010\n",
      "Epoch: 2982|steps:   60|Train Avg Loss: 0.6294 |Test Loss: 0.9763|lr = 0.00010\n",
      "Epoch: 2983|steps:   30|Train Avg Loss: 0.6221 |Test Loss: 0.9600|lr = 0.00010\n",
      "Epoch: 2983|steps:   60|Train Avg Loss: 0.6515 |Test Loss: 0.9891|lr = 0.00010\n",
      "Epoch: 2984|steps:   30|Train Avg Loss: 0.6273 |Test Loss: 0.9723|lr = 0.00010\n",
      "Epoch: 2984|steps:   60|Train Avg Loss: 0.6291 |Test Loss: 0.9785|lr = 0.00010\n",
      "Epoch: 2985|steps:   30|Train Avg Loss: 0.6213 |Test Loss: 0.9745|lr = 0.00010\n",
      "Epoch: 2985|steps:   60|Train Avg Loss: 0.6372 |Test Loss: 0.9858|lr = 0.00010\n",
      "Epoch: 2986|steps:   30|Train Avg Loss: 0.6442 |Test Loss: 0.9695|lr = 0.00010\n",
      "Epoch: 2986|steps:   60|Train Avg Loss: 0.6056 |Test Loss: 0.9887|lr = 0.00010\n",
      "Epoch: 2987|steps:   30|Train Avg Loss: 0.6185 |Test Loss: 0.9792|lr = 0.00010\n",
      "Epoch: 2987|steps:   60|Train Avg Loss: 0.6323 |Test Loss: 0.9767|lr = 0.00010\n",
      "Epoch: 2988|steps:   30|Train Avg Loss: 0.6121 |Test Loss: 0.9894|lr = 0.00010\n",
      "Epoch: 2988|steps:   60|Train Avg Loss: 0.6438 |Test Loss: 0.9606|lr = 0.00010\n",
      "Epoch: 2989|steps:   30|Train Avg Loss: 0.6203 |Test Loss: 0.9717|lr = 0.00010\n",
      "Epoch: 2989|steps:   60|Train Avg Loss: 0.6334 |Test Loss: 0.9750|lr = 0.00010\n",
      "Epoch: 2990|steps:   30|Train Avg Loss: 0.6128 |Test Loss: 0.9724|lr = 0.00010\n",
      "Epoch: 2990|steps:   60|Train Avg Loss: 0.6365 |Test Loss: 0.9774|lr = 0.00010\n",
      "Epoch: 2991|steps:   30|Train Avg Loss: 0.6181 |Test Loss: 0.9803|lr = 0.00010\n",
      "Epoch: 2991|steps:   60|Train Avg Loss: 0.6377 |Test Loss: 0.9563|lr = 0.00010\n",
      "Epoch: 2992|steps:   30|Train Avg Loss: 0.6245 |Test Loss: 0.9713|lr = 0.00010\n",
      "Epoch: 2992|steps:   60|Train Avg Loss: 0.6214 |Test Loss: 0.9787|lr = 0.00010\n",
      "Epoch: 2993|steps:   30|Train Avg Loss: 0.6172 |Test Loss: 0.9882|lr = 0.00010\n",
      "Epoch: 2993|steps:   60|Train Avg Loss: 0.6299 |Test Loss: 0.9797|lr = 0.00010\n",
      "Epoch: 2994|steps:   30|Train Avg Loss: 0.6305 |Test Loss: 0.9628|lr = 0.00010\n",
      "Epoch: 2994|steps:   60|Train Avg Loss: 0.6189 |Test Loss: 0.9831|lr = 0.00010\n",
      "Epoch: 2995|steps:   30|Train Avg Loss: 0.6062 |Test Loss: 0.9889|lr = 0.00010\n",
      "Epoch: 2995|steps:   60|Train Avg Loss: 0.6399 |Test Loss: 0.9642|lr = 0.00010\n",
      "Epoch: 2996|steps:   30|Train Avg Loss: 0.6148 |Test Loss: 0.9751|lr = 0.00010\n",
      "Epoch: 2996|steps:   60|Train Avg Loss: 0.6261 |Test Loss: 0.9824|lr = 0.00010\n",
      "Epoch: 2997|steps:   30|Train Avg Loss: 0.6113 |Test Loss: 0.9783|lr = 0.00010\n",
      "Epoch: 2997|steps:   60|Train Avg Loss: 0.6190 |Test Loss: 0.9701|lr = 0.00010\n",
      "Epoch: 2998|steps:   30|Train Avg Loss: 0.5889 |Test Loss: 0.9744|lr = 0.00010\n",
      "Epoch: 2998|steps:   60|Train Avg Loss: 0.6353 |Test Loss: 0.9759|lr = 0.00010\n",
      "Epoch: 2999|steps:   30|Train Avg Loss: 0.6283 |Test Loss: 0.9680|lr = 0.00010\n",
      "Epoch: 2999|steps:   60|Train Avg Loss: 0.6180 |Test Loss: 1.0014|lr = 0.00010\n",
      "Epoch: 3000|steps:   30|Train Avg Loss: 0.6073 |Test Loss: 0.9843|lr = 0.00010\n",
      "Epoch: 3000|steps:   60|Train Avg Loss: 0.6356 |Test Loss: 0.9888|lr = 0.00010\n",
      "Epoch: 3001|steps:   30|Train Avg Loss: 0.6176 |Test Loss: 0.9771|lr = 0.00010\n",
      "Epoch: 3001|steps:   60|Train Avg Loss: 0.6269 |Test Loss: 0.9785|lr = 0.00010\n",
      "Epoch: 3002|steps:   30|Train Avg Loss: 0.6034 |Test Loss: 0.9850|lr = 0.00010\n",
      "Epoch: 3002|steps:   60|Train Avg Loss: 0.6247 |Test Loss: 0.9766|lr = 0.00010\n",
      "Epoch: 3003|steps:   30|Train Avg Loss: 0.6151 |Test Loss: 0.9859|lr = 0.00010\n",
      "Epoch: 3003|steps:   60|Train Avg Loss: 0.6130 |Test Loss: 0.9790|lr = 0.00010\n",
      "Epoch: 3004|steps:   30|Train Avg Loss: 0.6246 |Test Loss: 0.9950|lr = 0.00010\n",
      "Epoch: 3004|steps:   60|Train Avg Loss: 0.6237 |Test Loss: 0.9810|lr = 0.00010\n",
      "Epoch: 3005|steps:   30|Train Avg Loss: 0.6147 |Test Loss: 0.9866|lr = 0.00010\n",
      "Epoch: 3005|steps:   60|Train Avg Loss: 0.6145 |Test Loss: 0.9893|lr = 0.00010\n",
      "Epoch: 3006|steps:   30|Train Avg Loss: 0.5960 |Test Loss: 0.9808|lr = 0.00010\n",
      "Epoch: 3006|steps:   60|Train Avg Loss: 0.6298 |Test Loss: 0.9910|lr = 0.00010\n",
      "Epoch: 3007|steps:   30|Train Avg Loss: 0.6014 |Test Loss: 0.9770|lr = 0.00010\n",
      "Epoch: 3007|steps:   60|Train Avg Loss: 0.6238 |Test Loss: 1.0026|lr = 0.00010\n",
      "Epoch: 3008|steps:   30|Train Avg Loss: 0.6026 |Test Loss: 0.9691|lr = 0.00010\n",
      "Epoch: 3008|steps:   60|Train Avg Loss: 0.6226 |Test Loss: 0.9947|lr = 0.00010\n",
      "Epoch: 3009|steps:   30|Train Avg Loss: 0.6370 |Test Loss: 0.9836|lr = 0.00010\n",
      "Epoch: 3009|steps:   60|Train Avg Loss: 0.6091 |Test Loss: 0.9924|lr = 0.00010\n",
      "Epoch: 3010|steps:   30|Train Avg Loss: 0.6264 |Test Loss: 1.0019|lr = 0.00010\n",
      "Epoch: 3010|steps:   60|Train Avg Loss: 0.5922 |Test Loss: 1.0111|lr = 0.00010\n",
      "Epoch: 3011|steps:   30|Train Avg Loss: 0.6029 |Test Loss: 0.9824|lr = 0.00010\n",
      "Epoch: 3011|steps:   60|Train Avg Loss: 0.6102 |Test Loss: 0.9942|lr = 0.00010\n",
      "Epoch: 3012|steps:   30|Train Avg Loss: 0.6086 |Test Loss: 0.9885|lr = 0.00010\n",
      "Epoch: 3012|steps:   60|Train Avg Loss: 0.6186 |Test Loss: 0.9808|lr = 0.00010\n",
      "Epoch: 3013|steps:   30|Train Avg Loss: 0.6226 |Test Loss: 0.9901|lr = 0.00010\n",
      "Epoch: 3013|steps:   60|Train Avg Loss: 0.5870 |Test Loss: 1.0021|lr = 0.00010\n",
      "Epoch: 3014|steps:   30|Train Avg Loss: 0.5899 |Test Loss: 0.9870|lr = 0.00010\n",
      "Epoch: 3014|steps:   60|Train Avg Loss: 0.6499 |Test Loss: 0.9968|lr = 0.00010\n",
      "Epoch: 3015|steps:   30|Train Avg Loss: 0.6183 |Test Loss: 0.9912|lr = 0.00010\n",
      "Epoch: 3015|steps:   60|Train Avg Loss: 0.6025 |Test Loss: 0.9801|lr = 0.00010\n",
      "Epoch: 3016|steps:   30|Train Avg Loss: 0.6061 |Test Loss: 0.9935|lr = 0.00010\n",
      "Epoch: 3016|steps:   60|Train Avg Loss: 0.6092 |Test Loss: 1.0010|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3017|steps:   30|Train Avg Loss: 0.6103 |Test Loss: 0.9845|lr = 0.00010\n",
      "Epoch: 3017|steps:   60|Train Avg Loss: 0.5967 |Test Loss: 0.9981|lr = 0.00010\n",
      "Epoch: 3018|steps:   30|Train Avg Loss: 0.6041 |Test Loss: 0.9795|lr = 0.00010\n",
      "Epoch: 3018|steps:   60|Train Avg Loss: 0.5937 |Test Loss: 1.0026|lr = 0.00010\n",
      "Epoch: 3019|steps:   30|Train Avg Loss: 0.5918 |Test Loss: 0.9907|lr = 0.00010\n",
      "Epoch: 3019|steps:   60|Train Avg Loss: 0.6180 |Test Loss: 0.9955|lr = 0.00010\n",
      "Epoch: 3020|steps:   30|Train Avg Loss: 0.6063 |Test Loss: 0.9970|lr = 0.00010\n",
      "Epoch: 3020|steps:   60|Train Avg Loss: 0.5997 |Test Loss: 0.9967|lr = 0.00010\n",
      "Epoch: 3021|steps:   30|Train Avg Loss: 0.5826 |Test Loss: 1.0127|lr = 0.00010\n",
      "Epoch: 3021|steps:   60|Train Avg Loss: 0.6356 |Test Loss: 0.9905|lr = 0.00010\n",
      "Epoch: 3022|steps:   30|Train Avg Loss: 0.5948 |Test Loss: 0.9922|lr = 0.00010\n",
      "Epoch: 3022|steps:   60|Train Avg Loss: 0.6167 |Test Loss: 1.0038|lr = 0.00010\n",
      "Epoch: 3023|steps:   30|Train Avg Loss: 0.5955 |Test Loss: 1.0044|lr = 0.00010\n",
      "Epoch: 3023|steps:   60|Train Avg Loss: 0.5990 |Test Loss: 0.9893|lr = 0.00010\n",
      "Epoch: 3024|steps:   30|Train Avg Loss: 0.6104 |Test Loss: 1.0226|lr = 0.00010\n",
      "Epoch: 3024|steps:   60|Train Avg Loss: 0.6063 |Test Loss: 0.9775|lr = 0.00010\n",
      "Epoch: 3025|steps:   30|Train Avg Loss: 0.5986 |Test Loss: 1.0021|lr = 0.00010\n",
      "Epoch: 3025|steps:   60|Train Avg Loss: 0.6109 |Test Loss: 1.0053|lr = 0.00010\n",
      "Epoch: 3026|steps:   30|Train Avg Loss: 0.5957 |Test Loss: 0.9956|lr = 0.00010\n",
      "Epoch: 3026|steps:   60|Train Avg Loss: 0.6166 |Test Loss: 0.9969|lr = 0.00010\n",
      "Epoch: 3027|steps:   30|Train Avg Loss: 0.5931 |Test Loss: 1.0069|lr = 0.00010\n",
      "Epoch: 3027|steps:   60|Train Avg Loss: 0.5936 |Test Loss: 1.0074|lr = 0.00010\n",
      "Epoch: 3028|steps:   30|Train Avg Loss: 0.5982 |Test Loss: 1.0031|lr = 0.00010\n",
      "Epoch: 3028|steps:   60|Train Avg Loss: 0.5929 |Test Loss: 1.0011|lr = 0.00010\n",
      "Epoch: 3029|steps:   30|Train Avg Loss: 0.6090 |Test Loss: 0.9940|lr = 0.00010\n",
      "Epoch: 3029|steps:   60|Train Avg Loss: 0.6125 |Test Loss: 1.0142|lr = 0.00010\n",
      "Epoch: 3030|steps:   30|Train Avg Loss: 0.5924 |Test Loss: 1.0098|lr = 0.00010\n",
      "Epoch: 3030|steps:   60|Train Avg Loss: 0.6011 |Test Loss: 0.9957|lr = 0.00010\n",
      "Epoch: 3031|steps:   30|Train Avg Loss: 0.6037 |Test Loss: 1.0105|lr = 0.00010\n",
      "Epoch: 3031|steps:   60|Train Avg Loss: 0.6039 |Test Loss: 1.0049|lr = 0.00010\n",
      "Epoch: 3032|steps:   30|Train Avg Loss: 0.6032 |Test Loss: 0.9988|lr = 0.00010\n",
      "Epoch: 3032|steps:   60|Train Avg Loss: 0.5949 |Test Loss: 1.0217|lr = 0.00010\n",
      "Epoch: 3033|steps:   30|Train Avg Loss: 0.5934 |Test Loss: 0.9885|lr = 0.00010\n",
      "Epoch: 3033|steps:   60|Train Avg Loss: 0.6052 |Test Loss: 1.0163|lr = 0.00010\n",
      "Epoch: 3034|steps:   30|Train Avg Loss: 0.5965 |Test Loss: 0.9864|lr = 0.00010\n",
      "Epoch: 3034|steps:   60|Train Avg Loss: 0.6012 |Test Loss: 0.9980|lr = 0.00010\n",
      "Epoch: 3035|steps:   30|Train Avg Loss: 0.5898 |Test Loss: 1.0076|lr = 0.00010\n",
      "Epoch: 3035|steps:   60|Train Avg Loss: 0.5991 |Test Loss: 0.9946|lr = 0.00010\n",
      "Epoch: 3036|steps:   30|Train Avg Loss: 0.5790 |Test Loss: 1.0043|lr = 0.00010\n",
      "Epoch: 3036|steps:   60|Train Avg Loss: 0.6092 |Test Loss: 1.0046|lr = 0.00010\n",
      "Epoch: 3037|steps:   30|Train Avg Loss: 0.6029 |Test Loss: 1.0222|lr = 0.00010\n",
      "Epoch: 3037|steps:   60|Train Avg Loss: 0.5875 |Test Loss: 1.0030|lr = 0.00010\n",
      "Epoch: 3038|steps:   30|Train Avg Loss: 0.6145 |Test Loss: 1.0378|lr = 0.00010\n",
      "Epoch: 3038|steps:   60|Train Avg Loss: 0.5788 |Test Loss: 1.0086|lr = 0.00010\n",
      "Epoch: 3039|steps:   30|Train Avg Loss: 0.5883 |Test Loss: 0.9811|lr = 0.00010\n",
      "Epoch: 3039|steps:   60|Train Avg Loss: 0.5933 |Test Loss: 1.0153|lr = 0.00010\n",
      "Epoch: 3040|steps:   30|Train Avg Loss: 0.5775 |Test Loss: 1.0145|lr = 0.00010\n",
      "Epoch: 3040|steps:   60|Train Avg Loss: 0.6123 |Test Loss: 1.0294|lr = 0.00010\n",
      "Epoch: 3041|steps:   30|Train Avg Loss: 0.5773 |Test Loss: 1.0085|lr = 0.00010\n",
      "Epoch: 3041|steps:   60|Train Avg Loss: 0.5938 |Test Loss: 1.0187|lr = 0.00010\n",
      "Epoch: 3042|steps:   30|Train Avg Loss: 0.5720 |Test Loss: 1.0001|lr = 0.00010\n",
      "Epoch: 3042|steps:   60|Train Avg Loss: 0.6254 |Test Loss: 1.0082|lr = 0.00010\n",
      "Epoch: 3043|steps:   30|Train Avg Loss: 0.5893 |Test Loss: 1.0214|lr = 0.00010\n",
      "Epoch: 3043|steps:   60|Train Avg Loss: 0.5966 |Test Loss: 1.0005|lr = 0.00010\n",
      "Epoch: 3044|steps:   30|Train Avg Loss: 0.5840 |Test Loss: 1.0053|lr = 0.00010\n",
      "Epoch: 3044|steps:   60|Train Avg Loss: 0.5959 |Test Loss: 1.0215|lr = 0.00010\n",
      "Epoch: 3045|steps:   30|Train Avg Loss: 0.5620 |Test Loss: 1.0136|lr = 0.00010\n",
      "Epoch: 3045|steps:   60|Train Avg Loss: 0.5978 |Test Loss: 1.0092|lr = 0.00010\n",
      "Epoch: 3046|steps:   30|Train Avg Loss: 0.5665 |Test Loss: 1.0121|lr = 0.00010\n",
      "Epoch: 3046|steps:   60|Train Avg Loss: 0.6014 |Test Loss: 1.0043|lr = 0.00010\n",
      "Epoch: 3047|steps:   30|Train Avg Loss: 0.5832 |Test Loss: 1.0380|lr = 0.00010\n",
      "Epoch: 3047|steps:   60|Train Avg Loss: 0.5916 |Test Loss: 1.0251|lr = 0.00010\n",
      "Epoch: 3048|steps:   30|Train Avg Loss: 0.5939 |Test Loss: 1.0076|lr = 0.00010\n",
      "Epoch: 3048|steps:   60|Train Avg Loss: 0.5744 |Test Loss: 1.0272|lr = 0.00010\n",
      "Epoch: 3049|steps:   30|Train Avg Loss: 0.5649 |Test Loss: 1.0160|lr = 0.00010\n",
      "Epoch: 3049|steps:   60|Train Avg Loss: 0.6203 |Test Loss: 1.0078|lr = 0.00010\n",
      "Epoch: 3050|steps:   30|Train Avg Loss: 0.5698 |Test Loss: 1.0451|lr = 0.00010\n",
      "Epoch: 3050|steps:   60|Train Avg Loss: 0.6061 |Test Loss: 1.0352|lr = 0.00010\n",
      "Epoch: 3051|steps:   30|Train Avg Loss: 0.5835 |Test Loss: 1.0044|lr = 0.00010\n",
      "Epoch: 3051|steps:   60|Train Avg Loss: 0.5842 |Test Loss: 1.0031|lr = 0.00010\n",
      "Epoch: 3052|steps:   30|Train Avg Loss: 0.5783 |Test Loss: 1.0384|lr = 0.00010\n",
      "Epoch: 3052|steps:   60|Train Avg Loss: 0.5957 |Test Loss: 1.0087|lr = 0.00010\n",
      "Epoch: 3053|steps:   30|Train Avg Loss: 0.5897 |Test Loss: 1.0454|lr = 0.00010\n",
      "Epoch: 3053|steps:   60|Train Avg Loss: 0.5911 |Test Loss: 1.0024|lr = 0.00010\n",
      "Epoch: 3054|steps:   30|Train Avg Loss: 0.5647 |Test Loss: 1.0127|lr = 0.00010\n",
      "Epoch: 3054|steps:   60|Train Avg Loss: 0.6108 |Test Loss: 1.0246|lr = 0.00010\n",
      "Epoch: 3055|steps:   30|Train Avg Loss: 0.5830 |Test Loss: 1.0300|lr = 0.00010\n",
      "Epoch: 3055|steps:   60|Train Avg Loss: 0.6028 |Test Loss: 1.0127|lr = 0.00010\n",
      "Epoch: 3056|steps:   30|Train Avg Loss: 0.5806 |Test Loss: 1.0206|lr = 0.00010\n",
      "Epoch: 3056|steps:   60|Train Avg Loss: 0.5856 |Test Loss: 1.0470|lr = 0.00010\n",
      "Epoch: 3057|steps:   30|Train Avg Loss: 0.5838 |Test Loss: 1.0154|lr = 0.00010\n",
      "Epoch: 3057|steps:   60|Train Avg Loss: 0.5820 |Test Loss: 1.0357|lr = 0.00010\n",
      "Epoch: 3058|steps:   30|Train Avg Loss: 0.5874 |Test Loss: 1.0276|lr = 0.00010\n",
      "Epoch: 3058|steps:   60|Train Avg Loss: 0.5862 |Test Loss: 1.0271|lr = 0.00010\n",
      "Epoch: 3059|steps:   30|Train Avg Loss: 0.5591 |Test Loss: 1.0137|lr = 0.00010\n",
      "Epoch: 3059|steps:   60|Train Avg Loss: 0.5993 |Test Loss: 1.0268|lr = 0.00010\n",
      "Epoch: 3060|steps:   30|Train Avg Loss: 0.5957 |Test Loss: 1.0286|lr = 0.00010\n",
      "Epoch: 3060|steps:   60|Train Avg Loss: 0.5719 |Test Loss: 1.0431|lr = 0.00010\n",
      "Epoch: 3061|steps:   30|Train Avg Loss: 0.5821 |Test Loss: 1.0272|lr = 0.00010\n",
      "Epoch: 3061|steps:   60|Train Avg Loss: 0.5831 |Test Loss: 1.0115|lr = 0.00010\n",
      "Epoch: 3062|steps:   30|Train Avg Loss: 0.6013 |Test Loss: 1.0179|lr = 0.00010\n",
      "Epoch: 3062|steps:   60|Train Avg Loss: 0.5733 |Test Loss: 1.0371|lr = 0.00010\n",
      "Epoch: 3063|steps:   30|Train Avg Loss: 0.5752 |Test Loss: 1.0248|lr = 0.00010\n",
      "Epoch: 3063|steps:   60|Train Avg Loss: 0.5835 |Test Loss: 1.0278|lr = 0.00010\n",
      "Epoch: 3064|steps:   30|Train Avg Loss: 0.5658 |Test Loss: 1.0158|lr = 0.00010\n",
      "Epoch: 3064|steps:   60|Train Avg Loss: 0.5768 |Test Loss: 1.0303|lr = 0.00010\n",
      "Epoch: 3065|steps:   30|Train Avg Loss: 0.5765 |Test Loss: 1.0145|lr = 0.00010\n",
      "Epoch: 3065|steps:   60|Train Avg Loss: 0.5918 |Test Loss: 1.0562|lr = 0.00010\n",
      "Epoch: 3066|steps:   30|Train Avg Loss: 0.5691 |Test Loss: 1.0230|lr = 0.00010\n",
      "Epoch: 3066|steps:   60|Train Avg Loss: 0.5788 |Test Loss: 1.0591|lr = 0.00010\n",
      "Epoch: 3067|steps:   30|Train Avg Loss: 0.5804 |Test Loss: 1.0422|lr = 0.00010\n",
      "Epoch: 3067|steps:   60|Train Avg Loss: 0.5625 |Test Loss: 1.0178|lr = 0.00010\n",
      "Epoch: 3068|steps:   30|Train Avg Loss: 0.5479 |Test Loss: 1.0215|lr = 0.00010\n",
      "Epoch: 3068|steps:   60|Train Avg Loss: 0.5945 |Test Loss: 1.0305|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3069|steps:   30|Train Avg Loss: 0.5599 |Test Loss: 1.0711|lr = 0.00010\n",
      "Epoch: 3069|steps:   60|Train Avg Loss: 0.5773 |Test Loss: 1.0310|lr = 0.00010\n",
      "Epoch: 3070|steps:   30|Train Avg Loss: 0.5492 |Test Loss: 1.0226|lr = 0.00010\n",
      "Epoch: 3070|steps:   60|Train Avg Loss: 0.5949 |Test Loss: 1.0597|lr = 0.00010\n",
      "Epoch: 3071|steps:   30|Train Avg Loss: 0.5779 |Test Loss: 1.0390|lr = 0.00010\n",
      "Epoch: 3071|steps:   60|Train Avg Loss: 0.5855 |Test Loss: 1.0460|lr = 0.00010\n",
      "Epoch: 3072|steps:   30|Train Avg Loss: 0.5738 |Test Loss: 1.0375|lr = 0.00010\n",
      "Epoch: 3072|steps:   60|Train Avg Loss: 0.5864 |Test Loss: 1.0500|lr = 0.00010\n",
      "Epoch: 3073|steps:   30|Train Avg Loss: 0.5595 |Test Loss: 1.0405|lr = 0.00010\n",
      "Epoch: 3073|steps:   60|Train Avg Loss: 0.5994 |Test Loss: 1.0417|lr = 0.00010\n",
      "Epoch: 3074|steps:   30|Train Avg Loss: 0.5733 |Test Loss: 1.0277|lr = 0.00010\n",
      "Epoch: 3074|steps:   60|Train Avg Loss: 0.5636 |Test Loss: 1.0295|lr = 0.00010\n",
      "Epoch: 3075|steps:   30|Train Avg Loss: 0.5650 |Test Loss: 1.0423|lr = 0.00010\n",
      "Epoch: 3075|steps:   60|Train Avg Loss: 0.5686 |Test Loss: 1.0776|lr = 0.00010\n",
      "Epoch: 3076|steps:   30|Train Avg Loss: 0.5817 |Test Loss: 1.0333|lr = 0.00010\n",
      "Epoch: 3076|steps:   60|Train Avg Loss: 0.5714 |Test Loss: 1.0395|lr = 0.00010\n",
      "Epoch: 3077|steps:   30|Train Avg Loss: 0.5630 |Test Loss: 1.0399|lr = 0.00010\n",
      "Epoch: 3077|steps:   60|Train Avg Loss: 0.5825 |Test Loss: 1.0260|lr = 0.00010\n",
      "Epoch: 3078|steps:   30|Train Avg Loss: 0.5687 |Test Loss: 1.0375|lr = 0.00010\n",
      "Epoch: 3078|steps:   60|Train Avg Loss: 0.5693 |Test Loss: 1.0572|lr = 0.00010\n",
      "Epoch: 3079|steps:   30|Train Avg Loss: 0.5391 |Test Loss: 1.0663|lr = 0.00010\n",
      "Epoch: 3079|steps:   60|Train Avg Loss: 0.5606 |Test Loss: 1.0358|lr = 0.00010\n",
      "Epoch: 3080|steps:   30|Train Avg Loss: 0.5632 |Test Loss: 1.0311|lr = 0.00010\n",
      "Epoch: 3080|steps:   60|Train Avg Loss: 0.5715 |Test Loss: 1.0400|lr = 0.00010\n",
      "Epoch: 3081|steps:   30|Train Avg Loss: 0.5523 |Test Loss: 1.0507|lr = 0.00010\n",
      "Epoch: 3081|steps:   60|Train Avg Loss: 0.5757 |Test Loss: 1.0445|lr = 0.00010\n",
      "Epoch: 3082|steps:   30|Train Avg Loss: 0.5583 |Test Loss: 1.0441|lr = 0.00010\n",
      "Epoch: 3082|steps:   60|Train Avg Loss: 0.5718 |Test Loss: 1.0530|lr = 0.00010\n",
      "Epoch: 3083|steps:   30|Train Avg Loss: 0.5539 |Test Loss: 1.0572|lr = 0.00010\n",
      "Epoch: 3083|steps:   60|Train Avg Loss: 0.5720 |Test Loss: 1.0521|lr = 0.00010\n",
      "Epoch: 3084|steps:   30|Train Avg Loss: 0.5654 |Test Loss: 1.0497|lr = 0.00010\n",
      "Epoch: 3084|steps:   60|Train Avg Loss: 0.5525 |Test Loss: 1.0434|lr = 0.00010\n",
      "Epoch: 3085|steps:   30|Train Avg Loss: 0.5572 |Test Loss: 1.0409|lr = 0.00010\n",
      "Epoch: 3085|steps:   60|Train Avg Loss: 0.5621 |Test Loss: 1.0393|lr = 0.00010\n",
      "Epoch: 3086|steps:   30|Train Avg Loss: 0.5504 |Test Loss: 1.0464|lr = 0.00010\n",
      "Epoch: 3086|steps:   60|Train Avg Loss: 0.5755 |Test Loss: 1.0613|lr = 0.00010\n",
      "Epoch: 3087|steps:   30|Train Avg Loss: 0.5792 |Test Loss: 1.0497|lr = 0.00010\n",
      "Epoch: 3087|steps:   60|Train Avg Loss: 0.5566 |Test Loss: 1.0736|lr = 0.00010\n",
      "Epoch: 3088|steps:   30|Train Avg Loss: 0.5391 |Test Loss: 1.0513|lr = 0.00010\n",
      "Epoch: 3088|steps:   60|Train Avg Loss: 0.5656 |Test Loss: 1.0615|lr = 0.00010\n",
      "Epoch: 3089|steps:   30|Train Avg Loss: 0.5438 |Test Loss: 1.0474|lr = 0.00010\n",
      "Epoch: 3089|steps:   60|Train Avg Loss: 0.5756 |Test Loss: 1.0615|lr = 0.00010\n",
      "Epoch: 3090|steps:   30|Train Avg Loss: 0.5276 |Test Loss: 1.0580|lr = 0.00010\n",
      "Epoch: 3090|steps:   60|Train Avg Loss: 0.5844 |Test Loss: 1.0580|lr = 0.00010\n",
      "Epoch: 3091|steps:   30|Train Avg Loss: 0.5417 |Test Loss: 1.0568|lr = 0.00010\n",
      "Epoch: 3091|steps:   60|Train Avg Loss: 0.5589 |Test Loss: 1.0618|lr = 0.00010\n",
      "Epoch: 3092|steps:   30|Train Avg Loss: 0.5438 |Test Loss: 1.0453|lr = 0.00010\n",
      "Epoch: 3092|steps:   60|Train Avg Loss: 0.5624 |Test Loss: 1.0649|lr = 0.00010\n",
      "Epoch: 3093|steps:   30|Train Avg Loss: 0.5425 |Test Loss: 1.0505|lr = 0.00010\n",
      "Epoch: 3093|steps:   60|Train Avg Loss: 0.5716 |Test Loss: 1.0621|lr = 0.00010\n",
      "Epoch: 3094|steps:   30|Train Avg Loss: 0.5290 |Test Loss: 1.0388|lr = 0.00010\n",
      "Epoch: 3094|steps:   60|Train Avg Loss: 0.5628 |Test Loss: 1.0562|lr = 0.00010\n",
      "Epoch: 3095|steps:   30|Train Avg Loss: 0.5677 |Test Loss: 1.0783|lr = 0.00010\n",
      "Epoch: 3095|steps:   60|Train Avg Loss: 0.5532 |Test Loss: 1.0513|lr = 0.00010\n",
      "Epoch: 3096|steps:   30|Train Avg Loss: 0.5651 |Test Loss: 1.0495|lr = 0.00010\n",
      "Epoch: 3096|steps:   60|Train Avg Loss: 0.5475 |Test Loss: 1.0712|lr = 0.00010\n",
      "Epoch: 3097|steps:   30|Train Avg Loss: 0.5201 |Test Loss: 1.0686|lr = 0.00010\n",
      "Epoch: 3097|steps:   60|Train Avg Loss: 0.5667 |Test Loss: 1.0389|lr = 0.00010\n",
      "Epoch: 3098|steps:   30|Train Avg Loss: 0.5440 |Test Loss: 1.0606|lr = 0.00010\n",
      "Epoch: 3098|steps:   60|Train Avg Loss: 0.5684 |Test Loss: 1.0852|lr = 0.00010\n",
      "Epoch: 3099|steps:   30|Train Avg Loss: 0.5593 |Test Loss: 1.0454|lr = 0.00010\n",
      "Epoch: 3099|steps:   60|Train Avg Loss: 0.5496 |Test Loss: 1.0609|lr = 0.00010\n",
      "Epoch: 3100|steps:   30|Train Avg Loss: 0.5423 |Test Loss: 1.0724|lr = 0.00010\n",
      "Epoch: 3100|steps:   60|Train Avg Loss: 0.5548 |Test Loss: 1.0761|lr = 0.00010\n",
      "Epoch: 3101|steps:   30|Train Avg Loss: 0.5354 |Test Loss: 1.0635|lr = 0.00010\n",
      "Epoch: 3101|steps:   60|Train Avg Loss: 0.5763 |Test Loss: 1.0481|lr = 0.00010\n",
      "Epoch: 3102|steps:   30|Train Avg Loss: 0.5311 |Test Loss: 1.0655|lr = 0.00010\n",
      "Epoch: 3102|steps:   60|Train Avg Loss: 0.5630 |Test Loss: 1.0823|lr = 0.00010\n",
      "Epoch: 3103|steps:   30|Train Avg Loss: 0.5680 |Test Loss: 1.0771|lr = 0.00010\n",
      "Epoch: 3103|steps:   60|Train Avg Loss: 0.5583 |Test Loss: 1.0671|lr = 0.00010\n",
      "Epoch: 3104|steps:   30|Train Avg Loss: 0.5426 |Test Loss: 1.0732|lr = 0.00010\n",
      "Epoch: 3104|steps:   60|Train Avg Loss: 0.5442 |Test Loss: 1.0707|lr = 0.00010\n",
      "Epoch: 3105|steps:   30|Train Avg Loss: 0.5235 |Test Loss: 1.0787|lr = 0.00010\n",
      "Epoch: 3105|steps:   60|Train Avg Loss: 0.5555 |Test Loss: 1.0763|lr = 0.00010\n",
      "Epoch: 3106|steps:   30|Train Avg Loss: 0.5615 |Test Loss: 1.0741|lr = 0.00010\n",
      "Epoch: 3106|steps:   60|Train Avg Loss: 0.5347 |Test Loss: 1.0556|lr = 0.00010\n",
      "Epoch: 3107|steps:   30|Train Avg Loss: 0.5222 |Test Loss: 1.0713|lr = 0.00010\n",
      "Epoch: 3107|steps:   60|Train Avg Loss: 0.5583 |Test Loss: 1.0720|lr = 0.00010\n",
      "Epoch: 3108|steps:   30|Train Avg Loss: 0.5304 |Test Loss: 1.0801|lr = 0.00010\n",
      "Epoch: 3108|steps:   60|Train Avg Loss: 0.5627 |Test Loss: 1.0865|lr = 0.00010\n",
      "Epoch: 3109|steps:   30|Train Avg Loss: 0.5280 |Test Loss: 1.0781|lr = 0.00010\n",
      "Epoch: 3109|steps:   60|Train Avg Loss: 0.5501 |Test Loss: 1.0732|lr = 0.00010\n",
      "Epoch: 3110|steps:   30|Train Avg Loss: 0.5309 |Test Loss: 1.0809|lr = 0.00010\n",
      "Epoch: 3110|steps:   60|Train Avg Loss: 0.5419 |Test Loss: 1.0625|lr = 0.00010\n",
      "Epoch: 3111|steps:   30|Train Avg Loss: 0.5298 |Test Loss: 1.0790|lr = 0.00010\n",
      "Epoch: 3111|steps:   60|Train Avg Loss: 0.5562 |Test Loss: 1.0708|lr = 0.00010\n",
      "Epoch: 3112|steps:   30|Train Avg Loss: 0.5204 |Test Loss: 1.1025|lr = 0.00010\n",
      "Epoch: 3112|steps:   60|Train Avg Loss: 0.5583 |Test Loss: 1.0681|lr = 0.00010\n",
      "Epoch: 3113|steps:   30|Train Avg Loss: 0.5378 |Test Loss: 1.0832|lr = 0.00010\n",
      "Epoch: 3113|steps:   60|Train Avg Loss: 0.5326 |Test Loss: 1.0795|lr = 0.00010\n",
      "Epoch: 3114|steps:   30|Train Avg Loss: 0.5116 |Test Loss: 1.0787|lr = 0.00010\n",
      "Epoch: 3114|steps:   60|Train Avg Loss: 0.5644 |Test Loss: 1.0994|lr = 0.00010\n",
      "Epoch: 3115|steps:   30|Train Avg Loss: 0.5328 |Test Loss: 1.0955|lr = 0.00010\n",
      "Epoch: 3115|steps:   60|Train Avg Loss: 0.5502 |Test Loss: 1.0824|lr = 0.00010\n",
      "Epoch: 3116|steps:   30|Train Avg Loss: 0.5351 |Test Loss: 1.0771|lr = 0.00010\n",
      "Epoch: 3116|steps:   60|Train Avg Loss: 0.5640 |Test Loss: 1.1011|lr = 0.00010\n",
      "Epoch: 3117|steps:   30|Train Avg Loss: 0.5466 |Test Loss: 1.1052|lr = 0.00010\n",
      "Epoch: 3117|steps:   60|Train Avg Loss: 0.5346 |Test Loss: 1.0838|lr = 0.00010\n",
      "Epoch: 3118|steps:   30|Train Avg Loss: 0.5350 |Test Loss: 1.0817|lr = 0.00010\n",
      "Epoch: 3118|steps:   60|Train Avg Loss: 0.5335 |Test Loss: 1.0765|lr = 0.00010\n",
      "Epoch: 3119|steps:   30|Train Avg Loss: 0.5472 |Test Loss: 1.1006|lr = 0.00010\n",
      "Epoch: 3119|steps:   60|Train Avg Loss: 0.5209 |Test Loss: 1.1005|lr = 0.00010\n",
      "Epoch: 3120|steps:   30|Train Avg Loss: 0.5540 |Test Loss: 1.1273|lr = 0.00010\n",
      "Epoch: 3120|steps:   60|Train Avg Loss: 0.5343 |Test Loss: 1.0708|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3121|steps:   30|Train Avg Loss: 0.5362 |Test Loss: 1.0866|lr = 0.00010\n",
      "Epoch: 3121|steps:   60|Train Avg Loss: 0.5418 |Test Loss: 1.1071|lr = 0.00010\n",
      "Epoch: 3122|steps:   30|Train Avg Loss: 0.5123 |Test Loss: 1.0728|lr = 0.00010\n",
      "Epoch: 3122|steps:   60|Train Avg Loss: 0.5570 |Test Loss: 1.1050|lr = 0.00010\n",
      "Epoch: 3123|steps:   30|Train Avg Loss: 0.5473 |Test Loss: 1.1082|lr = 0.00010\n",
      "Epoch: 3123|steps:   60|Train Avg Loss: 0.5389 |Test Loss: 1.0914|lr = 0.00010\n",
      "Epoch: 3124|steps:   30|Train Avg Loss: 0.5339 |Test Loss: 1.0747|lr = 0.00010\n",
      "Epoch: 3124|steps:   60|Train Avg Loss: 0.5280 |Test Loss: 1.0952|lr = 0.00010\n",
      "Epoch: 3125|steps:   30|Train Avg Loss: 0.5138 |Test Loss: 1.0926|lr = 0.00010\n",
      "Epoch: 3125|steps:   60|Train Avg Loss: 0.5545 |Test Loss: 1.0867|lr = 0.00010\n",
      "Epoch: 3126|steps:   30|Train Avg Loss: 0.5249 |Test Loss: 1.1037|lr = 0.00010\n",
      "Epoch: 3126|steps:   60|Train Avg Loss: 0.5377 |Test Loss: 1.0901|lr = 0.00010\n",
      "Epoch: 3127|steps:   30|Train Avg Loss: 0.5295 |Test Loss: 1.1068|lr = 0.00010\n",
      "Epoch: 3127|steps:   60|Train Avg Loss: 0.5307 |Test Loss: 1.0837|lr = 0.00010\n",
      "Epoch: 3128|steps:   30|Train Avg Loss: 0.5355 |Test Loss: 1.1068|lr = 0.00010\n",
      "Epoch: 3128|steps:   60|Train Avg Loss: 0.5034 |Test Loss: 1.1047|lr = 0.00010\n",
      "Epoch: 3129|steps:   30|Train Avg Loss: 0.5180 |Test Loss: 1.0972|lr = 0.00010\n",
      "Epoch: 3129|steps:   60|Train Avg Loss: 0.5318 |Test Loss: 1.1017|lr = 0.00010\n",
      "Epoch: 3130|steps:   30|Train Avg Loss: 0.5220 |Test Loss: 1.0929|lr = 0.00010\n",
      "Epoch: 3130|steps:   60|Train Avg Loss: 0.5259 |Test Loss: 1.1217|lr = 0.00010\n",
      "Epoch: 3131|steps:   30|Train Avg Loss: 0.5028 |Test Loss: 1.1122|lr = 0.00010\n",
      "Epoch: 3131|steps:   60|Train Avg Loss: 0.5335 |Test Loss: 1.1197|lr = 0.00010\n",
      "Epoch: 3132|steps:   30|Train Avg Loss: 0.5163 |Test Loss: 1.1092|lr = 0.00010\n",
      "Epoch: 3132|steps:   60|Train Avg Loss: 0.5360 |Test Loss: 1.0928|lr = 0.00010\n",
      "Epoch: 3133|steps:   30|Train Avg Loss: 0.5157 |Test Loss: 1.1066|lr = 0.00010\n",
      "Epoch: 3133|steps:   60|Train Avg Loss: 0.5354 |Test Loss: 1.0984|lr = 0.00010\n",
      "Epoch: 3134|steps:   30|Train Avg Loss: 0.5310 |Test Loss: 1.1227|lr = 0.00010\n",
      "Epoch: 3134|steps:   60|Train Avg Loss: 0.5155 |Test Loss: 1.1081|lr = 0.00010\n",
      "Epoch: 3135|steps:   30|Train Avg Loss: 0.5216 |Test Loss: 1.1055|lr = 0.00010\n",
      "Epoch: 3135|steps:   60|Train Avg Loss: 0.5318 |Test Loss: 1.1086|lr = 0.00010\n",
      "Epoch: 3136|steps:   30|Train Avg Loss: 0.5301 |Test Loss: 1.1061|lr = 0.00010\n",
      "Epoch: 3136|steps:   60|Train Avg Loss: 0.5208 |Test Loss: 1.1130|lr = 0.00010\n",
      "Epoch: 3137|steps:   30|Train Avg Loss: 0.5134 |Test Loss: 1.1060|lr = 0.00010\n",
      "Epoch: 3137|steps:   60|Train Avg Loss: 0.5281 |Test Loss: 1.1294|lr = 0.00010\n",
      "Epoch: 3138|steps:   30|Train Avg Loss: 0.5066 |Test Loss: 1.1131|lr = 0.00010\n",
      "Epoch: 3138|steps:   60|Train Avg Loss: 0.5380 |Test Loss: 1.1020|lr = 0.00010\n",
      "Epoch: 3139|steps:   30|Train Avg Loss: 0.5045 |Test Loss: 1.1462|lr = 0.00010\n",
      "Epoch: 3139|steps:   60|Train Avg Loss: 0.5376 |Test Loss: 1.0975|lr = 0.00010\n",
      "Epoch: 3140|steps:   30|Train Avg Loss: 0.5060 |Test Loss: 1.1207|lr = 0.00010\n",
      "Epoch: 3140|steps:   60|Train Avg Loss: 0.5263 |Test Loss: 1.0946|lr = 0.00010\n",
      "Epoch: 3141|steps:   30|Train Avg Loss: 0.5124 |Test Loss: 1.1029|lr = 0.00010\n",
      "Epoch: 3141|steps:   60|Train Avg Loss: 0.5220 |Test Loss: 1.1354|lr = 0.00010\n",
      "Epoch: 3142|steps:   30|Train Avg Loss: 0.4870 |Test Loss: 1.1324|lr = 0.00010\n",
      "Epoch: 3142|steps:   60|Train Avg Loss: 0.5480 |Test Loss: 1.1414|lr = 0.00010\n",
      "Epoch: 3143|steps:   30|Train Avg Loss: 0.5102 |Test Loss: 1.1332|lr = 0.00010\n",
      "Epoch: 3143|steps:   60|Train Avg Loss: 0.5246 |Test Loss: 1.1181|lr = 0.00010\n",
      "Epoch: 3144|steps:   30|Train Avg Loss: 0.5056 |Test Loss: 1.1108|lr = 0.00010\n",
      "Epoch: 3144|steps:   60|Train Avg Loss: 0.5217 |Test Loss: 1.1146|lr = 0.00010\n",
      "Epoch: 3145|steps:   30|Train Avg Loss: 0.4936 |Test Loss: 1.1426|lr = 0.00010\n",
      "Epoch: 3145|steps:   60|Train Avg Loss: 0.5168 |Test Loss: 1.1206|lr = 0.00010\n",
      "Epoch: 3146|steps:   30|Train Avg Loss: 0.4840 |Test Loss: 1.1503|lr = 0.00010\n",
      "Epoch: 3146|steps:   60|Train Avg Loss: 0.5347 |Test Loss: 1.1302|lr = 0.00010\n",
      "Epoch: 3147|steps:   30|Train Avg Loss: 0.5012 |Test Loss: 1.1319|lr = 0.00010\n",
      "Epoch: 3147|steps:   60|Train Avg Loss: 0.5195 |Test Loss: 1.1202|lr = 0.00010\n",
      "Epoch: 3148|steps:   30|Train Avg Loss: 0.5005 |Test Loss: 1.1098|lr = 0.00010\n",
      "Epoch: 3148|steps:   60|Train Avg Loss: 0.5290 |Test Loss: 1.1470|lr = 0.00010\n",
      "Epoch: 3149|steps:   30|Train Avg Loss: 0.5072 |Test Loss: 1.1361|lr = 0.00010\n",
      "Epoch: 3149|steps:   60|Train Avg Loss: 0.5134 |Test Loss: 1.1334|lr = 0.00010\n",
      "Epoch: 3150|steps:   30|Train Avg Loss: 0.4957 |Test Loss: 1.1307|lr = 0.00010\n",
      "Epoch: 3150|steps:   60|Train Avg Loss: 0.5200 |Test Loss: 1.1391|lr = 0.00010\n",
      "Epoch: 3151|steps:   30|Train Avg Loss: 0.5110 |Test Loss: 1.1243|lr = 0.00010\n",
      "Epoch: 3151|steps:   60|Train Avg Loss: 0.5170 |Test Loss: 1.1155|lr = 0.00010\n",
      "Epoch: 3152|steps:   30|Train Avg Loss: 0.5399 |Test Loss: 1.1299|lr = 0.00010\n",
      "Epoch: 3152|steps:   60|Train Avg Loss: 0.4949 |Test Loss: 1.1269|lr = 0.00010\n",
      "Epoch: 3153|steps:   30|Train Avg Loss: 0.5108 |Test Loss: 1.1417|lr = 0.00010\n",
      "Epoch: 3153|steps:   60|Train Avg Loss: 0.5121 |Test Loss: 1.1302|lr = 0.00010\n",
      "Epoch: 3154|steps:   30|Train Avg Loss: 0.4916 |Test Loss: 1.1459|lr = 0.00010\n",
      "Epoch: 3154|steps:   60|Train Avg Loss: 0.5258 |Test Loss: 1.1427|lr = 0.00010\n",
      "Epoch: 3155|steps:   30|Train Avg Loss: 0.5232 |Test Loss: 1.1507|lr = 0.00010\n",
      "Epoch: 3155|steps:   60|Train Avg Loss: 0.4947 |Test Loss: 1.1367|lr = 0.00010\n",
      "Epoch: 3156|steps:   30|Train Avg Loss: 0.5052 |Test Loss: 1.1398|lr = 0.00010\n",
      "Epoch: 3156|steps:   60|Train Avg Loss: 0.5053 |Test Loss: 1.1196|lr = 0.00010\n",
      "Epoch: 3157|steps:   30|Train Avg Loss: 0.5014 |Test Loss: 1.1389|lr = 0.00010\n",
      "Epoch: 3157|steps:   60|Train Avg Loss: 0.5132 |Test Loss: 1.1604|lr = 0.00010\n",
      "Epoch: 3158|steps:   30|Train Avg Loss: 0.4949 |Test Loss: 1.1597|lr = 0.00010\n",
      "Epoch: 3158|steps:   60|Train Avg Loss: 0.5212 |Test Loss: 1.1582|lr = 0.00010\n",
      "Epoch: 3159|steps:   30|Train Avg Loss: 0.4932 |Test Loss: 1.1351|lr = 0.00010\n",
      "Epoch: 3159|steps:   60|Train Avg Loss: 0.5168 |Test Loss: 1.1643|lr = 0.00010\n",
      "Epoch: 3160|steps:   30|Train Avg Loss: 0.4773 |Test Loss: 1.1646|lr = 0.00010\n",
      "Epoch: 3160|steps:   60|Train Avg Loss: 0.5055 |Test Loss: 1.1272|lr = 0.00010\n",
      "Epoch: 3161|steps:   30|Train Avg Loss: 0.5017 |Test Loss: 1.1461|lr = 0.00010\n",
      "Epoch: 3161|steps:   60|Train Avg Loss: 0.4941 |Test Loss: 1.1486|lr = 0.00010\n",
      "Epoch: 3162|steps:   30|Train Avg Loss: 0.4920 |Test Loss: 1.1425|lr = 0.00010\n",
      "Epoch: 3162|steps:   60|Train Avg Loss: 0.4947 |Test Loss: 1.1378|lr = 0.00010\n",
      "Epoch: 3163|steps:   30|Train Avg Loss: 0.4911 |Test Loss: 1.1766|lr = 0.00010\n",
      "Epoch: 3163|steps:   60|Train Avg Loss: 0.5058 |Test Loss: 1.1365|lr = 0.00010\n",
      "Epoch: 3164|steps:   30|Train Avg Loss: 0.4762 |Test Loss: 1.1432|lr = 0.00010\n",
      "Epoch: 3164|steps:   60|Train Avg Loss: 0.5148 |Test Loss: 1.1631|lr = 0.00010\n",
      "Epoch: 3165|steps:   30|Train Avg Loss: 0.4724 |Test Loss: 1.1634|lr = 0.00010\n",
      "Epoch: 3165|steps:   60|Train Avg Loss: 0.5183 |Test Loss: 1.1843|lr = 0.00010\n",
      "Epoch: 3166|steps:   30|Train Avg Loss: 0.4947 |Test Loss: 1.1584|lr = 0.00010\n",
      "Epoch: 3166|steps:   60|Train Avg Loss: 0.5181 |Test Loss: 1.1760|lr = 0.00010\n",
      "Epoch: 3167|steps:   30|Train Avg Loss: 0.4749 |Test Loss: 1.1609|lr = 0.00010\n",
      "Epoch: 3167|steps:   60|Train Avg Loss: 0.5146 |Test Loss: 1.1597|lr = 0.00010\n",
      "Epoch: 3168|steps:   30|Train Avg Loss: 0.4881 |Test Loss: 1.1753|lr = 0.00010\n",
      "Epoch: 3168|steps:   60|Train Avg Loss: 0.4798 |Test Loss: 1.1751|lr = 0.00010\n",
      "Epoch: 3169|steps:   30|Train Avg Loss: 0.4955 |Test Loss: 1.1899|lr = 0.00010\n",
      "Epoch: 3169|steps:   60|Train Avg Loss: 0.4990 |Test Loss: 1.1796|lr = 0.00010\n",
      "Epoch: 3170|steps:   30|Train Avg Loss: 0.5037 |Test Loss: 1.1881|lr = 0.00010\n",
      "Epoch: 3170|steps:   60|Train Avg Loss: 0.4944 |Test Loss: 1.1670|lr = 0.00010\n",
      "Epoch: 3171|steps:   30|Train Avg Loss: 0.4676 |Test Loss: 1.1506|lr = 0.00010\n",
      "Epoch: 3171|steps:   60|Train Avg Loss: 0.5089 |Test Loss: 1.1875|lr = 0.00010\n",
      "Epoch: 3172|steps:   30|Train Avg Loss: 0.4581 |Test Loss: 1.1856|lr = 0.00010\n",
      "Epoch: 3172|steps:   60|Train Avg Loss: 0.5126 |Test Loss: 1.1583|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3173|steps:   30|Train Avg Loss: 0.4779 |Test Loss: 1.1659|lr = 0.00010\n",
      "Epoch: 3173|steps:   60|Train Avg Loss: 0.5117 |Test Loss: 1.1615|lr = 0.00010\n",
      "Epoch: 3174|steps:   30|Train Avg Loss: 0.4906 |Test Loss: 1.1668|lr = 0.00010\n",
      "Epoch: 3174|steps:   60|Train Avg Loss: 0.4949 |Test Loss: 1.1747|lr = 0.00010\n",
      "Epoch: 3175|steps:   30|Train Avg Loss: 0.4820 |Test Loss: 1.2035|lr = 0.00010\n",
      "Epoch: 3175|steps:   60|Train Avg Loss: 0.5076 |Test Loss: 1.1731|lr = 0.00010\n",
      "Epoch: 3176|steps:   30|Train Avg Loss: 0.4822 |Test Loss: 1.2060|lr = 0.00010\n",
      "Epoch: 3176|steps:   60|Train Avg Loss: 0.4917 |Test Loss: 1.1686|lr = 0.00010\n",
      "Epoch: 3177|steps:   30|Train Avg Loss: 0.4759 |Test Loss: 1.1499|lr = 0.00010\n",
      "Epoch: 3177|steps:   60|Train Avg Loss: 0.4820 |Test Loss: 1.1996|lr = 0.00010\n",
      "Epoch: 3178|steps:   30|Train Avg Loss: 0.4999 |Test Loss: 1.1930|lr = 0.00010\n",
      "Epoch: 3178|steps:   60|Train Avg Loss: 0.4766 |Test Loss: 1.1791|lr = 0.00010\n",
      "Epoch: 3179|steps:   30|Train Avg Loss: 0.4741 |Test Loss: 1.1763|lr = 0.00010\n",
      "Epoch: 3179|steps:   60|Train Avg Loss: 0.4920 |Test Loss: 1.2171|lr = 0.00010\n",
      "Epoch: 3180|steps:   30|Train Avg Loss: 0.4851 |Test Loss: 1.1810|lr = 0.00010\n",
      "Epoch: 3180|steps:   60|Train Avg Loss: 0.4734 |Test Loss: 1.1990|lr = 0.00010\n",
      "Epoch: 3181|steps:   30|Train Avg Loss: 0.4911 |Test Loss: 1.1833|lr = 0.00010\n",
      "Epoch: 3181|steps:   60|Train Avg Loss: 0.4835 |Test Loss: 1.1696|lr = 0.00010\n",
      "Epoch: 3182|steps:   30|Train Avg Loss: 0.4765 |Test Loss: 1.2075|lr = 0.00010\n",
      "Epoch: 3182|steps:   60|Train Avg Loss: 0.4726 |Test Loss: 1.2020|lr = 0.00010\n",
      "Epoch: 3183|steps:   30|Train Avg Loss: 0.4748 |Test Loss: 1.1921|lr = 0.00010\n",
      "Epoch: 3183|steps:   60|Train Avg Loss: 0.4695 |Test Loss: 1.2084|lr = 0.00010\n",
      "Epoch: 3184|steps:   30|Train Avg Loss: 0.4764 |Test Loss: 1.1631|lr = 0.00010\n",
      "Epoch: 3184|steps:   60|Train Avg Loss: 0.4902 |Test Loss: 1.2289|lr = 0.00010\n",
      "Epoch: 3185|steps:   30|Train Avg Loss: 0.4848 |Test Loss: 1.1983|lr = 0.00010\n",
      "Epoch: 3185|steps:   60|Train Avg Loss: 0.4666 |Test Loss: 1.1802|lr = 0.00010\n",
      "Epoch: 3186|steps:   30|Train Avg Loss: 0.4671 |Test Loss: 1.1952|lr = 0.00010\n",
      "Epoch: 3186|steps:   60|Train Avg Loss: 0.4898 |Test Loss: 1.2181|lr = 0.00010\n",
      "Epoch: 3187|steps:   30|Train Avg Loss: 0.4737 |Test Loss: 1.2302|lr = 0.00010\n",
      "Epoch: 3187|steps:   60|Train Avg Loss: 0.4752 |Test Loss: 1.2088|lr = 0.00010\n",
      "Epoch: 3188|steps:   30|Train Avg Loss: 0.4657 |Test Loss: 1.2078|lr = 0.00010\n",
      "Epoch: 3188|steps:   60|Train Avg Loss: 0.4739 |Test Loss: 1.2075|lr = 0.00010\n",
      "Epoch: 3189|steps:   30|Train Avg Loss: 0.4704 |Test Loss: 1.2320|lr = 0.00010\n",
      "Epoch: 3189|steps:   60|Train Avg Loss: 0.4727 |Test Loss: 1.1903|lr = 0.00010\n",
      "Epoch: 3190|steps:   30|Train Avg Loss: 0.4829 |Test Loss: 1.2051|lr = 0.00010\n",
      "Epoch: 3190|steps:   60|Train Avg Loss: 0.4670 |Test Loss: 1.2048|lr = 0.00010\n",
      "Epoch: 3191|steps:   30|Train Avg Loss: 0.4463 |Test Loss: 1.2389|lr = 0.00010\n",
      "Epoch: 3191|steps:   60|Train Avg Loss: 0.4838 |Test Loss: 1.2045|lr = 0.00010\n",
      "Epoch: 3192|steps:   30|Train Avg Loss: 0.4785 |Test Loss: 1.2263|lr = 0.00010\n",
      "Epoch: 3192|steps:   60|Train Avg Loss: 0.4629 |Test Loss: 1.2142|lr = 0.00010\n",
      "Epoch: 3193|steps:   30|Train Avg Loss: 0.4688 |Test Loss: 1.2230|lr = 0.00010\n",
      "Epoch: 3193|steps:   60|Train Avg Loss: 0.4732 |Test Loss: 1.2211|lr = 0.00010\n",
      "Epoch: 3194|steps:   30|Train Avg Loss: 0.4514 |Test Loss: 1.2015|lr = 0.00010\n",
      "Epoch: 3194|steps:   60|Train Avg Loss: 0.4870 |Test Loss: 1.2241|lr = 0.00010\n",
      "Epoch: 3195|steps:   30|Train Avg Loss: 0.4529 |Test Loss: 1.1988|lr = 0.00010\n",
      "Epoch: 3195|steps:   60|Train Avg Loss: 0.5015 |Test Loss: 1.2278|lr = 0.00010\n",
      "Epoch: 3196|steps:   30|Train Avg Loss: 0.4694 |Test Loss: 1.2421|lr = 0.00010\n",
      "Epoch: 3196|steps:   60|Train Avg Loss: 0.4643 |Test Loss: 1.2093|lr = 0.00010\n",
      "Epoch: 3197|steps:   30|Train Avg Loss: 0.4688 |Test Loss: 1.2383|lr = 0.00010\n",
      "Epoch: 3197|steps:   60|Train Avg Loss: 0.4677 |Test Loss: 1.2148|lr = 0.00010\n",
      "Epoch: 3198|steps:   30|Train Avg Loss: 0.4525 |Test Loss: 1.2097|lr = 0.00010\n",
      "Epoch: 3198|steps:   60|Train Avg Loss: 0.5002 |Test Loss: 1.2189|lr = 0.00010\n",
      "Epoch: 3199|steps:   30|Train Avg Loss: 0.4654 |Test Loss: 1.2164|lr = 0.00010\n",
      "Epoch: 3199|steps:   60|Train Avg Loss: 0.4736 |Test Loss: 1.2411|lr = 0.00010\n",
      "Epoch: 3200|steps:   30|Train Avg Loss: 0.4535 |Test Loss: 1.2130|lr = 0.00010\n",
      "Epoch: 3200|steps:   60|Train Avg Loss: 0.4743 |Test Loss: 1.2175|lr = 0.00010\n",
      "Epoch: 3201|steps:   30|Train Avg Loss: 0.4499 |Test Loss: 1.2352|lr = 0.00010\n",
      "Epoch: 3201|steps:   60|Train Avg Loss: 0.4713 |Test Loss: 1.2411|lr = 0.00010\n",
      "Epoch: 3202|steps:   30|Train Avg Loss: 0.4436 |Test Loss: 1.2315|lr = 0.00010\n",
      "Epoch: 3202|steps:   60|Train Avg Loss: 0.4805 |Test Loss: 1.2232|lr = 0.00010\n",
      "Epoch: 3203|steps:   30|Train Avg Loss: 0.4662 |Test Loss: 1.2308|lr = 0.00010\n",
      "Epoch: 3203|steps:   60|Train Avg Loss: 0.4726 |Test Loss: 1.2502|lr = 0.00010\n",
      "Epoch: 3204|steps:   30|Train Avg Loss: 0.4580 |Test Loss: 1.2310|lr = 0.00010\n",
      "Epoch: 3204|steps:   60|Train Avg Loss: 0.4709 |Test Loss: 1.2352|lr = 0.00010\n",
      "Epoch: 3205|steps:   30|Train Avg Loss: 0.4374 |Test Loss: 1.2266|lr = 0.00010\n",
      "Epoch: 3205|steps:   60|Train Avg Loss: 0.4810 |Test Loss: 1.2652|lr = 0.00010\n",
      "Epoch: 3206|steps:   30|Train Avg Loss: 0.4765 |Test Loss: 1.2628|lr = 0.00010\n",
      "Epoch: 3206|steps:   60|Train Avg Loss: 0.4433 |Test Loss: 1.2288|lr = 0.00010\n",
      "Epoch: 3207|steps:   30|Train Avg Loss: 0.4877 |Test Loss: 1.2742|lr = 0.00010\n",
      "Epoch: 3207|steps:   60|Train Avg Loss: 0.4480 |Test Loss: 1.2303|lr = 0.00010\n",
      "Epoch: 3208|steps:   30|Train Avg Loss: 0.4459 |Test Loss: 1.2330|lr = 0.00010\n",
      "Epoch: 3208|steps:   60|Train Avg Loss: 0.4586 |Test Loss: 1.2597|lr = 0.00010\n",
      "Epoch: 3209|steps:   30|Train Avg Loss: 0.4421 |Test Loss: 1.2436|lr = 0.00010\n",
      "Epoch: 3209|steps:   60|Train Avg Loss: 0.4589 |Test Loss: 1.2376|lr = 0.00010\n",
      "Epoch: 3210|steps:   30|Train Avg Loss: 0.4397 |Test Loss: 1.2611|lr = 0.00010\n",
      "Epoch: 3210|steps:   60|Train Avg Loss: 0.4666 |Test Loss: 1.2539|lr = 0.00010\n",
      "Epoch: 3211|steps:   30|Train Avg Loss: 0.4321 |Test Loss: 1.2561|lr = 0.00010\n",
      "Epoch: 3211|steps:   60|Train Avg Loss: 0.4679 |Test Loss: 1.2646|lr = 0.00010\n",
      "Epoch: 3212|steps:   30|Train Avg Loss: 0.4613 |Test Loss: 1.2745|lr = 0.00010\n",
      "Epoch: 3212|steps:   60|Train Avg Loss: 0.4704 |Test Loss: 1.2720|lr = 0.00010\n",
      "Epoch: 3213|steps:   30|Train Avg Loss: 0.4418 |Test Loss: 1.2439|lr = 0.00010\n",
      "Epoch: 3213|steps:   60|Train Avg Loss: 0.4419 |Test Loss: 1.2855|lr = 0.00010\n",
      "Epoch: 3214|steps:   30|Train Avg Loss: 0.4356 |Test Loss: 1.2662|lr = 0.00010\n",
      "Epoch: 3214|steps:   60|Train Avg Loss: 0.4516 |Test Loss: 1.2582|lr = 0.00010\n",
      "Epoch: 3215|steps:   30|Train Avg Loss: 0.4414 |Test Loss: 1.2507|lr = 0.00010\n",
      "Epoch: 3215|steps:   60|Train Avg Loss: 0.4598 |Test Loss: 1.2623|lr = 0.00010\n",
      "Epoch: 3216|steps:   30|Train Avg Loss: 0.4463 |Test Loss: 1.2622|lr = 0.00010\n",
      "Epoch: 3216|steps:   60|Train Avg Loss: 0.4733 |Test Loss: 1.2938|lr = 0.00010\n",
      "Epoch: 3217|steps:   30|Train Avg Loss: 0.4590 |Test Loss: 1.2997|lr = 0.00010\n",
      "Epoch: 3217|steps:   60|Train Avg Loss: 0.4549 |Test Loss: 1.2986|lr = 0.00010\n",
      "Epoch: 3218|steps:   30|Train Avg Loss: 0.4441 |Test Loss: 1.2450|lr = 0.00010\n",
      "Epoch: 3218|steps:   60|Train Avg Loss: 0.4556 |Test Loss: 1.2864|lr = 0.00010\n",
      "Epoch: 3219|steps:   30|Train Avg Loss: 0.4591 |Test Loss: 1.2719|lr = 0.00010\n",
      "Epoch: 3219|steps:   60|Train Avg Loss: 0.4233 |Test Loss: 1.2700|lr = 0.00010\n",
      "Epoch: 3220|steps:   30|Train Avg Loss: 0.4576 |Test Loss: 1.2618|lr = 0.00010\n",
      "Epoch: 3220|steps:   60|Train Avg Loss: 0.4515 |Test Loss: 1.2833|lr = 0.00010\n",
      "Epoch: 3221|steps:   30|Train Avg Loss: 0.4172 |Test Loss: 1.2760|lr = 0.00010\n",
      "Epoch: 3221|steps:   60|Train Avg Loss: 0.4797 |Test Loss: 1.2768|lr = 0.00010\n",
      "Epoch: 3222|steps:   30|Train Avg Loss: 0.4368 |Test Loss: 1.3060|lr = 0.00010\n",
      "Epoch: 3222|steps:   60|Train Avg Loss: 0.4574 |Test Loss: 1.2926|lr = 0.00010\n",
      "Epoch: 3223|steps:   30|Train Avg Loss: 0.4372 |Test Loss: 1.2787|lr = 0.00010\n",
      "Epoch: 3223|steps:   60|Train Avg Loss: 0.4513 |Test Loss: 1.2753|lr = 0.00010\n",
      "Epoch: 3224|steps:   30|Train Avg Loss: 0.4482 |Test Loss: 1.2904|lr = 0.00010\n",
      "Epoch: 3224|steps:   60|Train Avg Loss: 0.4436 |Test Loss: 1.2631|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3225|steps:   30|Train Avg Loss: 0.4570 |Test Loss: 1.2980|lr = 0.00010\n",
      "Epoch: 3225|steps:   60|Train Avg Loss: 0.4575 |Test Loss: 1.2819|lr = 0.00010\n",
      "Epoch: 3226|steps:   30|Train Avg Loss: 0.4395 |Test Loss: 1.2978|lr = 0.00010\n",
      "Epoch: 3226|steps:   60|Train Avg Loss: 0.4436 |Test Loss: 1.2959|lr = 0.00010\n",
      "Epoch: 3227|steps:   30|Train Avg Loss: 0.4192 |Test Loss: 1.3019|lr = 0.00010\n",
      "Epoch: 3227|steps:   60|Train Avg Loss: 0.4687 |Test Loss: 1.2930|lr = 0.00010\n",
      "Epoch: 3228|steps:   30|Train Avg Loss: 0.4107 |Test Loss: 1.2742|lr = 0.00010\n",
      "Epoch: 3228|steps:   60|Train Avg Loss: 0.4506 |Test Loss: 1.3259|lr = 0.00010\n",
      "Epoch: 3229|steps:   30|Train Avg Loss: 0.4259 |Test Loss: 1.2728|lr = 0.00010\n",
      "Epoch: 3229|steps:   60|Train Avg Loss: 0.4530 |Test Loss: 1.3120|lr = 0.00010\n",
      "Epoch: 3230|steps:   30|Train Avg Loss: 0.4378 |Test Loss: 1.2980|lr = 0.00010\n",
      "Epoch: 3230|steps:   60|Train Avg Loss: 0.4543 |Test Loss: 1.3108|lr = 0.00010\n",
      "Epoch: 3231|steps:   30|Train Avg Loss: 0.4282 |Test Loss: 1.3511|lr = 0.00010\n",
      "Epoch: 3231|steps:   60|Train Avg Loss: 0.4429 |Test Loss: 1.2665|lr = 0.00010\n",
      "Epoch: 3232|steps:   30|Train Avg Loss: 0.4147 |Test Loss: 1.2786|lr = 0.00010\n",
      "Epoch: 3232|steps:   60|Train Avg Loss: 0.4349 |Test Loss: 1.3210|lr = 0.00010\n",
      "Epoch: 3233|steps:   30|Train Avg Loss: 0.4214 |Test Loss: 1.3003|lr = 0.00010\n",
      "Epoch: 3233|steps:   60|Train Avg Loss: 0.4429 |Test Loss: 1.3334|lr = 0.00010\n",
      "Epoch: 3234|steps:   30|Train Avg Loss: 0.4320 |Test Loss: 1.3078|lr = 0.00010\n",
      "Epoch: 3234|steps:   60|Train Avg Loss: 0.4361 |Test Loss: 1.3094|lr = 0.00010\n",
      "Epoch: 3235|steps:   30|Train Avg Loss: 0.4300 |Test Loss: 1.3254|lr = 0.00010\n",
      "Epoch: 3235|steps:   60|Train Avg Loss: 0.4354 |Test Loss: 1.3205|lr = 0.00010\n",
      "Epoch: 3236|steps:   30|Train Avg Loss: 0.4216 |Test Loss: 1.3221|lr = 0.00010\n",
      "Epoch: 3236|steps:   60|Train Avg Loss: 0.4499 |Test Loss: 1.2938|lr = 0.00010\n",
      "Epoch: 3237|steps:   30|Train Avg Loss: 0.4337 |Test Loss: 1.3479|lr = 0.00010\n",
      "Epoch: 3237|steps:   60|Train Avg Loss: 0.4441 |Test Loss: 1.3223|lr = 0.00010\n",
      "Epoch: 3238|steps:   30|Train Avg Loss: 0.4272 |Test Loss: 1.3097|lr = 0.00010\n",
      "Epoch: 3238|steps:   60|Train Avg Loss: 0.4295 |Test Loss: 1.3279|lr = 0.00010\n",
      "Epoch: 3239|steps:   30|Train Avg Loss: 0.4378 |Test Loss: 1.3053|lr = 0.00010\n",
      "Epoch: 3239|steps:   60|Train Avg Loss: 0.4258 |Test Loss: 1.3249|lr = 0.00010\n",
      "Epoch: 3240|steps:   30|Train Avg Loss: 0.4189 |Test Loss: 1.3215|lr = 0.00010\n",
      "Epoch: 3240|steps:   60|Train Avg Loss: 0.4462 |Test Loss: 1.3434|lr = 0.00010\n",
      "Epoch: 3241|steps:   30|Train Avg Loss: 0.4298 |Test Loss: 1.3253|lr = 0.00010\n",
      "Epoch: 3241|steps:   60|Train Avg Loss: 0.4206 |Test Loss: 1.3126|lr = 0.00010\n",
      "Epoch: 3242|steps:   30|Train Avg Loss: 0.4086 |Test Loss: 1.3135|lr = 0.00010\n",
      "Epoch: 3242|steps:   60|Train Avg Loss: 0.4402 |Test Loss: 1.3314|lr = 0.00010\n",
      "Epoch: 3243|steps:   30|Train Avg Loss: 0.4388 |Test Loss: 1.3294|lr = 0.00010\n",
      "Epoch: 3243|steps:   60|Train Avg Loss: 0.4072 |Test Loss: 1.3472|lr = 0.00010\n",
      "Epoch: 3244|steps:   30|Train Avg Loss: 0.4202 |Test Loss: 1.3765|lr = 0.00010\n",
      "Epoch: 3244|steps:   60|Train Avg Loss: 0.4410 |Test Loss: 1.3376|lr = 0.00010\n",
      "Epoch: 3245|steps:   30|Train Avg Loss: 0.4215 |Test Loss: 1.3162|lr = 0.00010\n",
      "Epoch: 3245|steps:   60|Train Avg Loss: 0.4195 |Test Loss: 1.3384|lr = 0.00010\n",
      "Epoch: 3246|steps:   30|Train Avg Loss: 0.4153 |Test Loss: 1.3747|lr = 0.00010\n",
      "Epoch: 3246|steps:   60|Train Avg Loss: 0.4221 |Test Loss: 1.3392|lr = 0.00010\n",
      "Epoch: 3247|steps:   30|Train Avg Loss: 0.4346 |Test Loss: 1.3509|lr = 0.00010\n",
      "Epoch: 3247|steps:   60|Train Avg Loss: 0.4177 |Test Loss: 1.3275|lr = 0.00010\n",
      "Epoch: 3248|steps:   30|Train Avg Loss: 0.4365 |Test Loss: 1.3102|lr = 0.00010\n",
      "Epoch: 3248|steps:   60|Train Avg Loss: 0.4284 |Test Loss: 1.3485|lr = 0.00010\n",
      "Epoch: 3249|steps:   30|Train Avg Loss: 0.4224 |Test Loss: 1.3388|lr = 0.00010\n",
      "Epoch: 3249|steps:   60|Train Avg Loss: 0.4166 |Test Loss: 1.3659|lr = 0.00010\n",
      "Epoch: 3250|steps:   30|Train Avg Loss: 0.3855 |Test Loss: 1.3354|lr = 0.00010\n",
      "Epoch: 3250|steps:   60|Train Avg Loss: 0.4404 |Test Loss: 1.3797|lr = 0.00010\n",
      "Epoch: 3251|steps:   30|Train Avg Loss: 0.4069 |Test Loss: 1.3579|lr = 0.00010\n",
      "Epoch: 3251|steps:   60|Train Avg Loss: 0.4367 |Test Loss: 1.4041|lr = 0.00010\n",
      "Epoch: 3252|steps:   30|Train Avg Loss: 0.4229 |Test Loss: 1.3772|lr = 0.00010\n",
      "Epoch: 3252|steps:   60|Train Avg Loss: 0.4249 |Test Loss: 1.3560|lr = 0.00010\n",
      "Epoch: 3253|steps:   30|Train Avg Loss: 0.4033 |Test Loss: 1.3564|lr = 0.00010\n",
      "Epoch: 3253|steps:   60|Train Avg Loss: 0.4224 |Test Loss: 1.3176|lr = 0.00010\n",
      "Epoch: 3254|steps:   30|Train Avg Loss: 0.4188 |Test Loss: 1.3509|lr = 0.00010\n",
      "Epoch: 3254|steps:   60|Train Avg Loss: 0.3987 |Test Loss: 1.3857|lr = 0.00010\n",
      "Epoch: 3255|steps:   30|Train Avg Loss: 0.4137 |Test Loss: 1.3914|lr = 0.00010\n",
      "Epoch: 3255|steps:   60|Train Avg Loss: 0.4147 |Test Loss: 1.3777|lr = 0.00010\n",
      "Epoch: 3256|steps:   30|Train Avg Loss: 0.4284 |Test Loss: 1.3750|lr = 0.00010\n",
      "Epoch: 3256|steps:   60|Train Avg Loss: 0.4102 |Test Loss: 1.3405|lr = 0.00010\n",
      "Epoch: 3257|steps:   30|Train Avg Loss: 0.3954 |Test Loss: 1.3914|lr = 0.00010\n",
      "Epoch: 3257|steps:   60|Train Avg Loss: 0.4260 |Test Loss: 1.3835|lr = 0.00010\n",
      "Epoch: 3258|steps:   30|Train Avg Loss: 0.4180 |Test Loss: 1.3799|lr = 0.00010\n",
      "Epoch: 3258|steps:   60|Train Avg Loss: 0.4153 |Test Loss: 1.3463|lr = 0.00010\n",
      "Epoch: 3259|steps:   30|Train Avg Loss: 0.3887 |Test Loss: 1.3835|lr = 0.00010\n",
      "Epoch: 3259|steps:   60|Train Avg Loss: 0.4450 |Test Loss: 1.3984|lr = 0.00010\n",
      "Epoch: 3260|steps:   30|Train Avg Loss: 0.3991 |Test Loss: 1.3877|lr = 0.00010\n",
      "Epoch: 3260|steps:   60|Train Avg Loss: 0.4115 |Test Loss: 1.3458|lr = 0.00010\n",
      "Epoch: 3261|steps:   30|Train Avg Loss: 0.4043 |Test Loss: 1.3915|lr = 0.00010\n",
      "Epoch: 3261|steps:   60|Train Avg Loss: 0.4206 |Test Loss: 1.3874|lr = 0.00010\n",
      "Epoch: 3262|steps:   30|Train Avg Loss: 0.3894 |Test Loss: 1.3800|lr = 0.00010\n",
      "Epoch: 3262|steps:   60|Train Avg Loss: 0.4165 |Test Loss: 1.4161|lr = 0.00010\n",
      "Epoch: 3263|steps:   30|Train Avg Loss: 0.3973 |Test Loss: 1.3844|lr = 0.00010\n",
      "Epoch: 3263|steps:   60|Train Avg Loss: 0.4023 |Test Loss: 1.3892|lr = 0.00010\n",
      "Epoch: 3264|steps:   30|Train Avg Loss: 0.3997 |Test Loss: 1.3828|lr = 0.00010\n",
      "Epoch: 3264|steps:   60|Train Avg Loss: 0.4251 |Test Loss: 1.3965|lr = 0.00010\n",
      "Epoch: 3265|steps:   30|Train Avg Loss: 0.4141 |Test Loss: 1.3900|lr = 0.00010\n",
      "Epoch: 3265|steps:   60|Train Avg Loss: 0.4066 |Test Loss: 1.4089|lr = 0.00010\n",
      "Epoch: 3266|steps:   30|Train Avg Loss: 0.3997 |Test Loss: 1.4133|lr = 0.00010\n",
      "Epoch: 3266|steps:   60|Train Avg Loss: 0.4130 |Test Loss: 1.4159|lr = 0.00010\n",
      "Epoch: 3267|steps:   30|Train Avg Loss: 0.4107 |Test Loss: 1.4264|lr = 0.00010\n",
      "Epoch: 3267|steps:   60|Train Avg Loss: 0.4070 |Test Loss: 1.3751|lr = 0.00010\n",
      "Epoch: 3268|steps:   30|Train Avg Loss: 0.3834 |Test Loss: 1.4285|lr = 0.00010\n",
      "Epoch: 3268|steps:   60|Train Avg Loss: 0.4204 |Test Loss: 1.3805|lr = 0.00010\n",
      "Epoch: 3269|steps:   30|Train Avg Loss: 0.4022 |Test Loss: 1.4083|lr = 0.00010\n",
      "Epoch: 3269|steps:   60|Train Avg Loss: 0.3805 |Test Loss: 1.3898|lr = 0.00010\n",
      "Epoch: 3270|steps:   30|Train Avg Loss: 0.3856 |Test Loss: 1.4072|lr = 0.00010\n",
      "Epoch: 3270|steps:   60|Train Avg Loss: 0.4200 |Test Loss: 1.3950|lr = 0.00010\n",
      "Epoch: 3271|steps:   30|Train Avg Loss: 0.3968 |Test Loss: 1.3835|lr = 0.00010\n",
      "Epoch: 3271|steps:   60|Train Avg Loss: 0.4061 |Test Loss: 1.3890|lr = 0.00010\n",
      "Epoch: 3272|steps:   30|Train Avg Loss: 0.3781 |Test Loss: 1.4310|lr = 0.00010\n",
      "Epoch: 3272|steps:   60|Train Avg Loss: 0.4052 |Test Loss: 1.3631|lr = 0.00010\n",
      "Epoch: 3273|steps:   30|Train Avg Loss: 0.3881 |Test Loss: 1.3983|lr = 0.00010\n",
      "Epoch: 3273|steps:   60|Train Avg Loss: 0.4091 |Test Loss: 1.3728|lr = 0.00010\n",
      "Epoch: 3274|steps:   30|Train Avg Loss: 0.4008 |Test Loss: 1.4478|lr = 0.00010\n",
      "Epoch: 3274|steps:   60|Train Avg Loss: 0.4000 |Test Loss: 1.4289|lr = 0.00010\n",
      "Epoch: 3275|steps:   30|Train Avg Loss: 0.4003 |Test Loss: 1.3846|lr = 0.00010\n",
      "Epoch: 3275|steps:   60|Train Avg Loss: 0.4036 |Test Loss: 1.4301|lr = 0.00010\n",
      "Epoch: 3276|steps:   30|Train Avg Loss: 0.4019 |Test Loss: 1.4432|lr = 0.00010\n",
      "Epoch: 3276|steps:   60|Train Avg Loss: 0.3978 |Test Loss: 1.4210|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3277|steps:   30|Train Avg Loss: 0.4047 |Test Loss: 1.4343|lr = 0.00010\n",
      "Epoch: 3277|steps:   60|Train Avg Loss: 0.3861 |Test Loss: 1.4015|lr = 0.00010\n",
      "Epoch: 3278|steps:   30|Train Avg Loss: 0.3788 |Test Loss: 1.4394|lr = 0.00010\n",
      "Epoch: 3278|steps:   60|Train Avg Loss: 0.4029 |Test Loss: 1.4145|lr = 0.00010\n",
      "Epoch: 3279|steps:   30|Train Avg Loss: 0.3707 |Test Loss: 1.4661|lr = 0.00010\n",
      "Epoch: 3279|steps:   60|Train Avg Loss: 0.4060 |Test Loss: 1.3996|lr = 0.00010\n",
      "Epoch: 3280|steps:   30|Train Avg Loss: 0.3806 |Test Loss: 1.4451|lr = 0.00010\n",
      "Epoch: 3280|steps:   60|Train Avg Loss: 0.4113 |Test Loss: 1.4420|lr = 0.00010\n",
      "Epoch: 3281|steps:   30|Train Avg Loss: 0.3805 |Test Loss: 1.4248|lr = 0.00010\n",
      "Epoch: 3281|steps:   60|Train Avg Loss: 0.3863 |Test Loss: 1.4461|lr = 0.00010\n",
      "Epoch: 3282|steps:   30|Train Avg Loss: 0.3616 |Test Loss: 1.4396|lr = 0.00010\n",
      "Epoch: 3282|steps:   60|Train Avg Loss: 0.4186 |Test Loss: 1.4794|lr = 0.00010\n",
      "Epoch: 3283|steps:   30|Train Avg Loss: 0.3831 |Test Loss: 1.4535|lr = 0.00010\n",
      "Epoch: 3283|steps:   60|Train Avg Loss: 0.3792 |Test Loss: 1.4774|lr = 0.00010\n",
      "Epoch: 3284|steps:   30|Train Avg Loss: 0.3836 |Test Loss: 1.4292|lr = 0.00010\n",
      "Epoch: 3284|steps:   60|Train Avg Loss: 0.3963 |Test Loss: 1.4607|lr = 0.00010\n",
      "Epoch: 3285|steps:   30|Train Avg Loss: 0.3793 |Test Loss: 1.4663|lr = 0.00010\n",
      "Epoch: 3285|steps:   60|Train Avg Loss: 0.3978 |Test Loss: 1.4232|lr = 0.00010\n",
      "Epoch: 3286|steps:   30|Train Avg Loss: 0.3647 |Test Loss: 1.4413|lr = 0.00010\n",
      "Epoch: 3286|steps:   60|Train Avg Loss: 0.4089 |Test Loss: 1.4614|lr = 0.00010\n",
      "Epoch: 3287|steps:   30|Train Avg Loss: 0.4047 |Test Loss: 1.4372|lr = 0.00010\n",
      "Epoch: 3287|steps:   60|Train Avg Loss: 0.3807 |Test Loss: 1.4698|lr = 0.00010\n",
      "Epoch: 3288|steps:   30|Train Avg Loss: 0.3982 |Test Loss: 1.4805|lr = 0.00010\n",
      "Epoch: 3288|steps:   60|Train Avg Loss: 0.4057 |Test Loss: 1.4609|lr = 0.00010\n",
      "Epoch: 3289|steps:   30|Train Avg Loss: 0.3757 |Test Loss: 1.4381|lr = 0.00010\n",
      "Epoch: 3289|steps:   60|Train Avg Loss: 0.3907 |Test Loss: 1.4742|lr = 0.00010\n",
      "Epoch: 3290|steps:   30|Train Avg Loss: 0.3569 |Test Loss: 1.4594|lr = 0.00010\n",
      "Epoch: 3290|steps:   60|Train Avg Loss: 0.4114 |Test Loss: 1.4358|lr = 0.00010\n",
      "Epoch: 3291|steps:   30|Train Avg Loss: 0.3599 |Test Loss: 1.4345|lr = 0.00010\n",
      "Epoch: 3291|steps:   60|Train Avg Loss: 0.3894 |Test Loss: 1.4716|lr = 0.00010\n",
      "Epoch: 3292|steps:   30|Train Avg Loss: 0.3751 |Test Loss: 1.4348|lr = 0.00010\n",
      "Epoch: 3292|steps:   60|Train Avg Loss: 0.3749 |Test Loss: 1.4906|lr = 0.00010\n",
      "Epoch: 3293|steps:   30|Train Avg Loss: 0.3840 |Test Loss: 1.4988|lr = 0.00010\n",
      "Epoch: 3293|steps:   60|Train Avg Loss: 0.4046 |Test Loss: 1.4499|lr = 0.00010\n",
      "Epoch: 3294|steps:   30|Train Avg Loss: 0.3731 |Test Loss: 1.4791|lr = 0.00010\n",
      "Epoch: 3294|steps:   60|Train Avg Loss: 0.3950 |Test Loss: 1.4805|lr = 0.00010\n",
      "Epoch: 3295|steps:   30|Train Avg Loss: 0.3860 |Test Loss: 1.4857|lr = 0.00010\n",
      "Epoch: 3295|steps:   60|Train Avg Loss: 0.3657 |Test Loss: 1.4737|lr = 0.00010\n",
      "Epoch: 3296|steps:   30|Train Avg Loss: 0.3570 |Test Loss: 1.4507|lr = 0.00010\n",
      "Epoch: 3296|steps:   60|Train Avg Loss: 0.3980 |Test Loss: 1.5149|lr = 0.00010\n",
      "Epoch: 3297|steps:   30|Train Avg Loss: 0.3462 |Test Loss: 1.4599|lr = 0.00010\n",
      "Epoch: 3297|steps:   60|Train Avg Loss: 0.3996 |Test Loss: 1.4786|lr = 0.00010\n",
      "Epoch: 3298|steps:   30|Train Avg Loss: 0.3616 |Test Loss: 1.4483|lr = 0.00010\n",
      "Epoch: 3298|steps:   60|Train Avg Loss: 0.4017 |Test Loss: 1.4777|lr = 0.00010\n",
      "Epoch: 3299|steps:   30|Train Avg Loss: 0.3722 |Test Loss: 1.5034|lr = 0.00010\n",
      "Epoch: 3299|steps:   60|Train Avg Loss: 0.3870 |Test Loss: 1.4445|lr = 0.00010\n",
      "Epoch: 3300|steps:   30|Train Avg Loss: 0.3680 |Test Loss: 1.5108|lr = 0.00010\n",
      "Epoch: 3300|steps:   60|Train Avg Loss: 0.3638 |Test Loss: 1.4900|lr = 0.00010\n",
      "Epoch: 3301|steps:   30|Train Avg Loss: 0.3600 |Test Loss: 1.4664|lr = 0.00010\n",
      "Epoch: 3301|steps:   60|Train Avg Loss: 0.3807 |Test Loss: 1.5518|lr = 0.00010\n",
      "Epoch: 3302|steps:   30|Train Avg Loss: 0.3717 |Test Loss: 1.5280|lr = 0.00010\n",
      "Epoch: 3302|steps:   60|Train Avg Loss: 0.3541 |Test Loss: 1.4876|lr = 0.00010\n",
      "Epoch: 3303|steps:   30|Train Avg Loss: 0.3525 |Test Loss: 1.4780|lr = 0.00010\n",
      "Epoch: 3303|steps:   60|Train Avg Loss: 0.3891 |Test Loss: 1.4860|lr = 0.00010\n",
      "Epoch: 3304|steps:   30|Train Avg Loss: 0.3627 |Test Loss: 1.5223|lr = 0.00010\n",
      "Epoch: 3304|steps:   60|Train Avg Loss: 0.3615 |Test Loss: 1.5280|lr = 0.00010\n",
      "Epoch: 3305|steps:   30|Train Avg Loss: 0.3642 |Test Loss: 1.5184|lr = 0.00010\n",
      "Epoch: 3305|steps:   60|Train Avg Loss: 0.3915 |Test Loss: 1.5037|lr = 0.00010\n",
      "Epoch: 3306|steps:   30|Train Avg Loss: 0.3414 |Test Loss: 1.5126|lr = 0.00010\n",
      "Epoch: 3306|steps:   60|Train Avg Loss: 0.3979 |Test Loss: 1.4780|lr = 0.00010\n",
      "Epoch: 3307|steps:   30|Train Avg Loss: 0.3531 |Test Loss: 1.5025|lr = 0.00010\n",
      "Epoch: 3307|steps:   60|Train Avg Loss: 0.3806 |Test Loss: 1.5670|lr = 0.00010\n",
      "Epoch: 3308|steps:   30|Train Avg Loss: 0.3638 |Test Loss: 1.5209|lr = 0.00010\n",
      "Epoch: 3308|steps:   60|Train Avg Loss: 0.3745 |Test Loss: 1.4920|lr = 0.00010\n",
      "Epoch: 3309|steps:   30|Train Avg Loss: 0.3542 |Test Loss: 1.5094|lr = 0.00010\n",
      "Epoch: 3309|steps:   60|Train Avg Loss: 0.3557 |Test Loss: 1.5223|lr = 0.00010\n",
      "Epoch: 3310|steps:   30|Train Avg Loss: 0.3485 |Test Loss: 1.5599|lr = 0.00010\n",
      "Epoch: 3310|steps:   60|Train Avg Loss: 0.3638 |Test Loss: 1.5496|lr = 0.00010\n",
      "Epoch: 3311|steps:   30|Train Avg Loss: 0.3696 |Test Loss: 1.5440|lr = 0.00010\n",
      "Epoch: 3311|steps:   60|Train Avg Loss: 0.3556 |Test Loss: 1.5022|lr = 0.00010\n",
      "Epoch: 3312|steps:   30|Train Avg Loss: 0.3575 |Test Loss: 1.5237|lr = 0.00010\n",
      "Epoch: 3312|steps:   60|Train Avg Loss: 0.3661 |Test Loss: 1.5326|lr = 0.00010\n",
      "Epoch: 3313|steps:   30|Train Avg Loss: 0.3474 |Test Loss: 1.5099|lr = 0.00010\n",
      "Epoch: 3313|steps:   60|Train Avg Loss: 0.3806 |Test Loss: 1.5554|lr = 0.00010\n",
      "Epoch: 3314|steps:   30|Train Avg Loss: 0.3703 |Test Loss: 1.5321|lr = 0.00010\n",
      "Epoch: 3314|steps:   60|Train Avg Loss: 0.3544 |Test Loss: 1.5448|lr = 0.00010\n",
      "Epoch: 3315|steps:   30|Train Avg Loss: 0.3481 |Test Loss: 1.5357|lr = 0.00010\n",
      "Epoch: 3315|steps:   60|Train Avg Loss: 0.3402 |Test Loss: 1.5487|lr = 0.00010\n",
      "Epoch: 3316|steps:   30|Train Avg Loss: 0.3304 |Test Loss: 1.5500|lr = 0.00010\n",
      "Epoch: 3316|steps:   60|Train Avg Loss: 0.3541 |Test Loss: 1.5190|lr = 0.00010\n",
      "Epoch: 3317|steps:   30|Train Avg Loss: 0.3585 |Test Loss: 1.5270|lr = 0.00010\n",
      "Epoch: 3317|steps:   60|Train Avg Loss: 0.3673 |Test Loss: 1.5547|lr = 0.00010\n",
      "Epoch: 3318|steps:   30|Train Avg Loss: 0.3416 |Test Loss: 1.5339|lr = 0.00010\n",
      "Epoch: 3318|steps:   60|Train Avg Loss: 0.3545 |Test Loss: 1.5426|lr = 0.00010\n",
      "Epoch: 3319|steps:   30|Train Avg Loss: 0.3568 |Test Loss: 1.5305|lr = 0.00010\n",
      "Epoch: 3319|steps:   60|Train Avg Loss: 0.3539 |Test Loss: 1.5513|lr = 0.00010\n",
      "Epoch: 3320|steps:   30|Train Avg Loss: 0.3410 |Test Loss: 1.5634|lr = 0.00010\n",
      "Epoch: 3320|steps:   60|Train Avg Loss: 0.3864 |Test Loss: 1.5929|lr = 0.00010\n",
      "Epoch: 3321|steps:   30|Train Avg Loss: 0.3428 |Test Loss: 1.5458|lr = 0.00010\n",
      "Epoch: 3321|steps:   60|Train Avg Loss: 0.3557 |Test Loss: 1.5972|lr = 0.00010\n",
      "Epoch: 3322|steps:   30|Train Avg Loss: 0.3497 |Test Loss: 1.5466|lr = 0.00010\n",
      "Epoch: 3322|steps:   60|Train Avg Loss: 0.3642 |Test Loss: 1.5610|lr = 0.00010\n",
      "Epoch: 3323|steps:   30|Train Avg Loss: 0.3491 |Test Loss: 1.5792|lr = 0.00010\n",
      "Epoch: 3323|steps:   60|Train Avg Loss: 0.3578 |Test Loss: 1.5991|lr = 0.00010\n",
      "Epoch: 3324|steps:   30|Train Avg Loss: 0.3507 |Test Loss: 1.5407|lr = 0.00010\n",
      "Epoch: 3324|steps:   60|Train Avg Loss: 0.3542 |Test Loss: 1.5888|lr = 0.00010\n",
      "Epoch: 3325|steps:   30|Train Avg Loss: 0.3559 |Test Loss: 1.5713|lr = 0.00010\n",
      "Epoch: 3325|steps:   60|Train Avg Loss: 0.3693 |Test Loss: 1.5894|lr = 0.00010\n",
      "Epoch: 3326|steps:   30|Train Avg Loss: 0.3348 |Test Loss: 1.5380|lr = 0.00010\n",
      "Epoch: 3326|steps:   60|Train Avg Loss: 0.3499 |Test Loss: 1.5639|lr = 0.00010\n",
      "Epoch: 3327|steps:   30|Train Avg Loss: 0.3495 |Test Loss: 1.5939|lr = 0.00010\n",
      "Epoch: 3327|steps:   60|Train Avg Loss: 0.3754 |Test Loss: 1.5710|lr = 0.00010\n",
      "Epoch: 3328|steps:   30|Train Avg Loss: 0.3495 |Test Loss: 1.6334|lr = 0.00010\n",
      "Epoch: 3328|steps:   60|Train Avg Loss: 0.3387 |Test Loss: 1.5592|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3329|steps:   30|Train Avg Loss: 0.3359 |Test Loss: 1.5699|lr = 0.00010\n",
      "Epoch: 3329|steps:   60|Train Avg Loss: 0.3665 |Test Loss: 1.5440|lr = 0.00010\n",
      "Epoch: 3330|steps:   30|Train Avg Loss: 0.3334 |Test Loss: 1.6030|lr = 0.00010\n",
      "Epoch: 3330|steps:   60|Train Avg Loss: 0.3503 |Test Loss: 1.5996|lr = 0.00010\n",
      "Epoch: 3331|steps:   30|Train Avg Loss: 0.3355 |Test Loss: 1.5566|lr = 0.00010\n",
      "Epoch: 3331|steps:   60|Train Avg Loss: 0.3491 |Test Loss: 1.5893|lr = 0.00010\n",
      "Epoch: 3332|steps:   30|Train Avg Loss: 0.3353 |Test Loss: 1.5657|lr = 0.00010\n",
      "Epoch: 3332|steps:   60|Train Avg Loss: 0.3680 |Test Loss: 1.5401|lr = 0.00010\n",
      "Epoch: 3333|steps:   30|Train Avg Loss: 0.3263 |Test Loss: 1.5961|lr = 0.00010\n",
      "Epoch: 3333|steps:   60|Train Avg Loss: 0.3550 |Test Loss: 1.5778|lr = 0.00010\n",
      "Epoch: 3334|steps:   30|Train Avg Loss: 0.3591 |Test Loss: 1.5610|lr = 0.00010\n",
      "Epoch: 3334|steps:   60|Train Avg Loss: 0.3227 |Test Loss: 1.5826|lr = 0.00010\n",
      "Epoch: 3335|steps:   30|Train Avg Loss: 0.3308 |Test Loss: 1.5720|lr = 0.00010\n",
      "Epoch: 3335|steps:   60|Train Avg Loss: 0.3450 |Test Loss: 1.6181|lr = 0.00010\n",
      "Epoch: 3336|steps:   30|Train Avg Loss: 0.3586 |Test Loss: 1.5991|lr = 0.00010\n",
      "Epoch: 3336|steps:   60|Train Avg Loss: 0.3491 |Test Loss: 1.5810|lr = 0.00010\n",
      "Epoch: 3337|steps:   30|Train Avg Loss: 0.3427 |Test Loss: 1.5808|lr = 0.00010\n",
      "Epoch: 3337|steps:   60|Train Avg Loss: 0.3457 |Test Loss: 1.6010|lr = 0.00010\n",
      "Epoch: 3338|steps:   30|Train Avg Loss: 0.3403 |Test Loss: 1.6317|lr = 0.00010\n",
      "Epoch: 3338|steps:   60|Train Avg Loss: 0.3323 |Test Loss: 1.5599|lr = 0.00010\n",
      "Epoch: 3339|steps:   30|Train Avg Loss: 0.3344 |Test Loss: 1.6249|lr = 0.00010\n",
      "Epoch: 3339|steps:   60|Train Avg Loss: 0.3411 |Test Loss: 1.5803|lr = 0.00010\n",
      "Epoch: 3340|steps:   30|Train Avg Loss: 0.3455 |Test Loss: 1.5924|lr = 0.00010\n",
      "Epoch: 3340|steps:   60|Train Avg Loss: 0.3269 |Test Loss: 1.5996|lr = 0.00010\n",
      "Epoch: 3341|steps:   30|Train Avg Loss: 0.3287 |Test Loss: 1.6428|lr = 0.00010\n",
      "Epoch: 3341|steps:   60|Train Avg Loss: 0.3497 |Test Loss: 1.6283|lr = 0.00010\n",
      "Epoch: 3342|steps:   30|Train Avg Loss: 0.3190 |Test Loss: 1.6344|lr = 0.00010\n",
      "Epoch: 3342|steps:   60|Train Avg Loss: 0.3376 |Test Loss: 1.6103|lr = 0.00010\n",
      "Epoch: 3343|steps:   30|Train Avg Loss: 0.3599 |Test Loss: 1.6212|lr = 0.00010\n",
      "Epoch: 3343|steps:   60|Train Avg Loss: 0.3335 |Test Loss: 1.6171|lr = 0.00010\n",
      "Epoch: 3344|steps:   30|Train Avg Loss: 0.3410 |Test Loss: 1.6265|lr = 0.00010\n",
      "Epoch: 3344|steps:   60|Train Avg Loss: 0.3167 |Test Loss: 1.6431|lr = 0.00010\n",
      "Epoch: 3345|steps:   30|Train Avg Loss: 0.3068 |Test Loss: 1.6448|lr = 0.00010\n",
      "Epoch: 3345|steps:   60|Train Avg Loss: 0.3404 |Test Loss: 1.6373|lr = 0.00010\n",
      "Epoch: 3346|steps:   30|Train Avg Loss: 0.3495 |Test Loss: 1.6783|lr = 0.00010\n",
      "Epoch: 3346|steps:   60|Train Avg Loss: 0.3277 |Test Loss: 1.6160|lr = 0.00010\n",
      "Epoch: 3347|steps:   30|Train Avg Loss: 0.3336 |Test Loss: 1.6121|lr = 0.00010\n",
      "Epoch: 3347|steps:   60|Train Avg Loss: 0.3210 |Test Loss: 1.6393|lr = 0.00010\n",
      "Epoch: 3348|steps:   30|Train Avg Loss: 0.3186 |Test Loss: 1.6415|lr = 0.00010\n",
      "Epoch: 3348|steps:   60|Train Avg Loss: 0.3152 |Test Loss: 1.6704|lr = 0.00010\n",
      "Epoch: 3349|steps:   30|Train Avg Loss: 0.3280 |Test Loss: 1.6227|lr = 0.00010\n",
      "Epoch: 3349|steps:   60|Train Avg Loss: 0.3270 |Test Loss: 1.6570|lr = 0.00010\n",
      "Epoch: 3350|steps:   30|Train Avg Loss: 0.3255 |Test Loss: 1.6843|lr = 0.00010\n",
      "Epoch: 3350|steps:   60|Train Avg Loss: 0.3308 |Test Loss: 1.6356|lr = 0.00010\n",
      "Epoch: 3351|steps:   30|Train Avg Loss: 0.3399 |Test Loss: 1.6505|lr = 0.00010\n",
      "Epoch: 3351|steps:   60|Train Avg Loss: 0.3243 |Test Loss: 1.6279|lr = 0.00010\n",
      "Epoch: 3352|steps:   30|Train Avg Loss: 0.3255 |Test Loss: 1.6900|lr = 0.00010\n",
      "Epoch: 3352|steps:   60|Train Avg Loss: 0.3281 |Test Loss: 1.6218|lr = 0.00010\n",
      "Epoch: 3353|steps:   30|Train Avg Loss: 0.3158 |Test Loss: 1.6418|lr = 0.00010\n",
      "Epoch: 3353|steps:   60|Train Avg Loss: 0.3406 |Test Loss: 1.6706|lr = 0.00010\n",
      "Epoch: 3354|steps:   30|Train Avg Loss: 0.3134 |Test Loss: 1.6911|lr = 0.00010\n",
      "Epoch: 3354|steps:   60|Train Avg Loss: 0.3282 |Test Loss: 1.6424|lr = 0.00010\n",
      "Epoch: 3355|steps:   30|Train Avg Loss: 0.3113 |Test Loss: 1.6498|lr = 0.00010\n",
      "Epoch: 3355|steps:   60|Train Avg Loss: 0.3274 |Test Loss: 1.6542|lr = 0.00010\n",
      "Epoch: 3356|steps:   30|Train Avg Loss: 0.3272 |Test Loss: 1.6669|lr = 0.00010\n",
      "Epoch: 3356|steps:   60|Train Avg Loss: 0.3330 |Test Loss: 1.6814|lr = 0.00010\n",
      "Epoch: 3357|steps:   30|Train Avg Loss: 0.3012 |Test Loss: 1.6541|lr = 0.00010\n",
      "Epoch: 3357|steps:   60|Train Avg Loss: 0.3262 |Test Loss: 1.6851|lr = 0.00010\n",
      "Epoch: 3358|steps:   30|Train Avg Loss: 0.3110 |Test Loss: 1.6994|lr = 0.00010\n",
      "Epoch: 3358|steps:   60|Train Avg Loss: 0.3512 |Test Loss: 1.6560|lr = 0.00010\n",
      "Epoch: 3359|steps:   30|Train Avg Loss: 0.3206 |Test Loss: 1.6847|lr = 0.00010\n",
      "Epoch: 3359|steps:   60|Train Avg Loss: 0.3309 |Test Loss: 1.7057|lr = 0.00010\n",
      "Epoch: 3360|steps:   30|Train Avg Loss: 0.3262 |Test Loss: 1.7153|lr = 0.00010\n",
      "Epoch: 3360|steps:   60|Train Avg Loss: 0.3411 |Test Loss: 1.6803|lr = 0.00010\n",
      "Epoch: 3361|steps:   30|Train Avg Loss: 0.3132 |Test Loss: 1.6711|lr = 0.00010\n",
      "Epoch: 3361|steps:   60|Train Avg Loss: 0.3310 |Test Loss: 1.7282|lr = 0.00010\n",
      "Epoch: 3362|steps:   30|Train Avg Loss: 0.3033 |Test Loss: 1.6638|lr = 0.00010\n",
      "Epoch: 3362|steps:   60|Train Avg Loss: 0.3299 |Test Loss: 1.6958|lr = 0.00010\n",
      "Epoch: 3363|steps:   30|Train Avg Loss: 0.3213 |Test Loss: 1.7035|lr = 0.00010\n",
      "Epoch: 3363|steps:   60|Train Avg Loss: 0.3112 |Test Loss: 1.6983|lr = 0.00010\n",
      "Epoch: 3364|steps:   30|Train Avg Loss: 0.2962 |Test Loss: 1.7121|lr = 0.00010\n",
      "Epoch: 3364|steps:   60|Train Avg Loss: 0.3364 |Test Loss: 1.7109|lr = 0.00010\n",
      "Epoch: 3365|steps:   30|Train Avg Loss: 0.3055 |Test Loss: 1.6966|lr = 0.00010\n",
      "Epoch: 3365|steps:   60|Train Avg Loss: 0.3504 |Test Loss: 1.7164|lr = 0.00010\n",
      "Epoch: 3366|steps:   30|Train Avg Loss: 0.3049 |Test Loss: 1.7074|lr = 0.00010\n",
      "Epoch: 3366|steps:   60|Train Avg Loss: 0.3266 |Test Loss: 1.7487|lr = 0.00010\n",
      "Epoch: 3367|steps:   30|Train Avg Loss: 0.3104 |Test Loss: 1.6709|lr = 0.00010\n",
      "Epoch: 3367|steps:   60|Train Avg Loss: 0.2941 |Test Loss: 1.6907|lr = 0.00010\n",
      "Epoch: 3368|steps:   30|Train Avg Loss: 0.3016 |Test Loss: 1.7393|lr = 0.00010\n",
      "Epoch: 3368|steps:   60|Train Avg Loss: 0.3267 |Test Loss: 1.6825|lr = 0.00010\n",
      "Epoch: 3369|steps:   30|Train Avg Loss: 0.3089 |Test Loss: 1.7173|lr = 0.00010\n",
      "Epoch: 3369|steps:   60|Train Avg Loss: 0.3189 |Test Loss: 1.7073|lr = 0.00010\n",
      "Epoch: 3370|steps:   30|Train Avg Loss: 0.3095 |Test Loss: 1.7847|lr = 0.00010\n",
      "Epoch: 3370|steps:   60|Train Avg Loss: 0.3020 |Test Loss: 1.6701|lr = 0.00010\n",
      "Epoch: 3371|steps:   30|Train Avg Loss: 0.2901 |Test Loss: 1.6647|lr = 0.00010\n",
      "Epoch: 3371|steps:   60|Train Avg Loss: 0.3198 |Test Loss: 1.6980|lr = 0.00010\n",
      "Epoch: 3372|steps:   30|Train Avg Loss: 0.2884 |Test Loss: 1.7493|lr = 0.00010\n",
      "Epoch: 3372|steps:   60|Train Avg Loss: 0.3183 |Test Loss: 1.6882|lr = 0.00010\n",
      "Epoch: 3373|steps:   30|Train Avg Loss: 0.2968 |Test Loss: 1.7383|lr = 0.00010\n",
      "Epoch: 3373|steps:   60|Train Avg Loss: 0.3289 |Test Loss: 1.7153|lr = 0.00010\n",
      "Epoch: 3374|steps:   30|Train Avg Loss: 0.2892 |Test Loss: 1.7101|lr = 0.00010\n",
      "Epoch: 3374|steps:   60|Train Avg Loss: 0.3279 |Test Loss: 1.7123|lr = 0.00010\n",
      "Epoch: 3375|steps:   30|Train Avg Loss: 0.2878 |Test Loss: 1.7680|lr = 0.00010\n",
      "Epoch: 3375|steps:   60|Train Avg Loss: 0.3139 |Test Loss: 1.7209|lr = 0.00010\n",
      "Epoch: 3376|steps:   30|Train Avg Loss: 0.3297 |Test Loss: 1.7346|lr = 0.00010\n",
      "Epoch: 3376|steps:   60|Train Avg Loss: 0.2961 |Test Loss: 1.6752|lr = 0.00010\n",
      "Epoch: 3377|steps:   30|Train Avg Loss: 0.2995 |Test Loss: 1.7902|lr = 0.00010\n",
      "Epoch: 3377|steps:   60|Train Avg Loss: 0.3179 |Test Loss: 1.7184|lr = 0.00010\n",
      "Epoch: 3378|steps:   30|Train Avg Loss: 0.2988 |Test Loss: 1.7474|lr = 0.00010\n",
      "Epoch: 3378|steps:   60|Train Avg Loss: 0.2962 |Test Loss: 1.7431|lr = 0.00010\n",
      "Epoch: 3379|steps:   30|Train Avg Loss: 0.3133 |Test Loss: 1.8108|lr = 0.00010\n",
      "Epoch: 3379|steps:   60|Train Avg Loss: 0.3401 |Test Loss: 1.6970|lr = 0.00010\n",
      "Epoch: 3380|steps:   30|Train Avg Loss: 0.2746 |Test Loss: 1.7055|lr = 0.00010\n",
      "Epoch: 3380|steps:   60|Train Avg Loss: 0.3301 |Test Loss: 1.7699|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3381|steps:   30|Train Avg Loss: 0.2988 |Test Loss: 1.7471|lr = 0.00010\n",
      "Epoch: 3381|steps:   60|Train Avg Loss: 0.2994 |Test Loss: 1.7444|lr = 0.00010\n",
      "Epoch: 3382|steps:   30|Train Avg Loss: 0.3012 |Test Loss: 1.7861|lr = 0.00010\n",
      "Epoch: 3382|steps:   60|Train Avg Loss: 0.3161 |Test Loss: 1.7462|lr = 0.00010\n",
      "Epoch: 3383|steps:   30|Train Avg Loss: 0.3211 |Test Loss: 1.7376|lr = 0.00010\n",
      "Epoch: 3383|steps:   60|Train Avg Loss: 0.3008 |Test Loss: 1.7623|lr = 0.00010\n",
      "Epoch: 3384|steps:   30|Train Avg Loss: 0.2851 |Test Loss: 1.7785|lr = 0.00010\n",
      "Epoch: 3384|steps:   60|Train Avg Loss: 0.3121 |Test Loss: 1.7594|lr = 0.00010\n",
      "Epoch: 3385|steps:   30|Train Avg Loss: 0.2875 |Test Loss: 1.7907|lr = 0.00010\n",
      "Epoch: 3385|steps:   60|Train Avg Loss: 0.2947 |Test Loss: 1.7670|lr = 0.00010\n",
      "Epoch: 3386|steps:   30|Train Avg Loss: 0.2669 |Test Loss: 1.7906|lr = 0.00010\n",
      "Epoch: 3386|steps:   60|Train Avg Loss: 0.3165 |Test Loss: 1.7917|lr = 0.00010\n",
      "Epoch: 3387|steps:   30|Train Avg Loss: 0.2813 |Test Loss: 1.7420|lr = 0.00010\n",
      "Epoch: 3387|steps:   60|Train Avg Loss: 0.3223 |Test Loss: 1.7622|lr = 0.00010\n",
      "Epoch: 3388|steps:   30|Train Avg Loss: 0.2752 |Test Loss: 1.7542|lr = 0.00010\n",
      "Epoch: 3388|steps:   60|Train Avg Loss: 0.2977 |Test Loss: 1.8014|lr = 0.00010\n",
      "Epoch: 3389|steps:   30|Train Avg Loss: 0.2728 |Test Loss: 1.8062|lr = 0.00010\n",
      "Epoch: 3389|steps:   60|Train Avg Loss: 0.3185 |Test Loss: 1.8524|lr = 0.00010\n",
      "Epoch: 3390|steps:   30|Train Avg Loss: 0.3016 |Test Loss: 1.7739|lr = 0.00010\n",
      "Epoch: 3390|steps:   60|Train Avg Loss: 0.2881 |Test Loss: 1.8150|lr = 0.00010\n",
      "Epoch: 3391|steps:   30|Train Avg Loss: 0.2864 |Test Loss: 1.7488|lr = 0.00010\n",
      "Epoch: 3391|steps:   60|Train Avg Loss: 0.2855 |Test Loss: 1.8062|lr = 0.00010\n",
      "Epoch: 3392|steps:   30|Train Avg Loss: 0.2808 |Test Loss: 1.7886|lr = 0.00010\n",
      "Epoch: 3392|steps:   60|Train Avg Loss: 0.3004 |Test Loss: 1.8335|lr = 0.00010\n",
      "Epoch: 3393|steps:   30|Train Avg Loss: 0.2769 |Test Loss: 1.7975|lr = 0.00010\n",
      "Epoch: 3393|steps:   60|Train Avg Loss: 0.2897 |Test Loss: 1.7928|lr = 0.00010\n",
      "Epoch: 3394|steps:   30|Train Avg Loss: 0.2859 |Test Loss: 1.8157|lr = 0.00010\n",
      "Epoch: 3394|steps:   60|Train Avg Loss: 0.2828 |Test Loss: 1.8023|lr = 0.00010\n",
      "Epoch: 3395|steps:   30|Train Avg Loss: 0.2756 |Test Loss: 1.8161|lr = 0.00010\n",
      "Epoch: 3395|steps:   60|Train Avg Loss: 0.2917 |Test Loss: 1.8114|lr = 0.00010\n",
      "Epoch: 3396|steps:   30|Train Avg Loss: 0.2729 |Test Loss: 1.7971|lr = 0.00010\n",
      "Epoch: 3396|steps:   60|Train Avg Loss: 0.2950 |Test Loss: 1.8259|lr = 0.00010\n",
      "Epoch: 3397|steps:   30|Train Avg Loss: 0.3049 |Test Loss: 1.7975|lr = 0.00010\n",
      "Epoch: 3397|steps:   60|Train Avg Loss: 0.2874 |Test Loss: 1.8184|lr = 0.00010\n",
      "Epoch: 3398|steps:   30|Train Avg Loss: 0.2889 |Test Loss: 1.7737|lr = 0.00010\n",
      "Epoch: 3398|steps:   60|Train Avg Loss: 0.2768 |Test Loss: 1.8160|lr = 0.00010\n",
      "Epoch: 3399|steps:   30|Train Avg Loss: 0.2848 |Test Loss: 1.7824|lr = 0.00010\n",
      "Epoch: 3399|steps:   60|Train Avg Loss: 0.3030 |Test Loss: 1.8230|lr = 0.00010\n",
      "Epoch: 3400|steps:   30|Train Avg Loss: 0.3053 |Test Loss: 1.8274|lr = 0.00010\n",
      "Epoch: 3400|steps:   60|Train Avg Loss: 0.2714 |Test Loss: 1.8523|lr = 0.00010\n",
      "Epoch: 3401|steps:   30|Train Avg Loss: 0.2852 |Test Loss: 1.8050|lr = 0.00010\n",
      "Epoch: 3401|steps:   60|Train Avg Loss: 0.2938 |Test Loss: 1.8748|lr = 0.00010\n",
      "Epoch: 3402|steps:   30|Train Avg Loss: 0.2830 |Test Loss: 1.8413|lr = 0.00010\n",
      "Epoch: 3402|steps:   60|Train Avg Loss: 0.2962 |Test Loss: 1.8318|lr = 0.00010\n",
      "Epoch: 3403|steps:   30|Train Avg Loss: 0.2763 |Test Loss: 1.7897|lr = 0.00010\n",
      "Epoch: 3403|steps:   60|Train Avg Loss: 0.2880 |Test Loss: 1.8467|lr = 0.00010\n",
      "Epoch: 3404|steps:   30|Train Avg Loss: 0.2765 |Test Loss: 1.8212|lr = 0.00010\n",
      "Epoch: 3404|steps:   60|Train Avg Loss: 0.2957 |Test Loss: 1.8543|lr = 0.00010\n",
      "Epoch: 3405|steps:   30|Train Avg Loss: 0.3217 |Test Loss: 1.8950|lr = 0.00010\n",
      "Epoch: 3405|steps:   60|Train Avg Loss: 0.3077 |Test Loss: 1.8187|lr = 0.00010\n",
      "Epoch: 3406|steps:   30|Train Avg Loss: 0.2647 |Test Loss: 1.8136|lr = 0.00010\n",
      "Epoch: 3406|steps:   60|Train Avg Loss: 0.2839 |Test Loss: 1.8345|lr = 0.00010\n",
      "Epoch: 3407|steps:   30|Train Avg Loss: 0.2563 |Test Loss: 1.8224|lr = 0.00010\n",
      "Epoch: 3407|steps:   60|Train Avg Loss: 0.3094 |Test Loss: 1.8490|lr = 0.00010\n",
      "Epoch: 3408|steps:   30|Train Avg Loss: 0.2535 |Test Loss: 1.8385|lr = 0.00010\n",
      "Epoch: 3408|steps:   60|Train Avg Loss: 0.2860 |Test Loss: 1.8708|lr = 0.00010\n",
      "Epoch: 3409|steps:   30|Train Avg Loss: 0.2738 |Test Loss: 1.9226|lr = 0.00010\n",
      "Epoch: 3409|steps:   60|Train Avg Loss: 0.2915 |Test Loss: 1.8163|lr = 0.00010\n",
      "Epoch: 3410|steps:   30|Train Avg Loss: 0.2851 |Test Loss: 1.8523|lr = 0.00010\n",
      "Epoch: 3410|steps:   60|Train Avg Loss: 0.2826 |Test Loss: 1.8909|lr = 0.00010\n",
      "Epoch: 3411|steps:   30|Train Avg Loss: 0.2657 |Test Loss: 1.8432|lr = 0.00010\n",
      "Epoch: 3411|steps:   60|Train Avg Loss: 0.2831 |Test Loss: 1.9288|lr = 0.00010\n",
      "Epoch: 3412|steps:   30|Train Avg Loss: 0.2566 |Test Loss: 1.8697|lr = 0.00010\n",
      "Epoch: 3412|steps:   60|Train Avg Loss: 0.2722 |Test Loss: 1.8269|lr = 0.00010\n",
      "Epoch: 3413|steps:   30|Train Avg Loss: 0.2789 |Test Loss: 1.8079|lr = 0.00010\n",
      "Epoch: 3413|steps:   60|Train Avg Loss: 0.2917 |Test Loss: 1.8763|lr = 0.00010\n",
      "Epoch: 3414|steps:   30|Train Avg Loss: 0.2644 |Test Loss: 1.9050|lr = 0.00010\n",
      "Epoch: 3414|steps:   60|Train Avg Loss: 0.2780 |Test Loss: 1.8984|lr = 0.00010\n",
      "Epoch: 3415|steps:   30|Train Avg Loss: 0.2642 |Test Loss: 1.8152|lr = 0.00010\n",
      "Epoch: 3415|steps:   60|Train Avg Loss: 0.2614 |Test Loss: 1.8947|lr = 0.00010\n",
      "Epoch: 3416|steps:   30|Train Avg Loss: 0.2645 |Test Loss: 1.9366|lr = 0.00010\n",
      "Epoch: 3416|steps:   60|Train Avg Loss: 0.3027 |Test Loss: 1.8806|lr = 0.00010\n",
      "Epoch: 3417|steps:   30|Train Avg Loss: 0.2637 |Test Loss: 1.8890|lr = 0.00010\n",
      "Epoch: 3417|steps:   60|Train Avg Loss: 0.2913 |Test Loss: 1.8750|lr = 0.00010\n",
      "Epoch: 3418|steps:   30|Train Avg Loss: 0.2473 |Test Loss: 1.8829|lr = 0.00010\n",
      "Epoch: 3418|steps:   60|Train Avg Loss: 0.2795 |Test Loss: 1.9243|lr = 0.00010\n",
      "Epoch: 3419|steps:   30|Train Avg Loss: 0.2492 |Test Loss: 1.8982|lr = 0.00010\n",
      "Epoch: 3419|steps:   60|Train Avg Loss: 0.2670 |Test Loss: 1.9199|lr = 0.00010\n",
      "Epoch: 3420|steps:   30|Train Avg Loss: 0.2797 |Test Loss: 1.9181|lr = 0.00010\n",
      "Epoch: 3420|steps:   60|Train Avg Loss: 0.2587 |Test Loss: 1.8798|lr = 0.00010\n",
      "Epoch: 3421|steps:   30|Train Avg Loss: 0.2603 |Test Loss: 1.8987|lr = 0.00010\n",
      "Epoch: 3421|steps:   60|Train Avg Loss: 0.2827 |Test Loss: 1.8838|lr = 0.00010\n",
      "Epoch: 3422|steps:   30|Train Avg Loss: 0.2724 |Test Loss: 1.8951|lr = 0.00010\n",
      "Epoch: 3422|steps:   60|Train Avg Loss: 0.2779 |Test Loss: 1.8726|lr = 0.00010\n",
      "Epoch: 3423|steps:   30|Train Avg Loss: 0.2719 |Test Loss: 1.9216|lr = 0.00010\n",
      "Epoch: 3423|steps:   60|Train Avg Loss: 0.2885 |Test Loss: 1.8972|lr = 0.00010\n",
      "Epoch: 3424|steps:   30|Train Avg Loss: 0.2536 |Test Loss: 1.8851|lr = 0.00010\n",
      "Epoch: 3424|steps:   60|Train Avg Loss: 0.2800 |Test Loss: 1.9577|lr = 0.00010\n",
      "Epoch: 3425|steps:   30|Train Avg Loss: 0.2505 |Test Loss: 1.8953|lr = 0.00010\n",
      "Epoch: 3425|steps:   60|Train Avg Loss: 0.2833 |Test Loss: 1.9248|lr = 0.00010\n",
      "Epoch: 3426|steps:   30|Train Avg Loss: 0.2588 |Test Loss: 1.9548|lr = 0.00010\n",
      "Epoch: 3426|steps:   60|Train Avg Loss: 0.2659 |Test Loss: 1.8649|lr = 0.00010\n",
      "Epoch: 3427|steps:   30|Train Avg Loss: 0.2634 |Test Loss: 1.9700|lr = 0.00010\n",
      "Epoch: 3427|steps:   60|Train Avg Loss: 0.2993 |Test Loss: 1.8494|lr = 0.00010\n",
      "Epoch: 3428|steps:   30|Train Avg Loss: 0.2568 |Test Loss: 1.9227|lr = 0.00010\n",
      "Epoch: 3428|steps:   60|Train Avg Loss: 0.2724 |Test Loss: 1.8795|lr = 0.00010\n",
      "Epoch: 3429|steps:   30|Train Avg Loss: 0.2383 |Test Loss: 1.8972|lr = 0.00010\n",
      "Epoch: 3429|steps:   60|Train Avg Loss: 0.2750 |Test Loss: 1.9432|lr = 0.00010\n",
      "Epoch: 3430|steps:   30|Train Avg Loss: 0.2670 |Test Loss: 1.8854|lr = 0.00010\n",
      "Epoch: 3430|steps:   60|Train Avg Loss: 0.2571 |Test Loss: 1.9407|lr = 0.00010\n",
      "Epoch: 3431|steps:   30|Train Avg Loss: 0.2456 |Test Loss: 1.9354|lr = 0.00010\n",
      "Epoch: 3431|steps:   60|Train Avg Loss: 0.2803 |Test Loss: 1.9698|lr = 0.00010\n",
      "Epoch: 3432|steps:   30|Train Avg Loss: 0.2617 |Test Loss: 1.9930|lr = 0.00010\n",
      "Epoch: 3432|steps:   60|Train Avg Loss: 0.2857 |Test Loss: 1.9415|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3433|steps:   30|Train Avg Loss: 0.2467 |Test Loss: 1.9583|lr = 0.00010\n",
      "Epoch: 3433|steps:   60|Train Avg Loss: 0.2557 |Test Loss: 1.9433|lr = 0.00010\n",
      "Epoch: 3434|steps:   30|Train Avg Loss: 0.2485 |Test Loss: 1.9730|lr = 0.00010\n",
      "Epoch: 3434|steps:   60|Train Avg Loss: 0.2723 |Test Loss: 1.9345|lr = 0.00010\n",
      "Epoch: 3435|steps:   30|Train Avg Loss: 0.2376 |Test Loss: 1.9914|lr = 0.00010\n",
      "Epoch: 3435|steps:   60|Train Avg Loss: 0.2588 |Test Loss: 2.0093|lr = 0.00010\n",
      "Epoch: 3436|steps:   30|Train Avg Loss: 0.2579 |Test Loss: 1.9523|lr = 0.00010\n",
      "Epoch: 3436|steps:   60|Train Avg Loss: 0.2570 |Test Loss: 1.9241|lr = 0.00010\n",
      "Epoch: 3437|steps:   30|Train Avg Loss: 0.2281 |Test Loss: 1.9861|lr = 0.00010\n",
      "Epoch: 3437|steps:   60|Train Avg Loss: 0.2805 |Test Loss: 1.9427|lr = 0.00010\n",
      "Epoch: 3438|steps:   30|Train Avg Loss: 0.2530 |Test Loss: 1.9285|lr = 0.00010\n",
      "Epoch: 3438|steps:   60|Train Avg Loss: 0.2657 |Test Loss: 1.9725|lr = 0.00010\n",
      "Epoch: 3439|steps:   30|Train Avg Loss: 0.2478 |Test Loss: 1.9262|lr = 0.00010\n",
      "Epoch: 3439|steps:   60|Train Avg Loss: 0.2606 |Test Loss: 1.9586|lr = 0.00010\n",
      "Epoch: 3440|steps:   30|Train Avg Loss: 0.2428 |Test Loss: 1.9512|lr = 0.00010\n",
      "Epoch: 3440|steps:   60|Train Avg Loss: 0.2495 |Test Loss: 1.9531|lr = 0.00010\n",
      "Epoch: 3441|steps:   30|Train Avg Loss: 0.2432 |Test Loss: 1.9959|lr = 0.00010\n",
      "Epoch: 3441|steps:   60|Train Avg Loss: 0.2584 |Test Loss: 1.9455|lr = 0.00010\n",
      "Epoch: 3442|steps:   30|Train Avg Loss: 0.2305 |Test Loss: 1.9995|lr = 0.00010\n",
      "Epoch: 3442|steps:   60|Train Avg Loss: 0.2636 |Test Loss: 1.9820|lr = 0.00010\n",
      "Epoch: 3443|steps:   30|Train Avg Loss: 0.2470 |Test Loss: 2.0222|lr = 0.00010\n",
      "Epoch: 3443|steps:   60|Train Avg Loss: 0.2675 |Test Loss: 1.9575|lr = 0.00010\n",
      "Epoch: 3444|steps:   30|Train Avg Loss: 0.2504 |Test Loss: 1.9577|lr = 0.00010\n",
      "Epoch: 3444|steps:   60|Train Avg Loss: 0.2565 |Test Loss: 1.9851|lr = 0.00010\n",
      "Epoch: 3445|steps:   30|Train Avg Loss: 0.2468 |Test Loss: 1.9626|lr = 0.00010\n",
      "Epoch: 3445|steps:   60|Train Avg Loss: 0.2542 |Test Loss: 1.9743|lr = 0.00010\n",
      "Epoch: 3446|steps:   30|Train Avg Loss: 0.2427 |Test Loss: 1.9831|lr = 0.00010\n",
      "Epoch: 3446|steps:   60|Train Avg Loss: 0.2465 |Test Loss: 1.9848|lr = 0.00010\n",
      "Epoch: 3447|steps:   30|Train Avg Loss: 0.2384 |Test Loss: 1.9656|lr = 0.00010\n",
      "Epoch: 3447|steps:   60|Train Avg Loss: 0.2492 |Test Loss: 2.0590|lr = 0.00010\n",
      "Epoch: 3448|steps:   30|Train Avg Loss: 0.2414 |Test Loss: 1.9613|lr = 0.00010\n",
      "Epoch: 3448|steps:   60|Train Avg Loss: 0.2591 |Test Loss: 1.9981|lr = 0.00010\n",
      "Epoch: 3449|steps:   30|Train Avg Loss: 0.2546 |Test Loss: 1.9764|lr = 0.00010\n",
      "Epoch: 3449|steps:   60|Train Avg Loss: 0.2459 |Test Loss: 2.0607|lr = 0.00010\n",
      "Epoch: 3450|steps:   30|Train Avg Loss: 0.2270 |Test Loss: 1.9749|lr = 0.00010\n",
      "Epoch: 3450|steps:   60|Train Avg Loss: 0.2414 |Test Loss: 2.0510|lr = 0.00010\n",
      "Epoch: 3451|steps:   30|Train Avg Loss: 0.2371 |Test Loss: 1.9428|lr = 0.00010\n",
      "Epoch: 3451|steps:   60|Train Avg Loss: 0.2443 |Test Loss: 2.0264|lr = 0.00010\n",
      "Epoch: 3452|steps:   30|Train Avg Loss: 0.2264 |Test Loss: 2.0992|lr = 0.00010\n",
      "Epoch: 3452|steps:   60|Train Avg Loss: 0.2539 |Test Loss: 1.9965|lr = 0.00010\n",
      "Epoch: 3453|steps:   30|Train Avg Loss: 0.2217 |Test Loss: 2.0123|lr = 0.00010\n",
      "Epoch: 3453|steps:   60|Train Avg Loss: 0.2941 |Test Loss: 2.0988|lr = 0.00010\n",
      "Epoch: 3454|steps:   30|Train Avg Loss: 0.2408 |Test Loss: 1.9986|lr = 0.00010\n",
      "Epoch: 3454|steps:   60|Train Avg Loss: 0.2451 |Test Loss: 2.0295|lr = 0.00010\n",
      "Epoch: 3455|steps:   30|Train Avg Loss: 0.2161 |Test Loss: 2.0297|lr = 0.00010\n",
      "Epoch: 3455|steps:   60|Train Avg Loss: 0.2571 |Test Loss: 2.0166|lr = 0.00010\n",
      "Epoch: 3456|steps:   30|Train Avg Loss: 0.2307 |Test Loss: 1.9970|lr = 0.00010\n",
      "Epoch: 3456|steps:   60|Train Avg Loss: 0.2447 |Test Loss: 2.0893|lr = 0.00010\n",
      "Epoch: 3457|steps:   30|Train Avg Loss: 0.2489 |Test Loss: 2.0402|lr = 0.00010\n",
      "Epoch: 3457|steps:   60|Train Avg Loss: 0.2459 |Test Loss: 2.0506|lr = 0.00010\n",
      "Epoch: 3458|steps:   30|Train Avg Loss: 0.2213 |Test Loss: 2.0657|lr = 0.00010\n",
      "Epoch: 3458|steps:   60|Train Avg Loss: 0.2517 |Test Loss: 1.9964|lr = 0.00010\n",
      "Epoch: 3459|steps:   30|Train Avg Loss: 0.2338 |Test Loss: 2.0206|lr = 0.00010\n",
      "Epoch: 3459|steps:   60|Train Avg Loss: 0.2330 |Test Loss: 2.0469|lr = 0.00010\n",
      "Epoch: 3460|steps:   30|Train Avg Loss: 0.2360 |Test Loss: 2.0427|lr = 0.00010\n",
      "Epoch: 3460|steps:   60|Train Avg Loss: 0.2305 |Test Loss: 2.0892|lr = 0.00010\n",
      "Epoch: 3461|steps:   30|Train Avg Loss: 0.2318 |Test Loss: 1.9953|lr = 0.00010\n",
      "Epoch: 3461|steps:   60|Train Avg Loss: 0.2496 |Test Loss: 2.0826|lr = 0.00010\n",
      "Epoch: 3462|steps:   30|Train Avg Loss: 0.2449 |Test Loss: 2.1289|lr = 0.00010\n",
      "Epoch: 3462|steps:   60|Train Avg Loss: 0.2293 |Test Loss: 2.0564|lr = 0.00010\n",
      "Epoch: 3463|steps:   30|Train Avg Loss: 0.2259 |Test Loss: 2.0410|lr = 0.00010\n",
      "Epoch: 3463|steps:   60|Train Avg Loss: 0.2308 |Test Loss: 2.0849|lr = 0.00010\n",
      "Epoch: 3464|steps:   30|Train Avg Loss: 0.2093 |Test Loss: 2.0275|lr = 0.00010\n",
      "Epoch: 3464|steps:   60|Train Avg Loss: 0.2607 |Test Loss: 2.0884|lr = 0.00010\n",
      "Epoch: 3465|steps:   30|Train Avg Loss: 0.2139 |Test Loss: 2.0979|lr = 0.00010\n",
      "Epoch: 3465|steps:   60|Train Avg Loss: 0.2400 |Test Loss: 2.0508|lr = 0.00010\n",
      "Epoch: 3466|steps:   30|Train Avg Loss: 0.2272 |Test Loss: 2.0606|lr = 0.00010\n",
      "Epoch: 3466|steps:   60|Train Avg Loss: 0.2400 |Test Loss: 2.1035|lr = 0.00010\n",
      "Epoch: 3467|steps:   30|Train Avg Loss: 0.2410 |Test Loss: 2.1127|lr = 0.00010\n",
      "Epoch: 3467|steps:   60|Train Avg Loss: 0.2399 |Test Loss: 2.0450|lr = 0.00010\n",
      "Epoch: 3468|steps:   30|Train Avg Loss: 0.2258 |Test Loss: 2.0480|lr = 0.00010\n",
      "Epoch: 3468|steps:   60|Train Avg Loss: 0.2274 |Test Loss: 2.1124|lr = 0.00010\n",
      "Epoch: 3469|steps:   30|Train Avg Loss: 0.2242 |Test Loss: 2.1509|lr = 0.00010\n",
      "Epoch: 3469|steps:   60|Train Avg Loss: 0.2355 |Test Loss: 2.0790|lr = 0.00010\n",
      "Epoch: 3470|steps:   30|Train Avg Loss: 0.2337 |Test Loss: 2.1190|lr = 0.00010\n",
      "Epoch: 3470|steps:   60|Train Avg Loss: 0.2334 |Test Loss: 2.0937|lr = 0.00010\n",
      "Epoch: 3471|steps:   30|Train Avg Loss: 0.2301 |Test Loss: 2.0730|lr = 0.00010\n",
      "Epoch: 3471|steps:   60|Train Avg Loss: 0.2278 |Test Loss: 2.0793|lr = 0.00010\n",
      "Epoch: 3472|steps:   30|Train Avg Loss: 0.2343 |Test Loss: 2.0711|lr = 0.00010\n",
      "Epoch: 3472|steps:   60|Train Avg Loss: 0.2163 |Test Loss: 2.1123|lr = 0.00010\n",
      "Epoch: 3473|steps:   30|Train Avg Loss: 0.2129 |Test Loss: 2.1131|lr = 0.00010\n",
      "Epoch: 3473|steps:   60|Train Avg Loss: 0.2277 |Test Loss: 2.1467|lr = 0.00010\n",
      "Epoch: 3474|steps:   30|Train Avg Loss: 0.2167 |Test Loss: 2.1726|lr = 0.00010\n",
      "Epoch: 3474|steps:   60|Train Avg Loss: 0.2304 |Test Loss: 2.0685|lr = 0.00010\n",
      "Epoch: 3475|steps:   30|Train Avg Loss: 0.2233 |Test Loss: 2.1159|lr = 0.00010\n",
      "Epoch: 3475|steps:   60|Train Avg Loss: 0.2343 |Test Loss: 2.1314|lr = 0.00010\n",
      "Epoch: 3476|steps:   30|Train Avg Loss: 0.2252 |Test Loss: 2.1276|lr = 0.00010\n",
      "Epoch: 3476|steps:   60|Train Avg Loss: 0.2310 |Test Loss: 2.1256|lr = 0.00010\n",
      "Epoch: 3477|steps:   30|Train Avg Loss: 0.2122 |Test Loss: 2.0815|lr = 0.00010\n",
      "Epoch: 3477|steps:   60|Train Avg Loss: 0.2309 |Test Loss: 2.1044|lr = 0.00010\n",
      "Epoch: 3478|steps:   30|Train Avg Loss: 0.2095 |Test Loss: 2.0800|lr = 0.00010\n",
      "Epoch: 3478|steps:   60|Train Avg Loss: 0.2296 |Test Loss: 2.1028|lr = 0.00010\n",
      "Epoch: 3479|steps:   30|Train Avg Loss: 0.2444 |Test Loss: 2.2085|lr = 0.00010\n",
      "Epoch: 3479|steps:   60|Train Avg Loss: 0.2344 |Test Loss: 2.1589|lr = 0.00010\n",
      "Epoch: 3480|steps:   30|Train Avg Loss: 0.2256 |Test Loss: 2.1503|lr = 0.00010\n",
      "Epoch: 3480|steps:   60|Train Avg Loss: 0.2238 |Test Loss: 2.1597|lr = 0.00010\n",
      "Epoch: 3481|steps:   30|Train Avg Loss: 0.2211 |Test Loss: 2.0996|lr = 0.00010\n",
      "Epoch: 3481|steps:   60|Train Avg Loss: 0.2096 |Test Loss: 2.1096|lr = 0.00010\n",
      "Epoch: 3482|steps:   30|Train Avg Loss: 0.2074 |Test Loss: 2.1699|lr = 0.00010\n",
      "Epoch: 3482|steps:   60|Train Avg Loss: 0.2361 |Test Loss: 2.0821|lr = 0.00010\n",
      "Epoch: 3483|steps:   30|Train Avg Loss: 0.2168 |Test Loss: 2.1433|lr = 0.00010\n",
      "Epoch: 3483|steps:   60|Train Avg Loss: 0.2175 |Test Loss: 2.1359|lr = 0.00010\n",
      "Epoch: 3484|steps:   30|Train Avg Loss: 0.2048 |Test Loss: 2.1002|lr = 0.00010\n",
      "Epoch: 3484|steps:   60|Train Avg Loss: 0.2181 |Test Loss: 2.1604|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3485|steps:   30|Train Avg Loss: 0.1946 |Test Loss: 2.1488|lr = 0.00010\n",
      "Epoch: 3485|steps:   60|Train Avg Loss: 0.2278 |Test Loss: 2.1279|lr = 0.00010\n",
      "Epoch: 3486|steps:   30|Train Avg Loss: 0.1949 |Test Loss: 2.1867|lr = 0.00010\n",
      "Epoch: 3486|steps:   60|Train Avg Loss: 0.2393 |Test Loss: 2.1410|lr = 0.00010\n",
      "Epoch: 3487|steps:   30|Train Avg Loss: 0.1983 |Test Loss: 2.2015|lr = 0.00010\n",
      "Epoch: 3487|steps:   60|Train Avg Loss: 0.2136 |Test Loss: 2.1861|lr = 0.00010\n",
      "Epoch: 3488|steps:   30|Train Avg Loss: 0.2166 |Test Loss: 2.1957|lr = 0.00010\n",
      "Epoch: 3488|steps:   60|Train Avg Loss: 0.2175 |Test Loss: 2.1429|lr = 0.00010\n",
      "Epoch: 3489|steps:   30|Train Avg Loss: 0.1967 |Test Loss: 2.1549|lr = 0.00010\n",
      "Epoch: 3489|steps:   60|Train Avg Loss: 0.2300 |Test Loss: 2.2714|lr = 0.00010\n",
      "Epoch: 3490|steps:   30|Train Avg Loss: 0.2043 |Test Loss: 2.2271|lr = 0.00010\n",
      "Epoch: 3490|steps:   60|Train Avg Loss: 0.1957 |Test Loss: 2.1465|lr = 0.00010\n",
      "Epoch: 3491|steps:   30|Train Avg Loss: 0.2271 |Test Loss: 2.1365|lr = 0.00010\n",
      "Epoch: 3491|steps:   60|Train Avg Loss: 0.2104 |Test Loss: 2.1683|lr = 0.00010\n",
      "Epoch: 3492|steps:   30|Train Avg Loss: 0.2002 |Test Loss: 2.1999|lr = 0.00010\n",
      "Epoch: 3492|steps:   60|Train Avg Loss: 0.2527 |Test Loss: 2.1554|lr = 0.00010\n",
      "Epoch: 3493|steps:   30|Train Avg Loss: 0.2017 |Test Loss: 2.2131|lr = 0.00010\n",
      "Epoch: 3493|steps:   60|Train Avg Loss: 0.2193 |Test Loss: 2.2004|lr = 0.00010\n",
      "Epoch: 3494|steps:   30|Train Avg Loss: 0.2180 |Test Loss: 2.1625|lr = 0.00010\n",
      "Epoch: 3494|steps:   60|Train Avg Loss: 0.2071 |Test Loss: 2.1936|lr = 0.00010\n",
      "Epoch: 3495|steps:   30|Train Avg Loss: 0.2268 |Test Loss: 2.2175|lr = 0.00010\n",
      "Epoch: 3495|steps:   60|Train Avg Loss: 0.2106 |Test Loss: 2.2173|lr = 0.00010\n",
      "Epoch: 3496|steps:   30|Train Avg Loss: 0.1881 |Test Loss: 2.2077|lr = 0.00010\n",
      "Epoch: 3496|steps:   60|Train Avg Loss: 0.2293 |Test Loss: 2.2185|lr = 0.00010\n",
      "Epoch: 3497|steps:   30|Train Avg Loss: 0.2050 |Test Loss: 2.2118|lr = 0.00010\n",
      "Epoch: 3497|steps:   60|Train Avg Loss: 0.2085 |Test Loss: 2.2358|lr = 0.00010\n",
      "Epoch: 3498|steps:   30|Train Avg Loss: 0.2000 |Test Loss: 2.2231|lr = 0.00010\n",
      "Epoch: 3498|steps:   60|Train Avg Loss: 0.2263 |Test Loss: 2.2238|lr = 0.00010\n",
      "Epoch: 3499|steps:   30|Train Avg Loss: 0.1979 |Test Loss: 2.2233|lr = 0.00010\n",
      "Epoch: 3499|steps:   60|Train Avg Loss: 0.2062 |Test Loss: 2.2608|lr = 0.00010\n",
      "Epoch: 3500|steps:   30|Train Avg Loss: 0.1973 |Test Loss: 2.2416|lr = 0.00010\n",
      "Epoch: 3500|steps:   60|Train Avg Loss: 0.1993 |Test Loss: 2.1895|lr = 0.00010\n",
      "Epoch: 3501|steps:   30|Train Avg Loss: 0.2104 |Test Loss: 2.1928|lr = 0.00010\n",
      "Epoch: 3501|steps:   60|Train Avg Loss: 0.2085 |Test Loss: 2.1966|lr = 0.00010\n",
      "Epoch: 3502|steps:   30|Train Avg Loss: 0.2036 |Test Loss: 2.2395|lr = 0.00010\n",
      "Epoch: 3502|steps:   60|Train Avg Loss: 0.2233 |Test Loss: 2.2432|lr = 0.00010\n",
      "Epoch: 3503|steps:   30|Train Avg Loss: 0.2093 |Test Loss: 2.2282|lr = 0.00010\n",
      "Epoch: 3503|steps:   60|Train Avg Loss: 0.2394 |Test Loss: 2.3441|lr = 0.00010\n",
      "Epoch: 3504|steps:   30|Train Avg Loss: 0.1961 |Test Loss: 2.2724|lr = 0.00010\n",
      "Epoch: 3504|steps:   60|Train Avg Loss: 0.1956 |Test Loss: 2.2931|lr = 0.00010\n",
      "Epoch: 3505|steps:   30|Train Avg Loss: 0.1927 |Test Loss: 2.2422|lr = 0.00010\n",
      "Epoch: 3505|steps:   60|Train Avg Loss: 0.1932 |Test Loss: 2.2633|lr = 0.00010\n",
      "Epoch: 3506|steps:   30|Train Avg Loss: 0.1905 |Test Loss: 2.2463|lr = 0.00010\n",
      "Epoch: 3506|steps:   60|Train Avg Loss: 0.2133 |Test Loss: 2.2800|lr = 0.00010\n",
      "Epoch: 3507|steps:   30|Train Avg Loss: 0.1973 |Test Loss: 2.2307|lr = 0.00010\n",
      "Epoch: 3507|steps:   60|Train Avg Loss: 0.1950 |Test Loss: 2.2676|lr = 0.00010\n",
      "Epoch: 3508|steps:   30|Train Avg Loss: 0.1881 |Test Loss: 2.2509|lr = 0.00010\n",
      "Epoch: 3508|steps:   60|Train Avg Loss: 0.2083 |Test Loss: 2.2877|lr = 0.00010\n",
      "Epoch: 3509|steps:   30|Train Avg Loss: 0.1931 |Test Loss: 2.2413|lr = 0.00010\n",
      "Epoch: 3509|steps:   60|Train Avg Loss: 0.2050 |Test Loss: 2.2851|lr = 0.00010\n",
      "Epoch: 3510|steps:   30|Train Avg Loss: 0.1958 |Test Loss: 2.1788|lr = 0.00010\n",
      "Epoch: 3510|steps:   60|Train Avg Loss: 0.1997 |Test Loss: 2.2569|lr = 0.00010\n",
      "Epoch: 3511|steps:   30|Train Avg Loss: 0.1806 |Test Loss: 2.2334|lr = 0.00010\n",
      "Epoch: 3511|steps:   60|Train Avg Loss: 0.2044 |Test Loss: 2.3697|lr = 0.00010\n",
      "Epoch: 3512|steps:   30|Train Avg Loss: 0.1901 |Test Loss: 2.2322|lr = 0.00010\n",
      "Epoch: 3512|steps:   60|Train Avg Loss: 0.2101 |Test Loss: 2.3392|lr = 0.00010\n",
      "Epoch: 3513|steps:   30|Train Avg Loss: 0.1808 |Test Loss: 2.3159|lr = 0.00010\n",
      "Epoch: 3513|steps:   60|Train Avg Loss: 0.1987 |Test Loss: 2.2477|lr = 0.00010\n",
      "Epoch: 3514|steps:   30|Train Avg Loss: 0.1736 |Test Loss: 2.3032|lr = 0.00010\n",
      "Epoch: 3514|steps:   60|Train Avg Loss: 0.1931 |Test Loss: 2.2883|lr = 0.00010\n",
      "Epoch: 3515|steps:   30|Train Avg Loss: 0.1765 |Test Loss: 2.2424|lr = 0.00010\n",
      "Epoch: 3515|steps:   60|Train Avg Loss: 0.1941 |Test Loss: 2.3278|lr = 0.00010\n",
      "Epoch: 3516|steps:   30|Train Avg Loss: 0.2017 |Test Loss: 2.2868|lr = 0.00010\n",
      "Epoch: 3516|steps:   60|Train Avg Loss: 0.1802 |Test Loss: 2.3467|lr = 0.00010\n",
      "Epoch: 3517|steps:   30|Train Avg Loss: 0.2090 |Test Loss: 2.3335|lr = 0.00010\n",
      "Epoch: 3517|steps:   60|Train Avg Loss: 0.1928 |Test Loss: 2.3423|lr = 0.00010\n",
      "Epoch: 3518|steps:   30|Train Avg Loss: 0.1892 |Test Loss: 2.2704|lr = 0.00010\n",
      "Epoch: 3518|steps:   60|Train Avg Loss: 0.1864 |Test Loss: 2.3008|lr = 0.00010\n",
      "Epoch: 3519|steps:   30|Train Avg Loss: 0.1973 |Test Loss: 2.3430|lr = 0.00010\n",
      "Epoch: 3519|steps:   60|Train Avg Loss: 0.2030 |Test Loss: 2.2702|lr = 0.00010\n",
      "Epoch: 3520|steps:   30|Train Avg Loss: 0.1883 |Test Loss: 2.2523|lr = 0.00010\n",
      "Epoch: 3520|steps:   60|Train Avg Loss: 0.1970 |Test Loss: 2.3368|lr = 0.00010\n",
      "Epoch: 3521|steps:   30|Train Avg Loss: 0.1846 |Test Loss: 2.2646|lr = 0.00010\n",
      "Epoch: 3521|steps:   60|Train Avg Loss: 0.2035 |Test Loss: 2.3790|lr = 0.00010\n",
      "Epoch: 3522|steps:   30|Train Avg Loss: 0.1939 |Test Loss: 2.3640|lr = 0.00010\n",
      "Epoch: 3522|steps:   60|Train Avg Loss: 0.2040 |Test Loss: 2.2592|lr = 0.00010\n",
      "Epoch: 3523|steps:   30|Train Avg Loss: 0.1773 |Test Loss: 2.3525|lr = 0.00010\n",
      "Epoch: 3523|steps:   60|Train Avg Loss: 0.1912 |Test Loss: 2.3142|lr = 0.00010\n",
      "Epoch: 3524|steps:   30|Train Avg Loss: 0.1644 |Test Loss: 2.3315|lr = 0.00010\n",
      "Epoch: 3524|steps:   60|Train Avg Loss: 0.1877 |Test Loss: 2.3397|lr = 0.00010\n",
      "Epoch: 3525|steps:   30|Train Avg Loss: 0.1914 |Test Loss: 2.3476|lr = 0.00010\n",
      "Epoch: 3525|steps:   60|Train Avg Loss: 0.1738 |Test Loss: 2.3595|lr = 0.00010\n",
      "Epoch: 3526|steps:   30|Train Avg Loss: 0.1663 |Test Loss: 2.3693|lr = 0.00010\n",
      "Epoch: 3526|steps:   60|Train Avg Loss: 0.2013 |Test Loss: 2.3737|lr = 0.00010\n",
      "Epoch: 3527|steps:   30|Train Avg Loss: 0.2064 |Test Loss: 2.4047|lr = 0.00010\n",
      "Epoch: 3527|steps:   60|Train Avg Loss: 0.2086 |Test Loss: 2.3258|lr = 0.00010\n",
      "Epoch: 3528|steps:   30|Train Avg Loss: 0.1832 |Test Loss: 2.3460|lr = 0.00010\n",
      "Epoch: 3528|steps:   60|Train Avg Loss: 0.1874 |Test Loss: 2.4688|lr = 0.00010\n",
      "Epoch: 3529|steps:   30|Train Avg Loss: 0.1801 |Test Loss: 2.3446|lr = 0.00010\n",
      "Epoch: 3529|steps:   60|Train Avg Loss: 0.1948 |Test Loss: 2.4338|lr = 0.00010\n",
      "Epoch: 3530|steps:   30|Train Avg Loss: 0.1922 |Test Loss: 2.4382|lr = 0.00010\n",
      "Epoch: 3530|steps:   60|Train Avg Loss: 0.1850 |Test Loss: 2.3282|lr = 0.00010\n",
      "Epoch: 3531|steps:   30|Train Avg Loss: 0.1627 |Test Loss: 2.3970|lr = 0.00010\n",
      "Epoch: 3531|steps:   60|Train Avg Loss: 0.2065 |Test Loss: 2.3821|lr = 0.00010\n",
      "Epoch: 3532|steps:   30|Train Avg Loss: 0.1783 |Test Loss: 2.3441|lr = 0.00010\n",
      "Epoch: 3532|steps:   60|Train Avg Loss: 0.1805 |Test Loss: 2.4187|lr = 0.00010\n",
      "Epoch: 3533|steps:   30|Train Avg Loss: 0.1731 |Test Loss: 2.3703|lr = 0.00010\n",
      "Epoch: 3533|steps:   60|Train Avg Loss: 0.1655 |Test Loss: 2.3837|lr = 0.00010\n",
      "Epoch: 3534|steps:   30|Train Avg Loss: 0.1839 |Test Loss: 2.3734|lr = 0.00010\n",
      "Epoch: 3534|steps:   60|Train Avg Loss: 0.1766 |Test Loss: 2.3337|lr = 0.00010\n",
      "Epoch: 3535|steps:   30|Train Avg Loss: 0.1731 |Test Loss: 2.3778|lr = 0.00010\n",
      "Epoch: 3535|steps:   60|Train Avg Loss: 0.1894 |Test Loss: 2.3973|lr = 0.00010\n",
      "Epoch: 3536|steps:   30|Train Avg Loss: 0.1697 |Test Loss: 2.3963|lr = 0.00010\n",
      "Epoch: 3536|steps:   60|Train Avg Loss: 0.1870 |Test Loss: 2.4121|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3537|steps:   30|Train Avg Loss: 0.1814 |Test Loss: 2.4433|lr = 0.00010\n",
      "Epoch: 3537|steps:   60|Train Avg Loss: 0.1718 |Test Loss: 2.4215|lr = 0.00010\n",
      "Epoch: 3538|steps:   30|Train Avg Loss: 0.1736 |Test Loss: 2.4677|lr = 0.00010\n",
      "Epoch: 3538|steps:   60|Train Avg Loss: 0.1701 |Test Loss: 2.4581|lr = 0.00010\n",
      "Epoch: 3539|steps:   30|Train Avg Loss: 0.1621 |Test Loss: 2.4097|lr = 0.00010\n",
      "Epoch: 3539|steps:   60|Train Avg Loss: 0.1749 |Test Loss: 2.4343|lr = 0.00010\n",
      "Epoch: 3540|steps:   30|Train Avg Loss: 0.1511 |Test Loss: 2.4118|lr = 0.00010\n",
      "Epoch: 3540|steps:   60|Train Avg Loss: 0.1819 |Test Loss: 2.4638|lr = 0.00010\n",
      "Epoch: 3541|steps:   30|Train Avg Loss: 0.1844 |Test Loss: 2.4153|lr = 0.00010\n",
      "Epoch: 3541|steps:   60|Train Avg Loss: 0.1758 |Test Loss: 2.4533|lr = 0.00010\n",
      "Epoch: 3542|steps:   30|Train Avg Loss: 0.1688 |Test Loss: 2.3979|lr = 0.00010\n",
      "Epoch: 3542|steps:   60|Train Avg Loss: 0.1728 |Test Loss: 2.4322|lr = 0.00010\n",
      "Epoch: 3543|steps:   30|Train Avg Loss: 0.1614 |Test Loss: 2.4438|lr = 0.00010\n",
      "Epoch: 3543|steps:   60|Train Avg Loss: 0.1779 |Test Loss: 2.4788|lr = 0.00010\n",
      "Epoch: 3544|steps:   30|Train Avg Loss: 0.1562 |Test Loss: 2.4118|lr = 0.00010\n",
      "Epoch: 3544|steps:   60|Train Avg Loss: 0.1926 |Test Loss: 2.4767|lr = 0.00010\n",
      "Epoch: 3545|steps:   30|Train Avg Loss: 0.1745 |Test Loss: 2.4896|lr = 0.00010\n",
      "Epoch: 3545|steps:   60|Train Avg Loss: 0.1716 |Test Loss: 2.4424|lr = 0.00010\n",
      "Epoch: 3546|steps:   30|Train Avg Loss: 0.1697 |Test Loss: 2.4532|lr = 0.00010\n",
      "Epoch: 3546|steps:   60|Train Avg Loss: 0.1823 |Test Loss: 2.4872|lr = 0.00010\n",
      "Epoch: 3547|steps:   30|Train Avg Loss: 0.1814 |Test Loss: 2.4694|lr = 0.00010\n",
      "Epoch: 3547|steps:   60|Train Avg Loss: 0.1832 |Test Loss: 2.5460|lr = 0.00010\n",
      "Epoch: 3548|steps:   30|Train Avg Loss: 0.1733 |Test Loss: 2.5197|lr = 0.00010\n",
      "Epoch: 3548|steps:   60|Train Avg Loss: 0.1582 |Test Loss: 2.4839|lr = 0.00010\n",
      "Epoch: 3549|steps:   30|Train Avg Loss: 0.1441 |Test Loss: 2.4672|lr = 0.00010\n",
      "Epoch: 3549|steps:   60|Train Avg Loss: 0.1926 |Test Loss: 2.4534|lr = 0.00010\n",
      "Epoch: 3550|steps:   30|Train Avg Loss: 0.1435 |Test Loss: 2.4645|lr = 0.00010\n",
      "Epoch: 3550|steps:   60|Train Avg Loss: 0.1846 |Test Loss: 2.4728|lr = 0.00010\n",
      "Epoch: 3551|steps:   30|Train Avg Loss: 0.1747 |Test Loss: 2.4937|lr = 0.00010\n",
      "Epoch: 3551|steps:   60|Train Avg Loss: 0.1670 |Test Loss: 2.4752|lr = 0.00010\n",
      "Epoch: 3552|steps:   30|Train Avg Loss: 0.1780 |Test Loss: 2.4619|lr = 0.00010\n",
      "Epoch: 3552|steps:   60|Train Avg Loss: 0.1764 |Test Loss: 2.5395|lr = 0.00010\n",
      "Epoch: 3553|steps:   30|Train Avg Loss: 0.1578 |Test Loss: 2.5726|lr = 0.00010\n",
      "Epoch: 3553|steps:   60|Train Avg Loss: 0.1785 |Test Loss: 2.4837|lr = 0.00010\n",
      "Epoch: 3554|steps:   30|Train Avg Loss: 0.1602 |Test Loss: 2.4403|lr = 0.00010\n",
      "Epoch: 3554|steps:   60|Train Avg Loss: 0.1784 |Test Loss: 2.4727|lr = 0.00010\n",
      "Epoch: 3555|steps:   30|Train Avg Loss: 0.1712 |Test Loss: 2.5344|lr = 0.00010\n",
      "Epoch: 3555|steps:   60|Train Avg Loss: 0.1781 |Test Loss: 2.5221|lr = 0.00010\n",
      "Epoch: 3556|steps:   30|Train Avg Loss: 0.1599 |Test Loss: 2.4944|lr = 0.00010\n",
      "Epoch: 3556|steps:   60|Train Avg Loss: 0.1690 |Test Loss: 2.5288|lr = 0.00010\n",
      "Epoch: 3557|steps:   30|Train Avg Loss: 0.1494 |Test Loss: 2.5408|lr = 0.00010\n",
      "Epoch: 3557|steps:   60|Train Avg Loss: 0.1651 |Test Loss: 2.5443|lr = 0.00010\n",
      "Epoch: 3558|steps:   30|Train Avg Loss: 0.1937 |Test Loss: 2.5417|lr = 0.00010\n",
      "Epoch: 3558|steps:   60|Train Avg Loss: 0.1777 |Test Loss: 2.5202|lr = 0.00010\n",
      "Epoch: 3559|steps:   30|Train Avg Loss: 0.1563 |Test Loss: 2.5466|lr = 0.00010\n",
      "Epoch: 3559|steps:   60|Train Avg Loss: 0.1839 |Test Loss: 2.4757|lr = 0.00010\n",
      "Epoch: 3560|steps:   30|Train Avg Loss: 0.1550 |Test Loss: 2.5600|lr = 0.00010\n",
      "Epoch: 3560|steps:   60|Train Avg Loss: 0.1755 |Test Loss: 2.6166|lr = 0.00010\n",
      "Epoch: 3561|steps:   30|Train Avg Loss: 0.1672 |Test Loss: 2.5418|lr = 0.00010\n",
      "Epoch: 3561|steps:   60|Train Avg Loss: 0.1813 |Test Loss: 2.4504|lr = 0.00010\n",
      "Epoch: 3562|steps:   30|Train Avg Loss: 0.1589 |Test Loss: 2.5835|lr = 0.00010\n",
      "Epoch: 3562|steps:   60|Train Avg Loss: 0.1791 |Test Loss: 2.5418|lr = 0.00010\n",
      "Epoch: 3563|steps:   30|Train Avg Loss: 0.1606 |Test Loss: 2.5504|lr = 0.00010\n",
      "Epoch: 3563|steps:   60|Train Avg Loss: 0.1597 |Test Loss: 2.5647|lr = 0.00010\n",
      "Epoch: 3564|steps:   30|Train Avg Loss: 0.1536 |Test Loss: 2.5342|lr = 0.00010\n",
      "Epoch: 3564|steps:   60|Train Avg Loss: 0.1542 |Test Loss: 2.5146|lr = 0.00010\n",
      "Epoch: 3565|steps:   30|Train Avg Loss: 0.1658 |Test Loss: 2.4957|lr = 0.00010\n",
      "Epoch: 3565|steps:   60|Train Avg Loss: 0.1589 |Test Loss: 2.5630|lr = 0.00010\n",
      "Epoch: 3566|steps:   30|Train Avg Loss: 0.1537 |Test Loss: 2.5651|lr = 0.00010\n",
      "Epoch: 3566|steps:   60|Train Avg Loss: 0.1578 |Test Loss: 2.5802|lr = 0.00010\n",
      "Epoch: 3567|steps:   30|Train Avg Loss: 0.1533 |Test Loss: 2.5304|lr = 0.00010\n",
      "Epoch: 3567|steps:   60|Train Avg Loss: 0.1524 |Test Loss: 2.6387|lr = 0.00010\n",
      "Epoch: 3568|steps:   30|Train Avg Loss: 0.1466 |Test Loss: 2.6679|lr = 0.00010\n",
      "Epoch: 3568|steps:   60|Train Avg Loss: 0.1669 |Test Loss: 2.6058|lr = 0.00010\n",
      "Epoch: 3569|steps:   30|Train Avg Loss: 0.1988 |Test Loss: 2.5780|lr = 0.00010\n",
      "Epoch: 3569|steps:   60|Train Avg Loss: 0.1766 |Test Loss: 2.6572|lr = 0.00010\n",
      "Epoch: 3570|steps:   30|Train Avg Loss: 0.1483 |Test Loss: 2.6372|lr = 0.00010\n",
      "Epoch: 3570|steps:   60|Train Avg Loss: 0.1840 |Test Loss: 2.6666|lr = 0.00010\n",
      "Epoch: 3571|steps:   30|Train Avg Loss: 0.1493 |Test Loss: 2.6294|lr = 0.00010\n",
      "Epoch: 3571|steps:   60|Train Avg Loss: 0.1536 |Test Loss: 2.5854|lr = 0.00010\n",
      "Epoch: 3572|steps:   30|Train Avg Loss: 0.1556 |Test Loss: 2.6134|lr = 0.00010\n",
      "Epoch: 3572|steps:   60|Train Avg Loss: 0.1570 |Test Loss: 2.5907|lr = 0.00010\n",
      "Epoch: 3573|steps:   30|Train Avg Loss: 0.1585 |Test Loss: 2.6092|lr = 0.00010\n",
      "Epoch: 3573|steps:   60|Train Avg Loss: 0.1438 |Test Loss: 2.6167|lr = 0.00010\n",
      "Epoch: 3574|steps:   30|Train Avg Loss: 0.1446 |Test Loss: 2.5509|lr = 0.00010\n",
      "Epoch: 3574|steps:   60|Train Avg Loss: 0.1517 |Test Loss: 2.6102|lr = 0.00010\n",
      "Epoch: 3575|steps:   30|Train Avg Loss: 0.1391 |Test Loss: 2.5395|lr = 0.00010\n",
      "Epoch: 3575|steps:   60|Train Avg Loss: 0.1565 |Test Loss: 2.6639|lr = 0.00010\n",
      "Epoch: 3576|steps:   30|Train Avg Loss: 0.1607 |Test Loss: 2.5838|lr = 0.00010\n",
      "Epoch: 3576|steps:   60|Train Avg Loss: 0.1491 |Test Loss: 2.6601|lr = 0.00010\n",
      "Epoch: 3577|steps:   30|Train Avg Loss: 0.1498 |Test Loss: 2.6282|lr = 0.00010\n",
      "Epoch: 3577|steps:   60|Train Avg Loss: 0.1457 |Test Loss: 2.6112|lr = 0.00010\n",
      "Epoch: 3578|steps:   30|Train Avg Loss: 0.1454 |Test Loss: 2.5410|lr = 0.00010\n",
      "Epoch: 3578|steps:   60|Train Avg Loss: 0.1497 |Test Loss: 2.6252|lr = 0.00010\n",
      "Epoch: 3579|steps:   30|Train Avg Loss: 0.1324 |Test Loss: 2.7025|lr = 0.00010\n",
      "Epoch: 3579|steps:   60|Train Avg Loss: 0.1524 |Test Loss: 2.6172|lr = 0.00010\n",
      "Epoch: 3580|steps:   30|Train Avg Loss: 0.1352 |Test Loss: 2.6103|lr = 0.00010\n",
      "Epoch: 3580|steps:   60|Train Avg Loss: 0.1576 |Test Loss: 2.6338|lr = 0.00010\n",
      "Epoch: 3581|steps:   30|Train Avg Loss: 0.1584 |Test Loss: 2.6403|lr = 0.00010\n",
      "Epoch: 3581|steps:   60|Train Avg Loss: 0.1682 |Test Loss: 2.7092|lr = 0.00010\n",
      "Epoch: 3582|steps:   30|Train Avg Loss: 0.1764 |Test Loss: 2.6029|lr = 0.00010\n",
      "Epoch: 3582|steps:   60|Train Avg Loss: 0.1784 |Test Loss: 2.6784|lr = 0.00010\n",
      "Epoch: 3583|steps:   30|Train Avg Loss: 0.1673 |Test Loss: 2.6804|lr = 0.00010\n",
      "Epoch: 3583|steps:   60|Train Avg Loss: 0.1516 |Test Loss: 2.6855|lr = 0.00010\n",
      "Epoch: 3584|steps:   30|Train Avg Loss: 0.1319 |Test Loss: 2.6810|lr = 0.00010\n",
      "Epoch: 3584|steps:   60|Train Avg Loss: 0.1604 |Test Loss: 2.7224|lr = 0.00010\n",
      "Epoch: 3585|steps:   30|Train Avg Loss: 0.1379 |Test Loss: 2.6883|lr = 0.00010\n",
      "Epoch: 3585|steps:   60|Train Avg Loss: 0.1480 |Test Loss: 2.6851|lr = 0.00010\n",
      "Epoch: 3586|steps:   30|Train Avg Loss: 0.1492 |Test Loss: 2.6417|lr = 0.00010\n",
      "Epoch: 3586|steps:   60|Train Avg Loss: 0.1315 |Test Loss: 2.6424|lr = 0.00010\n",
      "Epoch: 3587|steps:   30|Train Avg Loss: 0.1436 |Test Loss: 2.6891|lr = 0.00010\n",
      "Epoch: 3587|steps:   60|Train Avg Loss: 0.1303 |Test Loss: 2.6108|lr = 0.00010\n",
      "Epoch: 3588|steps:   30|Train Avg Loss: 0.1416 |Test Loss: 2.6122|lr = 0.00010\n",
      "Epoch: 3588|steps:   60|Train Avg Loss: 0.1457 |Test Loss: 2.6796|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3589|steps:   30|Train Avg Loss: 0.1324 |Test Loss: 2.5805|lr = 0.00010\n",
      "Epoch: 3589|steps:   60|Train Avg Loss: 0.1430 |Test Loss: 2.7205|lr = 0.00010\n",
      "Epoch: 3590|steps:   30|Train Avg Loss: 0.1377 |Test Loss: 2.6700|lr = 0.00010\n",
      "Epoch: 3590|steps:   60|Train Avg Loss: 0.1445 |Test Loss: 2.6799|lr = 0.00010\n",
      "Epoch: 3591|steps:   30|Train Avg Loss: 0.1281 |Test Loss: 2.6441|lr = 0.00010\n",
      "Epoch: 3591|steps:   60|Train Avg Loss: 0.1393 |Test Loss: 2.6932|lr = 0.00010\n",
      "Epoch: 3592|steps:   30|Train Avg Loss: 0.1304 |Test Loss: 2.7147|lr = 0.00010\n",
      "Epoch: 3592|steps:   60|Train Avg Loss: 0.1568 |Test Loss: 2.7458|lr = 0.00010\n",
      "Epoch: 3593|steps:   30|Train Avg Loss: 0.1442 |Test Loss: 2.6973|lr = 0.00010\n",
      "Epoch: 3593|steps:   60|Train Avg Loss: 0.1443 |Test Loss: 2.7057|lr = 0.00010\n",
      "Epoch: 3594|steps:   30|Train Avg Loss: 0.1371 |Test Loss: 2.6856|lr = 0.00010\n",
      "Epoch: 3594|steps:   60|Train Avg Loss: 0.1345 |Test Loss: 2.7529|lr = 0.00010\n",
      "Epoch: 3595|steps:   30|Train Avg Loss: 0.1354 |Test Loss: 2.7116|lr = 0.00010\n",
      "Epoch: 3595|steps:   60|Train Avg Loss: 0.1474 |Test Loss: 2.7880|lr = 0.00010\n",
      "Epoch: 3596|steps:   30|Train Avg Loss: 0.1488 |Test Loss: 2.7084|lr = 0.00010\n",
      "Epoch: 3596|steps:   60|Train Avg Loss: 0.1424 |Test Loss: 2.7376|lr = 0.00010\n",
      "Epoch: 3597|steps:   30|Train Avg Loss: 0.1426 |Test Loss: 2.6966|lr = 0.00010\n",
      "Epoch: 3597|steps:   60|Train Avg Loss: 0.1636 |Test Loss: 2.7289|lr = 0.00010\n",
      "Epoch: 3598|steps:   30|Train Avg Loss: 0.1360 |Test Loss: 2.7328|lr = 0.00010\n",
      "Epoch: 3598|steps:   60|Train Avg Loss: 0.1393 |Test Loss: 2.7179|lr = 0.00010\n",
      "Epoch: 3599|steps:   30|Train Avg Loss: 0.1363 |Test Loss: 2.7084|lr = 0.00010\n",
      "Epoch: 3599|steps:   60|Train Avg Loss: 0.1402 |Test Loss: 2.7365|lr = 0.00010\n",
      "Epoch: 3600|steps:   30|Train Avg Loss: 0.1322 |Test Loss: 2.7214|lr = 0.00010\n",
      "Epoch: 3600|steps:   60|Train Avg Loss: 0.1401 |Test Loss: 2.8053|lr = 0.00010\n",
      "Epoch: 3601|steps:   30|Train Avg Loss: 0.1270 |Test Loss: 2.7086|lr = 0.00010\n",
      "Epoch: 3601|steps:   60|Train Avg Loss: 0.1468 |Test Loss: 2.6948|lr = 0.00010\n",
      "Epoch: 3602|steps:   30|Train Avg Loss: 0.1270 |Test Loss: 2.8435|lr = 0.00010\n",
      "Epoch: 3602|steps:   60|Train Avg Loss: 0.1359 |Test Loss: 2.7462|lr = 0.00010\n",
      "Epoch: 3603|steps:   30|Train Avg Loss: 0.1273 |Test Loss: 2.7298|lr = 0.00010\n",
      "Epoch: 3603|steps:   60|Train Avg Loss: 0.1384 |Test Loss: 2.7261|lr = 0.00010\n",
      "Epoch: 3604|steps:   30|Train Avg Loss: 0.1210 |Test Loss: 2.8126|lr = 0.00010\n",
      "Epoch: 3604|steps:   60|Train Avg Loss: 0.1511 |Test Loss: 2.7697|lr = 0.00010\n",
      "Epoch: 3605|steps:   30|Train Avg Loss: 0.1558 |Test Loss: 2.7596|lr = 0.00010\n",
      "Epoch: 3605|steps:   60|Train Avg Loss: 0.1440 |Test Loss: 2.8256|lr = 0.00010\n",
      "Epoch: 3606|steps:   30|Train Avg Loss: 0.1338 |Test Loss: 2.7692|lr = 0.00010\n",
      "Epoch: 3606|steps:   60|Train Avg Loss: 0.1403 |Test Loss: 2.8614|lr = 0.00010\n",
      "Epoch: 3607|steps:   30|Train Avg Loss: 0.1187 |Test Loss: 2.7828|lr = 0.00010\n",
      "Epoch: 3607|steps:   60|Train Avg Loss: 0.1487 |Test Loss: 2.8998|lr = 0.00010\n",
      "Epoch: 3608|steps:   30|Train Avg Loss: 0.1422 |Test Loss: 2.7228|lr = 0.00010\n",
      "Epoch: 3608|steps:   60|Train Avg Loss: 0.1418 |Test Loss: 2.8430|lr = 0.00010\n",
      "Epoch: 3609|steps:   30|Train Avg Loss: 0.1132 |Test Loss: 2.8642|lr = 0.00010\n",
      "Epoch: 3609|steps:   60|Train Avg Loss: 0.1477 |Test Loss: 2.8649|lr = 0.00010\n",
      "Epoch: 3610|steps:   30|Train Avg Loss: 0.1360 |Test Loss: 2.7991|lr = 0.00010\n",
      "Epoch: 3610|steps:   60|Train Avg Loss: 0.1285 |Test Loss: 2.8875|lr = 0.00010\n",
      "Epoch: 3611|steps:   30|Train Avg Loss: 0.1344 |Test Loss: 2.8714|lr = 0.00010\n",
      "Epoch: 3611|steps:   60|Train Avg Loss: 0.1418 |Test Loss: 2.7640|lr = 0.00010\n",
      "Epoch: 3612|steps:   30|Train Avg Loss: 0.1138 |Test Loss: 2.7921|lr = 0.00010\n",
      "Epoch: 3612|steps:   60|Train Avg Loss: 0.1387 |Test Loss: 2.8298|lr = 0.00010\n",
      "Epoch: 3613|steps:   30|Train Avg Loss: 0.1280 |Test Loss: 2.8569|lr = 0.00010\n",
      "Epoch: 3613|steps:   60|Train Avg Loss: 0.1316 |Test Loss: 2.8094|lr = 0.00010\n",
      "Epoch: 3614|steps:   30|Train Avg Loss: 0.1282 |Test Loss: 2.8377|lr = 0.00010\n",
      "Epoch: 3614|steps:   60|Train Avg Loss: 0.1339 |Test Loss: 2.8115|lr = 0.00010\n",
      "Epoch: 3615|steps:   30|Train Avg Loss: 0.1548 |Test Loss: 2.8311|lr = 0.00010\n",
      "Epoch: 3615|steps:   60|Train Avg Loss: 0.1594 |Test Loss: 2.8870|lr = 0.00010\n",
      "Epoch: 3616|steps:   30|Train Avg Loss: 0.1403 |Test Loss: 2.8115|lr = 0.00010\n",
      "Epoch: 3616|steps:   60|Train Avg Loss: 0.1709 |Test Loss: 2.7686|lr = 0.00010\n",
      "Epoch: 3617|steps:   30|Train Avg Loss: 0.1290 |Test Loss: 2.9060|lr = 0.00010\n",
      "Epoch: 3617|steps:   60|Train Avg Loss: 0.1500 |Test Loss: 2.8845|lr = 0.00010\n",
      "Epoch: 3618|steps:   30|Train Avg Loss: 0.1243 |Test Loss: 2.9194|lr = 0.00010\n",
      "Epoch: 3618|steps:   60|Train Avg Loss: 0.1478 |Test Loss: 2.8140|lr = 0.00010\n",
      "Epoch: 3619|steps:   30|Train Avg Loss: 0.1220 |Test Loss: 2.8499|lr = 0.00010\n",
      "Epoch: 3619|steps:   60|Train Avg Loss: 0.1366 |Test Loss: 2.7852|lr = 0.00010\n",
      "Epoch: 3620|steps:   30|Train Avg Loss: 0.1151 |Test Loss: 2.9389|lr = 0.00010\n",
      "Epoch: 3620|steps:   60|Train Avg Loss: 0.1199 |Test Loss: 2.8719|lr = 0.00010\n",
      "Epoch: 3621|steps:   30|Train Avg Loss: 0.1088 |Test Loss: 2.7942|lr = 0.00010\n",
      "Epoch: 3621|steps:   60|Train Avg Loss: 0.1408 |Test Loss: 2.8624|lr = 0.00010\n",
      "Epoch: 3622|steps:   30|Train Avg Loss: 0.1284 |Test Loss: 2.8269|lr = 0.00010\n",
      "Epoch: 3622|steps:   60|Train Avg Loss: 0.1268 |Test Loss: 2.8560|lr = 0.00010\n",
      "Epoch: 3623|steps:   30|Train Avg Loss: 0.1246 |Test Loss: 2.8614|lr = 0.00010\n",
      "Epoch: 3623|steps:   60|Train Avg Loss: 0.1316 |Test Loss: 3.0235|lr = 0.00010\n",
      "Epoch: 3624|steps:   30|Train Avg Loss: 0.1227 |Test Loss: 2.8599|lr = 0.00010\n",
      "Epoch: 3624|steps:   60|Train Avg Loss: 0.1197 |Test Loss: 2.8114|lr = 0.00010\n",
      "Epoch: 3625|steps:   30|Train Avg Loss: 0.1149 |Test Loss: 2.8585|lr = 0.00010\n",
      "Epoch: 3625|steps:   60|Train Avg Loss: 0.1107 |Test Loss: 2.9021|lr = 0.00010\n",
      "Epoch: 3626|steps:   30|Train Avg Loss: 0.1230 |Test Loss: 2.8923|lr = 0.00010\n",
      "Epoch: 3626|steps:   60|Train Avg Loss: 0.1130 |Test Loss: 2.8644|lr = 0.00010\n",
      "Epoch: 3627|steps:   30|Train Avg Loss: 0.1159 |Test Loss: 2.9101|lr = 0.00010\n",
      "Epoch: 3627|steps:   60|Train Avg Loss: 0.1287 |Test Loss: 2.8801|lr = 0.00010\n",
      "Epoch: 3628|steps:   30|Train Avg Loss: 0.1227 |Test Loss: 2.9300|lr = 0.00010\n",
      "Epoch: 3628|steps:   60|Train Avg Loss: 0.1104 |Test Loss: 2.8939|lr = 0.00010\n",
      "Epoch: 3629|steps:   30|Train Avg Loss: 0.1142 |Test Loss: 2.8274|lr = 0.00010\n",
      "Epoch: 3629|steps:   60|Train Avg Loss: 0.1497 |Test Loss: 2.8992|lr = 0.00010\n",
      "Epoch: 3630|steps:   30|Train Avg Loss: 0.1653 |Test Loss: 2.8607|lr = 0.00010\n",
      "Epoch: 3630|steps:   60|Train Avg Loss: 0.1513 |Test Loss: 2.9647|lr = 0.00010\n",
      "Epoch: 3631|steps:   30|Train Avg Loss: 0.1089 |Test Loss: 2.9126|lr = 0.00010\n",
      "Epoch: 3631|steps:   60|Train Avg Loss: 0.1333 |Test Loss: 2.9645|lr = 0.00010\n",
      "Epoch: 3632|steps:   30|Train Avg Loss: 0.1243 |Test Loss: 2.9764|lr = 0.00010\n",
      "Epoch: 3632|steps:   60|Train Avg Loss: 0.1619 |Test Loss: 3.0081|lr = 0.00010\n",
      "Epoch: 3633|steps:   30|Train Avg Loss: 0.1326 |Test Loss: 2.8718|lr = 0.00010\n",
      "Epoch: 3633|steps:   60|Train Avg Loss: 0.1153 |Test Loss: 2.8073|lr = 0.00010\n",
      "Epoch: 3634|steps:   30|Train Avg Loss: 0.1289 |Test Loss: 2.9585|lr = 0.00010\n",
      "Epoch: 3634|steps:   60|Train Avg Loss: 0.1259 |Test Loss: 3.0237|lr = 0.00010\n",
      "Epoch: 3635|steps:   30|Train Avg Loss: 0.1177 |Test Loss: 2.9493|lr = 0.00010\n",
      "Epoch: 3635|steps:   60|Train Avg Loss: 0.1235 |Test Loss: 2.8961|lr = 0.00010\n",
      "Epoch: 3636|steps:   30|Train Avg Loss: 0.1399 |Test Loss: 2.9868|lr = 0.00010\n",
      "Epoch: 3636|steps:   60|Train Avg Loss: 0.1708 |Test Loss: 3.0561|lr = 0.00010\n",
      "Epoch: 3637|steps:   30|Train Avg Loss: 0.1243 |Test Loss: 2.9682|lr = 0.00010\n",
      "Epoch: 3637|steps:   60|Train Avg Loss: 0.1359 |Test Loss: 2.9288|lr = 0.00010\n",
      "Epoch: 3638|steps:   30|Train Avg Loss: 0.1167 |Test Loss: 2.9067|lr = 0.00010\n",
      "Epoch: 3638|steps:   60|Train Avg Loss: 0.1107 |Test Loss: 2.9634|lr = 0.00010\n",
      "Epoch: 3639|steps:   30|Train Avg Loss: 0.1170 |Test Loss: 2.9881|lr = 0.00010\n",
      "Epoch: 3639|steps:   60|Train Avg Loss: 0.1047 |Test Loss: 2.9925|lr = 0.00010\n",
      "Epoch: 3640|steps:   30|Train Avg Loss: 0.1052 |Test Loss: 2.9161|lr = 0.00010\n",
      "Epoch: 3640|steps:   60|Train Avg Loss: 0.1288 |Test Loss: 2.9925|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3641|steps:   30|Train Avg Loss: 0.1037 |Test Loss: 3.0418|lr = 0.00010\n",
      "Epoch: 3641|steps:   60|Train Avg Loss: 0.1209 |Test Loss: 2.9343|lr = 0.00010\n",
      "Epoch: 3642|steps:   30|Train Avg Loss: 0.1109 |Test Loss: 2.8833|lr = 0.00010\n",
      "Epoch: 3642|steps:   60|Train Avg Loss: 0.1090 |Test Loss: 3.0276|lr = 0.00010\n",
      "Epoch: 3643|steps:   30|Train Avg Loss: 0.1179 |Test Loss: 2.9733|lr = 0.00010\n",
      "Epoch: 3643|steps:   60|Train Avg Loss: 0.1124 |Test Loss: 2.9688|lr = 0.00010\n",
      "Epoch: 3644|steps:   30|Train Avg Loss: 0.1068 |Test Loss: 3.0176|lr = 0.00010\n",
      "Epoch: 3644|steps:   60|Train Avg Loss: 0.1135 |Test Loss: 2.9578|lr = 0.00010\n",
      "Epoch: 3645|steps:   30|Train Avg Loss: 0.1131 |Test Loss: 2.9516|lr = 0.00010\n",
      "Epoch: 3645|steps:   60|Train Avg Loss: 0.1072 |Test Loss: 3.0120|lr = 0.00010\n",
      "Epoch: 3646|steps:   30|Train Avg Loss: 0.1077 |Test Loss: 3.0259|lr = 0.00010\n",
      "Epoch: 3646|steps:   60|Train Avg Loss: 0.1193 |Test Loss: 3.0048|lr = 0.00010\n",
      "Epoch: 3647|steps:   30|Train Avg Loss: 0.1040 |Test Loss: 2.9696|lr = 0.00010\n",
      "Epoch: 3647|steps:   60|Train Avg Loss: 0.1264 |Test Loss: 2.9970|lr = 0.00010\n",
      "Epoch: 3648|steps:   30|Train Avg Loss: 0.1194 |Test Loss: 2.9971|lr = 0.00010\n",
      "Epoch: 3648|steps:   60|Train Avg Loss: 0.1350 |Test Loss: 3.0382|lr = 0.00010\n",
      "Epoch: 3649|steps:   30|Train Avg Loss: 0.1211 |Test Loss: 3.0520|lr = 0.00010\n",
      "Epoch: 3649|steps:   60|Train Avg Loss: 0.1554 |Test Loss: 3.0444|lr = 0.00010\n",
      "Epoch: 3650|steps:   30|Train Avg Loss: 0.1536 |Test Loss: 3.0344|lr = 0.00010\n",
      "Epoch: 3650|steps:   60|Train Avg Loss: 0.1342 |Test Loss: 3.1215|lr = 0.00010\n",
      "Epoch: 3651|steps:   30|Train Avg Loss: 0.1033 |Test Loss: 3.0304|lr = 0.00010\n",
      "Epoch: 3651|steps:   60|Train Avg Loss: 0.1202 |Test Loss: 3.0117|lr = 0.00010\n",
      "Epoch: 3652|steps:   30|Train Avg Loss: 0.1077 |Test Loss: 3.0568|lr = 0.00010\n",
      "Epoch: 3652|steps:   60|Train Avg Loss: 0.1119 |Test Loss: 2.9723|lr = 0.00010\n",
      "Epoch: 3653|steps:   30|Train Avg Loss: 0.1079 |Test Loss: 3.0688|lr = 0.00010\n",
      "Epoch: 3653|steps:   60|Train Avg Loss: 0.1274 |Test Loss: 3.0276|lr = 0.00010\n",
      "Epoch: 3654|steps:   30|Train Avg Loss: 0.1142 |Test Loss: 3.0569|lr = 0.00010\n",
      "Epoch: 3654|steps:   60|Train Avg Loss: 0.1133 |Test Loss: 2.9982|lr = 0.00010\n",
      "Epoch: 3655|steps:   30|Train Avg Loss: 0.1089 |Test Loss: 3.0432|lr = 0.00010\n",
      "Epoch: 3655|steps:   60|Train Avg Loss: 0.1125 |Test Loss: 3.0376|lr = 0.00010\n",
      "Epoch: 3656|steps:   30|Train Avg Loss: 0.1308 |Test Loss: 3.0233|lr = 0.00010\n",
      "Epoch: 3656|steps:   60|Train Avg Loss: 0.1190 |Test Loss: 3.1341|lr = 0.00010\n",
      "Epoch: 3657|steps:   30|Train Avg Loss: 0.1035 |Test Loss: 3.0856|lr = 0.00010\n",
      "Epoch: 3657|steps:   60|Train Avg Loss: 0.1115 |Test Loss: 3.0581|lr = 0.00010\n",
      "Epoch: 3658|steps:   30|Train Avg Loss: 0.0980 |Test Loss: 3.1416|lr = 0.00010\n",
      "Epoch: 3658|steps:   60|Train Avg Loss: 0.1021 |Test Loss: 3.0715|lr = 0.00010\n",
      "Epoch: 3659|steps:   30|Train Avg Loss: 0.0945 |Test Loss: 3.1033|lr = 0.00010\n",
      "Epoch: 3659|steps:   60|Train Avg Loss: 0.1198 |Test Loss: 3.1162|lr = 0.00010\n",
      "Epoch: 3660|steps:   30|Train Avg Loss: 0.1178 |Test Loss: 3.0543|lr = 0.00010\n",
      "Epoch: 3660|steps:   60|Train Avg Loss: 0.1032 |Test Loss: 3.0749|lr = 0.00010\n",
      "Epoch: 3661|steps:   30|Train Avg Loss: 0.1590 |Test Loss: 2.9988|lr = 0.00010\n",
      "Epoch: 3661|steps:   60|Train Avg Loss: 0.1396 |Test Loss: 2.9357|lr = 0.00010\n",
      "Epoch: 3662|steps:   30|Train Avg Loss: 0.1307 |Test Loss: 3.0486|lr = 0.00010\n",
      "Epoch: 3662|steps:   60|Train Avg Loss: 0.1534 |Test Loss: 3.1221|lr = 0.00010\n",
      "Epoch: 3663|steps:   30|Train Avg Loss: 0.1066 |Test Loss: 3.1058|lr = 0.00010\n",
      "Epoch: 3663|steps:   60|Train Avg Loss: 0.1197 |Test Loss: 3.0516|lr = 0.00010\n",
      "Epoch: 3664|steps:   30|Train Avg Loss: 0.0979 |Test Loss: 3.0156|lr = 0.00010\n",
      "Epoch: 3664|steps:   60|Train Avg Loss: 0.1067 |Test Loss: 3.0817|lr = 0.00010\n",
      "Epoch: 3665|steps:   30|Train Avg Loss: 0.1006 |Test Loss: 3.0911|lr = 0.00010\n",
      "Epoch: 3665|steps:   60|Train Avg Loss: 0.1075 |Test Loss: 3.0665|lr = 0.00010\n",
      "Epoch: 3666|steps:   30|Train Avg Loss: 0.1007 |Test Loss: 3.1089|lr = 0.00010\n",
      "Epoch: 3666|steps:   60|Train Avg Loss: 0.1090 |Test Loss: 3.0200|lr = 0.00010\n",
      "Epoch: 3667|steps:   30|Train Avg Loss: 0.1064 |Test Loss: 3.1254|lr = 0.00010\n",
      "Epoch: 3667|steps:   60|Train Avg Loss: 0.1076 |Test Loss: 3.0788|lr = 0.00010\n",
      "Epoch: 3668|steps:   30|Train Avg Loss: 0.0879 |Test Loss: 3.1185|lr = 0.00010\n",
      "Epoch: 3668|steps:   60|Train Avg Loss: 0.1173 |Test Loss: 3.0516|lr = 0.00010\n",
      "Epoch: 3669|steps:   30|Train Avg Loss: 0.0918 |Test Loss: 3.0886|lr = 0.00010\n",
      "Epoch: 3669|steps:   60|Train Avg Loss: 0.1016 |Test Loss: 3.0731|lr = 0.00010\n",
      "Epoch: 3670|steps:   30|Train Avg Loss: 0.0970 |Test Loss: 3.0639|lr = 0.00010\n",
      "Epoch: 3670|steps:   60|Train Avg Loss: 0.0977 |Test Loss: 3.0810|lr = 0.00010\n",
      "Epoch: 3671|steps:   30|Train Avg Loss: 0.0951 |Test Loss: 3.1120|lr = 0.00010\n",
      "Epoch: 3671|steps:   60|Train Avg Loss: 0.1013 |Test Loss: 3.1660|lr = 0.00010\n",
      "Epoch: 3672|steps:   30|Train Avg Loss: 0.0933 |Test Loss: 3.1332|lr = 0.00010\n",
      "Epoch: 3672|steps:   60|Train Avg Loss: 0.0923 |Test Loss: 3.1376|lr = 0.00010\n",
      "Epoch: 3673|steps:   30|Train Avg Loss: 0.0905 |Test Loss: 3.1146|lr = 0.00010\n",
      "Epoch: 3673|steps:   60|Train Avg Loss: 0.1060 |Test Loss: 3.2232|lr = 0.00010\n",
      "Epoch: 3674|steps:   30|Train Avg Loss: 0.1126 |Test Loss: 3.2287|lr = 0.00010\n",
      "Epoch: 3674|steps:   60|Train Avg Loss: 0.1111 |Test Loss: 3.0694|lr = 0.00010\n",
      "Epoch: 3675|steps:   30|Train Avg Loss: 0.1256 |Test Loss: 3.1425|lr = 0.00010\n",
      "Epoch: 3675|steps:   60|Train Avg Loss: 0.1298 |Test Loss: 3.2579|lr = 0.00010\n",
      "Epoch: 3676|steps:   30|Train Avg Loss: 0.1280 |Test Loss: 3.3877|lr = 0.00010\n",
      "Epoch: 3676|steps:   60|Train Avg Loss: 0.1362 |Test Loss: 3.2464|lr = 0.00010\n",
      "Epoch: 3677|steps:   30|Train Avg Loss: 0.1395 |Test Loss: 3.2454|lr = 0.00010\n",
      "Epoch: 3677|steps:   60|Train Avg Loss: 0.1412 |Test Loss: 3.1928|lr = 0.00010\n",
      "Epoch: 3678|steps:   30|Train Avg Loss: 0.1077 |Test Loss: 3.2012|lr = 0.00010\n",
      "Epoch: 3678|steps:   60|Train Avg Loss: 0.1099 |Test Loss: 3.1103|lr = 0.00010\n",
      "Epoch: 3679|steps:   30|Train Avg Loss: 0.0836 |Test Loss: 3.1207|lr = 0.00010\n",
      "Epoch: 3679|steps:   60|Train Avg Loss: 0.0993 |Test Loss: 3.1637|lr = 0.00010\n",
      "Epoch: 3680|steps:   30|Train Avg Loss: 0.0888 |Test Loss: 3.1520|lr = 0.00010\n",
      "Epoch: 3680|steps:   60|Train Avg Loss: 0.0996 |Test Loss: 3.1999|lr = 0.00010\n",
      "Epoch: 3681|steps:   30|Train Avg Loss: 0.0889 |Test Loss: 3.1965|lr = 0.00010\n",
      "Epoch: 3681|steps:   60|Train Avg Loss: 0.0947 |Test Loss: 3.1925|lr = 0.00010\n",
      "Epoch: 3682|steps:   30|Train Avg Loss: 0.0844 |Test Loss: 3.1513|lr = 0.00010\n",
      "Epoch: 3682|steps:   60|Train Avg Loss: 0.0977 |Test Loss: 3.2365|lr = 0.00010\n",
      "Epoch: 3683|steps:   30|Train Avg Loss: 0.0901 |Test Loss: 3.1944|lr = 0.00010\n",
      "Epoch: 3683|steps:   60|Train Avg Loss: 0.1110 |Test Loss: 3.2142|lr = 0.00010\n",
      "Epoch: 3684|steps:   30|Train Avg Loss: 0.1078 |Test Loss: 3.1806|lr = 0.00010\n",
      "Epoch: 3684|steps:   60|Train Avg Loss: 0.1011 |Test Loss: 3.2777|lr = 0.00010\n",
      "Epoch: 3685|steps:   30|Train Avg Loss: 0.1078 |Test Loss: 3.3025|lr = 0.00010\n",
      "Epoch: 3685|steps:   60|Train Avg Loss: 0.1187 |Test Loss: 3.1760|lr = 0.00010\n",
      "Epoch: 3686|steps:   30|Train Avg Loss: 0.1049 |Test Loss: 3.1998|lr = 0.00010\n",
      "Epoch: 3686|steps:   60|Train Avg Loss: 0.1291 |Test Loss: 3.1762|lr = 0.00010\n",
      "Epoch: 3687|steps:   30|Train Avg Loss: 0.0911 |Test Loss: 3.1814|lr = 0.00010\n",
      "Epoch: 3687|steps:   60|Train Avg Loss: 0.1022 |Test Loss: 3.2020|lr = 0.00010\n",
      "Epoch: 3688|steps:   30|Train Avg Loss: 0.0953 |Test Loss: 3.2391|lr = 0.00010\n",
      "Epoch: 3688|steps:   60|Train Avg Loss: 0.0860 |Test Loss: 3.1877|lr = 0.00010\n",
      "Epoch: 3689|steps:   30|Train Avg Loss: 0.0917 |Test Loss: 3.2345|lr = 0.00010\n",
      "Epoch: 3689|steps:   60|Train Avg Loss: 0.0878 |Test Loss: 3.1774|lr = 0.00010\n",
      "Epoch: 3690|steps:   30|Train Avg Loss: 0.1124 |Test Loss: 3.1233|lr = 0.00010\n",
      "Epoch: 3690|steps:   60|Train Avg Loss: 0.0990 |Test Loss: 3.1673|lr = 0.00010\n",
      "Epoch: 3691|steps:   30|Train Avg Loss: 0.0789 |Test Loss: 3.1579|lr = 0.00010\n",
      "Epoch: 3691|steps:   60|Train Avg Loss: 0.0923 |Test Loss: 3.2007|lr = 0.00010\n",
      "Epoch: 3692|steps:   30|Train Avg Loss: 0.0844 |Test Loss: 3.2016|lr = 0.00010\n",
      "Epoch: 3692|steps:   60|Train Avg Loss: 0.0971 |Test Loss: 3.1979|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3693|steps:   30|Train Avg Loss: 0.0941 |Test Loss: 3.2311|lr = 0.00010\n",
      "Epoch: 3693|steps:   60|Train Avg Loss: 0.0986 |Test Loss: 3.2095|lr = 0.00010\n",
      "Epoch: 3694|steps:   30|Train Avg Loss: 0.0951 |Test Loss: 3.2201|lr = 0.00010\n",
      "Epoch: 3694|steps:   60|Train Avg Loss: 0.1035 |Test Loss: 3.2420|lr = 0.00010\n",
      "Epoch: 3695|steps:   30|Train Avg Loss: 0.0784 |Test Loss: 3.1890|lr = 0.00010\n",
      "Epoch: 3695|steps:   60|Train Avg Loss: 0.0997 |Test Loss: 3.2868|lr = 0.00010\n",
      "Epoch: 3696|steps:   30|Train Avg Loss: 0.0864 |Test Loss: 3.2400|lr = 0.00010\n",
      "Epoch: 3696|steps:   60|Train Avg Loss: 0.0990 |Test Loss: 3.2607|lr = 0.00010\n",
      "Epoch: 3697|steps:   30|Train Avg Loss: 0.1009 |Test Loss: 3.2445|lr = 0.00010\n",
      "Epoch: 3697|steps:   60|Train Avg Loss: 0.1562 |Test Loss: 3.2321|lr = 0.00010\n",
      "Epoch: 3698|steps:   30|Train Avg Loss: 0.1025 |Test Loss: 3.3007|lr = 0.00010\n",
      "Epoch: 3698|steps:   60|Train Avg Loss: 0.1157 |Test Loss: 3.3550|lr = 0.00010\n",
      "Epoch: 3699|steps:   30|Train Avg Loss: 0.0874 |Test Loss: 3.3446|lr = 0.00010\n",
      "Epoch: 3699|steps:   60|Train Avg Loss: 0.0950 |Test Loss: 3.3238|lr = 0.00010\n",
      "Epoch: 3700|steps:   30|Train Avg Loss: 0.0859 |Test Loss: 3.1797|lr = 0.00010\n",
      "Epoch: 3700|steps:   60|Train Avg Loss: 0.0959 |Test Loss: 3.3154|lr = 0.00010\n",
      "Epoch: 3701|steps:   30|Train Avg Loss: 0.0987 |Test Loss: 3.3165|lr = 0.00010\n",
      "Epoch: 3701|steps:   60|Train Avg Loss: 0.1041 |Test Loss: 3.2974|lr = 0.00010\n",
      "Epoch: 3702|steps:   30|Train Avg Loss: 0.0809 |Test Loss: 3.2286|lr = 0.00010\n",
      "Epoch: 3702|steps:   60|Train Avg Loss: 0.0939 |Test Loss: 3.3124|lr = 0.00010\n",
      "Epoch: 3703|steps:   30|Train Avg Loss: 0.0878 |Test Loss: 3.2196|lr = 0.00010\n",
      "Epoch: 3703|steps:   60|Train Avg Loss: 0.0878 |Test Loss: 3.3865|lr = 0.00010\n",
      "Epoch: 3704|steps:   30|Train Avg Loss: 0.0868 |Test Loss: 3.3319|lr = 0.00010\n",
      "Epoch: 3704|steps:   60|Train Avg Loss: 0.0855 |Test Loss: 3.2536|lr = 0.00010\n",
      "Epoch: 3705|steps:   30|Train Avg Loss: 0.0855 |Test Loss: 3.3046|lr = 0.00010\n",
      "Epoch: 3705|steps:   60|Train Avg Loss: 0.0949 |Test Loss: 3.3438|lr = 0.00010\n",
      "Epoch: 3706|steps:   30|Train Avg Loss: 0.0948 |Test Loss: 3.3260|lr = 0.00010\n",
      "Epoch: 3706|steps:   60|Train Avg Loss: 0.0883 |Test Loss: 3.3495|lr = 0.00010\n",
      "Epoch: 3707|steps:   30|Train Avg Loss: 0.0831 |Test Loss: 3.2593|lr = 0.00010\n",
      "Epoch: 3707|steps:   60|Train Avg Loss: 0.0936 |Test Loss: 3.3644|lr = 0.00010\n",
      "Epoch: 3708|steps:   30|Train Avg Loss: 0.0779 |Test Loss: 3.3391|lr = 0.00010\n",
      "Epoch: 3708|steps:   60|Train Avg Loss: 0.1059 |Test Loss: 3.4355|lr = 0.00010\n",
      "Epoch: 3709|steps:   30|Train Avg Loss: 0.0945 |Test Loss: 3.3683|lr = 0.00010\n",
      "Epoch: 3709|steps:   60|Train Avg Loss: 0.1147 |Test Loss: 3.3835|lr = 0.00010\n",
      "Epoch: 3710|steps:   30|Train Avg Loss: 0.1440 |Test Loss: 3.2383|lr = 0.00010\n",
      "Epoch: 3710|steps:   60|Train Avg Loss: 0.1400 |Test Loss: 3.3304|lr = 0.00010\n",
      "Epoch: 3711|steps:   30|Train Avg Loss: 0.1147 |Test Loss: 3.4062|lr = 0.00010\n",
      "Epoch: 3711|steps:   60|Train Avg Loss: 0.1185 |Test Loss: 3.3848|lr = 0.00010\n",
      "Epoch: 3712|steps:   30|Train Avg Loss: 0.0998 |Test Loss: 3.3331|lr = 0.00010\n",
      "Epoch: 3712|steps:   60|Train Avg Loss: 0.0923 |Test Loss: 3.3410|lr = 0.00010\n",
      "Epoch: 3713|steps:   30|Train Avg Loss: 0.0818 |Test Loss: 3.3140|lr = 0.00010\n",
      "Epoch: 3713|steps:   60|Train Avg Loss: 0.1005 |Test Loss: 3.3942|lr = 0.00010\n",
      "Epoch: 3714|steps:   30|Train Avg Loss: 0.1123 |Test Loss: 3.4677|lr = 0.00010\n",
      "Epoch: 3714|steps:   60|Train Avg Loss: 0.1145 |Test Loss: 3.3528|lr = 0.00010\n",
      "Epoch: 3715|steps:   30|Train Avg Loss: 0.0954 |Test Loss: 3.3885|lr = 0.00010\n",
      "Epoch: 3715|steps:   60|Train Avg Loss: 0.0974 |Test Loss: 3.3232|lr = 0.00010\n",
      "Epoch: 3716|steps:   30|Train Avg Loss: 0.0771 |Test Loss: 3.3429|lr = 0.00010\n",
      "Epoch: 3716|steps:   60|Train Avg Loss: 0.1343 |Test Loss: 3.4550|lr = 0.00010\n",
      "Epoch: 3717|steps:   30|Train Avg Loss: 0.1163 |Test Loss: 3.3754|lr = 0.00010\n",
      "Epoch: 3717|steps:   60|Train Avg Loss: 0.1152 |Test Loss: 3.4064|lr = 0.00010\n",
      "Epoch: 3718|steps:   30|Train Avg Loss: 0.1767 |Test Loss: 3.4564|lr = 0.00010\n",
      "Epoch: 3718|steps:   60|Train Avg Loss: 0.1685 |Test Loss: 3.3918|lr = 0.00010\n",
      "Epoch: 3719|steps:   30|Train Avg Loss: 0.0993 |Test Loss: 3.4397|lr = 0.00010\n",
      "Epoch: 3719|steps:   60|Train Avg Loss: 0.1026 |Test Loss: 3.3753|lr = 0.00010\n",
      "Epoch: 3720|steps:   30|Train Avg Loss: 0.1010 |Test Loss: 3.3465|lr = 0.00010\n",
      "Epoch: 3720|steps:   60|Train Avg Loss: 0.0993 |Test Loss: 3.3928|lr = 0.00010\n",
      "Epoch: 3721|steps:   30|Train Avg Loss: 0.0792 |Test Loss: 3.3747|lr = 0.00010\n",
      "Epoch: 3721|steps:   60|Train Avg Loss: 0.0762 |Test Loss: 3.3227|lr = 0.00010\n",
      "Epoch: 3722|steps:   30|Train Avg Loss: 0.0770 |Test Loss: 3.3709|lr = 0.00010\n",
      "Epoch: 3722|steps:   60|Train Avg Loss: 0.0773 |Test Loss: 3.2770|lr = 0.00010\n",
      "Epoch: 3723|steps:   30|Train Avg Loss: 0.0762 |Test Loss: 3.3042|lr = 0.00010\n",
      "Epoch: 3723|steps:   60|Train Avg Loss: 0.0744 |Test Loss: 3.3213|lr = 0.00010\n",
      "Epoch: 3724|steps:   30|Train Avg Loss: 0.0712 |Test Loss: 3.3372|lr = 0.00010\n",
      "Epoch: 3724|steps:   60|Train Avg Loss: 0.0746 |Test Loss: 3.3849|lr = 0.00010\n",
      "Epoch: 3725|steps:   30|Train Avg Loss: 0.0730 |Test Loss: 3.4138|lr = 0.00010\n",
      "Epoch: 3725|steps:   60|Train Avg Loss: 0.0814 |Test Loss: 3.3864|lr = 0.00010\n",
      "Epoch: 3726|steps:   30|Train Avg Loss: 0.0818 |Test Loss: 3.4077|lr = 0.00010\n",
      "Epoch: 3726|steps:   60|Train Avg Loss: 0.0785 |Test Loss: 3.3825|lr = 0.00010\n",
      "Epoch: 3727|steps:   30|Train Avg Loss: 0.0769 |Test Loss: 3.4355|lr = 0.00010\n",
      "Epoch: 3727|steps:   60|Train Avg Loss: 0.0833 |Test Loss: 3.3737|lr = 0.00010\n",
      "Epoch: 3728|steps:   30|Train Avg Loss: 0.0794 |Test Loss: 3.4249|lr = 0.00010\n",
      "Epoch: 3728|steps:   60|Train Avg Loss: 0.0743 |Test Loss: 3.4086|lr = 0.00010\n",
      "Epoch: 3729|steps:   30|Train Avg Loss: 0.0813 |Test Loss: 3.3282|lr = 0.00010\n",
      "Epoch: 3729|steps:   60|Train Avg Loss: 0.1019 |Test Loss: 3.4805|lr = 0.00010\n",
      "Epoch: 3730|steps:   30|Train Avg Loss: 0.0771 |Test Loss: 3.3901|lr = 0.00010\n",
      "Epoch: 3730|steps:   60|Train Avg Loss: 0.0847 |Test Loss: 3.4044|lr = 0.00010\n",
      "Epoch: 3731|steps:   30|Train Avg Loss: 0.0938 |Test Loss: 3.4475|lr = 0.00010\n",
      "Epoch: 3731|steps:   60|Train Avg Loss: 0.1075 |Test Loss: 3.4385|lr = 0.00010\n",
      "Epoch: 3732|steps:   30|Train Avg Loss: 0.0801 |Test Loss: 3.3415|lr = 0.00010\n",
      "Epoch: 3732|steps:   60|Train Avg Loss: 0.0938 |Test Loss: 3.4083|lr = 0.00010\n",
      "Epoch: 3733|steps:   30|Train Avg Loss: 0.0869 |Test Loss: 3.4832|lr = 0.00010\n",
      "Epoch: 3733|steps:   60|Train Avg Loss: 0.0756 |Test Loss: 3.3669|lr = 0.00010\n",
      "Epoch: 3734|steps:   30|Train Avg Loss: 0.0672 |Test Loss: 3.3659|lr = 0.00010\n",
      "Epoch: 3734|steps:   60|Train Avg Loss: 0.0868 |Test Loss: 3.4853|lr = 0.00010\n",
      "Epoch: 3735|steps:   30|Train Avg Loss: 0.0725 |Test Loss: 3.4441|lr = 0.00010\n",
      "Epoch: 3735|steps:   60|Train Avg Loss: 0.1002 |Test Loss: 3.4478|lr = 0.00010\n",
      "Epoch: 3736|steps:   30|Train Avg Loss: 0.0850 |Test Loss: 3.4030|lr = 0.00010\n",
      "Epoch: 3736|steps:   60|Train Avg Loss: 0.0952 |Test Loss: 3.3959|lr = 0.00010\n",
      "Epoch: 3737|steps:   30|Train Avg Loss: 0.0883 |Test Loss: 3.3852|lr = 0.00010\n",
      "Epoch: 3737|steps:   60|Train Avg Loss: 0.0967 |Test Loss: 3.4229|lr = 0.00010\n",
      "Epoch: 3738|steps:   30|Train Avg Loss: 0.0638 |Test Loss: 3.4659|lr = 0.00010\n",
      "Epoch: 3738|steps:   60|Train Avg Loss: 0.0900 |Test Loss: 3.5149|lr = 0.00010\n",
      "Epoch: 3739|steps:   30|Train Avg Loss: 0.0722 |Test Loss: 3.4047|lr = 0.00010\n",
      "Epoch: 3739|steps:   60|Train Avg Loss: 0.0796 |Test Loss: 3.4540|lr = 0.00010\n",
      "Epoch: 3740|steps:   30|Train Avg Loss: 0.0697 |Test Loss: 3.4507|lr = 0.00010\n",
      "Epoch: 3740|steps:   60|Train Avg Loss: 0.0779 |Test Loss: 3.4935|lr = 0.00010\n",
      "Epoch: 3741|steps:   30|Train Avg Loss: 0.0743 |Test Loss: 3.4394|lr = 0.00010\n",
      "Epoch: 3741|steps:   60|Train Avg Loss: 0.0796 |Test Loss: 3.5351|lr = 0.00010\n",
      "Epoch: 3742|steps:   30|Train Avg Loss: 0.0851 |Test Loss: 3.4972|lr = 0.00010\n",
      "Epoch: 3742|steps:   60|Train Avg Loss: 0.0722 |Test Loss: 3.3961|lr = 0.00010\n",
      "Epoch: 3743|steps:   30|Train Avg Loss: 0.0860 |Test Loss: 3.4867|lr = 0.00010\n",
      "Epoch: 3743|steps:   60|Train Avg Loss: 0.0793 |Test Loss: 3.5234|lr = 0.00010\n",
      "Epoch: 3744|steps:   30|Train Avg Loss: 0.0745 |Test Loss: 3.5133|lr = 0.00010\n",
      "Epoch: 3744|steps:   60|Train Avg Loss: 0.0792 |Test Loss: 3.4520|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3745|steps:   30|Train Avg Loss: 0.0804 |Test Loss: 3.5631|lr = 0.00010\n",
      "Epoch: 3745|steps:   60|Train Avg Loss: 0.0992 |Test Loss: 3.4682|lr = 0.00010\n",
      "Epoch: 3746|steps:   30|Train Avg Loss: 0.1097 |Test Loss: 3.4895|lr = 0.00010\n",
      "Epoch: 3746|steps:   60|Train Avg Loss: 0.1771 |Test Loss: 3.4993|lr = 0.00010\n",
      "Epoch: 3747|steps:   30|Train Avg Loss: 0.2080 |Test Loss: 3.4547|lr = 0.00010\n",
      "Epoch: 3747|steps:   60|Train Avg Loss: 0.1833 |Test Loss: 3.5030|lr = 0.00010\n",
      "Epoch: 3748|steps:   30|Train Avg Loss: 0.1078 |Test Loss: 3.4952|lr = 0.00010\n",
      "Epoch: 3748|steps:   60|Train Avg Loss: 0.0921 |Test Loss: 3.4493|lr = 0.00010\n",
      "Epoch: 3749|steps:   30|Train Avg Loss: 0.0776 |Test Loss: 3.5290|lr = 0.00010\n",
      "Epoch: 3749|steps:   60|Train Avg Loss: 0.0783 |Test Loss: 3.5275|lr = 0.00010\n",
      "Epoch: 3750|steps:   30|Train Avg Loss: 0.0816 |Test Loss: 3.4312|lr = 0.00010\n",
      "Epoch: 3750|steps:   60|Train Avg Loss: 0.0642 |Test Loss: 3.4919|lr = 0.00010\n",
      "Epoch: 3751|steps:   30|Train Avg Loss: 0.0699 |Test Loss: 3.5621|lr = 0.00010\n",
      "Epoch: 3751|steps:   60|Train Avg Loss: 0.0780 |Test Loss: 3.5246|lr = 0.00010\n",
      "Epoch: 3752|steps:   30|Train Avg Loss: 0.0704 |Test Loss: 3.5108|lr = 0.00010\n",
      "Epoch: 3752|steps:   60|Train Avg Loss: 0.0668 |Test Loss: 3.4945|lr = 0.00010\n",
      "Epoch: 3753|steps:   30|Train Avg Loss: 0.0786 |Test Loss: 3.4891|lr = 0.00010\n",
      "Epoch: 3753|steps:   60|Train Avg Loss: 0.0754 |Test Loss: 3.5883|lr = 0.00010\n",
      "Epoch: 3754|steps:   30|Train Avg Loss: 0.0727 |Test Loss: 3.5194|lr = 0.00010\n",
      "Epoch: 3754|steps:   60|Train Avg Loss: 0.0738 |Test Loss: 3.5531|lr = 0.00010\n",
      "Epoch: 3755|steps:   30|Train Avg Loss: 0.0585 |Test Loss: 3.5287|lr = 0.00010\n",
      "Epoch: 3755|steps:   60|Train Avg Loss: 0.0770 |Test Loss: 3.5478|lr = 0.00010\n",
      "Epoch: 3756|steps:   30|Train Avg Loss: 0.0579 |Test Loss: 3.5508|lr = 0.00010\n",
      "Epoch: 3756|steps:   60|Train Avg Loss: 0.0718 |Test Loss: 3.5852|lr = 0.00010\n",
      "Epoch: 3757|steps:   30|Train Avg Loss: 0.0829 |Test Loss: 3.5383|lr = 0.00010\n",
      "Epoch: 3757|steps:   60|Train Avg Loss: 0.0696 |Test Loss: 3.4838|lr = 0.00010\n",
      "Epoch: 3758|steps:   30|Train Avg Loss: 0.0606 |Test Loss: 3.6051|lr = 0.00010\n",
      "Epoch: 3758|steps:   60|Train Avg Loss: 0.0773 |Test Loss: 3.4921|lr = 0.00010\n",
      "Epoch: 3759|steps:   30|Train Avg Loss: 0.0746 |Test Loss: 3.4936|lr = 0.00010\n",
      "Epoch: 3759|steps:   60|Train Avg Loss: 0.0796 |Test Loss: 3.5389|lr = 0.00010\n",
      "Epoch: 3760|steps:   30|Train Avg Loss: 0.0603 |Test Loss: 3.4963|lr = 0.00010\n",
      "Epoch: 3760|steps:   60|Train Avg Loss: 0.0735 |Test Loss: 3.5741|lr = 0.00010\n",
      "Epoch: 3761|steps:   30|Train Avg Loss: 0.0679 |Test Loss: 3.5373|lr = 0.00010\n",
      "Epoch: 3761|steps:   60|Train Avg Loss: 0.0758 |Test Loss: 3.5312|lr = 0.00010\n",
      "Epoch: 3762|steps:   30|Train Avg Loss: 0.0911 |Test Loss: 3.5825|lr = 0.00010\n",
      "Epoch: 3762|steps:   60|Train Avg Loss: 0.0784 |Test Loss: 3.5815|lr = 0.00010\n",
      "Epoch: 3763|steps:   30|Train Avg Loss: 0.0827 |Test Loss: 3.5813|lr = 0.00010\n",
      "Epoch: 3763|steps:   60|Train Avg Loss: 0.1046 |Test Loss: 3.5489|lr = 0.00010\n",
      "Epoch: 3764|steps:   30|Train Avg Loss: 0.0991 |Test Loss: 3.5826|lr = 0.00010\n",
      "Epoch: 3764|steps:   60|Train Avg Loss: 0.1031 |Test Loss: 3.5717|lr = 0.00010\n",
      "Epoch: 3765|steps:   30|Train Avg Loss: 0.0897 |Test Loss: 3.6484|lr = 0.00010\n",
      "Epoch: 3765|steps:   60|Train Avg Loss: 0.1357 |Test Loss: 3.5141|lr = 0.00010\n",
      "Epoch: 3766|steps:   30|Train Avg Loss: 0.0889 |Test Loss: 3.5985|lr = 0.00010\n",
      "Epoch: 3766|steps:   60|Train Avg Loss: 0.0825 |Test Loss: 3.6283|lr = 0.00010\n",
      "Epoch: 3767|steps:   30|Train Avg Loss: 0.0815 |Test Loss: 3.6051|lr = 0.00010\n",
      "Epoch: 3767|steps:   60|Train Avg Loss: 0.0802 |Test Loss: 3.5692|lr = 0.00010\n",
      "Epoch: 3768|steps:   30|Train Avg Loss: 0.0807 |Test Loss: 3.5494|lr = 0.00010\n",
      "Epoch: 3768|steps:   60|Train Avg Loss: 0.0743 |Test Loss: 3.5858|lr = 0.00010\n",
      "Epoch: 3769|steps:   30|Train Avg Loss: 0.0713 |Test Loss: 3.5481|lr = 0.00010\n",
      "Epoch: 3769|steps:   60|Train Avg Loss: 0.0714 |Test Loss: 3.5703|lr = 0.00010\n",
      "Epoch: 3770|steps:   30|Train Avg Loss: 0.0698 |Test Loss: 3.5986|lr = 0.00010\n",
      "Epoch: 3770|steps:   60|Train Avg Loss: 0.0671 |Test Loss: 3.6243|lr = 0.00010\n",
      "Epoch: 3771|steps:   30|Train Avg Loss: 0.0654 |Test Loss: 3.6497|lr = 0.00010\n",
      "Epoch: 3771|steps:   60|Train Avg Loss: 0.0661 |Test Loss: 3.5733|lr = 0.00010\n",
      "Epoch: 3772|steps:   30|Train Avg Loss: 0.0812 |Test Loss: 3.6032|lr = 0.00010\n",
      "Epoch: 3772|steps:   60|Train Avg Loss: 0.0587 |Test Loss: 3.5101|lr = 0.00010\n",
      "Epoch: 3773|steps:   30|Train Avg Loss: 0.0696 |Test Loss: 3.6571|lr = 0.00010\n",
      "Epoch: 3773|steps:   60|Train Avg Loss: 0.1012 |Test Loss: 3.7207|lr = 0.00010\n",
      "Epoch: 3774|steps:   30|Train Avg Loss: 0.0839 |Test Loss: 3.6049|lr = 0.00010\n",
      "Epoch: 3774|steps:   60|Train Avg Loss: 0.1429 |Test Loss: 3.5069|lr = 0.00010\n",
      "Epoch: 3775|steps:   30|Train Avg Loss: 0.2068 |Test Loss: 3.8464|lr = 0.00010\n",
      "Epoch: 3775|steps:   60|Train Avg Loss: 0.2242 |Test Loss: 3.4692|lr = 0.00010\n",
      "Epoch: 3776|steps:   30|Train Avg Loss: 0.1201 |Test Loss: 3.6145|lr = 0.00010\n",
      "Epoch: 3776|steps:   60|Train Avg Loss: 0.0889 |Test Loss: 3.5804|lr = 0.00010\n",
      "Epoch: 3777|steps:   30|Train Avg Loss: 0.0630 |Test Loss: 3.6169|lr = 0.00010\n",
      "Epoch: 3777|steps:   60|Train Avg Loss: 0.0771 |Test Loss: 3.4898|lr = 0.00010\n",
      "Epoch: 3778|steps:   30|Train Avg Loss: 0.0680 |Test Loss: 3.6272|lr = 0.00010\n",
      "Epoch: 3778|steps:   60|Train Avg Loss: 0.0648 |Test Loss: 3.5368|lr = 0.00010\n",
      "Epoch: 3779|steps:   30|Train Avg Loss: 0.0676 |Test Loss: 3.5648|lr = 0.00010\n",
      "Epoch: 3779|steps:   60|Train Avg Loss: 0.0644 |Test Loss: 3.5258|lr = 0.00010\n",
      "Epoch: 3780|steps:   30|Train Avg Loss: 0.0687 |Test Loss: 3.5736|lr = 0.00010\n",
      "Epoch: 3780|steps:   60|Train Avg Loss: 0.0636 |Test Loss: 3.5201|lr = 0.00010\n",
      "Epoch: 3781|steps:   30|Train Avg Loss: 0.0699 |Test Loss: 3.6271|lr = 0.00010\n",
      "Epoch: 3781|steps:   60|Train Avg Loss: 0.0790 |Test Loss: 3.5592|lr = 0.00010\n",
      "Epoch: 3782|steps:   30|Train Avg Loss: 0.0624 |Test Loss: 3.5982|lr = 0.00010\n",
      "Epoch: 3782|steps:   60|Train Avg Loss: 0.0703 |Test Loss: 3.5828|lr = 0.00010\n",
      "Epoch: 3783|steps:   30|Train Avg Loss: 0.0595 |Test Loss: 3.5552|lr = 0.00010\n",
      "Epoch: 3783|steps:   60|Train Avg Loss: 0.0620 |Test Loss: 3.6162|lr = 0.00010\n",
      "Epoch: 3784|steps:   30|Train Avg Loss: 0.0571 |Test Loss: 3.5796|lr = 0.00010\n",
      "Epoch: 3784|steps:   60|Train Avg Loss: 0.0739 |Test Loss: 3.6354|lr = 0.00010\n",
      "Epoch: 3785|steps:   30|Train Avg Loss: 0.0650 |Test Loss: 3.6049|lr = 0.00010\n",
      "Epoch: 3785|steps:   60|Train Avg Loss: 0.0627 |Test Loss: 3.5233|lr = 0.00010\n",
      "Epoch: 3786|steps:   30|Train Avg Loss: 0.0613 |Test Loss: 3.5297|lr = 0.00010\n",
      "Epoch: 3786|steps:   60|Train Avg Loss: 0.0646 |Test Loss: 3.6596|lr = 0.00010\n",
      "Epoch: 3787|steps:   30|Train Avg Loss: 0.0651 |Test Loss: 3.5939|lr = 0.00010\n",
      "Epoch: 3787|steps:   60|Train Avg Loss: 0.0870 |Test Loss: 3.7570|lr = 0.00010\n",
      "Epoch: 3788|steps:   30|Train Avg Loss: 0.1473 |Test Loss: 3.7694|lr = 0.00010\n",
      "Epoch: 3788|steps:   60|Train Avg Loss: 0.1471 |Test Loss: 3.6880|lr = 0.00010\n",
      "Epoch: 3789|steps:   30|Train Avg Loss: 0.1000 |Test Loss: 3.7167|lr = 0.00010\n",
      "Epoch: 3789|steps:   60|Train Avg Loss: 0.1068 |Test Loss: 3.5525|lr = 0.00010\n",
      "Epoch: 3790|steps:   30|Train Avg Loss: 0.0869 |Test Loss: 3.5802|lr = 0.00010\n",
      "Epoch: 3790|steps:   60|Train Avg Loss: 0.0779 |Test Loss: 3.6415|lr = 0.00010\n",
      "Epoch: 3791|steps:   30|Train Avg Loss: 0.0569 |Test Loss: 3.7005|lr = 0.00010\n",
      "Epoch: 3791|steps:   60|Train Avg Loss: 0.0762 |Test Loss: 3.6429|lr = 0.00010\n",
      "Epoch: 3792|steps:   30|Train Avg Loss: 0.0622 |Test Loss: 3.5663|lr = 0.00010\n",
      "Epoch: 3792|steps:   60|Train Avg Loss: 0.0676 |Test Loss: 3.6688|lr = 0.00010\n",
      "Epoch: 3793|steps:   30|Train Avg Loss: 0.0583 |Test Loss: 3.6992|lr = 0.00010\n",
      "Epoch: 3793|steps:   60|Train Avg Loss: 0.0560 |Test Loss: 3.6341|lr = 0.00010\n",
      "Epoch: 3794|steps:   30|Train Avg Loss: 0.0649 |Test Loss: 3.7069|lr = 0.00010\n",
      "Epoch: 3794|steps:   60|Train Avg Loss: 0.0660 |Test Loss: 3.6623|lr = 0.00010\n",
      "Epoch: 3795|steps:   30|Train Avg Loss: 0.0636 |Test Loss: 3.5841|lr = 0.00010\n",
      "Epoch: 3795|steps:   60|Train Avg Loss: 0.0575 |Test Loss: 3.5980|lr = 0.00010\n",
      "Epoch: 3796|steps:   30|Train Avg Loss: 0.0599 |Test Loss: 3.6334|lr = 0.00010\n",
      "Epoch: 3796|steps:   60|Train Avg Loss: 0.0620 |Test Loss: 3.7101|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3797|steps:   30|Train Avg Loss: 0.0601 |Test Loss: 3.5484|lr = 0.00010\n",
      "Epoch: 3797|steps:   60|Train Avg Loss: 0.0702 |Test Loss: 3.6732|lr = 0.00010\n",
      "Epoch: 3798|steps:   30|Train Avg Loss: 0.0653 |Test Loss: 3.7212|lr = 0.00010\n",
      "Epoch: 3798|steps:   60|Train Avg Loss: 0.0581 |Test Loss: 3.6805|lr = 0.00010\n",
      "Epoch: 3799|steps:   30|Train Avg Loss: 0.0738 |Test Loss: 3.6169|lr = 0.00010\n",
      "Epoch: 3799|steps:   60|Train Avg Loss: 0.0593 |Test Loss: 3.6348|lr = 0.00010\n",
      "Epoch: 3800|steps:   30|Train Avg Loss: 0.0654 |Test Loss: 3.6984|lr = 0.00010\n",
      "Epoch: 3800|steps:   60|Train Avg Loss: 0.0670 |Test Loss: 3.7317|lr = 0.00010\n",
      "Epoch: 3801|steps:   30|Train Avg Loss: 0.0644 |Test Loss: 3.6369|lr = 0.00010\n",
      "Epoch: 3801|steps:   60|Train Avg Loss: 0.0852 |Test Loss: 3.6533|lr = 0.00010\n",
      "Epoch: 3802|steps:   30|Train Avg Loss: 0.0668 |Test Loss: 3.7336|lr = 0.00010\n",
      "Epoch: 3802|steps:   60|Train Avg Loss: 0.0647 |Test Loss: 3.6494|lr = 0.00010\n",
      "Epoch: 3803|steps:   30|Train Avg Loss: 0.0632 |Test Loss: 3.7256|lr = 0.00010\n",
      "Epoch: 3803|steps:   60|Train Avg Loss: 0.0658 |Test Loss: 3.7067|lr = 0.00010\n",
      "Epoch: 3804|steps:   30|Train Avg Loss: 0.0802 |Test Loss: 3.7075|lr = 0.00010\n",
      "Epoch: 3804|steps:   60|Train Avg Loss: 0.0630 |Test Loss: 3.6534|lr = 0.00010\n",
      "Epoch: 3805|steps:   30|Train Avg Loss: 0.0711 |Test Loss: 3.6601|lr = 0.00010\n",
      "Epoch: 3805|steps:   60|Train Avg Loss: 0.0633 |Test Loss: 3.7024|lr = 0.00010\n",
      "Epoch: 3806|steps:   30|Train Avg Loss: 0.0614 |Test Loss: 3.7561|lr = 0.00010\n",
      "Epoch: 3806|steps:   60|Train Avg Loss: 0.0645 |Test Loss: 3.6904|lr = 0.00010\n",
      "Epoch: 3807|steps:   30|Train Avg Loss: 0.0610 |Test Loss: 3.7914|lr = 0.00010\n",
      "Epoch: 3807|steps:   60|Train Avg Loss: 0.0639 |Test Loss: 3.6929|lr = 0.00010\n",
      "Epoch: 3808|steps:   30|Train Avg Loss: 0.1170 |Test Loss: 3.5838|lr = 0.00010\n",
      "Epoch: 3808|steps:   60|Train Avg Loss: 0.1852 |Test Loss: 3.9155|lr = 0.00010\n",
      "Epoch: 3809|steps:   30|Train Avg Loss: 0.1161 |Test Loss: 3.7632|lr = 0.00010\n",
      "Epoch: 3809|steps:   60|Train Avg Loss: 0.1228 |Test Loss: 3.7926|lr = 0.00010\n",
      "Epoch: 3810|steps:   30|Train Avg Loss: 0.1101 |Test Loss: 3.7889|lr = 0.00010\n",
      "Epoch: 3810|steps:   60|Train Avg Loss: 0.1054 |Test Loss: 3.7246|lr = 0.00010\n",
      "Epoch: 3811|steps:   30|Train Avg Loss: 0.0660 |Test Loss: 3.7249|lr = 0.00010\n",
      "Epoch: 3811|steps:   60|Train Avg Loss: 0.0542 |Test Loss: 3.7882|lr = 0.00010\n",
      "Epoch: 3812|steps:   30|Train Avg Loss: 0.0625 |Test Loss: 3.7026|lr = 0.00010\n",
      "Epoch: 3812|steps:   60|Train Avg Loss: 0.0581 |Test Loss: 3.7093|lr = 0.00010\n",
      "Epoch: 3813|steps:   30|Train Avg Loss: 0.0553 |Test Loss: 3.7007|lr = 0.00010\n",
      "Epoch: 3813|steps:   60|Train Avg Loss: 0.0566 |Test Loss: 3.8052|lr = 0.00010\n",
      "Epoch: 3814|steps:   30|Train Avg Loss: 0.0539 |Test Loss: 3.8500|lr = 0.00010\n",
      "Epoch: 3814|steps:   60|Train Avg Loss: 0.0622 |Test Loss: 3.7407|lr = 0.00010\n",
      "Epoch: 3815|steps:   30|Train Avg Loss: 0.0555 |Test Loss: 3.7470|lr = 0.00010\n",
      "Epoch: 3815|steps:   60|Train Avg Loss: 0.0584 |Test Loss: 3.8085|lr = 0.00010\n",
      "Epoch: 3816|steps:   30|Train Avg Loss: 0.0582 |Test Loss: 3.7362|lr = 0.00010\n",
      "Epoch: 3816|steps:   60|Train Avg Loss: 0.0613 |Test Loss: 3.8010|lr = 0.00010\n",
      "Epoch: 3817|steps:   30|Train Avg Loss: 0.0631 |Test Loss: 3.7540|lr = 0.00010\n",
      "Epoch: 3817|steps:   60|Train Avg Loss: 0.0826 |Test Loss: 3.7503|lr = 0.00010\n",
      "Epoch: 3818|steps:   30|Train Avg Loss: 0.1125 |Test Loss: 3.7279|lr = 0.00010\n",
      "Epoch: 3818|steps:   60|Train Avg Loss: 0.1358 |Test Loss: 3.8443|lr = 0.00010\n",
      "Epoch: 3819|steps:   30|Train Avg Loss: 0.1067 |Test Loss: 3.7241|lr = 0.00010\n",
      "Epoch: 3819|steps:   60|Train Avg Loss: 0.0947 |Test Loss: 3.6749|lr = 0.00010\n",
      "Epoch: 3820|steps:   30|Train Avg Loss: 0.0594 |Test Loss: 3.7659|lr = 0.00010\n",
      "Epoch: 3820|steps:   60|Train Avg Loss: 0.0778 |Test Loss: 3.7398|lr = 0.00010\n",
      "Epoch: 3821|steps:   30|Train Avg Loss: 0.0662 |Test Loss: 3.8137|lr = 0.00010\n",
      "Epoch: 3821|steps:   60|Train Avg Loss: 0.0736 |Test Loss: 3.7299|lr = 0.00010\n",
      "Epoch: 3822|steps:   30|Train Avg Loss: 0.0609 |Test Loss: 3.7806|lr = 0.00010\n",
      "Epoch: 3822|steps:   60|Train Avg Loss: 0.0666 |Test Loss: 3.7928|lr = 0.00010\n",
      "Epoch: 3823|steps:   30|Train Avg Loss: 0.0666 |Test Loss: 3.8926|lr = 0.00010\n",
      "Epoch: 3823|steps:   60|Train Avg Loss: 0.0579 |Test Loss: 3.7565|lr = 0.00010\n",
      "Epoch: 3824|steps:   30|Train Avg Loss: 0.0524 |Test Loss: 3.7996|lr = 0.00010\n",
      "Epoch: 3824|steps:   60|Train Avg Loss: 0.0664 |Test Loss: 3.7789|lr = 0.00010\n",
      "Epoch: 3825|steps:   30|Train Avg Loss: 0.0646 |Test Loss: 3.8043|lr = 0.00010\n",
      "Epoch: 3825|steps:   60|Train Avg Loss: 0.0699 |Test Loss: 3.8181|lr = 0.00010\n",
      "Epoch: 3826|steps:   30|Train Avg Loss: 0.0671 |Test Loss: 3.8590|lr = 0.00010\n",
      "Epoch: 3826|steps:   60|Train Avg Loss: 0.0663 |Test Loss: 3.8441|lr = 0.00010\n",
      "Epoch: 3827|steps:   30|Train Avg Loss: 0.0505 |Test Loss: 3.8093|lr = 0.00010\n",
      "Epoch: 3827|steps:   60|Train Avg Loss: 0.0558 |Test Loss: 3.7776|lr = 0.00010\n",
      "Epoch: 3828|steps:   30|Train Avg Loss: 0.0505 |Test Loss: 3.8259|lr = 0.00010\n",
      "Epoch: 3828|steps:   60|Train Avg Loss: 0.0642 |Test Loss: 3.8171|lr = 0.00010\n",
      "Epoch: 3829|steps:   30|Train Avg Loss: 0.0556 |Test Loss: 3.8886|lr = 0.00010\n",
      "Epoch: 3829|steps:   60|Train Avg Loss: 0.0549 |Test Loss: 3.7805|lr = 0.00010\n",
      "Epoch: 3830|steps:   30|Train Avg Loss: 0.0591 |Test Loss: 3.8328|lr = 0.00010\n",
      "Epoch: 3830|steps:   60|Train Avg Loss: 0.0496 |Test Loss: 3.8467|lr = 0.00010\n",
      "Epoch: 3831|steps:   30|Train Avg Loss: 0.0476 |Test Loss: 3.8177|lr = 0.00010\n",
      "Epoch: 3831|steps:   60|Train Avg Loss: 0.0725 |Test Loss: 3.8123|lr = 0.00010\n",
      "Epoch: 3832|steps:   30|Train Avg Loss: 0.0592 |Test Loss: 3.8488|lr = 0.00010\n",
      "Epoch: 3832|steps:   60|Train Avg Loss: 0.0626 |Test Loss: 3.8726|lr = 0.00010\n",
      "Epoch: 3833|steps:   30|Train Avg Loss: 0.0752 |Test Loss: 3.8340|lr = 0.00010\n",
      "Epoch: 3833|steps:   60|Train Avg Loss: 0.0604 |Test Loss: 3.8709|lr = 0.00010\n",
      "Epoch: 3834|steps:   30|Train Avg Loss: 0.0481 |Test Loss: 3.7865|lr = 0.00010\n",
      "Epoch: 3834|steps:   60|Train Avg Loss: 0.0576 |Test Loss: 3.9451|lr = 0.00010\n",
      "Epoch: 3835|steps:   30|Train Avg Loss: 0.0519 |Test Loss: 3.8599|lr = 0.00010\n",
      "Epoch: 3835|steps:   60|Train Avg Loss: 0.0532 |Test Loss: 3.8592|lr = 0.00010\n",
      "Epoch: 3836|steps:   30|Train Avg Loss: 0.0601 |Test Loss: 3.8747|lr = 0.00010\n",
      "Epoch: 3836|steps:   60|Train Avg Loss: 0.0565 |Test Loss: 3.9289|lr = 0.00010\n",
      "Epoch: 3837|steps:   30|Train Avg Loss: 0.0472 |Test Loss: 3.9783|lr = 0.00010\n",
      "Epoch: 3837|steps:   60|Train Avg Loss: 0.0675 |Test Loss: 3.8541|lr = 0.00010\n",
      "Epoch: 3838|steps:   30|Train Avg Loss: 0.0605 |Test Loss: 3.8780|lr = 0.00010\n",
      "Epoch: 3838|steps:   60|Train Avg Loss: 0.0980 |Test Loss: 3.8223|lr = 0.00010\n",
      "Epoch: 3839|steps:   30|Train Avg Loss: 0.0911 |Test Loss: 3.8675|lr = 0.00010\n",
      "Epoch: 3839|steps:   60|Train Avg Loss: 0.1165 |Test Loss: 3.8283|lr = 0.00010\n",
      "Epoch: 3840|steps:   30|Train Avg Loss: 0.1715 |Test Loss: 3.8694|lr = 0.00010\n",
      "Epoch: 3840|steps:   60|Train Avg Loss: 0.1997 |Test Loss: 3.8701|lr = 0.00010\n",
      "Epoch: 3841|steps:   30|Train Avg Loss: 0.1626 |Test Loss: 3.8711|lr = 0.00010\n",
      "Epoch: 3841|steps:   60|Train Avg Loss: 0.1215 |Test Loss: 3.8101|lr = 0.00010\n",
      "Epoch: 3842|steps:   30|Train Avg Loss: 0.0706 |Test Loss: 3.9028|lr = 0.00010\n",
      "Epoch: 3842|steps:   60|Train Avg Loss: 0.0669 |Test Loss: 3.8267|lr = 0.00010\n",
      "Epoch: 3843|steps:   30|Train Avg Loss: 0.0583 |Test Loss: 3.7800|lr = 0.00010\n",
      "Epoch: 3843|steps:   60|Train Avg Loss: 0.0558 |Test Loss: 3.8532|lr = 0.00010\n",
      "Epoch: 3844|steps:   30|Train Avg Loss: 0.0490 |Test Loss: 3.7721|lr = 0.00010\n",
      "Epoch: 3844|steps:   60|Train Avg Loss: 0.0561 |Test Loss: 3.7853|lr = 0.00010\n",
      "Epoch: 3845|steps:   30|Train Avg Loss: 0.0524 |Test Loss: 3.7888|lr = 0.00010\n",
      "Epoch: 3845|steps:   60|Train Avg Loss: 0.0493 |Test Loss: 3.8365|lr = 0.00010\n",
      "Epoch: 3846|steps:   30|Train Avg Loss: 0.0426 |Test Loss: 3.8613|lr = 0.00010\n",
      "Epoch: 3846|steps:   60|Train Avg Loss: 0.0571 |Test Loss: 3.7881|lr = 0.00010\n",
      "Epoch: 3847|steps:   30|Train Avg Loss: 0.0441 |Test Loss: 3.8404|lr = 0.00010\n",
      "Epoch: 3847|steps:   60|Train Avg Loss: 0.0545 |Test Loss: 3.8676|lr = 0.00010\n",
      "Epoch: 3848|steps:   30|Train Avg Loss: 0.0482 |Test Loss: 3.8371|lr = 0.00010\n",
      "Epoch: 3848|steps:   60|Train Avg Loss: 0.0513 |Test Loss: 3.8004|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3849|steps:   30|Train Avg Loss: 0.0449 |Test Loss: 3.8069|lr = 0.00010\n",
      "Epoch: 3849|steps:   60|Train Avg Loss: 0.0586 |Test Loss: 3.8340|lr = 0.00010\n",
      "Epoch: 3850|steps:   30|Train Avg Loss: 0.0486 |Test Loss: 3.8218|lr = 0.00010\n",
      "Epoch: 3850|steps:   60|Train Avg Loss: 0.0536 |Test Loss: 3.8225|lr = 0.00010\n",
      "Epoch: 3851|steps:   30|Train Avg Loss: 0.0511 |Test Loss: 3.8404|lr = 0.00010\n",
      "Epoch: 3851|steps:   60|Train Avg Loss: 0.0519 |Test Loss: 3.8315|lr = 0.00010\n",
      "Epoch: 3852|steps:   30|Train Avg Loss: 0.0538 |Test Loss: 3.7958|lr = 0.00010\n",
      "Epoch: 3852|steps:   60|Train Avg Loss: 0.0568 |Test Loss: 3.8228|lr = 0.00010\n",
      "Epoch: 3853|steps:   30|Train Avg Loss: 0.0446 |Test Loss: 3.8521|lr = 0.00010\n",
      "Epoch: 3853|steps:   60|Train Avg Loss: 0.0542 |Test Loss: 3.8750|lr = 0.00010\n",
      "Epoch: 3854|steps:   30|Train Avg Loss: 0.0399 |Test Loss: 3.8892|lr = 0.00010\n",
      "Epoch: 3854|steps:   60|Train Avg Loss: 0.0663 |Test Loss: 3.7769|lr = 0.00010\n",
      "Epoch: 3855|steps:   30|Train Avg Loss: 0.0520 |Test Loss: 3.8736|lr = 0.00010\n",
      "Epoch: 3855|steps:   60|Train Avg Loss: 0.0615 |Test Loss: 3.8980|lr = 0.00010\n",
      "Epoch: 3856|steps:   30|Train Avg Loss: 0.0754 |Test Loss: 3.9783|lr = 0.00010\n",
      "Epoch: 3856|steps:   60|Train Avg Loss: 0.0669 |Test Loss: 3.8190|lr = 0.00010\n",
      "Epoch: 3857|steps:   30|Train Avg Loss: 0.0600 |Test Loss: 3.9131|lr = 0.00010\n",
      "Epoch: 3857|steps:   60|Train Avg Loss: 0.0544 |Test Loss: 3.8322|lr = 0.00010\n",
      "Epoch: 3858|steps:   30|Train Avg Loss: 0.0536 |Test Loss: 3.9003|lr = 0.00010\n",
      "Epoch: 3858|steps:   60|Train Avg Loss: 0.0544 |Test Loss: 3.9215|lr = 0.00010\n",
      "Epoch: 3859|steps:   30|Train Avg Loss: 0.0720 |Test Loss: 3.9512|lr = 0.00010\n",
      "Epoch: 3859|steps:   60|Train Avg Loss: 0.1135 |Test Loss: 3.9793|lr = 0.00010\n",
      "Epoch: 3860|steps:   30|Train Avg Loss: 0.1109 |Test Loss: 4.0182|lr = 0.00010\n",
      "Epoch: 3860|steps:   60|Train Avg Loss: 0.1169 |Test Loss: 4.1125|lr = 0.00010\n",
      "Epoch: 3861|steps:   30|Train Avg Loss: 0.0916 |Test Loss: 3.8386|lr = 0.00010\n",
      "Epoch: 3861|steps:   60|Train Avg Loss: 0.0955 |Test Loss: 3.9439|lr = 0.00010\n",
      "Epoch: 3862|steps:   30|Train Avg Loss: 0.0599 |Test Loss: 3.8578|lr = 0.00010\n",
      "Epoch: 3862|steps:   60|Train Avg Loss: 0.0525 |Test Loss: 3.9114|lr = 0.00010\n",
      "Epoch: 3863|steps:   30|Train Avg Loss: 0.0498 |Test Loss: 3.9437|lr = 0.00010\n",
      "Epoch: 3863|steps:   60|Train Avg Loss: 0.0468 |Test Loss: 3.9522|lr = 0.00010\n",
      "Epoch: 3864|steps:   30|Train Avg Loss: 0.0472 |Test Loss: 3.8788|lr = 0.00010\n",
      "Epoch: 3864|steps:   60|Train Avg Loss: 0.0548 |Test Loss: 3.8330|lr = 0.00010\n",
      "Epoch: 3865|steps:   30|Train Avg Loss: 0.0525 |Test Loss: 3.8818|lr = 0.00010\n",
      "Epoch: 3865|steps:   60|Train Avg Loss: 0.0483 |Test Loss: 3.9041|lr = 0.00010\n",
      "Epoch: 3866|steps:   30|Train Avg Loss: 0.0435 |Test Loss: 3.9330|lr = 0.00010\n",
      "Epoch: 3866|steps:   60|Train Avg Loss: 0.0514 |Test Loss: 3.9207|lr = 0.00010\n",
      "Epoch: 3867|steps:   30|Train Avg Loss: 0.0690 |Test Loss: 3.8821|lr = 0.00010\n",
      "Epoch: 3867|steps:   60|Train Avg Loss: 0.0842 |Test Loss: 4.0132|lr = 0.00010\n",
      "Epoch: 3868|steps:   30|Train Avg Loss: 0.0728 |Test Loss: 3.8809|lr = 0.00010\n",
      "Epoch: 3868|steps:   60|Train Avg Loss: 0.0912 |Test Loss: 3.9598|lr = 0.00010\n",
      "Epoch: 3869|steps:   30|Train Avg Loss: 0.0593 |Test Loss: 3.9213|lr = 0.00010\n",
      "Epoch: 3869|steps:   60|Train Avg Loss: 0.0537 |Test Loss: 3.8701|lr = 0.00010\n",
      "Epoch: 3870|steps:   30|Train Avg Loss: 0.0406 |Test Loss: 3.9679|lr = 0.00010\n",
      "Epoch: 3870|steps:   60|Train Avg Loss: 0.0603 |Test Loss: 3.9354|lr = 0.00010\n",
      "Epoch: 3871|steps:   30|Train Avg Loss: 0.0612 |Test Loss: 3.8879|lr = 0.00010\n",
      "Epoch: 3871|steps:   60|Train Avg Loss: 0.0500 |Test Loss: 3.9092|lr = 0.00010\n",
      "Epoch: 3872|steps:   30|Train Avg Loss: 0.0506 |Test Loss: 3.9176|lr = 0.00010\n",
      "Epoch: 3872|steps:   60|Train Avg Loss: 0.0674 |Test Loss: 3.8389|lr = 0.00010\n",
      "Epoch: 3873|steps:   30|Train Avg Loss: 0.0668 |Test Loss: 4.0884|lr = 0.00010\n",
      "Epoch: 3873|steps:   60|Train Avg Loss: 0.0640 |Test Loss: 3.9431|lr = 0.00010\n",
      "Epoch: 3874|steps:   30|Train Avg Loss: 0.0481 |Test Loss: 3.9651|lr = 0.00010\n",
      "Epoch: 3874|steps:   60|Train Avg Loss: 0.0532 |Test Loss: 3.8505|lr = 0.00010\n",
      "Epoch: 3875|steps:   30|Train Avg Loss: 0.0495 |Test Loss: 3.9187|lr = 0.00010\n",
      "Epoch: 3875|steps:   60|Train Avg Loss: 0.0538 |Test Loss: 3.9443|lr = 0.00010\n",
      "Epoch: 3876|steps:   30|Train Avg Loss: 0.0394 |Test Loss: 3.9946|lr = 0.00010\n",
      "Epoch: 3876|steps:   60|Train Avg Loss: 0.0544 |Test Loss: 3.8421|lr = 0.00010\n",
      "Epoch: 3877|steps:   30|Train Avg Loss: 0.0605 |Test Loss: 3.9279|lr = 0.00010\n",
      "Epoch: 3877|steps:   60|Train Avg Loss: 0.0700 |Test Loss: 4.0759|lr = 0.00010\n",
      "Epoch: 3878|steps:   30|Train Avg Loss: 0.0546 |Test Loss: 3.9207|lr = 0.00010\n",
      "Epoch: 3878|steps:   60|Train Avg Loss: 0.0682 |Test Loss: 3.9583|lr = 0.00010\n",
      "Epoch: 3879|steps:   30|Train Avg Loss: 0.0652 |Test Loss: 3.9781|lr = 0.00010\n",
      "Epoch: 3879|steps:   60|Train Avg Loss: 0.0645 |Test Loss: 3.9585|lr = 0.00010\n",
      "Epoch: 3880|steps:   30|Train Avg Loss: 0.1573 |Test Loss: 3.9978|lr = 0.00010\n",
      "Epoch: 3880|steps:   60|Train Avg Loss: 0.2123 |Test Loss: 4.0145|lr = 0.00010\n",
      "Epoch: 3881|steps:   30|Train Avg Loss: 0.0962 |Test Loss: 4.1462|lr = 0.00010\n",
      "Epoch: 3881|steps:   60|Train Avg Loss: 0.0804 |Test Loss: 4.0942|lr = 0.00010\n",
      "Epoch: 3882|steps:   30|Train Avg Loss: 0.0887 |Test Loss: 4.0287|lr = 0.00010\n",
      "Epoch: 3882|steps:   60|Train Avg Loss: 0.0683 |Test Loss: 4.1266|lr = 0.00010\n",
      "Epoch: 3883|steps:   30|Train Avg Loss: 0.0678 |Test Loss: 4.0235|lr = 0.00010\n",
      "Epoch: 3883|steps:   60|Train Avg Loss: 0.0650 |Test Loss: 3.9872|lr = 0.00010\n",
      "Epoch: 3884|steps:   30|Train Avg Loss: 0.0510 |Test Loss: 3.9826|lr = 0.00010\n",
      "Epoch: 3884|steps:   60|Train Avg Loss: 0.0479 |Test Loss: 3.9826|lr = 0.00010\n",
      "Epoch: 3885|steps:   30|Train Avg Loss: 0.0451 |Test Loss: 3.9822|lr = 0.00010\n",
      "Epoch: 3885|steps:   60|Train Avg Loss: 0.0489 |Test Loss: 4.0757|lr = 0.00010\n",
      "Epoch: 3886|steps:   30|Train Avg Loss: 0.0398 |Test Loss: 4.0071|lr = 0.00010\n",
      "Epoch: 3886|steps:   60|Train Avg Loss: 0.0483 |Test Loss: 4.0484|lr = 0.00010\n",
      "Epoch: 3887|steps:   30|Train Avg Loss: 0.0469 |Test Loss: 4.0072|lr = 0.00010\n",
      "Epoch: 3887|steps:   60|Train Avg Loss: 0.0438 |Test Loss: 4.1136|lr = 0.00010\n",
      "Epoch: 3888|steps:   30|Train Avg Loss: 0.0489 |Test Loss: 4.1122|lr = 0.00010\n",
      "Epoch: 3888|steps:   60|Train Avg Loss: 0.0479 |Test Loss: 4.0098|lr = 0.00010\n",
      "Epoch: 3889|steps:   30|Train Avg Loss: 0.0391 |Test Loss: 4.0324|lr = 0.00010\n",
      "Epoch: 3889|steps:   60|Train Avg Loss: 0.0521 |Test Loss: 3.9692|lr = 0.00010\n",
      "Epoch: 3890|steps:   30|Train Avg Loss: 0.0466 |Test Loss: 4.0604|lr = 0.00010\n",
      "Epoch: 3890|steps:   60|Train Avg Loss: 0.0452 |Test Loss: 3.9781|lr = 0.00010\n",
      "Epoch: 3891|steps:   30|Train Avg Loss: 0.0348 |Test Loss: 4.0371|lr = 0.00010\n",
      "Epoch: 3891|steps:   60|Train Avg Loss: 0.0520 |Test Loss: 3.9449|lr = 0.00010\n",
      "Epoch: 3892|steps:   30|Train Avg Loss: 0.0420 |Test Loss: 3.9530|lr = 0.00010\n",
      "Epoch: 3892|steps:   60|Train Avg Loss: 0.0566 |Test Loss: 3.9423|lr = 0.00010\n",
      "Epoch: 3893|steps:   30|Train Avg Loss: 0.0543 |Test Loss: 4.0122|lr = 0.00010\n",
      "Epoch: 3893|steps:   60|Train Avg Loss: 0.0418 |Test Loss: 4.0295|lr = 0.00010\n",
      "Epoch: 3894|steps:   30|Train Avg Loss: 0.0525 |Test Loss: 3.9671|lr = 0.00010\n",
      "Epoch: 3894|steps:   60|Train Avg Loss: 0.0502 |Test Loss: 4.0563|lr = 0.00010\n",
      "Epoch: 3895|steps:   30|Train Avg Loss: 0.0477 |Test Loss: 4.0236|lr = 0.00010\n",
      "Epoch: 3895|steps:   60|Train Avg Loss: 0.0500 |Test Loss: 4.0633|lr = 0.00010\n",
      "Epoch: 3896|steps:   30|Train Avg Loss: 0.0543 |Test Loss: 4.0241|lr = 0.00010\n",
      "Epoch: 3896|steps:   60|Train Avg Loss: 0.0411 |Test Loss: 3.9694|lr = 0.00010\n",
      "Epoch: 3897|steps:   30|Train Avg Loss: 0.0437 |Test Loss: 4.0601|lr = 0.00010\n",
      "Epoch: 3897|steps:   60|Train Avg Loss: 0.0514 |Test Loss: 4.0898|lr = 0.00010\n",
      "Epoch: 3898|steps:   30|Train Avg Loss: 0.0439 |Test Loss: 4.0418|lr = 0.00010\n",
      "Epoch: 3898|steps:   60|Train Avg Loss: 0.0575 |Test Loss: 4.0244|lr = 0.00010\n",
      "Epoch: 3899|steps:   30|Train Avg Loss: 0.0552 |Test Loss: 4.0349|lr = 0.00010\n",
      "Epoch: 3899|steps:   60|Train Avg Loss: 0.0562 |Test Loss: 4.0166|lr = 0.00010\n",
      "Epoch: 3900|steps:   30|Train Avg Loss: 0.0570 |Test Loss: 4.0287|lr = 0.00010\n",
      "Epoch: 3900|steps:   60|Train Avg Loss: 0.0566 |Test Loss: 4.0831|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3901|steps:   30|Train Avg Loss: 0.0401 |Test Loss: 4.0399|lr = 0.00010\n",
      "Epoch: 3901|steps:   60|Train Avg Loss: 0.0475 |Test Loss: 4.0776|lr = 0.00010\n",
      "Epoch: 3902|steps:   30|Train Avg Loss: 0.0514 |Test Loss: 4.1071|lr = 0.00010\n",
      "Epoch: 3902|steps:   60|Train Avg Loss: 0.0528 |Test Loss: 4.0949|lr = 0.00010\n",
      "Epoch: 3903|steps:   30|Train Avg Loss: 0.0559 |Test Loss: 4.1059|lr = 0.00010\n",
      "Epoch: 3903|steps:   60|Train Avg Loss: 0.0503 |Test Loss: 4.0821|lr = 0.00010\n",
      "Epoch: 3904|steps:   30|Train Avg Loss: 0.0475 |Test Loss: 4.1325|lr = 0.00010\n",
      "Epoch: 3904|steps:   60|Train Avg Loss: 0.0531 |Test Loss: 4.1543|lr = 0.00010\n",
      "Epoch: 3905|steps:   30|Train Avg Loss: 0.0603 |Test Loss: 4.1017|lr = 0.00010\n",
      "Epoch: 3905|steps:   60|Train Avg Loss: 0.1505 |Test Loss: 4.1159|lr = 0.00010\n",
      "Epoch: 3906|steps:   30|Train Avg Loss: 0.1188 |Test Loss: 4.1024|lr = 0.00010\n",
      "Epoch: 3906|steps:   60|Train Avg Loss: 0.1863 |Test Loss: 3.9762|lr = 0.00010\n",
      "Epoch: 3907|steps:   30|Train Avg Loss: 0.0776 |Test Loss: 3.9274|lr = 0.00010\n",
      "Epoch: 3907|steps:   60|Train Avg Loss: 0.0803 |Test Loss: 4.1679|lr = 0.00010\n",
      "Epoch: 3908|steps:   30|Train Avg Loss: 0.0900 |Test Loss: 4.0329|lr = 0.00010\n",
      "Epoch: 3908|steps:   60|Train Avg Loss: 0.0859 |Test Loss: 4.0079|lr = 0.00010\n",
      "Epoch: 3909|steps:   30|Train Avg Loss: 0.0453 |Test Loss: 3.9619|lr = 0.00010\n",
      "Epoch: 3909|steps:   60|Train Avg Loss: 0.0626 |Test Loss: 4.0383|lr = 0.00010\n",
      "Epoch: 3910|steps:   30|Train Avg Loss: 0.0395 |Test Loss: 4.0712|lr = 0.00010\n",
      "Epoch: 3910|steps:   60|Train Avg Loss: 0.0497 |Test Loss: 3.9885|lr = 0.00010\n",
      "Epoch: 3911|steps:   30|Train Avg Loss: 0.0390 |Test Loss: 4.0362|lr = 0.00010\n",
      "Epoch: 3911|steps:   60|Train Avg Loss: 0.0500 |Test Loss: 3.9823|lr = 0.00010\n",
      "Epoch: 3912|steps:   30|Train Avg Loss: 0.0539 |Test Loss: 3.9810|lr = 0.00010\n",
      "Epoch: 3912|steps:   60|Train Avg Loss: 0.0368 |Test Loss: 4.0831|lr = 0.00010\n",
      "Epoch: 3913|steps:   30|Train Avg Loss: 0.0352 |Test Loss: 4.0145|lr = 0.00010\n",
      "Epoch: 3913|steps:   60|Train Avg Loss: 0.0453 |Test Loss: 4.0273|lr = 0.00010\n",
      "Epoch: 3914|steps:   30|Train Avg Loss: 0.0376 |Test Loss: 4.0400|lr = 0.00010\n",
      "Epoch: 3914|steps:   60|Train Avg Loss: 0.0426 |Test Loss: 4.0196|lr = 0.00010\n",
      "Epoch: 3915|steps:   30|Train Avg Loss: 0.0489 |Test Loss: 4.0952|lr = 0.00010\n",
      "Epoch: 3915|steps:   60|Train Avg Loss: 0.0454 |Test Loss: 4.0433|lr = 0.00010\n",
      "Epoch: 3916|steps:   30|Train Avg Loss: 0.0562 |Test Loss: 4.0266|lr = 0.00010\n",
      "Epoch: 3916|steps:   60|Train Avg Loss: 0.0443 |Test Loss: 4.0733|lr = 0.00010\n",
      "Epoch: 3917|steps:   30|Train Avg Loss: 0.0399 |Test Loss: 4.0547|lr = 0.00010\n",
      "Epoch: 3917|steps:   60|Train Avg Loss: 0.0474 |Test Loss: 4.0454|lr = 0.00010\n",
      "Epoch: 3918|steps:   30|Train Avg Loss: 0.0355 |Test Loss: 4.0017|lr = 0.00010\n",
      "Epoch: 3918|steps:   60|Train Avg Loss: 0.0531 |Test Loss: 4.0337|lr = 0.00010\n",
      "Epoch: 3919|steps:   30|Train Avg Loss: 0.0479 |Test Loss: 4.0290|lr = 0.00010\n",
      "Epoch: 3919|steps:   60|Train Avg Loss: 0.0456 |Test Loss: 4.0547|lr = 0.00010\n",
      "Epoch: 3920|steps:   30|Train Avg Loss: 0.0436 |Test Loss: 4.0102|lr = 0.00010\n",
      "Epoch: 3920|steps:   60|Train Avg Loss: 0.0480 |Test Loss: 4.0636|lr = 0.00010\n",
      "Epoch: 3921|steps:   30|Train Avg Loss: 0.0420 |Test Loss: 4.0859|lr = 0.00010\n",
      "Epoch: 3921|steps:   60|Train Avg Loss: 0.0439 |Test Loss: 4.0585|lr = 0.00010\n",
      "Epoch: 3922|steps:   30|Train Avg Loss: 0.0379 |Test Loss: 4.0858|lr = 0.00010\n",
      "Epoch: 3922|steps:   60|Train Avg Loss: 0.0551 |Test Loss: 4.0461|lr = 0.00010\n",
      "Epoch: 3923|steps:   30|Train Avg Loss: 0.0382 |Test Loss: 4.0276|lr = 0.00010\n",
      "Epoch: 3923|steps:   60|Train Avg Loss: 0.0513 |Test Loss: 4.0908|lr = 0.00010\n",
      "Epoch: 3924|steps:   30|Train Avg Loss: 0.0384 |Test Loss: 4.0699|lr = 0.00010\n",
      "Epoch: 3924|steps:   60|Train Avg Loss: 0.0456 |Test Loss: 4.0768|lr = 0.00010\n",
      "Epoch: 3925|steps:   30|Train Avg Loss: 0.0455 |Test Loss: 4.1763|lr = 0.00010\n",
      "Epoch: 3925|steps:   60|Train Avg Loss: 0.1063 |Test Loss: 4.0543|lr = 0.00010\n",
      "Epoch: 3926|steps:   30|Train Avg Loss: 0.1975 |Test Loss: 4.2813|lr = 0.00010\n",
      "Epoch: 3926|steps:   60|Train Avg Loss: 0.1262 |Test Loss: 4.0694|lr = 0.00010\n",
      "Epoch: 3927|steps:   30|Train Avg Loss: 0.1983 |Test Loss: 4.0249|lr = 0.00010\n",
      "Epoch: 3927|steps:   60|Train Avg Loss: 0.1989 |Test Loss: 4.0767|lr = 0.00010\n",
      "Epoch: 3928|steps:   30|Train Avg Loss: 0.1119 |Test Loss: 4.1458|lr = 0.00010\n",
      "Epoch: 3928|steps:   60|Train Avg Loss: 0.0728 |Test Loss: 4.1103|lr = 0.00010\n",
      "Epoch: 3929|steps:   30|Train Avg Loss: 0.0555 |Test Loss: 3.9701|lr = 0.00010\n",
      "Epoch: 3929|steps:   60|Train Avg Loss: 0.0415 |Test Loss: 3.9419|lr = 0.00010\n",
      "Epoch: 3930|steps:   30|Train Avg Loss: 0.0458 |Test Loss: 3.9766|lr = 0.00010\n",
      "Epoch: 3930|steps:   60|Train Avg Loss: 0.0434 |Test Loss: 3.9881|lr = 0.00010\n",
      "Epoch: 3931|steps:   30|Train Avg Loss: 0.0381 |Test Loss: 4.0209|lr = 0.00010\n",
      "Epoch: 3931|steps:   60|Train Avg Loss: 0.0481 |Test Loss: 4.0399|lr = 0.00010\n",
      "Epoch: 3932|steps:   30|Train Avg Loss: 0.0434 |Test Loss: 4.0148|lr = 0.00010\n",
      "Epoch: 3932|steps:   60|Train Avg Loss: 0.0400 |Test Loss: 4.0581|lr = 0.00010\n",
      "Epoch: 3933|steps:   30|Train Avg Loss: 0.0393 |Test Loss: 4.0466|lr = 0.00010\n",
      "Epoch: 3933|steps:   60|Train Avg Loss: 0.0365 |Test Loss: 4.0379|lr = 0.00010\n",
      "Epoch: 3934|steps:   30|Train Avg Loss: 0.0420 |Test Loss: 4.0727|lr = 0.00010\n",
      "Epoch: 3934|steps:   60|Train Avg Loss: 0.0408 |Test Loss: 4.0996|lr = 0.00010\n",
      "Epoch: 3935|steps:   30|Train Avg Loss: 0.0358 |Test Loss: 4.0815|lr = 0.00010\n",
      "Epoch: 3935|steps:   60|Train Avg Loss: 0.0414 |Test Loss: 4.0433|lr = 0.00010\n",
      "Epoch: 3936|steps:   30|Train Avg Loss: 0.0340 |Test Loss: 4.1304|lr = 0.00010\n",
      "Epoch: 3936|steps:   60|Train Avg Loss: 0.0468 |Test Loss: 4.0686|lr = 0.00010\n",
      "Epoch: 3937|steps:   30|Train Avg Loss: 0.0364 |Test Loss: 4.0479|lr = 0.00010\n",
      "Epoch: 3937|steps:   60|Train Avg Loss: 0.0421 |Test Loss: 4.0627|lr = 0.00010\n",
      "Epoch: 3938|steps:   30|Train Avg Loss: 0.0365 |Test Loss: 4.0425|lr = 0.00010\n",
      "Epoch: 3938|steps:   60|Train Avg Loss: 0.0447 |Test Loss: 4.1263|lr = 0.00010\n",
      "Epoch: 3939|steps:   30|Train Avg Loss: 0.0402 |Test Loss: 4.0903|lr = 0.00010\n",
      "Epoch: 3939|steps:   60|Train Avg Loss: 0.0462 |Test Loss: 4.1484|lr = 0.00010\n",
      "Epoch: 3940|steps:   30|Train Avg Loss: 0.0414 |Test Loss: 4.1286|lr = 0.00010\n",
      "Epoch: 3940|steps:   60|Train Avg Loss: 0.0373 |Test Loss: 4.0027|lr = 0.00010\n",
      "Epoch: 3941|steps:   30|Train Avg Loss: 0.0431 |Test Loss: 4.0947|lr = 0.00010\n",
      "Epoch: 3941|steps:   60|Train Avg Loss: 0.0393 |Test Loss: 4.0588|lr = 0.00010\n",
      "Epoch: 3942|steps:   30|Train Avg Loss: 0.0429 |Test Loss: 4.0800|lr = 0.00010\n",
      "Epoch: 3942|steps:   60|Train Avg Loss: 0.0406 |Test Loss: 4.0848|lr = 0.00010\n",
      "Epoch: 3943|steps:   30|Train Avg Loss: 0.0395 |Test Loss: 4.1419|lr = 0.00010\n",
      "Epoch: 3943|steps:   60|Train Avg Loss: 0.0435 |Test Loss: 4.0561|lr = 0.00010\n",
      "Epoch: 3944|steps:   30|Train Avg Loss: 0.0406 |Test Loss: 4.0470|lr = 0.00010\n",
      "Epoch: 3944|steps:   60|Train Avg Loss: 0.0458 |Test Loss: 4.1144|lr = 0.00010\n",
      "Epoch: 3945|steps:   30|Train Avg Loss: 0.1498 |Test Loss: 4.0982|lr = 0.00010\n",
      "Epoch: 3945|steps:   60|Train Avg Loss: 0.2139 |Test Loss: 4.2267|lr = 0.00010\n",
      "Epoch: 3946|steps:   30|Train Avg Loss: 0.1539 |Test Loss: 3.9898|lr = 0.00010\n",
      "Epoch: 3946|steps:   60|Train Avg Loss: 0.1207 |Test Loss: 4.1445|lr = 0.00010\n",
      "Epoch: 3947|steps:   30|Train Avg Loss: 0.0890 |Test Loss: 4.0577|lr = 0.00010\n",
      "Epoch: 3947|steps:   60|Train Avg Loss: 0.0681 |Test Loss: 4.0908|lr = 0.00010\n",
      "Epoch: 3948|steps:   30|Train Avg Loss: 0.0400 |Test Loss: 4.0161|lr = 0.00010\n",
      "Epoch: 3948|steps:   60|Train Avg Loss: 0.0486 |Test Loss: 4.1063|lr = 0.00010\n",
      "Epoch: 3949|steps:   30|Train Avg Loss: 0.0465 |Test Loss: 4.1415|lr = 0.00010\n",
      "Epoch: 3949|steps:   60|Train Avg Loss: 0.0426 |Test Loss: 4.0469|lr = 0.00010\n",
      "Epoch: 3950|steps:   30|Train Avg Loss: 0.0442 |Test Loss: 4.0685|lr = 0.00010\n",
      "Epoch: 3950|steps:   60|Train Avg Loss: 0.0454 |Test Loss: 4.0717|lr = 0.00010\n",
      "Epoch: 3951|steps:   30|Train Avg Loss: 0.0411 |Test Loss: 4.0582|lr = 0.00010\n",
      "Epoch: 3951|steps:   60|Train Avg Loss: 0.0385 |Test Loss: 4.0237|lr = 0.00010\n",
      "Epoch: 3952|steps:   30|Train Avg Loss: 0.0398 |Test Loss: 4.0904|lr = 0.00010\n",
      "Epoch: 3952|steps:   60|Train Avg Loss: 0.0396 |Test Loss: 4.0198|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3953|steps:   30|Train Avg Loss: 0.0396 |Test Loss: 4.0543|lr = 0.00010\n",
      "Epoch: 3953|steps:   60|Train Avg Loss: 0.0495 |Test Loss: 4.1169|lr = 0.00010\n",
      "Epoch: 3954|steps:   30|Train Avg Loss: 0.0402 |Test Loss: 4.1455|lr = 0.00010\n",
      "Epoch: 3954|steps:   60|Train Avg Loss: 0.0392 |Test Loss: 4.0537|lr = 0.00010\n",
      "Epoch: 3955|steps:   30|Train Avg Loss: 0.0307 |Test Loss: 4.1145|lr = 0.00010\n",
      "Epoch: 3955|steps:   60|Train Avg Loss: 0.0450 |Test Loss: 4.0784|lr = 0.00010\n",
      "Epoch: 3956|steps:   30|Train Avg Loss: 0.0396 |Test Loss: 4.1441|lr = 0.00010\n",
      "Epoch: 3956|steps:   60|Train Avg Loss: 0.0405 |Test Loss: 4.1819|lr = 0.00010\n",
      "Epoch: 3957|steps:   30|Train Avg Loss: 0.0379 |Test Loss: 4.1841|lr = 0.00010\n",
      "Epoch: 3957|steps:   60|Train Avg Loss: 0.0400 |Test Loss: 4.1596|lr = 0.00010\n",
      "Epoch: 3958|steps:   30|Train Avg Loss: 0.0470 |Test Loss: 4.1447|lr = 0.00010\n",
      "Epoch: 3958|steps:   60|Train Avg Loss: 0.0359 |Test Loss: 4.1395|lr = 0.00010\n",
      "Epoch: 3959|steps:   30|Train Avg Loss: 0.0280 |Test Loss: 4.1791|lr = 0.00010\n",
      "Epoch: 3959|steps:   60|Train Avg Loss: 0.0461 |Test Loss: 4.1059|lr = 0.00010\n",
      "Epoch: 3960|steps:   30|Train Avg Loss: 0.0329 |Test Loss: 4.1309|lr = 0.00010\n",
      "Epoch: 3960|steps:   60|Train Avg Loss: 0.0426 |Test Loss: 4.0687|lr = 0.00010\n",
      "Epoch: 3961|steps:   30|Train Avg Loss: 0.0407 |Test Loss: 4.1513|lr = 0.00010\n",
      "Epoch: 3961|steps:   60|Train Avg Loss: 0.0458 |Test Loss: 4.0849|lr = 0.00010\n",
      "Epoch: 3962|steps:   30|Train Avg Loss: 0.0398 |Test Loss: 4.0900|lr = 0.00010\n",
      "Epoch: 3962|steps:   60|Train Avg Loss: 0.0398 |Test Loss: 4.1467|lr = 0.00010\n",
      "Epoch: 3963|steps:   30|Train Avg Loss: 0.0371 |Test Loss: 4.0944|lr = 0.00010\n",
      "Epoch: 3963|steps:   60|Train Avg Loss: 0.0437 |Test Loss: 4.1005|lr = 0.00010\n",
      "Epoch: 3964|steps:   30|Train Avg Loss: 0.0375 |Test Loss: 4.1794|lr = 0.00010\n",
      "Epoch: 3964|steps:   60|Train Avg Loss: 0.0346 |Test Loss: 4.1168|lr = 0.00010\n",
      "Epoch: 3965|steps:   30|Train Avg Loss: 0.0754 |Test Loss: 4.0704|lr = 0.00010\n",
      "Epoch: 3965|steps:   60|Train Avg Loss: 0.0971 |Test Loss: 4.3811|lr = 0.00010\n",
      "Epoch: 3966|steps:   30|Train Avg Loss: 0.0869 |Test Loss: 4.0265|lr = 0.00010\n",
      "Epoch: 3966|steps:   60|Train Avg Loss: 0.1590 |Test Loss: 4.0694|lr = 0.00010\n",
      "Epoch: 3967|steps:   30|Train Avg Loss: 0.3363 |Test Loss: 4.2893|lr = 0.00010\n",
      "Epoch: 3967|steps:   60|Train Avg Loss: 0.1772 |Test Loss: 4.1530|lr = 0.00010\n",
      "Epoch: 3968|steps:   30|Train Avg Loss: 0.0834 |Test Loss: 4.1424|lr = 0.00010\n",
      "Epoch: 3968|steps:   60|Train Avg Loss: 0.0953 |Test Loss: 4.3278|lr = 0.00010\n",
      "Epoch: 3969|steps:   30|Train Avg Loss: 0.0392 |Test Loss: 4.1420|lr = 0.00010\n",
      "Epoch: 3969|steps:   60|Train Avg Loss: 0.0471 |Test Loss: 4.1732|lr = 0.00010\n",
      "Epoch: 3970|steps:   30|Train Avg Loss: 0.0301 |Test Loss: 4.1327|lr = 0.00010\n",
      "Epoch: 3970|steps:   60|Train Avg Loss: 0.0438 |Test Loss: 4.1292|lr = 0.00010\n",
      "Epoch: 3971|steps:   30|Train Avg Loss: 0.0352 |Test Loss: 4.1588|lr = 0.00010\n",
      "Epoch: 3971|steps:   60|Train Avg Loss: 0.0330 |Test Loss: 4.1143|lr = 0.00010\n",
      "Epoch: 3972|steps:   30|Train Avg Loss: 0.0456 |Test Loss: 4.1546|lr = 0.00010\n",
      "Epoch: 3972|steps:   60|Train Avg Loss: 0.0330 |Test Loss: 4.1258|lr = 0.00010\n",
      "Epoch: 3973|steps:   30|Train Avg Loss: 0.0313 |Test Loss: 4.1151|lr = 0.00010\n",
      "Epoch: 3973|steps:   60|Train Avg Loss: 0.0382 |Test Loss: 4.1488|lr = 0.00010\n",
      "Epoch: 3974|steps:   30|Train Avg Loss: 0.0340 |Test Loss: 4.1082|lr = 0.00010\n",
      "Epoch: 3974|steps:   60|Train Avg Loss: 0.0391 |Test Loss: 4.1688|lr = 0.00010\n",
      "Epoch: 3975|steps:   30|Train Avg Loss: 0.0305 |Test Loss: 4.1361|lr = 0.00010\n",
      "Epoch: 3975|steps:   60|Train Avg Loss: 0.0379 |Test Loss: 4.1159|lr = 0.00010\n",
      "Epoch: 3976|steps:   30|Train Avg Loss: 0.0361 |Test Loss: 4.1596|lr = 0.00010\n",
      "Epoch: 3976|steps:   60|Train Avg Loss: 0.0349 |Test Loss: 4.1240|lr = 0.00010\n",
      "Epoch: 3977|steps:   30|Train Avg Loss: 0.0324 |Test Loss: 4.1147|lr = 0.00010\n",
      "Epoch: 3977|steps:   60|Train Avg Loss: 0.0375 |Test Loss: 4.1546|lr = 0.00010\n",
      "Epoch: 3978|steps:   30|Train Avg Loss: 0.0346 |Test Loss: 4.1155|lr = 0.00010\n",
      "Epoch: 3978|steps:   60|Train Avg Loss: 0.0365 |Test Loss: 4.1479|lr = 0.00010\n",
      "Epoch: 3979|steps:   30|Train Avg Loss: 0.0313 |Test Loss: 4.1757|lr = 0.00010\n",
      "Epoch: 3979|steps:   60|Train Avg Loss: 0.0368 |Test Loss: 4.1137|lr = 0.00010\n",
      "Epoch: 3980|steps:   30|Train Avg Loss: 0.0371 |Test Loss: 4.1283|lr = 0.00010\n",
      "Epoch: 3980|steps:   60|Train Avg Loss: 0.0347 |Test Loss: 4.1711|lr = 0.00010\n",
      "Epoch: 3981|steps:   30|Train Avg Loss: 0.0296 |Test Loss: 4.1811|lr = 0.00010\n",
      "Epoch: 3981|steps:   60|Train Avg Loss: 0.0498 |Test Loss: 4.0267|lr = 0.00010\n",
      "Epoch: 3982|steps:   30|Train Avg Loss: 0.0415 |Test Loss: 4.1415|lr = 0.00010\n",
      "Epoch: 3982|steps:   60|Train Avg Loss: 0.0455 |Test Loss: 4.2476|lr = 0.00010\n",
      "Epoch: 3983|steps:   30|Train Avg Loss: 0.0392 |Test Loss: 4.1324|lr = 0.00010\n",
      "Epoch: 3983|steps:   60|Train Avg Loss: 0.0337 |Test Loss: 4.0954|lr = 0.00010\n",
      "Epoch: 3984|steps:   30|Train Avg Loss: 0.0344 |Test Loss: 4.1758|lr = 0.00010\n",
      "Epoch: 3984|steps:   60|Train Avg Loss: 0.0390 |Test Loss: 4.1704|lr = 0.00010\n",
      "Epoch: 3985|steps:   30|Train Avg Loss: 0.0335 |Test Loss: 4.1338|lr = 0.00010\n",
      "Epoch: 3985|steps:   60|Train Avg Loss: 0.0396 |Test Loss: 4.1379|lr = 0.00010\n",
      "Epoch: 3986|steps:   30|Train Avg Loss: 0.0335 |Test Loss: 4.1084|lr = 0.00010\n",
      "Epoch: 3986|steps:   60|Train Avg Loss: 0.0407 |Test Loss: 4.2627|lr = 0.00010\n",
      "Epoch: 3987|steps:   30|Train Avg Loss: 0.0395 |Test Loss: 4.1265|lr = 0.00010\n",
      "Epoch: 3987|steps:   60|Train Avg Loss: 0.0370 |Test Loss: 4.1512|lr = 0.00010\n",
      "Epoch: 3988|steps:   30|Train Avg Loss: 0.0451 |Test Loss: 4.2129|lr = 0.00010\n",
      "Epoch: 3988|steps:   60|Train Avg Loss: 0.0450 |Test Loss: 4.1936|lr = 0.00010\n",
      "Epoch: 3989|steps:   30|Train Avg Loss: 0.0403 |Test Loss: 4.2299|lr = 0.00010\n",
      "Epoch: 3989|steps:   60|Train Avg Loss: 0.0393 |Test Loss: 4.1964|lr = 0.00010\n",
      "Epoch: 3990|steps:   30|Train Avg Loss: 0.0349 |Test Loss: 4.2690|lr = 0.00010\n",
      "Epoch: 3990|steps:   60|Train Avg Loss: 0.0450 |Test Loss: 4.1823|lr = 0.00010\n",
      "Epoch: 3991|steps:   30|Train Avg Loss: 0.0299 |Test Loss: 4.2644|lr = 0.00010\n",
      "Epoch: 3991|steps:   60|Train Avg Loss: 0.0401 |Test Loss: 4.0885|lr = 0.00010\n",
      "Epoch: 3992|steps:   30|Train Avg Loss: 0.0350 |Test Loss: 4.2341|lr = 0.00010\n",
      "Epoch: 3992|steps:   60|Train Avg Loss: 0.0379 |Test Loss: 4.2598|lr = 0.00010\n",
      "Epoch: 3993|steps:   30|Train Avg Loss: 0.0362 |Test Loss: 4.1747|lr = 0.00010\n",
      "Epoch: 3993|steps:   60|Train Avg Loss: 0.0342 |Test Loss: 4.1386|lr = 0.00010\n",
      "Epoch: 3994|steps:   30|Train Avg Loss: 0.0368 |Test Loss: 4.1764|lr = 0.00010\n",
      "Epoch: 3994|steps:   60|Train Avg Loss: 0.0297 |Test Loss: 4.2372|lr = 0.00010\n",
      "Epoch: 3995|steps:   30|Train Avg Loss: 0.0406 |Test Loss: 4.1433|lr = 0.00010\n",
      "Epoch: 3995|steps:   60|Train Avg Loss: 0.0340 |Test Loss: 4.2039|lr = 0.00010\n",
      "Epoch: 3996|steps:   30|Train Avg Loss: 0.0345 |Test Loss: 4.3372|lr = 0.00010\n",
      "Epoch: 3996|steps:   60|Train Avg Loss: 0.0491 |Test Loss: 4.2116|lr = 0.00010\n",
      "Epoch: 3997|steps:   30|Train Avg Loss: 0.0679 |Test Loss: 4.2107|lr = 0.00010\n",
      "Epoch: 3997|steps:   60|Train Avg Loss: 0.0946 |Test Loss: 4.3029|lr = 0.00010\n",
      "Epoch: 3998|steps:   30|Train Avg Loss: 0.2907 |Test Loss: 4.1786|lr = 0.00010\n",
      "Epoch: 3998|steps:   60|Train Avg Loss: 0.1748 |Test Loss: 4.2812|lr = 0.00010\n",
      "Epoch: 3999|steps:   30|Train Avg Loss: 0.1044 |Test Loss: 4.2540|lr = 0.00010\n",
      "Epoch: 3999|steps:   60|Train Avg Loss: 0.0675 |Test Loss: 4.1616|lr = 0.00010\n",
      "Epoch: 4000|steps:   30|Train Avg Loss: 0.0552 |Test Loss: 4.1337|lr = 0.00010\n",
      "Epoch: 4000|steps:   60|Train Avg Loss: 0.0574 |Test Loss: 4.1402|lr = 0.00010\n",
      "Epoch: 4001|steps:   30|Train Avg Loss: 0.0440 |Test Loss: 4.2360|lr = 0.00010\n",
      "Epoch: 4001|steps:   60|Train Avg Loss: 0.0355 |Test Loss: 4.1822|lr = 0.00010\n",
      "Epoch: 4002|steps:   30|Train Avg Loss: 0.0332 |Test Loss: 4.1276|lr = 0.00010\n",
      "Epoch: 4002|steps:   60|Train Avg Loss: 0.0311 |Test Loss: 4.1813|lr = 0.00010\n",
      "Epoch: 4003|steps:   30|Train Avg Loss: 0.0377 |Test Loss: 4.1447|lr = 0.00010\n",
      "Epoch: 4003|steps:   60|Train Avg Loss: 0.0311 |Test Loss: 4.1690|lr = 0.00010\n",
      "Epoch: 4004|steps:   30|Train Avg Loss: 0.0255 |Test Loss: 4.1548|lr = 0.00010\n",
      "Epoch: 4004|steps:   60|Train Avg Loss: 0.0377 |Test Loss: 4.0993|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4005|steps:   30|Train Avg Loss: 0.0339 |Test Loss: 4.1969|lr = 0.00010\n",
      "Epoch: 4005|steps:   60|Train Avg Loss: 0.0336 |Test Loss: 4.1717|lr = 0.00010\n",
      "Epoch: 4006|steps:   30|Train Avg Loss: 0.0302 |Test Loss: 4.1917|lr = 0.00010\n",
      "Epoch: 4006|steps:   60|Train Avg Loss: 0.0377 |Test Loss: 4.1300|lr = 0.00010\n",
      "Epoch: 4007|steps:   30|Train Avg Loss: 0.0348 |Test Loss: 4.1705|lr = 0.00010\n",
      "Epoch: 4007|steps:   60|Train Avg Loss: 0.0318 |Test Loss: 4.1310|lr = 0.00010\n",
      "Epoch: 4008|steps:   30|Train Avg Loss: 0.0340 |Test Loss: 4.1499|lr = 0.00010\n",
      "Epoch: 4008|steps:   60|Train Avg Loss: 0.0316 |Test Loss: 4.2228|lr = 0.00010\n",
      "Epoch: 4009|steps:   30|Train Avg Loss: 0.0330 |Test Loss: 4.1563|lr = 0.00010\n",
      "Epoch: 4009|steps:   60|Train Avg Loss: 0.0335 |Test Loss: 4.1477|lr = 0.00010\n",
      "Epoch: 4010|steps:   30|Train Avg Loss: 0.0289 |Test Loss: 4.1965|lr = 0.00010\n",
      "Epoch: 4010|steps:   60|Train Avg Loss: 0.0366 |Test Loss: 4.1522|lr = 0.00010\n",
      "Epoch: 4011|steps:   30|Train Avg Loss: 0.0383 |Test Loss: 4.2157|lr = 0.00010\n",
      "Epoch: 4011|steps:   60|Train Avg Loss: 0.0279 |Test Loss: 4.1522|lr = 0.00010\n",
      "Epoch: 4012|steps:   30|Train Avg Loss: 0.0331 |Test Loss: 4.1571|lr = 0.00010\n",
      "Epoch: 4012|steps:   60|Train Avg Loss: 0.0445 |Test Loss: 4.1783|lr = 0.00010\n",
      "Epoch: 4013|steps:   30|Train Avg Loss: 0.0397 |Test Loss: 4.2341|lr = 0.00010\n",
      "Epoch: 4013|steps:   60|Train Avg Loss: 0.0396 |Test Loss: 4.2510|lr = 0.00010\n",
      "Epoch: 4014|steps:   30|Train Avg Loss: 0.0855 |Test Loss: 4.1435|lr = 0.00010\n",
      "Epoch: 4014|steps:   60|Train Avg Loss: 0.0435 |Test Loss: 4.3187|lr = 0.00010\n",
      "Epoch: 4015|steps:   30|Train Avg Loss: 0.0503 |Test Loss: 4.2207|lr = 0.00010\n",
      "Epoch: 4015|steps:   60|Train Avg Loss: 0.0516 |Test Loss: 4.1837|lr = 0.00010\n",
      "Epoch: 4016|steps:   30|Train Avg Loss: 0.1077 |Test Loss: 4.3385|lr = 0.00010\n",
      "Epoch: 4016|steps:   60|Train Avg Loss: 0.1335 |Test Loss: 4.3000|lr = 0.00010\n",
      "Epoch: 4017|steps:   30|Train Avg Loss: 0.0850 |Test Loss: 4.0727|lr = 0.00010\n",
      "Epoch: 4017|steps:   60|Train Avg Loss: 0.1247 |Test Loss: 4.1857|lr = 0.00010\n",
      "Epoch: 4018|steps:   30|Train Avg Loss: 0.0689 |Test Loss: 4.1375|lr = 0.00010\n",
      "Epoch: 4018|steps:   60|Train Avg Loss: 0.0602 |Test Loss: 4.0544|lr = 0.00010\n",
      "Epoch: 4019|steps:   30|Train Avg Loss: 0.0493 |Test Loss: 4.1026|lr = 0.00010\n",
      "Epoch: 4019|steps:   60|Train Avg Loss: 0.0459 |Test Loss: 4.0977|lr = 0.00010\n",
      "Epoch: 4020|steps:   30|Train Avg Loss: 0.0394 |Test Loss: 4.1401|lr = 0.00010\n",
      "Epoch: 4020|steps:   60|Train Avg Loss: 0.0322 |Test Loss: 4.0542|lr = 0.00010\n",
      "Epoch: 4021|steps:   30|Train Avg Loss: 0.0366 |Test Loss: 4.1327|lr = 0.00010\n",
      "Epoch: 4021|steps:   60|Train Avg Loss: 0.0304 |Test Loss: 4.1482|lr = 0.00010\n",
      "Epoch: 4022|steps:   30|Train Avg Loss: 0.0342 |Test Loss: 4.2026|lr = 0.00010\n",
      "Epoch: 4022|steps:   60|Train Avg Loss: 0.0326 |Test Loss: 4.1765|lr = 0.00010\n",
      "Epoch: 4023|steps:   30|Train Avg Loss: 0.0313 |Test Loss: 4.1558|lr = 0.00010\n",
      "Epoch: 4023|steps:   60|Train Avg Loss: 0.0363 |Test Loss: 4.1499|lr = 0.00010\n",
      "Epoch: 4024|steps:   30|Train Avg Loss: 0.0285 |Test Loss: 4.1923|lr = 0.00010\n",
      "Epoch: 4024|steps:   60|Train Avg Loss: 0.0409 |Test Loss: 4.1521|lr = 0.00010\n",
      "Epoch: 4025|steps:   30|Train Avg Loss: 0.0318 |Test Loss: 4.1750|lr = 0.00010\n",
      "Epoch: 4025|steps:   60|Train Avg Loss: 0.0387 |Test Loss: 4.1948|lr = 0.00010\n",
      "Epoch: 4026|steps:   30|Train Avg Loss: 0.0288 |Test Loss: 4.1863|lr = 0.00010\n",
      "Epoch: 4026|steps:   60|Train Avg Loss: 0.0325 |Test Loss: 4.2071|lr = 0.00010\n",
      "Epoch: 4027|steps:   30|Train Avg Loss: 0.0276 |Test Loss: 4.1705|lr = 0.00010\n",
      "Epoch: 4027|steps:   60|Train Avg Loss: 0.0319 |Test Loss: 4.1343|lr = 0.00010\n",
      "Epoch: 4028|steps:   30|Train Avg Loss: 0.0322 |Test Loss: 4.1693|lr = 0.00010\n",
      "Epoch: 4028|steps:   60|Train Avg Loss: 0.0299 |Test Loss: 4.2041|lr = 0.00010\n",
      "Epoch: 4029|steps:   30|Train Avg Loss: 0.0362 |Test Loss: 4.2109|lr = 0.00010\n",
      "Epoch: 4029|steps:   60|Train Avg Loss: 0.0286 |Test Loss: 4.1418|lr = 0.00010\n",
      "Epoch: 4030|steps:   30|Train Avg Loss: 0.0343 |Test Loss: 4.2443|lr = 0.00010\n",
      "Epoch: 4030|steps:   60|Train Avg Loss: 0.0398 |Test Loss: 4.1434|lr = 0.00010\n",
      "Epoch: 4031|steps:   30|Train Avg Loss: 0.0348 |Test Loss: 4.2127|lr = 0.00010\n",
      "Epoch: 4031|steps:   60|Train Avg Loss: 0.0353 |Test Loss: 4.2454|lr = 0.00010\n",
      "Epoch: 4032|steps:   30|Train Avg Loss: 0.0290 |Test Loss: 4.1477|lr = 0.00010\n",
      "Epoch: 4032|steps:   60|Train Avg Loss: 0.0358 |Test Loss: 4.2085|lr = 0.00010\n",
      "Epoch: 4033|steps:   30|Train Avg Loss: 0.0340 |Test Loss: 4.2151|lr = 0.00010\n",
      "Epoch: 4033|steps:   60|Train Avg Loss: 0.0381 |Test Loss: 4.2243|lr = 0.00010\n",
      "Epoch: 4034|steps:   30|Train Avg Loss: 0.0313 |Test Loss: 4.1878|lr = 0.00010\n",
      "Epoch: 4034|steps:   60|Train Avg Loss: 0.0337 |Test Loss: 4.3303|lr = 0.00010\n",
      "Epoch: 4035|steps:   30|Train Avg Loss: 0.0377 |Test Loss: 4.2215|lr = 0.00010\n",
      "Epoch: 4035|steps:   60|Train Avg Loss: 0.0317 |Test Loss: 4.2366|lr = 0.00010\n",
      "Epoch: 4036|steps:   30|Train Avg Loss: 0.0351 |Test Loss: 4.3310|lr = 0.00010\n",
      "Epoch: 4036|steps:   60|Train Avg Loss: 0.0318 |Test Loss: 4.2496|lr = 0.00010\n",
      "Epoch: 4037|steps:   30|Train Avg Loss: 0.0382 |Test Loss: 4.2941|lr = 0.00010\n",
      "Epoch: 4037|steps:   60|Train Avg Loss: 0.0327 |Test Loss: 4.1901|lr = 0.00010\n",
      "Epoch: 4038|steps:   30|Train Avg Loss: 0.0731 |Test Loss: 4.4140|lr = 0.00010\n",
      "Epoch: 4038|steps:   60|Train Avg Loss: 0.1233 |Test Loss: 4.3714|lr = 0.00010\n",
      "Epoch: 4039|steps:   30|Train Avg Loss: 0.3412 |Test Loss: 4.3246|lr = 0.00010\n",
      "Epoch: 4039|steps:   60|Train Avg Loss: 0.2836 |Test Loss: 4.2090|lr = 0.00010\n",
      "Epoch: 4040|steps:   30|Train Avg Loss: 0.0869 |Test Loss: 4.0948|lr = 0.00010\n",
      "Epoch: 4040|steps:   60|Train Avg Loss: 0.0619 |Test Loss: 4.1183|lr = 0.00010\n",
      "Epoch: 4041|steps:   30|Train Avg Loss: 0.0430 |Test Loss: 4.1524|lr = 0.00010\n",
      "Epoch: 4041|steps:   60|Train Avg Loss: 0.0366 |Test Loss: 4.1038|lr = 0.00010\n",
      "Epoch: 4042|steps:   30|Train Avg Loss: 0.0356 |Test Loss: 4.0697|lr = 0.00010\n",
      "Epoch: 4042|steps:   60|Train Avg Loss: 0.0406 |Test Loss: 4.0907|lr = 0.00010\n",
      "Epoch: 4043|steps:   30|Train Avg Loss: 0.0318 |Test Loss: 4.1216|lr = 0.00010\n",
      "Epoch: 4043|steps:   60|Train Avg Loss: 0.0372 |Test Loss: 4.1234|lr = 0.00010\n",
      "Epoch: 4044|steps:   30|Train Avg Loss: 0.0243 |Test Loss: 4.1266|lr = 0.00010\n",
      "Epoch: 4044|steps:   60|Train Avg Loss: 0.0378 |Test Loss: 4.1574|lr = 0.00010\n",
      "Epoch: 4045|steps:   30|Train Avg Loss: 0.0347 |Test Loss: 4.1707|lr = 0.00010\n",
      "Epoch: 4045|steps:   60|Train Avg Loss: 0.0307 |Test Loss: 4.1588|lr = 0.00010\n",
      "Epoch: 4046|steps:   30|Train Avg Loss: 0.0273 |Test Loss: 4.1513|lr = 0.00010\n",
      "Epoch: 4046|steps:   60|Train Avg Loss: 0.0307 |Test Loss: 4.1895|lr = 0.00010\n",
      "Epoch: 4047|steps:   30|Train Avg Loss: 0.0225 |Test Loss: 4.1843|lr = 0.00010\n",
      "Epoch: 4047|steps:   60|Train Avg Loss: 0.0395 |Test Loss: 4.1896|lr = 0.00010\n",
      "Epoch: 4048|steps:   30|Train Avg Loss: 0.0308 |Test Loss: 4.1650|lr = 0.00010\n",
      "Epoch: 4048|steps:   60|Train Avg Loss: 0.0354 |Test Loss: 4.1049|lr = 0.00010\n",
      "Epoch: 4049|steps:   30|Train Avg Loss: 0.0347 |Test Loss: 4.2509|lr = 0.00010\n",
      "Epoch: 4049|steps:   60|Train Avg Loss: 0.0404 |Test Loss: 4.2128|lr = 0.00010\n",
      "Epoch: 4050|steps:   30|Train Avg Loss: 0.0269 |Test Loss: 4.1973|lr = 0.00010\n",
      "Epoch: 4050|steps:   60|Train Avg Loss: 0.0360 |Test Loss: 4.2024|lr = 0.00010\n",
      "Epoch: 4051|steps:   30|Train Avg Loss: 0.0446 |Test Loss: 4.2194|lr = 0.00010\n",
      "Epoch: 4051|steps:   60|Train Avg Loss: 0.0888 |Test Loss: 4.2075|lr = 0.00010\n",
      "Epoch: 4052|steps:   30|Train Avg Loss: 0.0706 |Test Loss: 4.1317|lr = 0.00010\n",
      "Epoch: 4052|steps:   60|Train Avg Loss: 0.0520 |Test Loss: 4.2886|lr = 0.00010\n",
      "Epoch: 4053|steps:   30|Train Avg Loss: 0.0288 |Test Loss: 4.2396|lr = 0.00010\n",
      "Epoch: 4053|steps:   60|Train Avg Loss: 0.0360 |Test Loss: 4.2703|lr = 0.00010\n",
      "Epoch: 4054|steps:   30|Train Avg Loss: 0.0325 |Test Loss: 4.2695|lr = 0.00010\n",
      "Epoch: 4054|steps:   60|Train Avg Loss: 0.0292 |Test Loss: 4.3362|lr = 0.00010\n",
      "Epoch: 4055|steps:   30|Train Avg Loss: 0.0373 |Test Loss: 4.2649|lr = 0.00010\n",
      "Epoch: 4055|steps:   60|Train Avg Loss: 0.0320 |Test Loss: 4.2873|lr = 0.00010\n",
      "Epoch: 4056|steps:   30|Train Avg Loss: 0.0297 |Test Loss: 4.2450|lr = 0.00010\n",
      "Epoch: 4056|steps:   60|Train Avg Loss: 0.0283 |Test Loss: 4.2315|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4057|steps:   30|Train Avg Loss: 0.0351 |Test Loss: 4.2780|lr = 0.00010\n",
      "Epoch: 4057|steps:   60|Train Avg Loss: 0.0306 |Test Loss: 4.2818|lr = 0.00010\n",
      "Epoch: 4058|steps:   30|Train Avg Loss: 0.0239 |Test Loss: 4.2843|lr = 0.00010\n",
      "Epoch: 4058|steps:   60|Train Avg Loss: 0.0339 |Test Loss: 4.2786|lr = 0.00010\n",
      "Epoch: 4059|steps:   30|Train Avg Loss: 0.0361 |Test Loss: 4.2432|lr = 0.00010\n",
      "Epoch: 4059|steps:   60|Train Avg Loss: 0.0454 |Test Loss: 4.3125|lr = 0.00010\n",
      "Epoch: 4060|steps:   30|Train Avg Loss: 0.0319 |Test Loss: 4.2875|lr = 0.00010\n",
      "Epoch: 4060|steps:   60|Train Avg Loss: 0.0342 |Test Loss: 4.2637|lr = 0.00010\n",
      "Epoch: 4061|steps:   30|Train Avg Loss: 0.0416 |Test Loss: 4.2827|lr = 0.00010\n",
      "Epoch: 4061|steps:   60|Train Avg Loss: 0.0379 |Test Loss: 4.2559|lr = 0.00010\n",
      "Epoch: 4062|steps:   30|Train Avg Loss: 0.0388 |Test Loss: 4.2686|lr = 0.00010\n",
      "Epoch: 4062|steps:   60|Train Avg Loss: 0.0396 |Test Loss: 4.1788|lr = 0.00010\n",
      "Epoch: 4063|steps:   30|Train Avg Loss: 0.0383 |Test Loss: 4.3677|lr = 0.00010\n",
      "Epoch: 4063|steps:   60|Train Avg Loss: 0.0359 |Test Loss: 4.3120|lr = 0.00010\n",
      "Epoch: 4064|steps:   30|Train Avg Loss: 0.0450 |Test Loss: 4.3919|lr = 0.00010\n",
      "Epoch: 4064|steps:   60|Train Avg Loss: 0.0634 |Test Loss: 4.3057|lr = 0.00010\n",
      "Epoch: 4065|steps:   30|Train Avg Loss: 0.1125 |Test Loss: 4.4268|lr = 0.00010\n",
      "Epoch: 4065|steps:   60|Train Avg Loss: 0.0746 |Test Loss: 4.4451|lr = 0.00010\n",
      "Epoch: 4066|steps:   30|Train Avg Loss: 0.1105 |Test Loss: 4.3555|lr = 0.00010\n",
      "Epoch: 4066|steps:   60|Train Avg Loss: 0.0596 |Test Loss: 4.2429|lr = 0.00010\n",
      "Epoch: 4067|steps:   30|Train Avg Loss: 0.0403 |Test Loss: 4.3227|lr = 0.00010\n",
      "Epoch: 4067|steps:   60|Train Avg Loss: 0.0495 |Test Loss: 4.3143|lr = 0.00010\n",
      "Epoch: 4068|steps:   30|Train Avg Loss: 0.0626 |Test Loss: 4.3016|lr = 0.00010\n",
      "Epoch: 4068|steps:   60|Train Avg Loss: 0.0547 |Test Loss: 4.4601|lr = 0.00010\n",
      "Epoch: 4069|steps:   30|Train Avg Loss: 0.0310 |Test Loss: 4.3466|lr = 0.00010\n",
      "Epoch: 4069|steps:   60|Train Avg Loss: 0.0394 |Test Loss: 4.3816|lr = 0.00010\n",
      "Epoch: 4070|steps:   30|Train Avg Loss: 0.0290 |Test Loss: 4.3556|lr = 0.00010\n",
      "Epoch: 4070|steps:   60|Train Avg Loss: 0.0340 |Test Loss: 4.3074|lr = 0.00010\n",
      "Epoch: 4071|steps:   30|Train Avg Loss: 0.0266 |Test Loss: 4.4175|lr = 0.00010\n",
      "Epoch: 4071|steps:   60|Train Avg Loss: 0.0328 |Test Loss: 4.4040|lr = 0.00010\n",
      "Epoch: 4072|steps:   30|Train Avg Loss: 0.0374 |Test Loss: 4.3506|lr = 0.00010\n",
      "Epoch: 4072|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 4.3675|lr = 0.00010\n",
      "Epoch: 4073|steps:   30|Train Avg Loss: 0.0317 |Test Loss: 4.4496|lr = 0.00010\n",
      "Epoch: 4073|steps:   60|Train Avg Loss: 0.0348 |Test Loss: 4.2944|lr = 0.00010\n",
      "Epoch: 4074|steps:   30|Train Avg Loss: 0.0248 |Test Loss: 4.3479|lr = 0.00010\n",
      "Epoch: 4074|steps:   60|Train Avg Loss: 0.0306 |Test Loss: 4.3473|lr = 0.00010\n",
      "Epoch: 4075|steps:   30|Train Avg Loss: 0.0333 |Test Loss: 4.3476|lr = 0.00010\n",
      "Epoch: 4075|steps:   60|Train Avg Loss: 0.0282 |Test Loss: 4.3837|lr = 0.00010\n",
      "Epoch: 4076|steps:   30|Train Avg Loss: 0.0362 |Test Loss: 4.3945|lr = 0.00010\n",
      "Epoch: 4076|steps:   60|Train Avg Loss: 0.0305 |Test Loss: 4.4249|lr = 0.00010\n",
      "Epoch: 4077|steps:   30|Train Avg Loss: 0.0236 |Test Loss: 4.4317|lr = 0.00010\n",
      "Epoch: 4077|steps:   60|Train Avg Loss: 0.0360 |Test Loss: 4.4333|lr = 0.00010\n",
      "Epoch: 4078|steps:   30|Train Avg Loss: 0.0274 |Test Loss: 4.3826|lr = 0.00010\n",
      "Epoch: 4078|steps:   60|Train Avg Loss: 0.0365 |Test Loss: 4.4478|lr = 0.00010\n",
      "Epoch: 4079|steps:   30|Train Avg Loss: 0.0300 |Test Loss: 4.3387|lr = 0.00010\n",
      "Epoch: 4079|steps:   60|Train Avg Loss: 0.0295 |Test Loss: 4.4254|lr = 0.00010\n",
      "Epoch: 4080|steps:   30|Train Avg Loss: 0.0344 |Test Loss: 4.4786|lr = 0.00010\n",
      "Epoch: 4080|steps:   60|Train Avg Loss: 0.0345 |Test Loss: 4.4232|lr = 0.00010\n",
      "Epoch: 4081|steps:   30|Train Avg Loss: 0.0327 |Test Loss: 4.4580|lr = 0.00010\n",
      "Epoch: 4081|steps:   60|Train Avg Loss: 0.0337 |Test Loss: 4.4895|lr = 0.00010\n",
      "Epoch: 4082|steps:   30|Train Avg Loss: 0.0245 |Test Loss: 4.3635|lr = 0.00010\n",
      "Epoch: 4082|steps:   60|Train Avg Loss: 0.0418 |Test Loss: 4.3683|lr = 0.00010\n",
      "Epoch: 4083|steps:   30|Train Avg Loss: 0.0414 |Test Loss: 4.3901|lr = 0.00010\n",
      "Epoch: 4083|steps:   60|Train Avg Loss: 0.1093 |Test Loss: 4.6604|lr = 0.00010\n",
      "Epoch: 4084|steps:   30|Train Avg Loss: 0.5791 |Test Loss: 4.7748|lr = 0.00010\n",
      "Epoch: 4084|steps:   60|Train Avg Loss: 0.3023 |Test Loss: 4.4589|lr = 0.00010\n",
      "Epoch: 4085|steps:   30|Train Avg Loss: 0.0986 |Test Loss: 4.4019|lr = 0.00010\n",
      "Epoch: 4085|steps:   60|Train Avg Loss: 0.0587 |Test Loss: 4.4289|lr = 0.00010\n",
      "Epoch: 4086|steps:   30|Train Avg Loss: 0.0579 |Test Loss: 4.2905|lr = 0.00010\n",
      "Epoch: 4086|steps:   60|Train Avg Loss: 0.0424 |Test Loss: 4.2928|lr = 0.00010\n",
      "Epoch: 4087|steps:   30|Train Avg Loss: 0.0345 |Test Loss: 4.4181|lr = 0.00010\n",
      "Epoch: 4087|steps:   60|Train Avg Loss: 0.0315 |Test Loss: 4.4074|lr = 0.00010\n",
      "Epoch: 4088|steps:   30|Train Avg Loss: 0.0269 |Test Loss: 4.3852|lr = 0.00010\n",
      "Epoch: 4088|steps:   60|Train Avg Loss: 0.0295 |Test Loss: 4.3547|lr = 0.00010\n",
      "Epoch: 4089|steps:   30|Train Avg Loss: 0.0259 |Test Loss: 4.3450|lr = 0.00010\n",
      "Epoch: 4089|steps:   60|Train Avg Loss: 0.0268 |Test Loss: 4.3949|lr = 0.00010\n",
      "Epoch: 4090|steps:   30|Train Avg Loss: 0.0276 |Test Loss: 4.3198|lr = 0.00010\n",
      "Epoch: 4090|steps:   60|Train Avg Loss: 0.0299 |Test Loss: 4.3545|lr = 0.00010\n",
      "Epoch: 4091|steps:   30|Train Avg Loss: 0.0228 |Test Loss: 4.3699|lr = 0.00010\n",
      "Epoch: 4091|steps:   60|Train Avg Loss: 0.0281 |Test Loss: 4.3447|lr = 0.00010\n",
      "Epoch: 4092|steps:   30|Train Avg Loss: 0.0312 |Test Loss: 4.3595|lr = 0.00010\n",
      "Epoch: 4092|steps:   60|Train Avg Loss: 0.0231 |Test Loss: 4.3618|lr = 0.00010\n",
      "Epoch: 4093|steps:   30|Train Avg Loss: 0.0259 |Test Loss: 4.3252|lr = 0.00010\n",
      "Epoch: 4093|steps:   60|Train Avg Loss: 0.0267 |Test Loss: 4.3589|lr = 0.00010\n",
      "Epoch: 4094|steps:   30|Train Avg Loss: 0.0250 |Test Loss: 4.3109|lr = 0.00010\n",
      "Epoch: 4094|steps:   60|Train Avg Loss: 0.0234 |Test Loss: 4.3830|lr = 0.00010\n",
      "Epoch: 4095|steps:   30|Train Avg Loss: 0.0265 |Test Loss: 4.3326|lr = 0.00010\n",
      "Epoch: 4095|steps:   60|Train Avg Loss: 0.0283 |Test Loss: 4.3443|lr = 0.00010\n",
      "Epoch: 4096|steps:   30|Train Avg Loss: 0.0278 |Test Loss: 4.3400|lr = 0.00010\n",
      "Epoch: 4096|steps:   60|Train Avg Loss: 0.0322 |Test Loss: 4.3055|lr = 0.00010\n",
      "Epoch: 4097|steps:   30|Train Avg Loss: 0.0240 |Test Loss: 4.3993|lr = 0.00010\n",
      "Epoch: 4097|steps:   60|Train Avg Loss: 0.0304 |Test Loss: 4.3648|lr = 0.00010\n",
      "Epoch: 4098|steps:   30|Train Avg Loss: 0.0244 |Test Loss: 4.3458|lr = 0.00010\n",
      "Epoch: 4098|steps:   60|Train Avg Loss: 0.0336 |Test Loss: 4.3234|lr = 0.00010\n",
      "Epoch: 4099|steps:   30|Train Avg Loss: 0.0356 |Test Loss: 4.3079|lr = 0.00010\n",
      "Epoch: 4099|steps:   60|Train Avg Loss: 0.0267 |Test Loss: 4.3952|lr = 0.00010\n",
      "Epoch: 4100|steps:   30|Train Avg Loss: 0.0236 |Test Loss: 4.3719|lr = 0.00010\n",
      "Epoch: 4100|steps:   60|Train Avg Loss: 0.0318 |Test Loss: 4.3517|lr = 0.00010\n",
      "Epoch: 4101|steps:   30|Train Avg Loss: 0.0282 |Test Loss: 4.3158|lr = 0.00010\n",
      "Epoch: 4101|steps:   60|Train Avg Loss: 0.0249 |Test Loss: 4.3870|lr = 0.00010\n",
      "Epoch: 4102|steps:   30|Train Avg Loss: 0.0307 |Test Loss: 4.3581|lr = 0.00010\n",
      "Epoch: 4102|steps:   60|Train Avg Loss: 0.0280 |Test Loss: 4.3329|lr = 0.00010\n",
      "Epoch: 4103|steps:   30|Train Avg Loss: 0.0307 |Test Loss: 4.3142|lr = 0.00010\n",
      "Epoch: 4103|steps:   60|Train Avg Loss: 0.0266 |Test Loss: 4.3226|lr = 0.00010\n",
      "Epoch: 4104|steps:   30|Train Avg Loss: 0.0261 |Test Loss: 4.3341|lr = 0.00010\n",
      "Epoch: 4104|steps:   60|Train Avg Loss: 0.0288 |Test Loss: 4.2891|lr = 0.00010\n",
      "Epoch: 4105|steps:   30|Train Avg Loss: 0.0332 |Test Loss: 4.3387|lr = 0.00010\n",
      "Epoch: 4105|steps:   60|Train Avg Loss: 0.0277 |Test Loss: 4.2995|lr = 0.00010\n",
      "Epoch: 4106|steps:   30|Train Avg Loss: 0.0237 |Test Loss: 4.3354|lr = 0.00010\n",
      "Epoch: 4106|steps:   60|Train Avg Loss: 0.0296 |Test Loss: 4.3377|lr = 0.00010\n",
      "Epoch: 4107|steps:   30|Train Avg Loss: 0.0290 |Test Loss: 4.3663|lr = 0.00010\n",
      "Epoch: 4107|steps:   60|Train Avg Loss: 0.0280 |Test Loss: 4.4349|lr = 0.00010\n",
      "Epoch: 4108|steps:   30|Train Avg Loss: 0.0187 |Test Loss: 4.3867|lr = 0.00010\n",
      "Epoch: 4108|steps:   60|Train Avg Loss: 0.0404 |Test Loss: 4.3209|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4109|steps:   30|Train Avg Loss: 0.0342 |Test Loss: 4.4659|lr = 0.00010\n",
      "Epoch: 4109|steps:   60|Train Avg Loss: 0.0521 |Test Loss: 4.4011|lr = 0.00010\n",
      "Epoch: 4110|steps:   30|Train Avg Loss: 0.0394 |Test Loss: 4.5825|lr = 0.00010\n",
      "Epoch: 4110|steps:   60|Train Avg Loss: 0.0533 |Test Loss: 4.5498|lr = 0.00010\n",
      "Epoch: 4111|steps:   30|Train Avg Loss: 0.0606 |Test Loss: 4.5526|lr = 0.00010\n",
      "Epoch: 4111|steps:   60|Train Avg Loss: 0.0880 |Test Loss: 4.4259|lr = 0.00010\n",
      "Epoch: 4112|steps:   30|Train Avg Loss: 0.0516 |Test Loss: 4.4077|lr = 0.00010\n",
      "Epoch: 4112|steps:   60|Train Avg Loss: 0.0682 |Test Loss: 4.4552|lr = 0.00010\n",
      "Epoch: 4113|steps:   30|Train Avg Loss: 0.0883 |Test Loss: 4.5396|lr = 0.00010\n",
      "Epoch: 4113|steps:   60|Train Avg Loss: 0.0878 |Test Loss: 4.5568|lr = 0.00010\n",
      "Epoch: 4114|steps:   30|Train Avg Loss: 0.0683 |Test Loss: 4.4951|lr = 0.00010\n",
      "Epoch: 4114|steps:   60|Train Avg Loss: 0.0417 |Test Loss: 4.3677|lr = 0.00010\n",
      "Epoch: 4115|steps:   30|Train Avg Loss: 0.0516 |Test Loss: 4.4432|lr = 0.00010\n",
      "Epoch: 4115|steps:   60|Train Avg Loss: 0.0422 |Test Loss: 4.4240|lr = 0.00010\n",
      "Epoch: 4116|steps:   30|Train Avg Loss: 0.2221 |Test Loss: 4.4306|lr = 0.00010\n",
      "Epoch: 4116|steps:   60|Train Avg Loss: 0.1109 |Test Loss: 4.3938|lr = 0.00010\n",
      "Epoch: 4117|steps:   30|Train Avg Loss: 0.0772 |Test Loss: 4.3185|lr = 0.00010\n",
      "Epoch: 4117|steps:   60|Train Avg Loss: 0.0922 |Test Loss: 4.4403|lr = 0.00010\n",
      "Epoch: 4118|steps:   30|Train Avg Loss: 0.0381 |Test Loss: 4.3082|lr = 0.00010\n",
      "Epoch: 4118|steps:   60|Train Avg Loss: 0.0274 |Test Loss: 4.3323|lr = 0.00010\n",
      "Epoch: 4119|steps:   30|Train Avg Loss: 0.0285 |Test Loss: 4.3439|lr = 0.00010\n",
      "Epoch: 4119|steps:   60|Train Avg Loss: 0.0300 |Test Loss: 4.3438|lr = 0.00010\n",
      "Epoch: 4120|steps:   30|Train Avg Loss: 0.0263 |Test Loss: 4.3386|lr = 0.00010\n",
      "Epoch: 4120|steps:   60|Train Avg Loss: 0.0270 |Test Loss: 4.3675|lr = 0.00010\n",
      "Epoch: 4121|steps:   30|Train Avg Loss: 0.0247 |Test Loss: 4.3568|lr = 0.00010\n",
      "Epoch: 4121|steps:   60|Train Avg Loss: 0.0312 |Test Loss: 4.3952|lr = 0.00010\n",
      "Epoch: 4122|steps:   30|Train Avg Loss: 0.0275 |Test Loss: 4.3739|lr = 0.00010\n",
      "Epoch: 4122|steps:   60|Train Avg Loss: 0.0288 |Test Loss: 4.3503|lr = 0.00010\n",
      "Epoch: 4123|steps:   30|Train Avg Loss: 0.0236 |Test Loss: 4.4036|lr = 0.00010\n",
      "Epoch: 4123|steps:   60|Train Avg Loss: 0.0265 |Test Loss: 4.3370|lr = 0.00010\n",
      "Epoch: 4124|steps:   30|Train Avg Loss: 0.0301 |Test Loss: 4.3952|lr = 0.00010\n",
      "Epoch: 4124|steps:   60|Train Avg Loss: 0.0250 |Test Loss: 4.3605|lr = 0.00010\n",
      "Epoch: 4125|steps:   30|Train Avg Loss: 0.0273 |Test Loss: 4.4028|lr = 0.00010\n",
      "Epoch: 4125|steps:   60|Train Avg Loss: 0.0287 |Test Loss: 4.4014|lr = 0.00010\n",
      "Epoch: 4126|steps:   30|Train Avg Loss: 0.0270 |Test Loss: 4.4039|lr = 0.00010\n",
      "Epoch: 4126|steps:   60|Train Avg Loss: 0.0228 |Test Loss: 4.3766|lr = 0.00010\n",
      "Epoch: 4127|steps:   30|Train Avg Loss: 0.0318 |Test Loss: 4.3755|lr = 0.00010\n",
      "Epoch: 4127|steps:   60|Train Avg Loss: 0.0251 |Test Loss: 4.3551|lr = 0.00010\n",
      "Epoch: 4128|steps:   30|Train Avg Loss: 0.0295 |Test Loss: 4.4347|lr = 0.00010\n",
      "Epoch: 4128|steps:   60|Train Avg Loss: 0.0262 |Test Loss: 4.3952|lr = 0.00010\n",
      "Epoch: 4129|steps:   30|Train Avg Loss: 0.0241 |Test Loss: 4.3842|lr = 0.00010\n",
      "Epoch: 4129|steps:   60|Train Avg Loss: 0.0277 |Test Loss: 4.3983|lr = 0.00010\n",
      "Epoch: 4130|steps:   30|Train Avg Loss: 0.0274 |Test Loss: 4.4261|lr = 0.00010\n",
      "Epoch: 4130|steps:   60|Train Avg Loss: 0.0392 |Test Loss: 4.4009|lr = 0.00010\n",
      "Epoch: 4131|steps:   30|Train Avg Loss: 0.0663 |Test Loss: 4.3097|lr = 0.00010\n",
      "Epoch: 4131|steps:   60|Train Avg Loss: 0.0470 |Test Loss: 4.4329|lr = 0.00010\n",
      "Epoch: 4132|steps:   30|Train Avg Loss: 0.0482 |Test Loss: 4.4042|lr = 0.00010\n",
      "Epoch: 4132|steps:   60|Train Avg Loss: 0.1453 |Test Loss: 4.4088|lr = 0.00010\n",
      "Epoch: 4133|steps:   30|Train Avg Loss: 0.1443 |Test Loss: 4.1767|lr = 0.00010\n",
      "Epoch: 4133|steps:   60|Train Avg Loss: 0.0814 |Test Loss: 4.4679|lr = 0.00010\n",
      "Epoch: 4134|steps:   30|Train Avg Loss: 0.0408 |Test Loss: 4.3484|lr = 0.00010\n",
      "Epoch: 4134|steps:   60|Train Avg Loss: 0.0462 |Test Loss: 4.2825|lr = 0.00010\n",
      "Epoch: 4135|steps:   30|Train Avg Loss: 0.0380 |Test Loss: 4.4295|lr = 0.00010\n",
      "Epoch: 4135|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 4.3937|lr = 0.00010\n",
      "Epoch: 4136|steps:   30|Train Avg Loss: 0.0269 |Test Loss: 4.4008|lr = 0.00010\n",
      "Epoch: 4136|steps:   60|Train Avg Loss: 0.0283 |Test Loss: 4.3953|lr = 0.00010\n",
      "Epoch: 4137|steps:   30|Train Avg Loss: 0.0237 |Test Loss: 4.4313|lr = 0.00010\n",
      "Epoch: 4137|steps:   60|Train Avg Loss: 0.0279 |Test Loss: 4.3781|lr = 0.00010\n",
      "Epoch: 4138|steps:   30|Train Avg Loss: 0.0191 |Test Loss: 4.3666|lr = 0.00010\n",
      "Epoch: 4138|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 4.4131|lr = 0.00010\n",
      "Epoch: 4139|steps:   30|Train Avg Loss: 0.0196 |Test Loss: 4.4335|lr = 0.00010\n",
      "Epoch: 4139|steps:   60|Train Avg Loss: 0.0315 |Test Loss: 4.4121|lr = 0.00010\n",
      "Epoch: 4140|steps:   30|Train Avg Loss: 0.0229 |Test Loss: 4.3779|lr = 0.00010\n",
      "Epoch: 4140|steps:   60|Train Avg Loss: 0.0268 |Test Loss: 4.3987|lr = 0.00010\n",
      "Epoch: 4141|steps:   30|Train Avg Loss: 0.0234 |Test Loss: 4.3859|lr = 0.00010\n",
      "Epoch: 4141|steps:   60|Train Avg Loss: 0.0280 |Test Loss: 4.4146|lr = 0.00010\n",
      "Epoch: 4142|steps:   30|Train Avg Loss: 0.0261 |Test Loss: 4.3627|lr = 0.00010\n",
      "Epoch: 4142|steps:   60|Train Avg Loss: 0.0266 |Test Loss: 4.4105|lr = 0.00010\n",
      "Epoch: 4143|steps:   30|Train Avg Loss: 0.0220 |Test Loss: 4.3888|lr = 0.00010\n",
      "Epoch: 4143|steps:   60|Train Avg Loss: 0.0296 |Test Loss: 4.3808|lr = 0.00010\n",
      "Epoch: 4144|steps:   30|Train Avg Loss: 0.0278 |Test Loss: 4.4602|lr = 0.00010\n",
      "Epoch: 4144|steps:   60|Train Avg Loss: 0.0225 |Test Loss: 4.3761|lr = 0.00010\n",
      "Epoch: 4145|steps:   30|Train Avg Loss: 0.0292 |Test Loss: 4.4550|lr = 0.00010\n",
      "Epoch: 4145|steps:   60|Train Avg Loss: 0.0342 |Test Loss: 4.4646|lr = 0.00010\n",
      "Epoch: 4146|steps:   30|Train Avg Loss: 0.0238 |Test Loss: 4.3953|lr = 0.00010\n",
      "Epoch: 4146|steps:   60|Train Avg Loss: 0.0237 |Test Loss: 4.3850|lr = 0.00010\n",
      "Epoch: 4147|steps:   30|Train Avg Loss: 0.0242 |Test Loss: 4.5132|lr = 0.00010\n",
      "Epoch: 4147|steps:   60|Train Avg Loss: 0.0275 |Test Loss: 4.4360|lr = 0.00010\n",
      "Epoch: 4148|steps:   30|Train Avg Loss: 0.0225 |Test Loss: 4.4446|lr = 0.00010\n",
      "Epoch: 4148|steps:   60|Train Avg Loss: 0.0295 |Test Loss: 4.3817|lr = 0.00010\n",
      "Epoch: 4149|steps:   30|Train Avg Loss: 0.0325 |Test Loss: 4.3989|lr = 0.00010\n",
      "Epoch: 4149|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 4.4552|lr = 0.00010\n",
      "Epoch: 4150|steps:   30|Train Avg Loss: 0.0261 |Test Loss: 4.4534|lr = 0.00010\n",
      "Epoch: 4150|steps:   60|Train Avg Loss: 0.0281 |Test Loss: 4.4780|lr = 0.00010\n",
      "Epoch: 4151|steps:   30|Train Avg Loss: 0.0270 |Test Loss: 4.4004|lr = 0.00010\n",
      "Epoch: 4151|steps:   60|Train Avg Loss: 0.0290 |Test Loss: 4.4880|lr = 0.00010\n",
      "Epoch: 4152|steps:   30|Train Avg Loss: 0.0248 |Test Loss: 4.4426|lr = 0.00010\n",
      "Epoch: 4152|steps:   60|Train Avg Loss: 0.0236 |Test Loss: 4.4111|lr = 0.00010\n",
      "Epoch: 4153|steps:   30|Train Avg Loss: 0.0286 |Test Loss: 4.3658|lr = 0.00010\n",
      "Epoch: 4153|steps:   60|Train Avg Loss: 0.0408 |Test Loss: 4.5324|lr = 0.00010\n",
      "Epoch: 4154|steps:   30|Train Avg Loss: 0.2388 |Test Loss: 4.8523|lr = 0.00010\n",
      "Epoch: 4154|steps:   60|Train Avg Loss: 0.5245 |Test Loss: 4.2860|lr = 0.00010\n",
      "Epoch: 4155|steps:   30|Train Avg Loss: 0.5025 |Test Loss: 4.4856|lr = 0.00010\n",
      "Epoch: 4155|steps:   60|Train Avg Loss: 0.1931 |Test Loss: 4.5210|lr = 0.00010\n",
      "Epoch: 4156|steps:   30|Train Avg Loss: 0.0890 |Test Loss: 4.4810|lr = 0.00010\n",
      "Epoch: 4156|steps:   60|Train Avg Loss: 0.0502 |Test Loss: 4.4527|lr = 0.00010\n",
      "Epoch: 4157|steps:   30|Train Avg Loss: 0.0265 |Test Loss: 4.4325|lr = 0.00010\n",
      "Epoch: 4157|steps:   60|Train Avg Loss: 0.0266 |Test Loss: 4.4120|lr = 0.00010\n",
      "Epoch: 4158|steps:   30|Train Avg Loss: 0.0229 |Test Loss: 4.3843|lr = 0.00010\n",
      "Epoch: 4158|steps:   60|Train Avg Loss: 0.0277 |Test Loss: 4.4419|lr = 0.00010\n",
      "Epoch: 4159|steps:   30|Train Avg Loss: 0.0209 |Test Loss: 4.4049|lr = 0.00010\n",
      "Epoch: 4159|steps:   60|Train Avg Loss: 0.0243 |Test Loss: 4.4106|lr = 0.00010\n",
      "Epoch: 4160|steps:   30|Train Avg Loss: 0.0257 |Test Loss: 4.4191|lr = 0.00010\n",
      "Epoch: 4160|steps:   60|Train Avg Loss: 0.0269 |Test Loss: 4.4237|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4161|steps:   30|Train Avg Loss: 0.0222 |Test Loss: 4.4161|lr = 0.00010\n",
      "Epoch: 4161|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 4.4343|lr = 0.00010\n",
      "Epoch: 4162|steps:   30|Train Avg Loss: 0.0209 |Test Loss: 4.4240|lr = 0.00010\n",
      "Epoch: 4162|steps:   60|Train Avg Loss: 0.0237 |Test Loss: 4.4684|lr = 0.00010\n",
      "Epoch: 4163|steps:   30|Train Avg Loss: 0.0259 |Test Loss: 4.3837|lr = 0.00010\n",
      "Epoch: 4163|steps:   60|Train Avg Loss: 0.0262 |Test Loss: 4.4373|lr = 0.00010\n",
      "Epoch: 4164|steps:   30|Train Avg Loss: 0.0217 |Test Loss: 4.4430|lr = 0.00010\n",
      "Epoch: 4164|steps:   60|Train Avg Loss: 0.0235 |Test Loss: 4.4077|lr = 0.00010\n",
      "Epoch: 4165|steps:   30|Train Avg Loss: 0.0232 |Test Loss: 4.4441|lr = 0.00010\n",
      "Epoch: 4165|steps:   60|Train Avg Loss: 0.0252 |Test Loss: 4.4178|lr = 0.00010\n",
      "Epoch: 4166|steps:   30|Train Avg Loss: 0.0235 |Test Loss: 4.4117|lr = 0.00010\n",
      "Epoch: 4166|steps:   60|Train Avg Loss: 0.0238 |Test Loss: 4.4211|lr = 0.00010\n",
      "Epoch: 4167|steps:   30|Train Avg Loss: 0.0256 |Test Loss: 4.4138|lr = 0.00010\n",
      "Epoch: 4167|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 4.4575|lr = 0.00010\n",
      "Epoch: 4168|steps:   30|Train Avg Loss: 0.0217 |Test Loss: 4.5148|lr = 0.00010\n",
      "Epoch: 4168|steps:   60|Train Avg Loss: 0.0283 |Test Loss: 4.3884|lr = 0.00010\n",
      "Epoch: 4169|steps:   30|Train Avg Loss: 0.0247 |Test Loss: 4.4623|lr = 0.00010\n",
      "Epoch: 4169|steps:   60|Train Avg Loss: 0.0224 |Test Loss: 4.4364|lr = 0.00010\n",
      "Epoch: 4170|steps:   30|Train Avg Loss: 0.0248 |Test Loss: 4.4481|lr = 0.00010\n",
      "Epoch: 4170|steps:   60|Train Avg Loss: 0.0207 |Test Loss: 4.4433|lr = 0.00010\n",
      "Epoch: 4171|steps:   30|Train Avg Loss: 0.0375 |Test Loss: 4.4257|lr = 0.00010\n",
      "Epoch: 4171|steps:   60|Train Avg Loss: 0.0283 |Test Loss: 4.4259|lr = 0.00010\n",
      "Epoch: 4172|steps:   30|Train Avg Loss: 0.0204 |Test Loss: 4.4463|lr = 0.00010\n",
      "Epoch: 4172|steps:   60|Train Avg Loss: 0.0257 |Test Loss: 4.4366|lr = 0.00010\n",
      "Epoch: 4173|steps:   30|Train Avg Loss: 0.0262 |Test Loss: 4.4275|lr = 0.00010\n",
      "Epoch: 4173|steps:   60|Train Avg Loss: 0.0225 |Test Loss: 4.4324|lr = 0.00010\n",
      "Epoch: 4174|steps:   30|Train Avg Loss: 0.0547 |Test Loss: 4.6223|lr = 0.00010\n",
      "Epoch: 4174|steps:   60|Train Avg Loss: 0.1037 |Test Loss: 4.5972|lr = 0.00010\n",
      "Epoch: 4175|steps:   30|Train Avg Loss: 0.0524 |Test Loss: 4.5483|lr = 0.00010\n",
      "Epoch: 4175|steps:   60|Train Avg Loss: 0.0668 |Test Loss: 4.5884|lr = 0.00010\n",
      "Epoch: 4176|steps:   30|Train Avg Loss: 0.0375 |Test Loss: 4.4836|lr = 0.00010\n",
      "Epoch: 4176|steps:   60|Train Avg Loss: 0.0988 |Test Loss: 4.5418|lr = 0.00010\n",
      "Epoch: 4177|steps:   30|Train Avg Loss: 0.1699 |Test Loss: 4.4103|lr = 0.00010\n",
      "Epoch: 4177|steps:   60|Train Avg Loss: 0.1145 |Test Loss: 4.5515|lr = 0.00010\n",
      "Epoch: 4178|steps:   30|Train Avg Loss: 0.0515 |Test Loss: 4.3491|lr = 0.00010\n",
      "Epoch: 4178|steps:   60|Train Avg Loss: 0.0478 |Test Loss: 4.3664|lr = 0.00010\n",
      "Epoch: 4179|steps:   30|Train Avg Loss: 0.0299 |Test Loss: 4.4219|lr = 0.00010\n",
      "Epoch: 4179|steps:   60|Train Avg Loss: 0.0271 |Test Loss: 4.3721|lr = 0.00010\n",
      "Epoch: 4180|steps:   30|Train Avg Loss: 0.0285 |Test Loss: 4.4029|lr = 0.00010\n",
      "Epoch: 4180|steps:   60|Train Avg Loss: 0.0237 |Test Loss: 4.3787|lr = 0.00010\n",
      "Epoch: 4181|steps:   30|Train Avg Loss: 0.0255 |Test Loss: 4.3621|lr = 0.00010\n",
      "Epoch: 4181|steps:   60|Train Avg Loss: 0.0186 |Test Loss: 4.3706|lr = 0.00010\n",
      "Epoch: 4182|steps:   30|Train Avg Loss: 0.0250 |Test Loss: 4.3557|lr = 0.00010\n",
      "Epoch: 4182|steps:   60|Train Avg Loss: 0.0250 |Test Loss: 4.4259|lr = 0.00010\n",
      "Epoch: 4183|steps:   30|Train Avg Loss: 0.0180 |Test Loss: 4.3831|lr = 0.00010\n",
      "Epoch: 4183|steps:   60|Train Avg Loss: 0.0227 |Test Loss: 4.4174|lr = 0.00010\n",
      "Epoch: 4184|steps:   30|Train Avg Loss: 0.0232 |Test Loss: 4.4613|lr = 0.00010\n",
      "Epoch: 4184|steps:   60|Train Avg Loss: 0.0249 |Test Loss: 4.3608|lr = 0.00010\n",
      "Epoch: 4185|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 4.4068|lr = 0.00010\n",
      "Epoch: 4185|steps:   60|Train Avg Loss: 0.0250 |Test Loss: 4.4099|lr = 0.00010\n",
      "Epoch: 4186|steps:   30|Train Avg Loss: 0.0240 |Test Loss: 4.4523|lr = 0.00010\n",
      "Epoch: 4186|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 4.4021|lr = 0.00010\n",
      "Epoch: 4187|steps:   30|Train Avg Loss: 0.0226 |Test Loss: 4.4344|lr = 0.00010\n",
      "Epoch: 4187|steps:   60|Train Avg Loss: 0.0254 |Test Loss: 4.4640|lr = 0.00010\n",
      "Epoch: 4188|steps:   30|Train Avg Loss: 0.0210 |Test Loss: 4.4429|lr = 0.00010\n",
      "Epoch: 4188|steps:   60|Train Avg Loss: 0.0280 |Test Loss: 4.4302|lr = 0.00010\n",
      "Epoch: 4189|steps:   30|Train Avg Loss: 0.0168 |Test Loss: 4.4481|lr = 0.00010\n",
      "Epoch: 4189|steps:   60|Train Avg Loss: 0.0270 |Test Loss: 4.3976|lr = 0.00010\n",
      "Epoch: 4190|steps:   30|Train Avg Loss: 0.0234 |Test Loss: 4.3826|lr = 0.00010\n",
      "Epoch: 4190|steps:   60|Train Avg Loss: 0.0302 |Test Loss: 4.4533|lr = 0.00010\n",
      "Epoch: 4191|steps:   30|Train Avg Loss: 0.0243 |Test Loss: 4.4847|lr = 0.00010\n",
      "Epoch: 4191|steps:   60|Train Avg Loss: 0.0312 |Test Loss: 4.3655|lr = 0.00010\n",
      "Epoch: 4192|steps:   30|Train Avg Loss: 0.0300 |Test Loss: 4.4951|lr = 0.00010\n",
      "Epoch: 4192|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 4.4386|lr = 0.00010\n",
      "Epoch: 4193|steps:   30|Train Avg Loss: 0.0240 |Test Loss: 4.4099|lr = 0.00010\n",
      "Epoch: 4193|steps:   60|Train Avg Loss: 0.0245 |Test Loss: 4.4824|lr = 0.00010\n",
      "Epoch: 4194|steps:   30|Train Avg Loss: 0.0343 |Test Loss: 4.6031|lr = 0.00010\n",
      "Epoch: 4194|steps:   60|Train Avg Loss: 0.0329 |Test Loss: 4.5722|lr = 0.00010\n",
      "Epoch: 4195|steps:   30|Train Avg Loss: 0.0450 |Test Loss: 4.5762|lr = 0.00010\n",
      "Epoch: 4195|steps:   60|Train Avg Loss: 0.0387 |Test Loss: 4.5641|lr = 0.00010\n",
      "Epoch: 4196|steps:   30|Train Avg Loss: 0.0408 |Test Loss: 4.5248|lr = 0.00010\n",
      "Epoch: 4196|steps:   60|Train Avg Loss: 0.0653 |Test Loss: 4.4723|lr = 0.00010\n",
      "Epoch: 4197|steps:   30|Train Avg Loss: 0.0349 |Test Loss: 4.4188|lr = 0.00010\n",
      "Epoch: 4197|steps:   60|Train Avg Loss: 0.0264 |Test Loss: 4.3846|lr = 0.00010\n",
      "Epoch: 4198|steps:   30|Train Avg Loss: 0.0241 |Test Loss: 4.5058|lr = 0.00010\n",
      "Epoch: 4198|steps:   60|Train Avg Loss: 0.0297 |Test Loss: 4.5761|lr = 0.00010\n",
      "Epoch: 4199|steps:   30|Train Avg Loss: 0.0255 |Test Loss: 4.5090|lr = 0.00010\n",
      "Epoch: 4199|steps:   60|Train Avg Loss: 0.0310 |Test Loss: 4.4578|lr = 0.00010\n",
      "Epoch: 4200|steps:   30|Train Avg Loss: 0.0279 |Test Loss: 4.4203|lr = 0.00010\n",
      "Epoch: 4200|steps:   60|Train Avg Loss: 0.0336 |Test Loss: 4.5275|lr = 0.00010\n",
      "Epoch: 4201|steps:   30|Train Avg Loss: 0.0224 |Test Loss: 4.4411|lr = 0.00010\n",
      "Epoch: 4201|steps:   60|Train Avg Loss: 0.0286 |Test Loss: 4.5181|lr = 0.00010\n",
      "Epoch: 4202|steps:   30|Train Avg Loss: 0.0264 |Test Loss: 4.5756|lr = 0.00010\n",
      "Epoch: 4202|steps:   60|Train Avg Loss: 0.0221 |Test Loss: 4.4925|lr = 0.00010\n",
      "Epoch: 4203|steps:   30|Train Avg Loss: 0.0239 |Test Loss: 4.5074|lr = 0.00010\n",
      "Epoch: 4203|steps:   60|Train Avg Loss: 0.0257 |Test Loss: 4.4580|lr = 0.00010\n",
      "Epoch: 4204|steps:   30|Train Avg Loss: 0.0214 |Test Loss: 4.5472|lr = 0.00010\n",
      "Epoch: 4204|steps:   60|Train Avg Loss: 0.0305 |Test Loss: 4.5058|lr = 0.00010\n",
      "Epoch: 4205|steps:   30|Train Avg Loss: 0.0276 |Test Loss: 4.5425|lr = 0.00010\n",
      "Epoch: 4205|steps:   60|Train Avg Loss: 0.0255 |Test Loss: 4.5207|lr = 0.00010\n",
      "Epoch: 4206|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 4.5508|lr = 0.00010\n",
      "Epoch: 4206|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 4.5240|lr = 0.00010\n",
      "Epoch: 4207|steps:   30|Train Avg Loss: 0.0255 |Test Loss: 4.4962|lr = 0.00010\n",
      "Epoch: 4207|steps:   60|Train Avg Loss: 0.0265 |Test Loss: 4.5233|lr = 0.00010\n",
      "Epoch: 4208|steps:   30|Train Avg Loss: 0.0218 |Test Loss: 4.5956|lr = 0.00010\n",
      "Epoch: 4208|steps:   60|Train Avg Loss: 0.0343 |Test Loss: 4.4828|lr = 0.00010\n",
      "Epoch: 4209|steps:   30|Train Avg Loss: 0.0348 |Test Loss: 4.4455|lr = 0.00010\n",
      "Epoch: 4209|steps:   60|Train Avg Loss: 0.0339 |Test Loss: 4.4640|lr = 0.00010\n",
      "Epoch: 4210|steps:   30|Train Avg Loss: 0.0242 |Test Loss: 4.5621|lr = 0.00010\n",
      "Epoch: 4210|steps:   60|Train Avg Loss: 0.0211 |Test Loss: 4.5426|lr = 0.00010\n",
      "Epoch: 4211|steps:   30|Train Avg Loss: 0.0255 |Test Loss: 4.6671|lr = 0.00010\n",
      "Epoch: 4211|steps:   60|Train Avg Loss: 0.0225 |Test Loss: 4.4654|lr = 0.00010\n",
      "Epoch: 4212|steps:   30|Train Avg Loss: 0.0301 |Test Loss: 4.5243|lr = 0.00010\n",
      "Epoch: 4212|steps:   60|Train Avg Loss: 0.0387 |Test Loss: 4.5774|lr = 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4213|steps:   30|Train Avg Loss: 0.0771 |Test Loss: 4.5211|lr = 0.00010\n",
      "Epoch: 4213|steps:   60|Train Avg Loss: 0.1216 |Test Loss: 4.4937|lr = 0.00010\n",
      "Epoch: 4214|steps:   30|Train Avg Loss: 0.2738 |Test Loss: 4.4876|lr = 0.00010\n",
      "Epoch: 4214|steps:   60|Train Avg Loss: 0.1933 |Test Loss: 4.5812|lr = 0.00010\n",
      "Epoch: 4215|steps:   30|Train Avg Loss: 0.1849 |Test Loss: 4.4436|lr = 0.00010\n",
      "Epoch: 4215|steps:   60|Train Avg Loss: 0.1076 |Test Loss: 4.6233|lr = 0.00010\n",
      "Epoch: 4216|steps:   30|Train Avg Loss: 0.0490 |Test Loss: 4.6385|lr = 0.00010\n",
      "Epoch: 4216|steps:   60|Train Avg Loss: 0.0461 |Test Loss: 4.4510|lr = 0.00010\n",
      "Epoch: 4217|steps:   30|Train Avg Loss: 0.0368 |Test Loss: 4.5432|lr = 0.00010\n",
      "Epoch: 4217|steps:   60|Train Avg Loss: 0.0606 |Test Loss: 4.3947|lr = 0.00010\n",
      "Epoch: 4218|steps:   30|Train Avg Loss: 0.0579 |Test Loss: 4.5590|lr = 0.00010\n",
      "Epoch: 4218|steps:   60|Train Avg Loss: 0.0454 |Test Loss: 4.4666|lr = 0.00010\n",
      "Epoch: 4219|steps:   30|Train Avg Loss: 0.0257 |Test Loss: 4.5474|lr = 0.00010\n",
      "Epoch: 4219|steps:   60|Train Avg Loss: 0.0275 |Test Loss: 4.5101|lr = 0.00010\n",
      "Epoch: 4220|steps:   30|Train Avg Loss: 0.0231 |Test Loss: 4.5091|lr = 0.00010\n",
      "Epoch: 4220|steps:   60|Train Avg Loss: 0.0232 |Test Loss: 4.5061|lr = 0.00010\n",
      "Epoch: 4221|steps:   30|Train Avg Loss: 0.0209 |Test Loss: 4.5536|lr = 0.00010\n",
      "Epoch: 4221|steps:   60|Train Avg Loss: 0.0223 |Test Loss: 4.4514|lr = 0.00010\n",
      "Epoch: 4222|steps:   30|Train Avg Loss: 0.0268 |Test Loss: 4.4707|lr = 0.00010\n",
      "Epoch: 4222|steps:   60|Train Avg Loss: 0.0175 |Test Loss: 4.5492|lr = 0.00010\n",
      "Epoch: 4223|steps:   30|Train Avg Loss: 0.0179 |Test Loss: 4.5253|lr = 0.00010\n",
      "Epoch: 4223|steps:   60|Train Avg Loss: 0.0262 |Test Loss: 4.5283|lr = 0.00010\n",
      "Epoch: 4224|steps:   30|Train Avg Loss: 0.0186 |Test Loss: 4.5424|lr = 0.00010\n",
      "Epoch: 4224|steps:   60|Train Avg Loss: 0.0230 |Test Loss: 4.5068|lr = 0.00010\n",
      "Epoch: 4225|steps:   30|Train Avg Loss: 0.0175 |Test Loss: 4.5388|lr = 0.00010\n",
      "Epoch: 4225|steps:   60|Train Avg Loss: 0.0232 |Test Loss: 4.5021|lr = 0.00010\n",
      "Epoch: 4226|steps:   30|Train Avg Loss: 0.0191 |Test Loss: 4.4961|lr = 0.00010\n",
      "Epoch: 4226|steps:   60|Train Avg Loss: 0.0264 |Test Loss: 4.4966|lr = 0.00010\n",
      "Epoch: 4227|steps:   30|Train Avg Loss: 0.0231 |Test Loss: 4.4697|lr = 0.00010\n",
      "Epoch: 4227|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 4.5395|lr = 0.00010\n",
      "Epoch: 4228|steps:   30|Train Avg Loss: 0.0181 |Test Loss: 4.5185|lr = 0.00010\n",
      "Epoch: 4228|steps:   60|Train Avg Loss: 0.0223 |Test Loss: 4.5082|lr = 0.00010\n",
      "Epoch: 4229|steps:   30|Train Avg Loss: 0.0278 |Test Loss: 4.4378|lr = 0.00010\n",
      "Epoch: 4229|steps:   60|Train Avg Loss: 0.0222 |Test Loss: 4.5234|lr = 0.00010\n",
      "Epoch: 4230|steps:   30|Train Avg Loss: 0.0191 |Test Loss: 4.5157|lr = 0.00010\n",
      "Epoch: 4230|steps:   60|Train Avg Loss: 0.0214 |Test Loss: 4.4946|lr = 0.00010\n",
      "Epoch: 4231|steps:   30|Train Avg Loss: 0.0184 |Test Loss: 4.5407|lr = 0.00010\n",
      "Epoch: 4231|steps:   60|Train Avg Loss: 0.0222 |Test Loss: 4.5209|lr = 0.00010\n",
      "Epoch: 4232|steps:   30|Train Avg Loss: 0.0215 |Test Loss: 4.4714|lr = 0.00010\n",
      "Epoch: 4232|steps:   60|Train Avg Loss: 0.0256 |Test Loss: 4.5055|lr = 0.00010\n",
      "Epoch: 4233|steps:   30|Train Avg Loss: 0.0241 |Test Loss: 4.5261|lr = 0.00010\n",
      "Epoch: 4233|steps:   60|Train Avg Loss: 0.0226 |Test Loss: 4.5161|lr = 0.00010\n",
      "Epoch: 4234|steps:   30|Train Avg Loss: 0.0202 |Test Loss: 4.5524|lr = 0.00010\n",
      "Epoch: 4234|steps:   60|Train Avg Loss: 0.0229 |Test Loss: 4.5328|lr = 0.00010\n",
      "Epoch: 4235|steps:   30|Train Avg Loss: 0.0189 |Test Loss: 4.5076|lr = 0.00010\n",
      "Epoch: 4235|steps:   60|Train Avg Loss: 0.0299 |Test Loss: 4.5499|lr = 0.00010\n",
      "Epoch: 4236|steps:   30|Train Avg Loss: 0.1294 |Test Loss: 4.4773|lr = 0.00010\n",
      "Epoch: 4236|steps:   60|Train Avg Loss: 0.1170 |Test Loss: 4.6700|lr = 0.00010\n",
      "Epoch: 4237|steps:   30|Train Avg Loss: 0.0677 |Test Loss: 4.5495|lr = 0.00010\n",
      "Epoch: 4237|steps:   60|Train Avg Loss: 0.0701 |Test Loss: 4.5408|lr = 0.00010\n",
      "Epoch: 4238|steps:   30|Train Avg Loss: 0.0312 |Test Loss: 4.6295|lr = 0.00010\n",
      "Epoch: 4238|steps:   60|Train Avg Loss: 0.0409 |Test Loss: 4.5524|lr = 0.00010\n",
      "Epoch: 4239|steps:   30|Train Avg Loss: 0.0233 |Test Loss: 4.5275|lr = 0.00010\n",
      "Epoch: 4239|steps:   60|Train Avg Loss: 0.0259 |Test Loss: 4.5362|lr = 0.00010\n",
      "Epoch: 4240|steps:   30|Train Avg Loss: 0.0265 |Test Loss: 4.5066|lr = 0.00010\n",
      "Epoch: 4240|steps:   60|Train Avg Loss: 0.0242 |Test Loss: 4.5192|lr = 0.00010\n",
      "Epoch: 4241|steps:   30|Train Avg Loss: 0.0183 |Test Loss: 4.5461|lr = 0.00010\n",
      "Epoch: 4241|steps:   60|Train Avg Loss: 0.0243 |Test Loss: 4.5442|lr = 0.00010\n",
      "Epoch: 4242|steps:   30|Train Avg Loss: 0.0236 |Test Loss: 4.5391|lr = 0.00010\n",
      "Epoch: 4242|steps:   60|Train Avg Loss: 0.0215 |Test Loss: 4.4781|lr = 0.00010\n",
      "Epoch: 4243|steps:   30|Train Avg Loss: 0.0155 |Test Loss: 4.5493|lr = 0.00010\n",
      "Epoch: 4243|steps:   60|Train Avg Loss: 0.0266 |Test Loss: 4.5199|lr = 0.00010\n",
      "Epoch: 4244|steps:   30|Train Avg Loss: 0.0205 |Test Loss: 4.5447|lr = 0.00010\n",
      "Epoch: 4244|steps:   60|Train Avg Loss: 0.0200 |Test Loss: 4.5153|lr = 0.00010\n",
      "Epoch: 4245|steps:   30|Train Avg Loss: 0.0220 |Test Loss: 4.5880|lr = 0.00010\n",
      "Epoch: 4245|steps:   60|Train Avg Loss: 0.0194 |Test Loss: 4.4631|lr = 0.00010\n",
      "Epoch: 4246|steps:   30|Train Avg Loss: 0.0207 |Test Loss: 4.5049|lr = 0.00010\n",
      "Epoch: 4246|steps:   60|Train Avg Loss: 0.0285 |Test Loss: 4.5223|lr = 0.00010\n",
      "Epoch: 4247|steps:   30|Train Avg Loss: 0.0265 |Test Loss: 4.5285|lr = 0.00010\n",
      "Epoch: 4247|steps:   60|Train Avg Loss: 0.0288 |Test Loss: 4.5134|lr = 0.00010\n",
      "Epoch: 4248|steps:   30|Train Avg Loss: 0.0185 |Test Loss: 4.5242|lr = 0.00010\n",
      "Epoch: 4248|steps:   60|Train Avg Loss: 0.0251 |Test Loss: 4.5193|lr = 0.00010\n",
      "Epoch: 4249|steps:   30|Train Avg Loss: 0.0185 |Test Loss: 4.5192|lr = 0.00010\n",
      "Epoch: 4249|steps:   60|Train Avg Loss: 0.0204 |Test Loss: 4.5223|lr = 0.00010\n",
      "Epoch: 4250|steps:   30|Train Avg Loss: 0.0205 |Test Loss: 4.5441|lr = 0.00010\n",
      "Epoch: 4250|steps:   60|Train Avg Loss: 0.0210 |Test Loss: 4.5016|lr = 0.00010\n",
      "Epoch: 4251|steps:   30|Train Avg Loss: 0.0203 |Test Loss: 4.5768|lr = 0.00010\n",
      "Epoch: 4251|steps:   60|Train Avg Loss: 0.0327 |Test Loss: 4.5448|lr = 0.00010\n",
      "Epoch: 4252|steps:   30|Train Avg Loss: 0.0211 |Test Loss: 4.4967|lr = 0.00010\n",
      "Epoch: 4252|steps:   60|Train Avg Loss: 0.0264 |Test Loss: 4.5900|lr = 0.00010\n",
      "Epoch: 4253|steps:   30|Train Avg Loss: 0.0486 |Test Loss: 4.5474|lr = 0.00010\n",
      "Epoch: 4253|steps:   60|Train Avg Loss: 0.1252 |Test Loss: 4.9677|lr = 0.00010\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "TEST_LOSS = []\n",
    "TEST_ACC = []\n",
    "TRAIN_ACC = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,TIME_STEP,INPUT_SIZE)\n",
    "        # start trainnig \n",
    "        output = model(inputs)\n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,targets)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 30 steps\n",
    "        if((step+1) %30 == 0):\n",
    "            # average of loss in 30 steps\n",
    "            avg = loss_total / 30\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # calculate the accuracy of training \n",
    "            pred_train_y = torch.max(output, 1)[1].data.numpy()\n",
    "            train_accuracy = float((pred_train_y == targets.numpy()).astype(int).sum()) / float(targets.numpy().size)\n",
    "            TRAIN_ACC.append(train_accuracy)\n",
    "            \n",
    "            # calculate the accuracy of using testing data as inputs\n",
    "            test_output = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "            pred_test_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            test_accuracy = float((pred_test_y == test_labels.numpy()).astype(int).sum()) / float(test_labels.numpy().size)\n",
    "            TEST_ACC.append(test_accuracy)\n",
    "            test_loss = loss_func(test_output,test_labels)\n",
    "            TEST_LOSS.append(test_loss.tolist())\n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f |Test Loss: %.4f|lr = %.5f\"\n",
    "                  %(epoch+1,step+1,avg,test_loss,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "test = test.detach().numpy()\n",
    "test = scaler.inverse_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,231,231)\n",
    "y1 = stock_df[2208:2439][\"Close\"].values\n",
    "y2 = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(labels=['Real', 'Predicted'],  loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,200,200)\n",
    "y1 = np.array(LOSS)\n",
    "y2 = np.array(TEST_LOSS)\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(labels=['Train loss', 'Test loss'],  loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t =stock_df[2208:2439]\n",
    "t[\"predict\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv(\"C:/Users/acer/Desktop/LAB/123.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
