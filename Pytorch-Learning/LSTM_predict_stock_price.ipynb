{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "stock_path = \"C:/Users/acer/Desktop/LAB/lab2.csv\"\n",
    "stock_df = pd.read_csv(stock_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df[[\"Date\",\"Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler(feature_range = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = scaler.fit_transform(stock_df[[\"Close\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1465188 ]\n",
      " [0.12814316]\n",
      " [0.13684748]\n",
      " ...\n",
      " [0.39703834]\n",
      " [0.45126814]\n",
      " [0.47148938]]\n"
     ]
    }
   ],
   "source": [
    "print(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = pd.DataFrame(minmax_df,columns = [\"Close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df[\"Date\"] = stock_df[\"Date\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.146519</td>\n",
       "      <td>2010/1/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.128143</td>\n",
       "      <td>2010/1/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.136847</td>\n",
       "      <td>2010/1/22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117021</td>\n",
       "      <td>2010/1/25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098646</td>\n",
       "      <td>2010/1/26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.098646</td>\n",
       "      <td>2010/1/27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.115571</td>\n",
       "      <td>2010/1/28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.121857</td>\n",
       "      <td>2010/1/29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.119439</td>\n",
       "      <td>2010/2/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.104448</td>\n",
       "      <td>2010/2/2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Close       Date\n",
       "0  0.146519  2010/1/20\n",
       "1  0.128143  2010/1/21\n",
       "2  0.136847  2010/1/22\n",
       "3  0.117021  2010/1/25\n",
       "4  0.098646  2010/1/26\n",
       "5  0.098646  2010/1/27\n",
       "6  0.115571  2010/1/28\n",
       "7  0.121857  2010/1/29\n",
       "8  0.119439   2010/2/1\n",
       "9  0.104448   2010/2/2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = minmax_df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hy parameter\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEP = 30\n",
    "INPUT_SIZE = 1\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2438"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of feature:  torch.Size([2402, 30, 1])\n",
      "size of label:  torch.Size([2402, 7])\n"
     ]
    }
   ],
   "source": [
    "# declear training features data\n",
    "features = []\n",
    "for i in range(TIME_STEP,len(minmax_df)-6):\n",
    "    x = minmax_df[i-TIME_STEP:i][[\"Close\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = torch.FloatTensor(features)\n",
    "print(\"size of feature: \",features.size())\n",
    "\n",
    "# declear trainging labels data\n",
    "labels = []\n",
    "for i in range(TIME_STEP,len(minmax_df)-6):\n",
    "    x = minmax_df[i:i+7][\"Close\"]\n",
    "    labels.append(x.tolist())\n",
    "labels = torch.FloatTensor(labels)\n",
    "print(\"size of label: \",labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features[:2200]\n",
    "train_labels = labels[:2200]\n",
    "test_features = features[2200:]\n",
    "test_labels = labels[2200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mini-Batch\n",
    "\n",
    "torch_dataset = Data.TensorDataset(train_features,train_labels)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define NN architecture\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        # define lstm layer\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=128,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        self.lstm2 = torch.nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=64,         \n",
    "            num_layers=1,\n",
    "            batch_first=True, \n",
    "        )\n",
    "        # dropout layer\n",
    "        self.Relu =  torch.nn.ReLU()\n",
    "        self.hidden1 = torch.nn.Linear(64,32)\n",
    "        self.out = torch.nn.Linear(32,7)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,(h_n,h_c) = self.lstm(x,None)\n",
    "        lstm_out = self.Relu(lstm_out)\n",
    "        lstm_out,_ = self.lstm2(lstm_out,None)\n",
    "        lstm_out = self.Relu(lstm_out[:,-1,:])\n",
    "        # only need last output of lstm layer\n",
    "        h1_out = self.hidden1(lstm_out)\n",
    "        h1_out = self.Relu(h1_out)\n",
    "        out = self.out(h1_out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(1, 128, batch_first=True)\n",
      "  (lstm2): LSTM(128, 64, batch_first=True)\n",
      "  (Relu): ReLU()\n",
      "  (hidden1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=0.00015)\n",
    "# adject learning rate . when loss don't fall , lr = lr * factor  , min lr = 0.0001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.98,min_lr=0.0001)\n",
    "# crossentroy loss \n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1|steps:   30|Train Avg Loss: 0.1030 |Test Loss: 0.0051|lr = 0.01000\n",
      "Epoch:    1|steps:   60|Train Avg Loss: 0.0786 |Test Loss: 0.0035|lr = 0.01000\n",
      "Epoch:    2|steps:   30|Train Avg Loss: 0.0304 |Test Loss: 0.0050|lr = 0.01000\n",
      "Epoch:    2|steps:   60|Train Avg Loss: 0.0045 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:    3|steps:   30|Train Avg Loss: 0.0023 |Test Loss: 0.0010|lr = 0.01000\n",
      "Epoch:    3|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 0.0019|lr = 0.01000\n",
      "Epoch:    4|steps:   30|Train Avg Loss: 0.0019 |Test Loss: 0.0013|lr = 0.01000\n",
      "Epoch:    4|steps:   60|Train Avg Loss: 0.0022 |Test Loss: 0.0023|lr = 0.01000\n",
      "Epoch:    5|steps:   30|Train Avg Loss: 0.0022 |Test Loss: 0.0009|lr = 0.01000\n",
      "Epoch:    5|steps:   60|Train Avg Loss: 0.0018 |Test Loss: 0.0006|lr = 0.01000\n",
      "Epoch:    6|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:    6|steps:   60|Train Avg Loss: 0.0038 |Test Loss: 0.0009|lr = 0.01000\n",
      "Epoch:    7|steps:   30|Train Avg Loss: 0.0029 |Test Loss: 0.0020|lr = 0.01000\n",
      "Epoch:    7|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 0.0008|lr = 0.01000\n",
      "Epoch:    8|steps:   30|Train Avg Loss: 0.0021 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:    8|steps:   60|Train Avg Loss: 0.0021 |Test Loss: 0.0014|lr = 0.01000\n",
      "Epoch:    9|steps:   30|Train Avg Loss: 0.0020 |Test Loss: 0.0010|lr = 0.01000\n",
      "Epoch:    9|steps:   60|Train Avg Loss: 0.0020 |Test Loss: 0.0006|lr = 0.01000\n",
      "Epoch:   10|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:   10|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 0.0018|lr = 0.01000\n",
      "Epoch:   11|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 0.0012|lr = 0.01000\n",
      "Epoch:   11|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0010|lr = 0.01000\n",
      "Epoch:   12|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:   12|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0014|lr = 0.01000\n",
      "Epoch:   13|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:   13|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0010|lr = 0.01000\n",
      "Epoch:   14|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:   14|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.01000\n",
      "Epoch:   15|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0011|lr = 0.01000\n",
      "Epoch:   15|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 0.0011|lr = 0.01000\n",
      "Epoch:   16|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0012|lr = 0.01000\n",
      "Epoch:   16|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:   17|steps:   30|Train Avg Loss: 0.0024 |Test Loss: 0.0014|lr = 0.01000\n",
      "Epoch:   17|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0011|lr = 0.01000\n",
      "Epoch:   18|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0010|lr = 0.01000\n",
      "Epoch:   18|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0011|lr = 0.01000\n",
      "Epoch:   19|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.01000\n",
      "Epoch:   19|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0009|lr = 0.01000\n",
      "Epoch:   20|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0012|lr = 0.01000\n",
      "Epoch:   20|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.01000\n",
      "Epoch:   21|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0009|lr = 0.01000\n",
      "Epoch:   21|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0008|lr = 0.01000\n",
      "Epoch:   22|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0011|lr = 0.01000\n",
      "Epoch:   22|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0016|lr = 0.01000\n",
      "Epoch:   23|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:   23|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0016|lr = 0.01000\n",
      "Epoch:   24|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 0.0007|lr = 0.01000\n",
      "Epoch:   24|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 0.0009|lr = 0.01000\n",
      "Epoch:   25|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.01000\n",
      "Epoch:   25|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.01000\n",
      "Epoch:   26|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   26|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0010|lr = 0.00980\n",
      "Epoch:   27|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0010|lr = 0.00980\n",
      "Epoch:   27|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0016|lr = 0.00980\n",
      "Epoch:   28|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0011|lr = 0.00980\n",
      "Epoch:   28|steps:   60|Train Avg Loss: 0.0017 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   29|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00980\n",
      "Epoch:   29|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   30|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   30|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0020|lr = 0.00980\n",
      "Epoch:   31|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0009|lr = 0.00980\n",
      "Epoch:   31|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   32|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 0.0013|lr = 0.00980\n",
      "Epoch:   32|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0008|lr = 0.00980\n",
      "Epoch:   33|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   33|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0007|lr = 0.00980\n",
      "Epoch:   34|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   34|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0010|lr = 0.00980\n",
      "Epoch:   35|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   35|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   36|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   36|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   37|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0023|lr = 0.00980\n",
      "Epoch:   37|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0010|lr = 0.00980\n",
      "Epoch:   38|steps:   30|Train Avg Loss: 0.0017 |Test Loss: 0.0006|lr = 0.00980\n",
      "Epoch:   38|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0011|lr = 0.00980\n",
      "Epoch:   39|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0009|lr = 0.00960\n",
      "Epoch:   39|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   40|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0013|lr = 0.00960\n",
      "Epoch:   40|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0008|lr = 0.00960\n",
      "Epoch:   41|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   41|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0010|lr = 0.00960\n",
      "Epoch:   42|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 0.0014|lr = 0.00960\n",
      "Epoch:   42|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   43|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0014|lr = 0.00960\n",
      "Epoch:   43|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   44|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   44|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   45|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0010|lr = 0.00960\n",
      "Epoch:   45|steps:   60|Train Avg Loss: 0.0019 |Test Loss: 0.0015|lr = 0.00960\n",
      "Epoch:   46|steps:   30|Train Avg Loss: 0.0018 |Test Loss: 0.0009|lr = 0.00960\n",
      "Epoch:   46|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00960\n",
      "Epoch:   47|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0005|lr = 0.00960\n",
      "Epoch:   47|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0010|lr = 0.00960\n",
      "Epoch:   48|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   48|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0034|lr = 0.00960\n",
      "Epoch:   49|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00960\n",
      "Epoch:   49|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0009|lr = 0.00960\n",
      "Epoch:   50|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0009|lr = 0.00941\n",
      "Epoch:   50|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0010|lr = 0.00941\n",
      "Epoch:   51|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   51|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0005|lr = 0.00941\n",
      "Epoch:   52|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   52|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0030|lr = 0.00941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   53|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0007|lr = 0.00941\n",
      "Epoch:   53|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   54|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0014|lr = 0.00941\n",
      "Epoch:   54|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   55|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   55|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00941\n",
      "Epoch:   56|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   56|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0009|lr = 0.00941\n",
      "Epoch:   57|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0014|lr = 0.00941\n",
      "Epoch:   57|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0007|lr = 0.00941\n",
      "Epoch:   58|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0007|lr = 0.00941\n",
      "Epoch:   58|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   59|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   59|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0009|lr = 0.00941\n",
      "Epoch:   60|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   60|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   61|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   61|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0008|lr = 0.00941\n",
      "Epoch:   62|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0010|lr = 0.00941\n",
      "Epoch:   62|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   63|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   63|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   64|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0008|lr = 0.00941\n",
      "Epoch:   64|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0019|lr = 0.00941\n",
      "Epoch:   65|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   65|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0018|lr = 0.00941\n",
      "Epoch:   66|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0008|lr = 0.00941\n",
      "Epoch:   66|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   67|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0009|lr = 0.00941\n",
      "Epoch:   67|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   68|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0006|lr = 0.00941\n",
      "Epoch:   68|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0014|lr = 0.00941\n",
      "Epoch:   69|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   69|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   70|steps:   30|Train Avg Loss: 0.0011 |Test Loss: 0.0009|lr = 0.00922\n",
      "Epoch:   70|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   71|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0008|lr = 0.00922\n",
      "Epoch:   71|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0007|lr = 0.00922\n",
      "Epoch:   72|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   72|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   73|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   73|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0008|lr = 0.00922\n",
      "Epoch:   74|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0007|lr = 0.00922\n",
      "Epoch:   74|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0007|lr = 0.00922\n",
      "Epoch:   75|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0009|lr = 0.00922\n",
      "Epoch:   75|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   76|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   76|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0007|lr = 0.00922\n",
      "Epoch:   77|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0021|lr = 0.00922\n",
      "Epoch:   77|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0015|lr = 0.00922\n",
      "Epoch:   78|steps:   30|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   78|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00922\n",
      "Epoch:   79|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0006|lr = 0.00922\n",
      "Epoch:   79|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0011|lr = 0.00922\n",
      "Epoch:   80|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0008|lr = 0.00904\n",
      "Epoch:   80|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0009|lr = 0.00904\n",
      "Epoch:   81|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00904\n",
      "Epoch:   81|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00904\n",
      "Epoch:   82|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00904\n",
      "Epoch:   82|steps:   60|Train Avg Loss: 0.0011 |Test Loss: 0.0006|lr = 0.00904\n",
      "Epoch:   83|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00904\n",
      "Epoch:   83|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0005|lr = 0.00904\n",
      "Epoch:   84|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0008|lr = 0.00904\n",
      "Epoch:   84|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0014|lr = 0.00904\n",
      "Epoch:   85|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0008|lr = 0.00904\n",
      "Epoch:   85|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0008|lr = 0.00904\n",
      "Epoch:   86|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0021|lr = 0.00904\n",
      "Epoch:   86|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0007|lr = 0.00904\n",
      "Epoch:   87|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00904\n",
      "Epoch:   87|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00904\n",
      "Epoch:   88|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0005|lr = 0.00904\n",
      "Epoch:   88|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00904\n",
      "Epoch:   89|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0008|lr = 0.00904\n",
      "Epoch:   89|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0008|lr = 0.00904\n",
      "Epoch:   90|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0021|lr = 0.00904\n",
      "Epoch:   90|steps:   60|Train Avg Loss: 0.0014 |Test Loss: 0.0006|lr = 0.00904\n",
      "Epoch:   91|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00886\n",
      "Epoch:   91|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0009|lr = 0.00886\n",
      "Epoch:   92|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0015|lr = 0.00886\n",
      "Epoch:   92|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00886\n",
      "Epoch:   93|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0009|lr = 0.00886\n",
      "Epoch:   93|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00886\n",
      "Epoch:   94|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0007|lr = 0.00886\n",
      "Epoch:   94|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0007|lr = 0.00886\n",
      "Epoch:   95|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0008|lr = 0.00886\n",
      "Epoch:   95|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0010|lr = 0.00886\n",
      "Epoch:   96|steps:   30|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00886\n",
      "Epoch:   96|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0011|lr = 0.00886\n",
      "Epoch:   97|steps:   30|Train Avg Loss: 0.0016 |Test Loss: 0.0015|lr = 0.00886\n",
      "Epoch:   97|steps:   60|Train Avg Loss: 0.0015 |Test Loss: 0.0006|lr = 0.00886\n",
      "Epoch:   98|steps:   30|Train Avg Loss: 0.0013 |Test Loss: 0.0009|lr = 0.00886\n",
      "Epoch:   98|steps:   60|Train Avg Loss: 0.0012 |Test Loss: 0.0006|lr = 0.00886\n",
      "Epoch:   99|steps:   30|Train Avg Loss: 0.0014 |Test Loss: 0.0009|lr = 0.00886\n",
      "Epoch:   99|steps:   60|Train Avg Loss: 0.0016 |Test Loss: 0.0015|lr = 0.00886\n",
      "Epoch:  100|steps:   30|Train Avg Loss: 0.0011 |Test Loss: 0.0009|lr = 0.00886\n",
      "Epoch:  100|steps:   60|Train Avg Loss: 0.0013 |Test Loss: 0.0006|lr = 0.00886\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training \n",
    "'''\n",
    "LOSS = []\n",
    "TEST_LOSS = []\n",
    "TEST_ACC = []\n",
    "TRAIN_ACC = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    for step,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,TIME_STEP,INPUT_SIZE)\n",
    "        # start trainnig \n",
    "        output = model(inputs)\n",
    "        # calculate loss  (cross entroy)\n",
    "        loss = loss_func(output,targets)\n",
    "        # clear the gradients of all optimized variables(from last training)\n",
    "        optimizer.zero_grad()\n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # sum of loss\n",
    "        loss_total = loss_total + loss\n",
    "        \n",
    "        # print training info every 30 steps\n",
    "        if((step+1) %30 == 0):\n",
    "            # average of loss in 30 steps\n",
    "            avg = loss_total / 30\n",
    "            LOSS.append(avg.tolist())\n",
    "            \n",
    "            # calculate the accuracy of training \n",
    "            pred_train_y = torch.max(output, 1)[1].data.numpy()\n",
    "            #train_accuracy = float((pred_train_y == targets.numpy()).astype(int).sum()) / float(targets.numpy().size)\n",
    "            #TRAIN_ACC.append(train_accuracy)\n",
    "            \n",
    "            # calculate the accuracy of using testing data as inputs\n",
    "            test_output = model(test_features.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "            pred_test_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            #test_accuracy = float((pred_test_y == test_labels.numpy()).astype(int).sum()) / float(test_labels.numpy().size)\n",
    "            #TEST_ACC.append(test_accuracy)\n",
    "            test_loss = loss_func(test_output,test_labels)\n",
    "            TEST_LOSS.append(test_loss.tolist())\n",
    "            # print the epoch , steps , average loss , accuracy \n",
    "            print(\"Epoch: %4d|steps: %4d|Train Avg Loss: %.4f |Test Loss: %.4f|lr = %.5f\"\n",
    "                  %(epoch+1,step+1,avg,test_loss,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # inital variable\n",
    "            loss_total = 0\n",
    "    # updata learning rate\n",
    "    scheduler.step(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price =train_features[len(train_features)-1].numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4365617334842682],\n",
       " [0.4531064033508301],\n",
       " [0.4577021598815918],\n",
       " [0.4816000163555145],\n",
       " [0.4907914996147156],\n",
       " [0.4834383428096771],\n",
       " [0.4788426160812378],\n",
       " [0.48987236618995667],\n",
       " [0.48895323276519775],\n",
       " [0.47057026624679565],\n",
       " [0.46321702003479004],\n",
       " [0.46413615345954895],\n",
       " [0.44942978024482727],\n",
       " [0.455863893032074],\n",
       " [0.44851067662239075],\n",
       " [0.4586212933063507],\n",
       " [0.46413615345954895],\n",
       " [0.4549446702003479],\n",
       " [0.455863893032074],\n",
       " [0.46321702003479004],\n",
       " [0.4760851263999939],\n",
       " [0.46873190999031067],\n",
       " [0.48251914978027344],\n",
       " [0.48527660965919495],\n",
       " [0.48527660965919495],\n",
       " [0.4907914996147156],\n",
       " [0.5542128086090088],\n",
       " [0.5376680493354797],\n",
       " [0.5413447022438049],\n",
       " [0.5486978888511658]]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30\n",
      "37\n",
      "success\n",
      "7 37\n",
      "44\n",
      "success\n",
      "14 44\n",
      "51\n",
      "success\n",
      "21 51\n",
      "58\n",
      "success\n",
      "28 58\n",
      "65\n",
      "success\n",
      "35 65\n",
      "72\n",
      "success\n",
      "42 72\n",
      "79\n",
      "success\n",
      "49 79\n",
      "86\n",
      "success\n",
      "56 86\n",
      "93\n",
      "success\n",
      "63 93\n",
      "100\n",
      "success\n",
      "70 100\n",
      "107\n",
      "success\n",
      "77 107\n",
      "114\n",
      "success\n",
      "84 114\n",
      "121\n",
      "success\n",
      "91 121\n",
      "128\n",
      "success\n",
      "98 128\n",
      "135\n",
      "success\n",
      "105 135\n",
      "142\n",
      "success\n",
      "112 142\n",
      "149\n",
      "success\n",
      "119 149\n",
      "156\n",
      "success\n",
      "126 156\n",
      "163\n",
      "success\n",
      "133 163\n",
      "170\n",
      "success\n",
      "140 170\n",
      "177\n",
      "success\n",
      "147 177\n",
      "184\n",
      "success\n",
      "154 184\n",
      "191\n",
      "success\n",
      "161 191\n",
      "198\n",
      "success\n",
      "168 198\n",
      "205\n",
      "success\n",
      "175 205\n",
      "212\n",
      "success\n",
      "182 212\n",
      "219\n",
      "success\n",
      "189 219\n",
      "226\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,28):\n",
    "    pred_input = torch.FloatTensor(np.array(predict_price[0+i*7:30+i*7]))\n",
    "    print(i*7+0 , 30+i*7)\n",
    "    pred_output = model(pred_input.view(-1,TIME_STEP,INPUT_SIZE))\n",
    "    pred_output = pred_output.view(-1,1).detach().numpy()\n",
    "    predict_price.extend(pred_output.tolist())\n",
    "    print(len(predict_price))\n",
    "    print(\"success\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price = np.array(predict_price[30:])\n",
    "predict_price = predict_price.reshape(-1,1)\n",
    "predict_price =  scaler.inverse_transform(predict_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = test_labels[:196].numpy().reshape(-1,1)\n",
    "y1 = scaler.inverse_transform(y1)\n",
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (196,) and (1372, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-cfb7606a6057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m196\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m196\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2763\u001b[1;33m \u001b[1;31m# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2764\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphase_spectrum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m def phase_spectrum(\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1645\u001b[0m         \u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m=\u001b[0m    \u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m         \u001b[0mcharacter\u001b[0m        \u001b[0mcolor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m         \u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m=\u001b[0m    \u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m          \u001b[0mblue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                 kwargs[\"label\"] = mpl._label_from_arg(\n\u001b[0;32m    215\u001b[0m                     replaced[label_namer_idx], args[label_namer_idx])\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplaced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;31m# Repeatedly grab (x, y) or (x, y, format) from the front of args and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;31m# Looks like we don't want \"color\" to be interpreted to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;31m# mean both facecolor and edgecolor for some reason.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[1;31m# So the \"kw\" dictionary is thrown out, and only its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;31m# 'color' value is kept and translated as a 'facecolor'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (196,) and (1372, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,196,196)\n",
    "y2 = predict_price\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
